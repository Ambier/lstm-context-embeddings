WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f2284ebde90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f2284ebde50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
CV_INDEX=5
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=30
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9594/1066
Writing to /home/cil/lstm-context-embeddings/runs/1473177961

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-07T00:06:21.464720: step 1, loss 0.693147, acc 0.38
2016-09-07T00:06:22.161209: step 2, loss 0.7206, acc 0.46
2016-09-07T00:06:22.866813: step 3, loss 0.706399, acc 0.48
2016-09-07T00:06:23.540617: step 4, loss 0.708075, acc 0.44
2016-09-07T00:06:24.230151: step 5, loss 0.693359, acc 0.48
2016-09-07T00:06:24.919826: step 6, loss 0.690996, acc 0.52
2016-09-07T00:06:25.603334: step 7, loss 0.692419, acc 0.52
2016-09-07T00:06:26.292032: step 8, loss 0.705022, acc 0.4
2016-09-07T00:06:26.977918: step 9, loss 0.702348, acc 0.46
2016-09-07T00:06:27.688579: step 10, loss 0.690672, acc 0.5
2016-09-07T00:06:28.378356: step 11, loss 0.698178, acc 0.44
2016-09-07T00:06:29.049719: step 12, loss 0.700575, acc 0.46
2016-09-07T00:06:29.742449: step 13, loss 0.684605, acc 0.54
2016-09-07T00:06:30.431640: step 14, loss 0.695014, acc 0.52
2016-09-07T00:06:31.126013: step 15, loss 0.689293, acc 0.6
2016-09-07T00:06:31.820097: step 16, loss 0.698126, acc 0.5
2016-09-07T00:06:32.522784: step 17, loss 0.672419, acc 0.68
2016-09-07T00:06:33.196888: step 18, loss 0.694167, acc 0.5
2016-09-07T00:06:33.905971: step 19, loss 0.682695, acc 0.56
2016-09-07T00:06:34.603126: step 20, loss 0.696725, acc 0.5
2016-09-07T00:06:35.272202: step 21, loss 0.654071, acc 0.62
2016-09-07T00:06:35.934718: step 22, loss 0.679905, acc 0.6
2016-09-07T00:06:36.607345: step 23, loss 0.639729, acc 0.74
2016-09-07T00:06:37.315850: step 24, loss 0.648368, acc 0.64
2016-09-07T00:06:37.986902: step 25, loss 0.654262, acc 0.64
2016-09-07T00:06:38.671360: step 26, loss 0.643522, acc 0.64
2016-09-07T00:06:39.346936: step 27, loss 0.592814, acc 0.7
2016-09-07T00:06:40.044477: step 28, loss 0.660673, acc 0.64
2016-09-07T00:06:40.718999: step 29, loss 0.699306, acc 0.54
2016-09-07T00:06:41.397790: step 30, loss 0.626899, acc 0.6
2016-09-07T00:06:42.070810: step 31, loss 0.665205, acc 0.6
2016-09-07T00:06:42.758823: step 32, loss 0.623697, acc 0.62
2016-09-07T00:06:43.459130: step 33, loss 0.643414, acc 0.64
2016-09-07T00:06:44.131788: step 34, loss 0.602519, acc 0.66
2016-09-07T00:06:44.805584: step 35, loss 0.619016, acc 0.68
2016-09-07T00:06:45.495335: step 36, loss 0.614242, acc 0.62
2016-09-07T00:06:46.181298: step 37, loss 0.630798, acc 0.66
2016-09-07T00:06:46.879052: step 38, loss 0.616902, acc 0.62
2016-09-07T00:06:47.545947: step 39, loss 0.604981, acc 0.68
2016-09-07T00:06:48.241623: step 40, loss 0.539263, acc 0.64
2016-09-07T00:06:48.927920: step 41, loss 0.641529, acc 0.64
2016-09-07T00:06:49.621657: step 42, loss 0.710715, acc 0.64
2016-09-07T00:06:50.315375: step 43, loss 0.626122, acc 0.72
2016-09-07T00:06:50.990213: step 44, loss 0.650735, acc 0.58
2016-09-07T00:06:51.674715: step 45, loss 0.631104, acc 0.62
2016-09-07T00:06:52.345895: step 46, loss 0.580404, acc 0.66
2016-09-07T00:06:53.038639: step 47, loss 0.571711, acc 0.7
2016-09-07T00:06:53.722440: step 48, loss 0.492804, acc 0.8
2016-09-07T00:06:54.432702: step 49, loss 0.632059, acc 0.64
2016-09-07T00:06:55.140665: step 50, loss 0.622494, acc 0.66
2016-09-07T00:06:55.809752: step 51, loss 0.61571, acc 0.66
2016-09-07T00:06:56.490901: step 52, loss 0.576942, acc 0.74
2016-09-07T00:06:57.152727: step 53, loss 0.569495, acc 0.72
2016-09-07T00:06:57.842181: step 54, loss 0.492618, acc 0.8
2016-09-07T00:06:58.520045: step 55, loss 0.72334, acc 0.54
2016-09-07T00:06:59.211946: step 56, loss 0.535949, acc 0.72
2016-09-07T00:06:59.898480: step 57, loss 0.443763, acc 0.72
2016-09-07T00:07:00.616437: step 58, loss 0.609563, acc 0.62
2016-09-07T00:07:01.320342: step 59, loss 0.658749, acc 0.66
2016-09-07T00:07:01.992065: step 60, loss 0.605294, acc 0.7
2016-09-07T00:07:02.690062: step 61, loss 0.727768, acc 0.54
2016-09-07T00:07:03.388091: step 62, loss 0.391838, acc 0.8
2016-09-07T00:07:04.060659: step 63, loss 0.569706, acc 0.7
2016-09-07T00:07:04.776731: step 64, loss 0.603852, acc 0.74
2016-09-07T00:07:05.465194: step 65, loss 0.607283, acc 0.62
2016-09-07T00:07:06.149971: step 66, loss 0.538994, acc 0.7
2016-09-07T00:07:06.822289: step 67, loss 0.543958, acc 0.68
2016-09-07T00:07:07.522374: step 68, loss 0.609094, acc 0.62
2016-09-07T00:07:08.206684: step 69, loss 0.518303, acc 0.7
2016-09-07T00:07:08.884354: step 70, loss 0.603147, acc 0.64
2016-09-07T00:07:09.550929: step 71, loss 0.571422, acc 0.66
2016-09-07T00:07:10.242948: step 72, loss 0.667674, acc 0.64
2016-09-07T00:07:10.931478: step 73, loss 0.52364, acc 0.82
2016-09-07T00:07:11.586953: step 74, loss 0.431717, acc 0.84
2016-09-07T00:07:12.295684: step 75, loss 0.566904, acc 0.78
2016-09-07T00:07:12.985978: step 76, loss 0.481111, acc 0.8
2016-09-07T00:07:13.672061: step 77, loss 0.521704, acc 0.72
2016-09-07T00:07:14.354439: step 78, loss 0.51214, acc 0.76
2016-09-07T00:07:15.039098: step 79, loss 0.671202, acc 0.62
2016-09-07T00:07:15.729820: step 80, loss 0.631638, acc 0.7
2016-09-07T00:07:16.395997: step 81, loss 0.533317, acc 0.76
2016-09-07T00:07:17.110472: step 82, loss 0.608948, acc 0.7
2016-09-07T00:07:17.783912: step 83, loss 0.613454, acc 0.72
2016-09-07T00:07:18.462390: step 84, loss 0.5075, acc 0.76
2016-09-07T00:07:19.136938: step 85, loss 0.575382, acc 0.72
2016-09-07T00:07:19.837720: step 86, loss 0.396828, acc 0.86
2016-09-07T00:07:20.524913: step 87, loss 0.513965, acc 0.7
2016-09-07T00:07:21.248177: step 88, loss 0.656985, acc 0.6
2016-09-07T00:07:21.967044: step 89, loss 0.496853, acc 0.78
2016-09-07T00:07:22.662050: step 90, loss 0.502063, acc 0.8
2016-09-07T00:07:23.335937: step 91, loss 0.416246, acc 0.84
2016-09-07T00:07:24.016103: step 92, loss 0.469757, acc 0.74
2016-09-07T00:07:24.699225: step 93, loss 0.87506, acc 0.52
2016-09-07T00:07:25.403182: step 94, loss 0.662005, acc 0.68
2016-09-07T00:07:26.089621: step 95, loss 0.581037, acc 0.66
2016-09-07T00:07:26.795736: step 96, loss 0.536138, acc 0.62
2016-09-07T00:07:27.474382: step 97, loss 0.553102, acc 0.68
2016-09-07T00:07:28.167971: step 98, loss 0.574368, acc 0.74
2016-09-07T00:07:28.857850: step 99, loss 0.601481, acc 0.66
2016-09-07T00:07:29.554243: step 100, loss 0.459948, acc 0.74

Evaluation:
2016-09-07T00:07:32.753178: step 100, loss 0.496451, acc 0.788931

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-100

2016-09-07T00:07:34.455029: step 101, loss 0.494045, acc 0.78
2016-09-07T00:07:35.178575: step 102, loss 0.584389, acc 0.76
2016-09-07T00:07:35.874707: step 103, loss 0.585159, acc 0.7
2016-09-07T00:07:36.574399: step 104, loss 0.533069, acc 0.74
2016-09-07T00:07:37.262685: step 105, loss 0.520318, acc 0.74
2016-09-07T00:07:37.945015: step 106, loss 0.523976, acc 0.74
2016-09-07T00:07:38.631693: step 107, loss 0.533826, acc 0.74
2016-09-07T00:07:39.298269: step 108, loss 0.395577, acc 0.84
2016-09-07T00:07:40.009740: step 109, loss 0.50896, acc 0.8
2016-09-07T00:07:40.693462: step 110, loss 0.577669, acc 0.74
2016-09-07T00:07:41.372786: step 111, loss 0.394897, acc 0.84
2016-09-07T00:07:42.070175: step 112, loss 0.459063, acc 0.76
2016-09-07T00:07:42.742073: step 113, loss 0.589184, acc 0.7
2016-09-07T00:07:43.457857: step 114, loss 0.650751, acc 0.64
2016-09-07T00:07:44.134653: step 115, loss 0.573784, acc 0.68
2016-09-07T00:07:44.802544: step 116, loss 0.393501, acc 0.8
2016-09-07T00:07:45.496168: step 117, loss 0.436353, acc 0.84
2016-09-07T00:07:46.171919: step 118, loss 0.517369, acc 0.7
2016-09-07T00:07:46.869028: step 119, loss 0.45851, acc 0.74
2016-09-07T00:07:47.567867: step 120, loss 0.375814, acc 0.92
2016-09-07T00:07:48.278053: step 121, loss 0.562405, acc 0.72
2016-09-07T00:07:48.941597: step 122, loss 0.469136, acc 0.72
2016-09-07T00:07:49.631721: step 123, loss 0.488128, acc 0.76
2016-09-07T00:07:50.327037: step 124, loss 0.607891, acc 0.8
2016-09-07T00:07:51.044601: step 125, loss 0.509001, acc 0.72
2016-09-07T00:07:51.721394: step 126, loss 0.537811, acc 0.72
2016-09-07T00:07:52.404686: step 127, loss 0.543677, acc 0.7
2016-09-07T00:07:53.125082: step 128, loss 0.445233, acc 0.78
2016-09-07T00:07:53.800994: step 129, loss 0.558366, acc 0.7
2016-09-07T00:07:54.476674: step 130, loss 0.516119, acc 0.76
2016-09-07T00:07:55.155473: step 131, loss 0.419435, acc 0.74
2016-09-07T00:07:55.834526: step 132, loss 0.669408, acc 0.6
2016-09-07T00:07:56.522532: step 133, loss 0.545684, acc 0.74
2016-09-07T00:07:57.217534: step 134, loss 0.453949, acc 0.78
2016-09-07T00:07:57.908474: step 135, loss 0.460036, acc 0.7
2016-09-07T00:07:58.582445: step 136, loss 0.630676, acc 0.68
2016-09-07T00:07:59.281837: step 137, loss 0.559486, acc 0.74
2016-09-07T00:07:59.973018: step 138, loss 0.503295, acc 0.76
2016-09-07T00:08:00.706035: step 139, loss 0.537493, acc 0.74
2016-09-07T00:08:01.402181: step 140, loss 0.490003, acc 0.76
2016-09-07T00:08:02.097769: step 141, loss 0.586938, acc 0.7
2016-09-07T00:08:02.793955: step 142, loss 0.472495, acc 0.76
2016-09-07T00:08:03.474550: step 143, loss 0.480318, acc 0.74
2016-09-07T00:08:04.140746: step 144, loss 0.456122, acc 0.78
2016-09-07T00:08:04.823328: step 145, loss 0.486476, acc 0.82
2016-09-07T00:08:05.513120: step 146, loss 0.530972, acc 0.74
2016-09-07T00:08:06.188045: step 147, loss 0.441787, acc 0.74
2016-09-07T00:08:06.849459: step 148, loss 0.63857, acc 0.66
2016-09-07T00:08:07.537445: step 149, loss 0.568273, acc 0.66
2016-09-07T00:08:08.208465: step 150, loss 0.526512, acc 0.72
2016-09-07T00:08:08.890160: step 151, loss 0.550946, acc 0.68
2016-09-07T00:08:09.569153: step 152, loss 0.49558, acc 0.78
2016-09-07T00:08:10.263638: step 153, loss 0.527882, acc 0.74
2016-09-07T00:08:10.964459: step 154, loss 0.600275, acc 0.7
2016-09-07T00:08:11.647928: step 155, loss 0.420535, acc 0.82
2016-09-07T00:08:12.338127: step 156, loss 0.41717, acc 0.84
2016-09-07T00:08:12.993561: step 157, loss 0.476563, acc 0.76
2016-09-07T00:08:13.666083: step 158, loss 0.446039, acc 0.76
2016-09-07T00:08:14.350794: step 159, loss 0.429103, acc 0.78
2016-09-07T00:08:15.051144: step 160, loss 0.487275, acc 0.86
2016-09-07T00:08:15.729811: step 161, loss 0.465059, acc 0.78
2016-09-07T00:08:16.421329: step 162, loss 0.416606, acc 0.74
2016-09-07T00:08:17.133259: step 163, loss 0.41813, acc 0.78
2016-09-07T00:08:17.812701: step 164, loss 0.465379, acc 0.74
2016-09-07T00:08:18.491394: step 165, loss 0.453788, acc 0.76
2016-09-07T00:08:19.183863: step 166, loss 0.480691, acc 0.82
2016-09-07T00:08:19.858497: step 167, loss 0.586337, acc 0.72
2016-09-07T00:08:20.543257: step 168, loss 0.367612, acc 0.82
2016-09-07T00:08:21.236769: step 169, loss 0.664327, acc 0.66
2016-09-07T00:08:21.945967: step 170, loss 0.613729, acc 0.7
2016-09-07T00:08:22.619315: step 171, loss 0.371312, acc 0.84
2016-09-07T00:08:23.292857: step 172, loss 0.452791, acc 0.74
2016-09-07T00:08:23.968403: step 173, loss 0.570642, acc 0.68
2016-09-07T00:08:24.641500: step 174, loss 0.412853, acc 0.76
2016-09-07T00:08:25.305294: step 175, loss 0.55123, acc 0.74
2016-09-07T00:08:25.978866: step 176, loss 0.520465, acc 0.72
2016-09-07T00:08:26.664230: step 177, loss 0.521572, acc 0.8
2016-09-07T00:08:27.330446: step 178, loss 0.561836, acc 0.74
2016-09-07T00:08:28.022148: step 179, loss 0.46896, acc 0.74
2016-09-07T00:08:28.705348: step 180, loss 0.616326, acc 0.74
2016-09-07T00:08:29.394608: step 181, loss 0.516991, acc 0.76
2016-09-07T00:08:30.077713: step 182, loss 0.679118, acc 0.62
2016-09-07T00:08:30.742055: step 183, loss 0.558966, acc 0.72
2016-09-07T00:08:31.458101: step 184, loss 0.488857, acc 0.76
2016-09-07T00:08:32.140843: step 185, loss 0.557349, acc 0.68
2016-09-07T00:08:32.855020: step 186, loss 0.401152, acc 0.8
2016-09-07T00:08:33.557768: step 187, loss 0.537351, acc 0.74
2016-09-07T00:08:34.240126: step 188, loss 0.382236, acc 0.82
2016-09-07T00:08:34.932625: step 189, loss 0.569387, acc 0.68
2016-09-07T00:08:35.606020: step 190, loss 0.431688, acc 0.8
2016-09-07T00:08:36.283189: step 191, loss 0.565216, acc 0.66
2016-09-07T00:08:36.933747: step 192, loss 0.521188, acc 0.704545
2016-09-07T00:08:37.628256: step 193, loss 0.550447, acc 0.74
2016-09-07T00:08:38.316467: step 194, loss 0.478342, acc 0.78
2016-09-07T00:08:39.018294: step 195, loss 0.503752, acc 0.74
2016-09-07T00:08:39.717906: step 196, loss 0.384686, acc 0.84
2016-09-07T00:08:40.411362: step 197, loss 0.393558, acc 0.84
2016-09-07T00:08:41.099573: step 198, loss 0.473116, acc 0.8
2016-09-07T00:08:41.775060: step 199, loss 0.328369, acc 0.84
2016-09-07T00:08:42.501493: step 200, loss 0.381165, acc 0.86

Evaluation:
2016-09-07T00:08:45.637454: step 200, loss 0.445434, acc 0.787992

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-200

2016-09-07T00:08:47.312134: step 201, loss 0.503752, acc 0.78
2016-09-07T00:08:48.003694: step 202, loss 0.47105, acc 0.82
2016-09-07T00:08:48.714177: step 203, loss 0.239128, acc 0.92
2016-09-07T00:08:49.386591: step 204, loss 0.493228, acc 0.78
2016-09-07T00:08:50.076422: step 205, loss 0.310689, acc 0.9
2016-09-07T00:08:50.803080: step 206, loss 0.340878, acc 0.82
2016-09-07T00:08:51.485776: step 207, loss 0.306991, acc 0.86
2016-09-07T00:08:52.177063: step 208, loss 0.313248, acc 0.84
2016-09-07T00:08:52.874871: step 209, loss 0.343031, acc 0.88
2016-09-07T00:08:53.574054: step 210, loss 0.3215, acc 0.84
2016-09-07T00:08:54.242513: step 211, loss 0.373301, acc 0.84
2016-09-07T00:08:54.943342: step 212, loss 0.408324, acc 0.88
2016-09-07T00:08:55.643199: step 213, loss 0.277488, acc 0.88
2016-09-07T00:08:56.325728: step 214, loss 0.295042, acc 0.88
2016-09-07T00:08:57.020503: step 215, loss 0.345391, acc 0.86
2016-09-07T00:08:57.712992: step 216, loss 0.311145, acc 0.9
2016-09-07T00:08:58.388022: step 217, loss 0.23498, acc 0.88
2016-09-07T00:08:59.061275: step 218, loss 0.416995, acc 0.8
2016-09-07T00:08:59.729457: step 219, loss 0.283201, acc 0.88
2016-09-07T00:09:00.470188: step 220, loss 0.303762, acc 0.9
2016-09-07T00:09:01.164442: step 221, loss 0.574091, acc 0.7
2016-09-07T00:09:01.826478: step 222, loss 0.201051, acc 0.96
2016-09-07T00:09:02.514073: step 223, loss 0.239472, acc 0.88
2016-09-07T00:09:03.196536: step 224, loss 0.33546, acc 0.88
2016-09-07T00:09:03.870776: step 225, loss 0.265898, acc 0.9
2016-09-07T00:09:04.568082: step 226, loss 0.305263, acc 0.86
2016-09-07T00:09:05.257068: step 227, loss 0.250118, acc 0.86
2016-09-07T00:09:05.915267: step 228, loss 0.298049, acc 0.94
2016-09-07T00:09:06.611848: step 229, loss 0.305126, acc 0.86
2016-09-07T00:09:07.304294: step 230, loss 0.393545, acc 0.8
2016-09-07T00:09:07.990862: step 231, loss 0.277064, acc 0.86
2016-09-07T00:09:08.688405: step 232, loss 0.166858, acc 0.94
2016-09-07T00:09:09.377945: step 233, loss 0.266265, acc 0.9
2016-09-07T00:09:10.084781: step 234, loss 0.337448, acc 0.86
2016-09-07T00:09:10.754159: step 235, loss 0.488618, acc 0.76
2016-09-07T00:09:11.424017: step 236, loss 0.380291, acc 0.84
2016-09-07T00:09:12.104591: step 237, loss 0.29238, acc 0.86
2016-09-07T00:09:12.780889: step 238, loss 0.463605, acc 0.8
2016-09-07T00:09:13.455522: step 239, loss 0.438233, acc 0.86
2016-09-07T00:09:14.152206: step 240, loss 0.332716, acc 0.84
2016-09-07T00:09:14.837996: step 241, loss 0.27747, acc 0.84
2016-09-07T00:09:15.525414: step 242, loss 0.283708, acc 0.92
2016-09-07T00:09:16.226871: step 243, loss 0.439017, acc 0.76
2016-09-07T00:09:16.912678: step 244, loss 0.390463, acc 0.82
2016-09-07T00:09:17.583220: step 245, loss 0.357802, acc 0.86
2016-09-07T00:09:18.273487: step 246, loss 0.373717, acc 0.82
2016-09-07T00:09:18.961622: step 247, loss 0.295806, acc 0.88
2016-09-07T00:09:19.671146: step 248, loss 0.403897, acc 0.78
2016-09-07T00:09:20.329572: step 249, loss 0.498911, acc 0.76
2016-09-07T00:09:21.033274: step 250, loss 0.321015, acc 0.92
2016-09-07T00:09:21.734888: step 251, loss 0.345521, acc 0.8
2016-09-07T00:09:22.422048: step 252, loss 0.224729, acc 0.9
2016-09-07T00:09:23.119196: step 253, loss 0.364136, acc 0.88
2016-09-07T00:09:23.807486: step 254, loss 0.442597, acc 0.82
2016-09-07T00:09:24.519772: step 255, loss 0.335501, acc 0.82
2016-09-07T00:09:25.187010: step 256, loss 0.476508, acc 0.8
2016-09-07T00:09:25.877198: step 257, loss 0.476742, acc 0.78
2016-09-07T00:09:26.577836: step 258, loss 0.285465, acc 0.86
2016-09-07T00:09:27.259079: step 259, loss 0.293028, acc 0.88
2016-09-07T00:09:27.950373: step 260, loss 0.362794, acc 0.86
2016-09-07T00:09:28.632566: step 261, loss 0.363333, acc 0.86
2016-09-07T00:09:29.326935: step 262, loss 0.342113, acc 0.88
2016-09-07T00:09:30.002405: step 263, loss 0.40924, acc 0.78
2016-09-07T00:09:30.689428: step 264, loss 0.281569, acc 0.88
2016-09-07T00:09:31.375956: step 265, loss 0.328542, acc 0.84
2016-09-07T00:09:32.070906: step 266, loss 0.270825, acc 0.9
2016-09-07T00:09:32.785482: step 267, loss 0.523012, acc 0.74
2016-09-07T00:09:33.476790: step 268, loss 0.46406, acc 0.76
2016-09-07T00:09:34.170755: step 269, loss 0.23387, acc 0.88
2016-09-07T00:09:34.841626: step 270, loss 0.26571, acc 0.86
2016-09-07T00:09:35.514683: step 271, loss 0.389112, acc 0.82
2016-09-07T00:09:36.191427: step 272, loss 0.433986, acc 0.82
2016-09-07T00:09:36.868088: step 273, loss 0.448496, acc 0.78
2016-09-07T00:09:37.526909: step 274, loss 0.239521, acc 0.88
2016-09-07T00:09:38.199642: step 275, loss 0.40939, acc 0.78
2016-09-07T00:09:38.909323: step 276, loss 0.436047, acc 0.76
2016-09-07T00:09:39.571070: step 277, loss 0.492879, acc 0.8
2016-09-07T00:09:40.259969: step 278, loss 0.242218, acc 0.92
2016-09-07T00:09:40.945099: step 279, loss 0.442907, acc 0.8
2016-09-07T00:09:41.630663: step 280, loss 0.316404, acc 0.88
2016-09-07T00:09:42.316230: step 281, loss 0.250802, acc 0.92
2016-09-07T00:09:43.002188: step 282, loss 0.385679, acc 0.86
2016-09-07T00:09:43.706268: step 283, loss 0.356219, acc 0.82
2016-09-07T00:09:44.377921: step 284, loss 0.333889, acc 0.84
2016-09-07T00:09:45.070830: step 285, loss 0.366146, acc 0.8
2016-09-07T00:09:45.765932: step 286, loss 0.289049, acc 0.84
2016-09-07T00:09:46.466671: step 287, loss 0.489987, acc 0.84
2016-09-07T00:09:47.149835: step 288, loss 0.295918, acc 0.88
2016-09-07T00:09:47.837407: step 289, loss 0.31047, acc 0.84
2016-09-07T00:09:48.532942: step 290, loss 0.473015, acc 0.74
2016-09-07T00:09:49.195691: step 291, loss 0.387959, acc 0.84
2016-09-07T00:09:49.890932: step 292, loss 0.330763, acc 0.9
2016-09-07T00:09:50.559817: step 293, loss 0.531742, acc 0.74
2016-09-07T00:09:51.245404: step 294, loss 0.283133, acc 0.88
2016-09-07T00:09:51.933340: step 295, loss 0.258684, acc 0.92
2016-09-07T00:09:52.617621: step 296, loss 0.4069, acc 0.8
2016-09-07T00:09:53.322134: step 297, loss 0.361055, acc 0.8
2016-09-07T00:09:53.994675: step 298, loss 0.415999, acc 0.8
2016-09-07T00:09:54.689128: step 299, loss 0.389687, acc 0.84
2016-09-07T00:09:55.366774: step 300, loss 0.345763, acc 0.82

Evaluation:
2016-09-07T00:09:58.496955: step 300, loss 0.47722, acc 0.792683

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-300

2016-09-07T00:10:00.243145: step 301, loss 0.394276, acc 0.78
2016-09-07T00:10:00.923390: step 302, loss 0.538796, acc 0.72
2016-09-07T00:10:01.622172: step 303, loss 0.490082, acc 0.76
2016-09-07T00:10:02.311284: step 304, loss 0.260062, acc 0.9
2016-09-07T00:10:03.019720: step 305, loss 0.346601, acc 0.8
2016-09-07T00:10:03.677193: step 306, loss 0.304191, acc 0.86
2016-09-07T00:10:04.353025: step 307, loss 0.367755, acc 0.88
2016-09-07T00:10:05.027746: step 308, loss 0.489101, acc 0.74
2016-09-07T00:10:05.737675: step 309, loss 0.285112, acc 0.88
2016-09-07T00:10:06.421960: step 310, loss 0.403867, acc 0.78
2016-09-07T00:10:07.105713: step 311, loss 0.342005, acc 0.88
2016-09-07T00:10:07.809284: step 312, loss 0.315553, acc 0.9
2016-09-07T00:10:08.494076: step 313, loss 0.265447, acc 0.84
2016-09-07T00:10:09.176761: step 314, loss 0.356127, acc 0.8
2016-09-07T00:10:09.869210: step 315, loss 0.326166, acc 0.92
2016-09-07T00:10:10.564831: step 316, loss 0.444407, acc 0.82
2016-09-07T00:10:11.274890: step 317, loss 0.324165, acc 0.88
2016-09-07T00:10:11.964793: step 318, loss 0.452766, acc 0.82
2016-09-07T00:10:12.688599: step 319, loss 0.275426, acc 0.94
2016-09-07T00:10:13.367106: step 320, loss 0.430279, acc 0.76
2016-09-07T00:10:14.022407: step 321, loss 0.340293, acc 0.8
2016-09-07T00:10:14.693414: step 322, loss 0.357675, acc 0.8
2016-09-07T00:10:15.353550: step 323, loss 0.299161, acc 0.84
2016-09-07T00:10:16.029340: step 324, loss 0.474749, acc 0.78
2016-09-07T00:10:16.721189: step 325, loss 0.364479, acc 0.84
2016-09-07T00:10:17.413926: step 326, loss 0.35234, acc 0.8
2016-09-07T00:10:18.056194: step 327, loss 0.320271, acc 0.86
2016-09-07T00:10:18.749979: step 328, loss 0.386987, acc 0.8
2016-09-07T00:10:19.439148: step 329, loss 0.335597, acc 0.84
2016-09-07T00:10:20.124655: step 330, loss 0.202537, acc 0.92
2016-09-07T00:10:20.808731: step 331, loss 0.339902, acc 0.84
2016-09-07T00:10:21.499774: step 332, loss 0.48445, acc 0.82
2016-09-07T00:10:22.201778: step 333, loss 0.341623, acc 0.82
2016-09-07T00:10:22.874277: step 334, loss 0.304421, acc 0.86
2016-09-07T00:10:23.580532: step 335, loss 0.365942, acc 0.86
2016-09-07T00:10:24.276775: step 336, loss 0.291512, acc 0.88
2016-09-07T00:10:24.981242: step 337, loss 0.382455, acc 0.84
2016-09-07T00:10:25.667543: step 338, loss 0.482954, acc 0.76
2016-09-07T00:10:26.337208: step 339, loss 0.222805, acc 0.92
2016-09-07T00:10:27.041571: step 340, loss 0.365301, acc 0.88
2016-09-07T00:10:27.740533: step 341, loss 0.306908, acc 0.92
2016-09-07T00:10:28.473723: step 342, loss 0.298359, acc 0.84
2016-09-07T00:10:29.179081: step 343, loss 0.611063, acc 0.68
2016-09-07T00:10:29.880675: step 344, loss 0.43494, acc 0.78
2016-09-07T00:10:30.592704: step 345, loss 0.241879, acc 0.9
2016-09-07T00:10:31.255287: step 346, loss 0.393452, acc 0.84
2016-09-07T00:10:31.965981: step 347, loss 0.398458, acc 0.76
2016-09-07T00:10:32.643293: step 348, loss 0.364804, acc 0.82
2016-09-07T00:10:33.324662: step 349, loss 0.458933, acc 0.86
2016-09-07T00:10:34.018323: step 350, loss 0.442059, acc 0.78
2016-09-07T00:10:34.701474: step 351, loss 0.30055, acc 0.88
2016-09-07T00:10:35.392058: step 352, loss 0.411462, acc 0.78
2016-09-07T00:10:36.059058: step 353, loss 0.34834, acc 0.86
2016-09-07T00:10:36.802530: step 354, loss 0.382075, acc 0.82
2016-09-07T00:10:37.489374: step 355, loss 0.320322, acc 0.86
2016-09-07T00:10:38.175447: step 356, loss 0.426577, acc 0.78
2016-09-07T00:10:38.875120: step 357, loss 0.512394, acc 0.78
2016-09-07T00:10:39.560894: step 358, loss 0.351191, acc 0.82
2016-09-07T00:10:40.252274: step 359, loss 0.508083, acc 0.74
2016-09-07T00:10:40.958129: step 360, loss 0.44219, acc 0.74
2016-09-07T00:10:41.680213: step 361, loss 0.289556, acc 0.84
2016-09-07T00:10:42.381133: step 362, loss 0.32264, acc 0.86
2016-09-07T00:10:43.063824: step 363, loss 0.330399, acc 0.86
2016-09-07T00:10:43.764140: step 364, loss 0.465688, acc 0.82
2016-09-07T00:10:44.413502: step 365, loss 0.488443, acc 0.74
2016-09-07T00:10:45.132486: step 366, loss 0.458668, acc 0.82
2016-09-07T00:10:45.816726: step 367, loss 0.408716, acc 0.82
2016-09-07T00:10:46.501589: step 368, loss 0.350223, acc 0.8
2016-09-07T00:10:47.187833: step 369, loss 0.38946, acc 0.84
2016-09-07T00:10:47.876773: step 370, loss 0.3543, acc 0.82
2016-09-07T00:10:48.573340: step 371, loss 0.375147, acc 0.86
2016-09-07T00:10:49.226371: step 372, loss 0.363151, acc 0.82
2016-09-07T00:10:49.938397: step 373, loss 0.462579, acc 0.7
2016-09-07T00:10:50.587603: step 374, loss 0.270167, acc 0.84
2016-09-07T00:10:51.267463: step 375, loss 0.450092, acc 0.78
2016-09-07T00:10:51.943016: step 376, loss 0.377723, acc 0.86
2016-09-07T00:10:52.623489: step 377, loss 0.380403, acc 0.8
2016-09-07T00:10:53.308260: step 378, loss 0.272425, acc 0.84
2016-09-07T00:10:54.020170: step 379, loss 0.457797, acc 0.78
2016-09-07T00:10:54.741324: step 380, loss 0.402849, acc 0.82
2016-09-07T00:10:55.408861: step 381, loss 0.462544, acc 0.8
2016-09-07T00:10:56.096643: step 382, loss 0.418963, acc 0.76
2016-09-07T00:10:56.784951: step 383, loss 0.328735, acc 0.86
2016-09-07T00:10:57.429284: step 384, loss 0.366097, acc 0.818182
2016-09-07T00:10:58.133861: step 385, loss 0.231261, acc 0.92
2016-09-07T00:10:58.824618: step 386, loss 0.205063, acc 0.9
2016-09-07T00:10:59.533203: step 387, loss 0.407646, acc 0.86
2016-09-07T00:11:00.223964: step 388, loss 0.209425, acc 0.9
2016-09-07T00:11:00.915013: step 389, loss 0.283877, acc 0.92
2016-09-07T00:11:01.604264: step 390, loss 0.171686, acc 0.96
2016-09-07T00:11:02.274993: step 391, loss 0.224653, acc 0.88
2016-09-07T00:11:02.941184: step 392, loss 0.171802, acc 0.94
2016-09-07T00:11:03.598483: step 393, loss 0.162998, acc 0.96
2016-09-07T00:11:04.306066: step 394, loss 0.136139, acc 0.96
2016-09-07T00:11:04.977376: step 395, loss 0.187173, acc 0.88
2016-09-07T00:11:05.665982: step 396, loss 0.311157, acc 0.86
2016-09-07T00:11:06.355115: step 397, loss 0.318749, acc 0.86
2016-09-07T00:11:07.032999: step 398, loss 0.246432, acc 0.9
2016-09-07T00:11:07.741526: step 399, loss 0.181674, acc 0.92
2016-09-07T00:11:08.442607: step 400, loss 0.272359, acc 0.88

Evaluation:
2016-09-07T00:11:11.596491: step 400, loss 0.498677, acc 0.790807

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-400

2016-09-07T00:11:13.234505: step 401, loss 0.239676, acc 0.88
2016-09-07T00:11:13.937857: step 402, loss 0.303113, acc 0.9
2016-09-07T00:11:14.613399: step 403, loss 0.225731, acc 0.92
2016-09-07T00:11:15.292425: step 404, loss 0.0961673, acc 0.98
2016-09-07T00:11:15.982342: step 405, loss 0.261837, acc 0.92
2016-09-07T00:11:16.661111: step 406, loss 0.287217, acc 0.86
2016-09-07T00:11:17.351512: step 407, loss 0.141048, acc 0.96
2016-09-07T00:11:18.049384: step 408, loss 0.264594, acc 0.86
2016-09-07T00:11:18.745647: step 409, loss 0.326625, acc 0.88
2016-09-07T00:11:19.399550: step 410, loss 0.248306, acc 0.92
2016-09-07T00:11:20.086305: step 411, loss 0.232004, acc 0.88
2016-09-07T00:11:20.776157: step 412, loss 0.253806, acc 0.88
2016-09-07T00:11:21.480229: step 413, loss 0.213494, acc 0.9
2016-09-07T00:11:22.163269: step 414, loss 0.22893, acc 0.92
2016-09-07T00:11:22.849593: step 415, loss 0.204516, acc 0.9
2016-09-07T00:11:23.559260: step 416, loss 0.344396, acc 0.88
2016-09-07T00:11:24.241658: step 417, loss 0.22588, acc 0.86
2016-09-07T00:11:24.906682: step 418, loss 0.237322, acc 0.88
2016-09-07T00:11:25.583143: step 419, loss 0.174607, acc 0.94
2016-09-07T00:11:26.278977: step 420, loss 0.193127, acc 0.9
2016-09-07T00:11:26.969400: step 421, loss 0.153192, acc 0.96
2016-09-07T00:11:27.670205: step 422, loss 0.247964, acc 0.88
2016-09-07T00:11:28.373013: step 423, loss 0.26236, acc 0.9
2016-09-07T00:11:29.056621: step 424, loss 0.38819, acc 0.84
2016-09-07T00:11:29.745356: step 425, loss 0.237403, acc 0.92
2016-09-07T00:11:30.419095: step 426, loss 0.289459, acc 0.86
2016-09-07T00:11:31.111996: step 427, loss 0.155627, acc 0.94
2016-09-07T00:11:31.794266: step 428, loss 0.427592, acc 0.84
2016-09-07T00:11:32.492705: step 429, loss 0.208034, acc 0.88
2016-09-07T00:11:33.202893: step 430, loss 0.221483, acc 0.94
2016-09-07T00:11:33.871864: step 431, loss 0.231527, acc 0.9
2016-09-07T00:11:34.541968: step 432, loss 0.242011, acc 0.9
2016-09-07T00:11:35.222521: step 433, loss 0.271492, acc 0.86
2016-09-07T00:11:35.895024: step 434, loss 0.228718, acc 0.88
2016-09-07T00:11:36.571265: step 435, loss 0.205772, acc 0.88
2016-09-07T00:11:37.246140: step 436, loss 0.474954, acc 0.78
2016-09-07T00:11:37.930213: step 437, loss 0.160784, acc 0.92
2016-09-07T00:11:38.596611: step 438, loss 0.146073, acc 0.92
2016-09-07T00:11:39.314441: step 439, loss 0.226828, acc 0.92
2016-09-07T00:11:39.996385: step 440, loss 0.309854, acc 0.86
2016-09-07T00:11:40.694651: step 441, loss 0.341015, acc 0.84
2016-09-07T00:11:41.376750: step 442, loss 0.288756, acc 0.88
2016-09-07T00:11:42.068686: step 443, loss 0.172306, acc 0.88
2016-09-07T00:11:42.793599: step 444, loss 0.270408, acc 0.88
2016-09-07T00:11:43.469372: step 445, loss 0.248902, acc 0.92
2016-09-07T00:11:44.158512: step 446, loss 0.143283, acc 0.92
2016-09-07T00:11:44.865664: step 447, loss 0.207075, acc 0.9
2016-09-07T00:11:45.590167: step 448, loss 0.229636, acc 0.92
2016-09-07T00:11:46.285936: step 449, loss 0.219622, acc 0.92
2016-09-07T00:11:46.951631: step 450, loss 0.208831, acc 0.94
2016-09-07T00:11:47.656616: step 451, loss 0.141759, acc 0.94
2016-09-07T00:11:48.349732: step 452, loss 0.16834, acc 0.92
2016-09-07T00:11:49.005875: step 453, loss 0.237817, acc 0.86
2016-09-07T00:11:49.700123: step 454, loss 0.298587, acc 0.9
2016-09-07T00:11:50.393980: step 455, loss 0.236708, acc 0.92
2016-09-07T00:11:51.087901: step 456, loss 0.307328, acc 0.9
2016-09-07T00:11:51.749986: step 457, loss 0.313291, acc 0.8
2016-09-07T00:11:52.443284: step 458, loss 0.210069, acc 0.84
2016-09-07T00:11:53.099696: step 459, loss 0.316174, acc 0.9
2016-09-07T00:11:53.782007: step 460, loss 0.104237, acc 1
2016-09-07T00:11:54.480909: step 461, loss 0.218506, acc 0.88
2016-09-07T00:11:55.174620: step 462, loss 0.173106, acc 0.92
2016-09-07T00:11:55.866311: step 463, loss 0.36096, acc 0.82
2016-09-07T00:11:56.543479: step 464, loss 0.171565, acc 0.92
2016-09-07T00:11:57.233187: step 465, loss 0.378262, acc 0.84
2016-09-07T00:11:57.910652: step 466, loss 0.43714, acc 0.82
2016-09-07T00:11:58.614927: step 467, loss 0.246485, acc 0.9
2016-09-07T00:11:59.292726: step 468, loss 0.214983, acc 0.86
2016-09-07T00:11:59.967890: step 469, loss 0.263429, acc 0.9
2016-09-07T00:12:00.676448: step 470, loss 0.192515, acc 0.9
2016-09-07T00:12:01.366482: step 471, loss 0.178262, acc 0.92
2016-09-07T00:12:02.054728: step 472, loss 0.223602, acc 0.94
2016-09-07T00:12:02.721593: step 473, loss 0.187707, acc 0.94
2016-09-07T00:12:03.403417: step 474, loss 0.465304, acc 0.8
2016-09-07T00:12:04.089796: step 475, loss 0.12902, acc 0.96
2016-09-07T00:12:04.770202: step 476, loss 0.268189, acc 0.9
2016-09-07T00:12:05.454470: step 477, loss 0.265948, acc 0.9
2016-09-07T00:12:06.140185: step 478, loss 0.302552, acc 0.88
2016-09-07T00:12:06.839267: step 479, loss 0.236292, acc 0.92
2016-09-07T00:12:07.526378: step 480, loss 0.288259, acc 0.9
2016-09-07T00:12:08.193948: step 481, loss 0.267932, acc 0.9
2016-09-07T00:12:08.871475: step 482, loss 0.257799, acc 0.9
2016-09-07T00:12:09.566754: step 483, loss 0.24509, acc 0.9
2016-09-07T00:12:10.249255: step 484, loss 0.265169, acc 0.88
2016-09-07T00:12:10.920060: step 485, loss 0.269882, acc 0.86
2016-09-07T00:12:11.611890: step 486, loss 0.30076, acc 0.86
2016-09-07T00:12:12.280029: step 487, loss 0.229795, acc 0.92
2016-09-07T00:12:12.999765: step 488, loss 0.126673, acc 0.94
2016-09-07T00:12:13.692317: step 489, loss 0.33417, acc 0.84
2016-09-07T00:12:14.370767: step 490, loss 0.26808, acc 0.88
2016-09-07T00:12:15.062392: step 491, loss 0.328386, acc 0.86
2016-09-07T00:12:15.736259: step 492, loss 0.207637, acc 0.96
2016-09-07T00:12:16.456027: step 493, loss 0.135038, acc 0.94
2016-09-07T00:12:17.123185: step 494, loss 0.367111, acc 0.86
2016-09-07T00:12:17.813918: step 495, loss 0.429131, acc 0.78
2016-09-07T00:12:18.504416: step 496, loss 0.143631, acc 0.98
2016-09-07T00:12:19.172377: step 497, loss 0.210229, acc 0.94
2016-09-07T00:12:19.870117: step 498, loss 0.281534, acc 0.9
2016-09-07T00:12:20.563754: step 499, loss 0.132271, acc 0.92
2016-09-07T00:12:21.255727: step 500, loss 0.254441, acc 0.9

Evaluation:
2016-09-07T00:12:24.373039: step 500, loss 0.510886, acc 0.778612

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-500

2016-09-07T00:12:26.139354: step 501, loss 0.238674, acc 0.88
2016-09-07T00:12:26.802387: step 502, loss 0.234822, acc 0.9
2016-09-07T00:12:27.479516: step 503, loss 0.234335, acc 0.9
2016-09-07T00:12:28.184759: step 504, loss 0.176615, acc 0.94
2016-09-07T00:12:28.870898: step 505, loss 0.238864, acc 0.88
2016-09-07T00:12:29.565559: step 506, loss 0.38159, acc 0.86
2016-09-07T00:12:30.221189: step 507, loss 0.284916, acc 0.88
2016-09-07T00:12:30.912974: step 508, loss 0.262959, acc 0.9
2016-09-07T00:12:31.595247: step 509, loss 0.402187, acc 0.88
2016-09-07T00:12:32.302382: step 510, loss 0.393193, acc 0.84
2016-09-07T00:12:33.007988: step 511, loss 0.215452, acc 0.94
2016-09-07T00:12:33.728179: step 512, loss 0.242678, acc 0.92
2016-09-07T00:12:34.421045: step 513, loss 0.311173, acc 0.88
2016-09-07T00:12:35.085387: step 514, loss 0.315767, acc 0.86
2016-09-07T00:12:35.780822: step 515, loss 0.234734, acc 0.88
2016-09-07T00:12:36.462251: step 516, loss 0.281543, acc 0.9
2016-09-07T00:12:37.148779: step 517, loss 0.409653, acc 0.78
2016-09-07T00:12:37.823069: step 518, loss 0.336107, acc 0.82
2016-09-07T00:12:38.503403: step 519, loss 0.278987, acc 0.88
2016-09-07T00:12:39.205454: step 520, loss 0.207106, acc 0.94
2016-09-07T00:12:39.877227: step 521, loss 0.292077, acc 0.88
2016-09-07T00:12:40.570917: step 522, loss 0.24894, acc 0.92
2016-09-07T00:12:41.252097: step 523, loss 0.163564, acc 0.96
2016-09-07T00:12:41.950741: step 524, loss 0.374498, acc 0.82
2016-09-07T00:12:42.628038: step 525, loss 0.292058, acc 0.88
2016-09-07T00:12:43.313963: step 526, loss 0.213566, acc 0.94
2016-09-07T00:12:44.002499: step 527, loss 0.472252, acc 0.78
2016-09-07T00:12:44.651601: step 528, loss 0.118942, acc 0.94
2016-09-07T00:12:45.355323: step 529, loss 0.198105, acc 0.94
2016-09-07T00:12:46.059661: step 530, loss 0.203567, acc 0.92
2016-09-07T00:12:46.741941: step 531, loss 0.234332, acc 0.88
2016-09-07T00:12:47.408063: step 532, loss 0.32171, acc 0.84
2016-09-07T00:12:48.126586: step 533, loss 0.192676, acc 0.9
2016-09-07T00:12:48.820932: step 534, loss 0.215385, acc 0.92
2016-09-07T00:12:49.496437: step 535, loss 0.415103, acc 0.84
2016-09-07T00:12:50.189647: step 536, loss 0.246779, acc 0.92
2016-09-07T00:12:50.867981: step 537, loss 0.159219, acc 0.96
2016-09-07T00:12:51.541935: step 538, loss 0.251045, acc 0.86
2016-09-07T00:12:52.217690: step 539, loss 0.347201, acc 0.84
2016-09-07T00:12:52.901429: step 540, loss 0.10397, acc 0.98
2016-09-07T00:12:53.619309: step 541, loss 0.201952, acc 0.9
2016-09-07T00:12:54.284952: step 542, loss 0.318247, acc 0.84
2016-09-07T00:12:54.979889: step 543, loss 0.248114, acc 0.94
2016-09-07T00:12:55.662178: step 544, loss 0.23558, acc 0.9
2016-09-07T00:12:56.338439: step 545, loss 0.355777, acc 0.88
2016-09-07T00:12:57.031365: step 546, loss 0.156773, acc 0.9
2016-09-07T00:12:57.734255: step 547, loss 0.240094, acc 0.88
2016-09-07T00:12:58.424202: step 548, loss 0.260491, acc 0.92
2016-09-07T00:12:59.125513: step 549, loss 0.390656, acc 0.86
2016-09-07T00:12:59.821540: step 550, loss 0.182702, acc 0.92
2016-09-07T00:13:00.530580: step 551, loss 0.203906, acc 0.9
2016-09-07T00:13:01.206069: step 552, loss 0.325508, acc 0.82
2016-09-07T00:13:01.875332: step 553, loss 0.239433, acc 0.88
2016-09-07T00:13:02.544643: step 554, loss 0.128232, acc 0.98
2016-09-07T00:13:03.233309: step 555, loss 0.253211, acc 0.88
2016-09-07T00:13:03.886679: step 556, loss 0.281894, acc 0.92
2016-09-07T00:13:04.605149: step 557, loss 0.247977, acc 0.88
2016-09-07T00:13:05.278708: step 558, loss 0.302325, acc 0.86
2016-09-07T00:13:05.957699: step 559, loss 0.361312, acc 0.88
2016-09-07T00:13:06.627981: step 560, loss 0.15008, acc 0.92
2016-09-07T00:13:07.285877: step 561, loss 0.293273, acc 0.82
2016-09-07T00:13:07.973326: step 562, loss 0.352563, acc 0.84
2016-09-07T00:13:08.668941: step 563, loss 0.21106, acc 0.9
2016-09-07T00:13:09.361968: step 564, loss 0.210748, acc 0.88
2016-09-07T00:13:10.040432: step 565, loss 0.159172, acc 0.92
2016-09-07T00:13:10.718218: step 566, loss 0.209934, acc 0.92
2016-09-07T00:13:11.398877: step 567, loss 0.197807, acc 0.94
2016-09-07T00:13:12.075133: step 568, loss 0.10601, acc 0.98
2016-09-07T00:13:12.749582: step 569, loss 0.292097, acc 0.86
2016-09-07T00:13:13.427922: step 570, loss 0.236532, acc 0.92
2016-09-07T00:13:14.091858: step 571, loss 0.199545, acc 0.9
2016-09-07T00:13:14.764160: step 572, loss 0.341523, acc 0.84
2016-09-07T00:13:15.466855: step 573, loss 0.228012, acc 0.92
2016-09-07T00:13:16.157861: step 574, loss 0.194124, acc 0.92
2016-09-07T00:13:16.836886: step 575, loss 0.240182, acc 0.94
2016-09-07T00:13:17.492506: step 576, loss 0.247674, acc 0.909091
2016-09-07T00:13:18.204321: step 577, loss 0.115493, acc 0.96
2016-09-07T00:13:18.939975: step 578, loss 0.145821, acc 0.9
2016-09-07T00:13:19.602484: step 579, loss 0.213129, acc 0.9
2016-09-07T00:13:20.298302: step 580, loss 0.226292, acc 0.88
2016-09-07T00:13:20.982479: step 581, loss 0.273313, acc 0.9
2016-09-07T00:13:21.687822: step 582, loss 0.11011, acc 0.94
2016-09-07T00:13:22.366656: step 583, loss 0.0737911, acc 1
2016-09-07T00:13:23.053529: step 584, loss 0.116632, acc 0.96
2016-09-07T00:13:23.750692: step 585, loss 0.187141, acc 0.94
2016-09-07T00:13:24.449357: step 586, loss 0.196553, acc 0.9
2016-09-07T00:13:25.149802: step 587, loss 0.174984, acc 0.92
2016-09-07T00:13:25.833239: step 588, loss 0.210833, acc 0.88
2016-09-07T00:13:26.528649: step 589, loss 0.260411, acc 0.92
2016-09-07T00:13:27.215053: step 590, loss 0.121654, acc 0.98
2016-09-07T00:13:27.887014: step 591, loss 0.162404, acc 0.92
2016-09-07T00:13:28.590880: step 592, loss 0.133601, acc 0.92
2016-09-07T00:13:29.253508: step 593, loss 0.128353, acc 0.94
2016-09-07T00:13:29.963054: step 594, loss 0.163903, acc 0.9
2016-09-07T00:13:30.655967: step 595, loss 0.0389467, acc 1
2016-09-07T00:13:31.330722: step 596, loss 0.0840453, acc 0.98
2016-09-07T00:13:32.016847: step 597, loss 0.163977, acc 0.92
2016-09-07T00:13:32.703268: step 598, loss 0.101847, acc 0.96
2016-09-07T00:13:33.404147: step 599, loss 0.188159, acc 0.9
2016-09-07T00:13:34.060872: step 600, loss 0.139649, acc 0.92

Evaluation:
2016-09-07T00:13:37.211473: step 600, loss 0.693244, acc 0.772983

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-600

2016-09-07T00:13:38.833120: step 601, loss 0.0748947, acc 0.98
2016-09-07T00:13:39.543889: step 602, loss 0.27131, acc 0.9
2016-09-07T00:13:40.244688: step 603, loss 0.0769533, acc 0.94
2016-09-07T00:13:40.926061: step 604, loss 0.133525, acc 0.92
2016-09-07T00:13:41.626708: step 605, loss 0.225777, acc 0.86
2016-09-07T00:13:42.305775: step 606, loss 0.173255, acc 0.9
2016-09-07T00:13:43.010053: step 607, loss 0.0640142, acc 0.98
2016-09-07T00:13:43.681699: step 608, loss 0.174624, acc 0.88
2016-09-07T00:13:44.371737: step 609, loss 0.107568, acc 0.96
2016-09-07T00:13:45.038295: step 610, loss 0.057005, acc 0.98
2016-09-07T00:13:45.751466: step 611, loss 0.252276, acc 0.86
2016-09-07T00:13:46.449576: step 612, loss 0.0805723, acc 0.98
2016-09-07T00:13:47.129647: step 613, loss 0.118713, acc 0.94
2016-09-07T00:13:47.844799: step 614, loss 0.176731, acc 0.92
2016-09-07T00:13:48.523841: step 615, loss 0.0841839, acc 0.96
2016-09-07T00:13:49.218534: step 616, loss 0.146148, acc 0.94
2016-09-07T00:13:49.904282: step 617, loss 0.126853, acc 0.92
2016-09-07T00:13:50.602660: step 618, loss 0.072104, acc 0.98
2016-09-07T00:13:51.286737: step 619, loss 0.172981, acc 0.9
2016-09-07T00:13:51.967454: step 620, loss 0.15887, acc 0.88
2016-09-07T00:13:52.684088: step 621, loss 0.12259, acc 0.96
2016-09-07T00:13:53.349974: step 622, loss 0.207223, acc 0.94
2016-09-07T00:13:54.033007: step 623, loss 0.240601, acc 0.9
2016-09-07T00:13:54.730226: step 624, loss 0.0751604, acc 0.96
2016-09-07T00:13:55.427458: step 625, loss 0.138466, acc 0.9
2016-09-07T00:13:56.103062: step 626, loss 0.329254, acc 0.9
2016-09-07T00:13:56.767420: step 627, loss 0.150249, acc 0.96
2016-09-07T00:13:57.478440: step 628, loss 0.166307, acc 0.94
2016-09-07T00:13:58.141613: step 629, loss 0.206757, acc 0.88
2016-09-07T00:13:58.858407: step 630, loss 0.226469, acc 0.86
2016-09-07T00:13:59.569942: step 631, loss 0.235783, acc 0.94
2016-09-07T00:14:00.294208: step 632, loss 0.152171, acc 0.96
2016-09-07T00:14:00.990981: step 633, loss 0.0987121, acc 0.96
2016-09-07T00:14:01.664893: step 634, loss 0.132249, acc 0.98
2016-09-07T00:14:02.367679: step 635, loss 0.168639, acc 0.92
2016-09-07T00:14:03.046734: step 636, loss 0.256279, acc 0.9
2016-09-07T00:14:03.745076: step 637, loss 0.176148, acc 0.94
2016-09-07T00:14:04.431332: step 638, loss 0.12399, acc 0.96
2016-09-07T00:14:05.118706: step 639, loss 0.218765, acc 0.9
2016-09-07T00:14:05.829194: step 640, loss 0.223582, acc 0.88
2016-09-07T00:14:06.479262: step 641, loss 0.247135, acc 0.9
2016-09-07T00:14:07.208807: step 642, loss 0.160115, acc 0.94
2016-09-07T00:14:07.900692: step 643, loss 0.128056, acc 0.94
2016-09-07T00:14:08.597928: step 644, loss 0.188019, acc 0.92
2016-09-07T00:14:09.282622: step 645, loss 0.208307, acc 0.9
2016-09-07T00:14:09.982853: step 646, loss 0.285536, acc 0.9
2016-09-07T00:14:10.700697: step 647, loss 0.0744635, acc 0.96
2016-09-07T00:14:11.364483: step 648, loss 0.139235, acc 0.92
2016-09-07T00:14:12.064857: step 649, loss 0.162355, acc 0.96
2016-09-07T00:14:12.765142: step 650, loss 0.256487, acc 0.92
2016-09-07T00:14:13.446375: step 651, loss 0.21133, acc 0.92
2016-09-07T00:14:14.130991: step 652, loss 0.15161, acc 0.94
2016-09-07T00:14:14.841465: step 653, loss 0.108145, acc 0.94
2016-09-07T00:14:15.537887: step 654, loss 0.153476, acc 0.96
2016-09-07T00:14:16.208780: step 655, loss 0.0916951, acc 0.96
2016-09-07T00:14:16.951719: step 656, loss 0.127887, acc 0.94
2016-09-07T00:14:17.634561: step 657, loss 0.137641, acc 0.92
2016-09-07T00:14:18.298724: step 658, loss 0.164374, acc 0.96
2016-09-07T00:14:18.981612: step 659, loss 0.393258, acc 0.86
2016-09-07T00:14:19.663162: step 660, loss 0.176259, acc 0.94
2016-09-07T00:14:20.353686: step 661, loss 0.154362, acc 0.92
2016-09-07T00:14:21.034902: step 662, loss 0.128274, acc 0.94
2016-09-07T00:14:21.733577: step 663, loss 0.193445, acc 0.96
2016-09-07T00:14:22.419498: step 664, loss 0.312815, acc 0.9
2016-09-07T00:14:23.116248: step 665, loss 0.134668, acc 0.98
2016-09-07T00:14:23.812120: step 666, loss 0.0839192, acc 0.98
2016-09-07T00:14:24.480133: step 667, loss 0.106143, acc 0.94
2016-09-07T00:14:25.196157: step 668, loss 0.347368, acc 0.86
2016-09-07T00:14:25.878897: step 669, loss 0.240385, acc 0.9
2016-09-07T00:14:26.563950: step 670, loss 0.11132, acc 0.96
2016-09-07T00:14:27.246498: step 671, loss 0.369906, acc 0.82
2016-09-07T00:14:27.917656: step 672, loss 0.196588, acc 0.86
2016-09-07T00:14:28.595869: step 673, loss 0.136621, acc 0.94
2016-09-07T00:14:29.257257: step 674, loss 0.198915, acc 0.9
2016-09-07T00:14:29.947229: step 675, loss 0.159, acc 0.94
2016-09-07T00:14:30.629505: step 676, loss 0.224543, acc 0.86
2016-09-07T00:14:31.328395: step 677, loss 0.200996, acc 0.9
2016-09-07T00:14:32.030531: step 678, loss 0.148473, acc 0.94
2016-09-07T00:14:32.721239: step 679, loss 0.268253, acc 0.88
2016-09-07T00:14:33.392864: step 680, loss 0.176857, acc 0.9
2016-09-07T00:14:34.065292: step 681, loss 0.176909, acc 0.92
2016-09-07T00:14:34.752028: step 682, loss 0.139701, acc 0.98
2016-09-07T00:14:35.419298: step 683, loss 0.0760769, acc 0.98
2016-09-07T00:14:36.091518: step 684, loss 0.227542, acc 0.92
2016-09-07T00:14:36.795144: step 685, loss 0.12036, acc 0.9
2016-09-07T00:14:37.485239: step 686, loss 0.0992273, acc 0.96
2016-09-07T00:14:38.166186: step 687, loss 0.135028, acc 0.94
2016-09-07T00:14:38.853992: step 688, loss 0.159198, acc 0.9
2016-09-07T00:14:39.546357: step 689, loss 0.1517, acc 0.96
2016-09-07T00:14:40.212019: step 690, loss 0.0908489, acc 0.98
2016-09-07T00:14:40.905426: step 691, loss 0.228888, acc 0.88
2016-09-07T00:14:41.588158: step 692, loss 0.17483, acc 0.88
2016-09-07T00:14:42.287206: step 693, loss 0.117942, acc 0.98
2016-09-07T00:14:42.975220: step 694, loss 0.198372, acc 0.92
2016-09-07T00:14:43.666448: step 695, loss 0.206064, acc 0.9
2016-09-07T00:14:44.370409: step 696, loss 0.187304, acc 0.92
2016-09-07T00:14:45.046303: step 697, loss 0.112669, acc 0.94
2016-09-07T00:14:45.732873: step 698, loss 0.11443, acc 0.94
2016-09-07T00:14:46.411893: step 699, loss 0.146868, acc 0.94
2016-09-07T00:14:47.111104: step 700, loss 0.295496, acc 0.88

Evaluation:
2016-09-07T00:14:50.261939: step 700, loss 0.616645, acc 0.77955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-700

2016-09-07T00:14:51.888285: step 701, loss 0.456277, acc 0.78
2016-09-07T00:14:52.590893: step 702, loss 0.100402, acc 0.96
2016-09-07T00:14:53.276639: step 703, loss 0.128314, acc 0.94
2016-09-07T00:14:53.990424: step 704, loss 0.283514, acc 0.88
2016-09-07T00:14:54.700650: step 705, loss 0.291267, acc 0.88
2016-09-07T00:14:55.372736: step 706, loss 0.294315, acc 0.86
2016-09-07T00:14:56.067438: step 707, loss 0.250198, acc 0.88
2016-09-07T00:14:56.755685: step 708, loss 0.216744, acc 0.92
2016-09-07T00:14:57.426616: step 709, loss 0.221203, acc 0.96
2016-09-07T00:14:58.120779: step 710, loss 0.308844, acc 0.88
2016-09-07T00:14:58.826647: step 711, loss 0.142482, acc 0.96
2016-09-07T00:14:59.494333: step 712, loss 0.271297, acc 0.9
2016-09-07T00:15:00.175971: step 713, loss 0.272209, acc 0.9
2016-09-07T00:15:00.889828: step 714, loss 0.250093, acc 0.9
2016-09-07T00:15:01.594914: step 715, loss 0.275682, acc 0.86
2016-09-07T00:15:02.298946: step 716, loss 0.158647, acc 0.94
2016-09-07T00:15:03.015888: step 717, loss 0.148824, acc 0.94
2016-09-07T00:15:03.711466: step 718, loss 0.203433, acc 0.88
2016-09-07T00:15:04.395305: step 719, loss 0.150362, acc 0.96
2016-09-07T00:15:05.086186: step 720, loss 0.183789, acc 0.92
2016-09-07T00:15:05.770073: step 721, loss 0.175002, acc 0.92
2016-09-07T00:15:06.453397: step 722, loss 0.140557, acc 0.98
2016-09-07T00:15:07.143264: step 723, loss 0.0732044, acc 0.98
2016-09-07T00:15:07.824358: step 724, loss 0.201887, acc 0.9
2016-09-07T00:15:08.547248: step 725, loss 0.276813, acc 0.88
2016-09-07T00:15:09.246824: step 726, loss 0.283231, acc 0.96
2016-09-07T00:15:09.938894: step 727, loss 0.228005, acc 0.9
2016-09-07T00:15:10.626308: step 728, loss 0.159385, acc 0.94
2016-09-07T00:15:11.308470: step 729, loss 0.156637, acc 0.94
2016-09-07T00:15:12.004806: step 730, loss 0.194599, acc 0.92
2016-09-07T00:15:12.670212: step 731, loss 0.191068, acc 0.94
2016-09-07T00:15:13.352589: step 732, loss 0.201928, acc 0.92
2016-09-07T00:15:14.040719: step 733, loss 0.19134, acc 0.96
2016-09-07T00:15:14.713705: step 734, loss 0.165834, acc 0.94
2016-09-07T00:15:15.408357: step 735, loss 0.128853, acc 0.96
2016-09-07T00:15:16.091466: step 736, loss 0.121335, acc 0.96
2016-09-07T00:15:16.789135: step 737, loss 0.185631, acc 0.92
2016-09-07T00:15:17.451767: step 738, loss 0.182923, acc 0.9
2016-09-07T00:15:18.139738: step 739, loss 0.0773737, acc 0.96
2016-09-07T00:15:18.840736: step 740, loss 0.0998654, acc 0.98
2016-09-07T00:15:19.527653: step 741, loss 0.195597, acc 0.92
2016-09-07T00:15:20.226034: step 742, loss 0.229119, acc 0.88
2016-09-07T00:15:20.910128: step 743, loss 0.137281, acc 0.96
2016-09-07T00:15:21.611200: step 744, loss 0.176401, acc 0.9
2016-09-07T00:15:22.276900: step 745, loss 0.220549, acc 0.9
2016-09-07T00:15:22.984343: step 746, loss 0.357326, acc 0.86
2016-09-07T00:15:23.679001: step 747, loss 0.252627, acc 0.88
2016-09-07T00:15:24.397461: step 748, loss 0.0674108, acc 1
2016-09-07T00:15:25.083670: step 749, loss 0.140908, acc 0.92
2016-09-07T00:15:25.757527: step 750, loss 0.148801, acc 0.88
2016-09-07T00:15:26.467300: step 751, loss 0.25321, acc 0.88
2016-09-07T00:15:27.139110: step 752, loss 0.246329, acc 0.96
2016-09-07T00:15:27.818270: step 753, loss 0.129267, acc 0.94
2016-09-07T00:15:28.503666: step 754, loss 0.344458, acc 0.82
2016-09-07T00:15:29.157630: step 755, loss 0.159532, acc 0.94
2016-09-07T00:15:29.835426: step 756, loss 0.159955, acc 0.94
2016-09-07T00:15:30.518788: step 757, loss 0.210302, acc 0.9
2016-09-07T00:15:31.246130: step 758, loss 0.0668551, acc 1
2016-09-07T00:15:31.930252: step 759, loss 0.138791, acc 0.92
2016-09-07T00:15:32.598083: step 760, loss 0.178134, acc 0.9
2016-09-07T00:15:33.262489: step 761, loss 0.202909, acc 0.9
2016-09-07T00:15:33.936826: step 762, loss 0.0616887, acc 0.98
2016-09-07T00:15:34.631364: step 763, loss 0.259558, acc 0.94
2016-09-07T00:15:35.327124: step 764, loss 0.11276, acc 0.96
2016-09-07T00:15:36.052142: step 765, loss 0.172225, acc 0.9
2016-09-07T00:15:36.733630: step 766, loss 0.231798, acc 0.88
2016-09-07T00:15:37.411331: step 767, loss 0.115791, acc 0.98
2016-09-07T00:15:38.055388: step 768, loss 0.0493515, acc 0.977273
2016-09-07T00:15:38.748924: step 769, loss 0.109728, acc 0.98
2016-09-07T00:15:39.426911: step 770, loss 0.0920883, acc 1
2016-09-07T00:15:40.148930: step 771, loss 0.163583, acc 0.9
2016-09-07T00:15:40.843149: step 772, loss 0.116175, acc 0.96
2016-09-07T00:15:41.514236: step 773, loss 0.0972828, acc 0.96
2016-09-07T00:15:42.220602: step 774, loss 0.213834, acc 0.9
2016-09-07T00:15:42.905241: step 775, loss 0.0733058, acc 0.98
2016-09-07T00:15:43.602073: step 776, loss 0.128833, acc 0.94
2016-09-07T00:15:44.283477: step 777, loss 0.0526171, acc 0.96
2016-09-07T00:15:44.987760: step 778, loss 0.0280866, acc 1
2016-09-07T00:15:45.675486: step 779, loss 0.0798803, acc 0.98
2016-09-07T00:15:46.350256: step 780, loss 0.0854806, acc 0.98
2016-09-07T00:15:47.028315: step 781, loss 0.197386, acc 0.92
2016-09-07T00:15:47.723010: step 782, loss 0.152008, acc 0.92
2016-09-07T00:15:48.415211: step 783, loss 0.113675, acc 0.96
2016-09-07T00:15:49.091350: step 784, loss 0.0493993, acc 0.98
2016-09-07T00:15:49.778124: step 785, loss 0.19147, acc 0.92
2016-09-07T00:15:50.489644: step 786, loss 0.0807955, acc 0.96
2016-09-07T00:15:51.179526: step 787, loss 0.0847189, acc 0.94
2016-09-07T00:15:51.859620: step 788, loss 0.114493, acc 0.92
2016-09-07T00:15:52.537752: step 789, loss 0.125023, acc 0.94
2016-09-07T00:15:53.207165: step 790, loss 0.128272, acc 0.96
2016-09-07T00:15:53.881938: step 791, loss 0.0371557, acc 1
2016-09-07T00:15:54.561777: step 792, loss 0.069669, acc 0.96
2016-09-07T00:15:55.273286: step 793, loss 0.228159, acc 0.94
2016-09-07T00:15:55.948034: step 794, loss 0.0704172, acc 0.98
2016-09-07T00:15:56.651535: step 795, loss 0.0883159, acc 0.98
2016-09-07T00:15:57.337925: step 796, loss 0.0538754, acc 0.98
2016-09-07T00:15:58.029512: step 797, loss 0.187523, acc 0.9
2016-09-07T00:15:58.719788: step 798, loss 0.0303592, acc 1
2016-09-07T00:15:59.402651: step 799, loss 0.116258, acc 0.94
2016-09-07T00:16:00.113525: step 800, loss 0.14926, acc 0.94

Evaluation:
2016-09-07T00:16:03.277063: step 800, loss 0.740668, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-800

2016-09-07T00:16:05.006342: step 801, loss 0.0921785, acc 0.96
2016-09-07T00:16:05.692085: step 802, loss 0.0775967, acc 0.96
2016-09-07T00:16:06.378670: step 803, loss 0.149997, acc 0.96
2016-09-07T00:16:07.061999: step 804, loss 0.0897545, acc 0.96
2016-09-07T00:16:07.740119: step 805, loss 0.142731, acc 0.96
2016-09-07T00:16:08.438174: step 806, loss 0.15358, acc 0.94
2016-09-07T00:16:09.083130: step 807, loss 0.0378866, acc 1
2016-09-07T00:16:09.787229: step 808, loss 0.0754054, acc 0.96
2016-09-07T00:16:10.473199: step 809, loss 0.122664, acc 0.96
2016-09-07T00:16:11.151273: step 810, loss 0.176903, acc 0.92
2016-09-07T00:16:11.829184: step 811, loss 0.141436, acc 0.94
2016-09-07T00:16:12.508845: step 812, loss 0.109783, acc 0.98
2016-09-07T00:16:13.198758: step 813, loss 0.102787, acc 0.94
2016-09-07T00:16:13.853729: step 814, loss 0.0658537, acc 0.98
2016-09-07T00:16:14.570654: step 815, loss 0.0931682, acc 0.92
2016-09-07T00:16:15.219324: step 816, loss 0.0819978, acc 0.96
2016-09-07T00:16:15.881996: step 817, loss 0.0693705, acc 0.98
2016-09-07T00:16:16.561157: step 818, loss 0.117535, acc 0.92
2016-09-07T00:16:17.234817: step 819, loss 0.129886, acc 0.92
2016-09-07T00:16:17.933730: step 820, loss 0.0380514, acc 0.98
2016-09-07T00:16:18.611409: step 821, loss 0.162196, acc 0.94
2016-09-07T00:16:19.334815: step 822, loss 0.139733, acc 0.96
2016-09-07T00:16:20.024747: step 823, loss 0.137432, acc 0.94
2016-09-07T00:16:20.713123: step 824, loss 0.11628, acc 0.96
2016-09-07T00:16:21.408365: step 825, loss 0.148787, acc 0.94
2016-09-07T00:16:22.098375: step 826, loss 0.0721286, acc 0.96
2016-09-07T00:16:22.773575: step 827, loss 0.248418, acc 0.86
2016-09-07T00:16:23.467305: step 828, loss 0.116394, acc 0.96
2016-09-07T00:16:24.174477: step 829, loss 0.254742, acc 0.88
2016-09-07T00:16:24.866974: step 830, loss 0.10168, acc 0.94
2016-09-07T00:16:25.551340: step 831, loss 0.102195, acc 0.94
2016-09-07T00:16:26.234244: step 832, loss 0.209832, acc 0.92
2016-09-07T00:16:26.924749: step 833, loss 0.0556484, acc 0.98
2016-09-07T00:16:27.609930: step 834, loss 0.21003, acc 0.9
2016-09-07T00:16:28.315298: step 835, loss 0.14838, acc 0.94
2016-09-07T00:16:29.025798: step 836, loss 0.0647599, acc 0.98
2016-09-07T00:16:29.715526: step 837, loss 0.089049, acc 0.96
2016-09-07T00:16:30.392042: step 838, loss 0.170748, acc 0.92
2016-09-07T00:16:31.092380: step 839, loss 0.105397, acc 0.96
2016-09-07T00:16:31.792794: step 840, loss 0.0902605, acc 0.98
2016-09-07T00:16:32.474417: step 841, loss 0.128747, acc 0.94
2016-09-07T00:16:33.119799: step 842, loss 0.0618692, acc 0.98
2016-09-07T00:16:33.819370: step 843, loss 0.115601, acc 0.92
2016-09-07T00:16:34.483458: step 844, loss 0.197091, acc 0.92
2016-09-07T00:16:35.150577: step 845, loss 0.18297, acc 0.88
2016-09-07T00:16:35.831250: step 846, loss 0.0547948, acc 0.98
2016-09-07T00:16:36.538963: step 847, loss 0.192249, acc 0.92
2016-09-07T00:16:37.245415: step 848, loss 0.105797, acc 0.94
2016-09-07T00:16:37.928862: step 849, loss 0.0919627, acc 0.98
2016-09-07T00:16:38.627005: step 850, loss 0.155986, acc 0.92
2016-09-07T00:16:39.297022: step 851, loss 0.0998591, acc 0.96
2016-09-07T00:16:39.979183: step 852, loss 0.116198, acc 0.94
2016-09-07T00:16:40.660969: step 853, loss 0.110404, acc 0.98
2016-09-07T00:16:41.348807: step 854, loss 0.122033, acc 0.96
2016-09-07T00:16:42.018717: step 855, loss 0.0275196, acc 1
2016-09-07T00:16:42.702274: step 856, loss 0.138171, acc 0.94
2016-09-07T00:16:43.392758: step 857, loss 0.152261, acc 0.96
2016-09-07T00:16:44.048918: step 858, loss 0.051414, acc 1
2016-09-07T00:16:44.733504: step 859, loss 0.0759251, acc 0.96
2016-09-07T00:16:45.422983: step 860, loss 0.0975265, acc 0.98
2016-09-07T00:16:46.131678: step 861, loss 0.144455, acc 0.98
2016-09-07T00:16:46.829579: step 862, loss 0.239788, acc 0.88
2016-09-07T00:16:47.510360: step 863, loss 0.129923, acc 0.94
2016-09-07T00:16:48.219472: step 864, loss 0.147964, acc 0.96
2016-09-07T00:16:48.912227: step 865, loss 0.0867301, acc 0.96
2016-09-07T00:16:49.600399: step 866, loss 0.143514, acc 0.96
2016-09-07T00:16:50.312402: step 867, loss 0.153378, acc 0.92
2016-09-07T00:16:50.999407: step 868, loss 0.116548, acc 0.96
2016-09-07T00:16:51.688474: step 869, loss 0.0966636, acc 0.98
2016-09-07T00:16:52.347310: step 870, loss 0.0482188, acc 1
2016-09-07T00:16:53.049192: step 871, loss 0.114679, acc 0.96
2016-09-07T00:16:53.727126: step 872, loss 0.332364, acc 0.88
2016-09-07T00:16:54.419329: step 873, loss 0.169598, acc 0.92
2016-09-07T00:16:55.116912: step 874, loss 0.152761, acc 0.92
2016-09-07T00:16:55.798131: step 875, loss 0.32006, acc 0.96
2016-09-07T00:16:56.499164: step 876, loss 0.072621, acc 0.98
2016-09-07T00:16:57.168756: step 877, loss 0.125677, acc 0.92
2016-09-07T00:16:57.884961: step 878, loss 0.264907, acc 0.9
2016-09-07T00:16:58.545484: step 879, loss 0.195525, acc 0.9
2016-09-07T00:16:59.216689: step 880, loss 0.210166, acc 0.86
2016-09-07T00:16:59.893907: step 881, loss 0.101137, acc 0.96
2016-09-07T00:17:00.608306: step 882, loss 0.224746, acc 0.88
2016-09-07T00:17:01.290090: step 883, loss 0.130237, acc 0.96
2016-09-07T00:17:01.953650: step 884, loss 0.143822, acc 0.94
2016-09-07T00:17:02.633787: step 885, loss 0.185557, acc 0.9
2016-09-07T00:17:03.312683: step 886, loss 0.250091, acc 0.92
2016-09-07T00:17:03.997214: step 887, loss 0.210477, acc 0.92
2016-09-07T00:17:04.685278: step 888, loss 0.0574781, acc 1
2016-09-07T00:17:05.360916: step 889, loss 0.0676156, acc 0.98
2016-09-07T00:17:06.065448: step 890, loss 0.188721, acc 0.94
2016-09-07T00:17:06.762065: step 891, loss 0.242199, acc 0.9
2016-09-07T00:17:07.460454: step 892, loss 0.124269, acc 0.94
2016-09-07T00:17:08.161545: step 893, loss 0.104355, acc 0.96
2016-09-07T00:17:08.841886: step 894, loss 0.144942, acc 0.94
2016-09-07T00:17:09.538696: step 895, loss 0.139349, acc 0.94
2016-09-07T00:17:10.222752: step 896, loss 0.085955, acc 0.94
2016-09-07T00:17:10.920461: step 897, loss 0.050605, acc 0.98
2016-09-07T00:17:11.619081: step 898, loss 0.135135, acc 0.94
2016-09-07T00:17:12.315149: step 899, loss 0.190079, acc 0.86
2016-09-07T00:17:12.955730: step 900, loss 0.211861, acc 0.92

Evaluation:
2016-09-07T00:17:16.124641: step 900, loss 0.868612, acc 0.76454

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-900

2016-09-07T00:17:17.770012: step 901, loss 0.0787047, acc 0.98
2016-09-07T00:17:18.453696: step 902, loss 0.0668836, acc 0.96
2016-09-07T00:17:19.138400: step 903, loss 0.0428542, acc 1
2016-09-07T00:17:19.834796: step 904, loss 0.0944618, acc 0.98
2016-09-07T00:17:20.535522: step 905, loss 0.166414, acc 0.92
2016-09-07T00:17:21.207628: step 906, loss 0.125829, acc 0.96
2016-09-07T00:17:21.913998: step 907, loss 0.118406, acc 0.9
2016-09-07T00:17:22.599009: step 908, loss 0.19402, acc 0.88
2016-09-07T00:17:23.292908: step 909, loss 0.102049, acc 0.94
2016-09-07T00:17:23.990785: step 910, loss 0.120687, acc 0.94
2016-09-07T00:17:24.688668: step 911, loss 0.228129, acc 0.9
2016-09-07T00:17:25.430611: step 912, loss 0.497247, acc 0.82
2016-09-07T00:17:26.093134: step 913, loss 0.150939, acc 0.94
2016-09-07T00:17:26.777830: step 914, loss 0.136278, acc 0.94
2016-09-07T00:17:27.443527: step 915, loss 0.0826994, acc 0.98
2016-09-07T00:17:28.126792: step 916, loss 0.251475, acc 0.86
2016-09-07T00:17:28.807482: step 917, loss 0.0813017, acc 0.98
2016-09-07T00:17:29.495366: step 918, loss 0.121485, acc 0.94
2016-09-07T00:17:30.166062: step 919, loss 0.0667628, acc 0.96
2016-09-07T00:17:30.817644: step 920, loss 0.071739, acc 1
2016-09-07T00:17:31.509266: step 921, loss 0.0918263, acc 0.96
2016-09-07T00:17:32.186797: step 922, loss 0.0966825, acc 0.94
2016-09-07T00:17:32.859508: step 923, loss 0.20042, acc 0.9
2016-09-07T00:17:33.532989: step 924, loss 0.0785017, acc 0.98
2016-09-07T00:17:34.209968: step 925, loss 0.11614, acc 0.92
2016-09-07T00:17:34.896076: step 926, loss 0.179504, acc 0.94
2016-09-07T00:17:35.572649: step 927, loss 0.109208, acc 0.96
2016-09-07T00:17:36.274237: step 928, loss 0.0887984, acc 0.96
2016-09-07T00:17:36.950513: step 929, loss 0.105157, acc 0.98
2016-09-07T00:17:37.642445: step 930, loss 0.128406, acc 0.94
2016-09-07T00:17:38.341849: step 931, loss 0.147707, acc 0.92
2016-09-07T00:17:39.025483: step 932, loss 0.0507775, acc 1
2016-09-07T00:17:39.706840: step 933, loss 0.238123, acc 0.94
2016-09-07T00:17:40.418905: step 934, loss 0.164896, acc 0.94
2016-09-07T00:17:41.155970: step 935, loss 0.131496, acc 0.98
2016-09-07T00:17:41.832898: step 936, loss 0.0958324, acc 1
2016-09-07T00:17:42.492018: step 937, loss 0.150837, acc 0.92
2016-09-07T00:17:43.160676: step 938, loss 0.20988, acc 0.94
2016-09-07T00:17:43.848162: step 939, loss 0.133388, acc 0.94
2016-09-07T00:17:44.526349: step 940, loss 0.05312, acc 0.98
2016-09-07T00:17:45.205848: step 941, loss 0.148442, acc 0.94
2016-09-07T00:17:45.893875: step 942, loss 0.141105, acc 0.9
2016-09-07T00:17:46.547093: step 943, loss 0.136373, acc 0.92
2016-09-07T00:17:47.260265: step 944, loss 0.0480703, acc 1
2016-09-07T00:17:47.933630: step 945, loss 0.194623, acc 0.92
2016-09-07T00:17:48.618483: step 946, loss 0.108442, acc 0.94
2016-09-07T00:17:49.300897: step 947, loss 0.265242, acc 0.92
2016-09-07T00:17:49.978728: step 948, loss 0.0814799, acc 0.98
2016-09-07T00:17:50.653406: step 949, loss 0.0894015, acc 0.96
2016-09-07T00:17:51.312520: step 950, loss 0.0825888, acc 0.96
2016-09-07T00:17:52.007492: step 951, loss 0.117658, acc 0.94
2016-09-07T00:17:52.707538: step 952, loss 0.115005, acc 0.94
2016-09-07T00:17:53.381613: step 953, loss 0.0682715, acc 0.98
2016-09-07T00:17:54.056939: step 954, loss 0.116363, acc 0.96
2016-09-07T00:17:54.751043: step 955, loss 0.0705269, acc 0.98
2016-09-07T00:17:55.449988: step 956, loss 0.274194, acc 0.92
2016-09-07T00:17:56.111510: step 957, loss 0.0899151, acc 0.98
2016-09-07T00:17:56.798169: step 958, loss 0.0855281, acc 0.96
2016-09-07T00:17:57.476082: step 959, loss 0.127808, acc 0.94
2016-09-07T00:17:58.111437: step 960, loss 0.228017, acc 0.863636
2016-09-07T00:17:58.801672: step 961, loss 0.11317, acc 0.98
2016-09-07T00:17:59.476191: step 962, loss 0.0662728, acc 0.98
2016-09-07T00:18:00.175012: step 963, loss 0.175365, acc 0.92
2016-09-07T00:18:00.911664: step 964, loss 0.0363313, acc 1
2016-09-07T00:18:01.609108: step 965, loss 0.0505888, acc 1
2016-09-07T00:18:02.275824: step 966, loss 0.073717, acc 0.98
2016-09-07T00:18:02.952974: step 967, loss 0.111161, acc 0.96
2016-09-07T00:18:03.638244: step 968, loss 0.109071, acc 0.96
2016-09-07T00:18:04.321624: step 969, loss 0.0490687, acc 1
2016-09-07T00:18:04.998585: step 970, loss 0.0826807, acc 0.98
2016-09-07T00:18:05.676676: step 971, loss 0.0346418, acc 0.98
2016-09-07T00:18:06.359561: step 972, loss 0.111026, acc 0.96
2016-09-07T00:18:07.047889: step 973, loss 0.113442, acc 0.94
2016-09-07T00:18:07.730823: step 974, loss 0.0923828, acc 0.94
2016-09-07T00:18:08.425515: step 975, loss 0.0988128, acc 0.94
2016-09-07T00:18:09.125778: step 976, loss 0.0621167, acc 0.98
2016-09-07T00:18:09.813711: step 977, loss 0.152357, acc 0.94
2016-09-07T00:18:10.503589: step 978, loss 0.0625823, acc 0.96
2016-09-07T00:18:11.209734: step 979, loss 0.0402899, acc 1
2016-09-07T00:18:11.877781: step 980, loss 0.0346533, acc 0.98
2016-09-07T00:18:12.607555: step 981, loss 0.194539, acc 0.92
2016-09-07T00:18:13.302532: step 982, loss 0.124587, acc 0.92
2016-09-07T00:18:13.985691: step 983, loss 0.027326, acc 1
2016-09-07T00:18:14.702964: step 984, loss 0.0185431, acc 1
2016-09-07T00:18:15.385381: step 985, loss 0.0561284, acc 1
2016-09-07T00:18:16.080647: step 986, loss 0.0306671, acc 1
2016-09-07T00:18:16.760478: step 987, loss 0.0358023, acc 1
2016-09-07T00:18:17.452598: step 988, loss 0.0659455, acc 0.96
2016-09-07T00:18:18.150075: step 989, loss 0.0305972, acc 1
2016-09-07T00:18:18.826562: step 990, loss 0.159905, acc 0.9
2016-09-07T00:18:19.516325: step 991, loss 0.0551419, acc 0.98
2016-09-07T00:18:20.181370: step 992, loss 0.151593, acc 0.9
2016-09-07T00:18:20.876592: step 993, loss 0.0412348, acc 0.98
2016-09-07T00:18:21.543174: step 994, loss 0.122526, acc 0.94
2016-09-07T00:18:22.246187: step 995, loss 0.137504, acc 0.92
2016-09-07T00:18:22.912801: step 996, loss 0.0382287, acc 1
2016-09-07T00:18:23.590624: step 997, loss 0.0579207, acc 0.96
2016-09-07T00:18:24.280282: step 998, loss 0.026635, acc 1
2016-09-07T00:18:24.953369: step 999, loss 0.072206, acc 0.98
2016-09-07T00:18:25.657252: step 1000, loss 0.0423687, acc 0.98

Evaluation:
2016-09-07T00:18:28.795176: step 1000, loss 0.923916, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-1000

2016-09-07T00:18:30.452533: step 1001, loss 0.117091, acc 0.96
2016-09-07T00:18:31.130479: step 1002, loss 0.0828543, acc 0.98
2016-09-07T00:18:31.845597: step 1003, loss 0.0837721, acc 0.96
2016-09-07T00:18:32.537500: step 1004, loss 0.057078, acc 0.96
2016-09-07T00:18:33.215282: step 1005, loss 0.20121, acc 0.88
2016-09-07T00:18:33.887299: step 1006, loss 0.144755, acc 0.96
2016-09-07T00:18:34.563905: step 1007, loss 0.0465563, acc 1
2016-09-07T00:18:35.272389: step 1008, loss 0.0613292, acc 0.98
2016-09-07T00:18:35.956272: step 1009, loss 0.19538, acc 0.96
2016-09-07T00:18:36.644124: step 1010, loss 0.125692, acc 0.94
2016-09-07T00:18:37.320346: step 1011, loss 0.0643356, acc 0.96
2016-09-07T00:18:37.996998: step 1012, loss 0.262839, acc 0.92
2016-09-07T00:18:38.688263: step 1013, loss 0.0687597, acc 0.96
2016-09-07T00:18:39.379607: step 1014, loss 0.122874, acc 0.96
2016-09-07T00:18:40.084412: step 1015, loss 0.05888, acc 0.98
2016-09-07T00:18:40.759127: step 1016, loss 0.113655, acc 0.92
2016-09-07T00:18:41.431758: step 1017, loss 0.259842, acc 0.9
2016-09-07T00:18:42.114132: step 1018, loss 0.0502844, acc 1
2016-09-07T00:18:42.803610: step 1019, loss 0.0644635, acc 0.96
2016-09-07T00:18:43.492329: step 1020, loss 0.0372049, acc 1
2016-09-07T00:18:44.184935: step 1021, loss 0.264131, acc 0.88
2016-09-07T00:18:44.879202: step 1022, loss 0.0746206, acc 0.98
2016-09-07T00:18:45.559420: step 1023, loss 0.194911, acc 0.92
2016-09-07T00:18:46.227452: step 1024, loss 0.126609, acc 0.96
2016-09-07T00:18:46.915773: step 1025, loss 0.0789605, acc 0.98
2016-09-07T00:18:47.597567: step 1026, loss 0.156536, acc 0.94
2016-09-07T00:18:48.315485: step 1027, loss 0.0538049, acc 0.98
2016-09-07T00:18:48.996255: step 1028, loss 0.184547, acc 0.9
2016-09-07T00:18:49.708463: step 1029, loss 0.109354, acc 0.98
2016-09-07T00:18:50.375814: step 1030, loss 0.0989282, acc 0.96
2016-09-07T00:18:51.075365: step 1031, loss 0.0561416, acc 0.96
2016-09-07T00:18:51.762221: step 1032, loss 0.237878, acc 0.94
2016-09-07T00:18:52.459547: step 1033, loss 0.064779, acc 0.96
2016-09-07T00:18:53.159476: step 1034, loss 0.133117, acc 0.94
2016-09-07T00:18:53.811451: step 1035, loss 0.0763972, acc 0.96
2016-09-07T00:18:54.502977: step 1036, loss 0.0892931, acc 0.94
2016-09-07T00:18:55.192029: step 1037, loss 0.166523, acc 0.94
2016-09-07T00:18:55.871947: step 1038, loss 0.0325667, acc 0.98
2016-09-07T00:18:56.543448: step 1039, loss 0.0840929, acc 0.94
2016-09-07T00:18:57.221996: step 1040, loss 0.0481771, acc 1
2016-09-07T00:18:57.893872: step 1041, loss 0.0915839, acc 0.94
2016-09-07T00:18:58.583392: step 1042, loss 0.0547274, acc 0.98
2016-09-07T00:18:59.282183: step 1043, loss 0.100258, acc 0.98
2016-09-07T00:18:59.953428: step 1044, loss 0.263981, acc 0.9
2016-09-07T00:19:00.676441: step 1045, loss 0.152936, acc 0.96
2016-09-07T00:19:01.369970: step 1046, loss 0.109794, acc 0.94
2016-09-07T00:19:02.077295: step 1047, loss 0.0686201, acc 0.96
2016-09-07T00:19:02.766550: step 1048, loss 0.0508946, acc 0.98
2016-09-07T00:19:03.429583: step 1049, loss 0.0630163, acc 0.98
2016-09-07T00:19:04.137842: step 1050, loss 0.133027, acc 0.96
2016-09-07T00:19:04.801841: step 1051, loss 0.0792546, acc 0.98
2016-09-07T00:19:05.508702: step 1052, loss 0.0387645, acc 1
2016-09-07T00:19:06.208706: step 1053, loss 0.190777, acc 0.96
2016-09-07T00:19:06.910127: step 1054, loss 0.174394, acc 0.9
2016-09-07T00:19:07.622403: step 1055, loss 0.0588505, acc 0.98
2016-09-07T00:19:08.294205: step 1056, loss 0.078459, acc 0.98
2016-09-07T00:19:09.007650: step 1057, loss 0.0828075, acc 0.96
2016-09-07T00:19:09.710142: step 1058, loss 0.435732, acc 0.84
2016-09-07T00:19:10.423578: step 1059, loss 0.0514664, acc 0.98
2016-09-07T00:19:11.116887: step 1060, loss 0.0196395, acc 1
2016-09-07T00:19:11.795394: step 1061, loss 0.12012, acc 0.94
2016-09-07T00:19:12.490867: step 1062, loss 0.0205578, acc 1
2016-09-07T00:19:13.163393: step 1063, loss 0.0930063, acc 0.94
2016-09-07T00:19:13.859550: step 1064, loss 0.237251, acc 0.94
2016-09-07T00:19:14.545797: step 1065, loss 0.0389497, acc 0.98
2016-09-07T00:19:15.245346: step 1066, loss 0.0877808, acc 0.94
2016-09-07T00:19:15.928478: step 1067, loss 0.0571875, acc 0.98
2016-09-07T00:19:16.641153: step 1068, loss 0.0934421, acc 0.94
2016-09-07T00:19:17.363952: step 1069, loss 0.226247, acc 0.94
2016-09-07T00:19:18.034524: step 1070, loss 0.0571641, acc 0.96
2016-09-07T00:19:18.707609: step 1071, loss 0.0331589, acc 1
2016-09-07T00:19:19.406327: step 1072, loss 0.119109, acc 0.96
2016-09-07T00:19:20.105813: step 1073, loss 0.0692359, acc 0.98
2016-09-07T00:19:20.826649: step 1074, loss 0.16597, acc 0.92
2016-09-07T00:19:21.486865: step 1075, loss 0.0896079, acc 0.96
2016-09-07T00:19:22.193429: step 1076, loss 0.0528873, acc 0.98
2016-09-07T00:19:22.898175: step 1077, loss 0.0922737, acc 0.96
2016-09-07T00:19:23.578972: step 1078, loss 0.101344, acc 0.96
2016-09-07T00:19:24.255951: step 1079, loss 0.196996, acc 0.86
2016-09-07T00:19:24.938522: step 1080, loss 0.065401, acc 0.96
2016-09-07T00:19:25.619539: step 1081, loss 0.124395, acc 0.96
2016-09-07T00:19:26.284809: step 1082, loss 0.250077, acc 0.92
2016-09-07T00:19:26.989908: step 1083, loss 0.0633749, acc 0.98
2016-09-07T00:19:27.704873: step 1084, loss 0.0990906, acc 0.96
2016-09-07T00:19:28.394002: step 1085, loss 0.115333, acc 0.98
2016-09-07T00:19:29.068596: step 1086, loss 0.115314, acc 0.92
2016-09-07T00:19:29.763289: step 1087, loss 0.0697974, acc 0.96
2016-09-07T00:19:30.446490: step 1088, loss 0.226629, acc 0.84
2016-09-07T00:19:31.136214: step 1089, loss 0.0326611, acc 1
2016-09-07T00:19:31.847346: step 1090, loss 0.0958031, acc 0.94
2016-09-07T00:19:32.519090: step 1091, loss 0.220227, acc 0.92
2016-09-07T00:19:33.222494: step 1092, loss 0.0466958, acc 0.98
2016-09-07T00:19:33.905406: step 1093, loss 0.207658, acc 0.94
2016-09-07T00:19:34.579874: step 1094, loss 0.130254, acc 0.9
2016-09-07T00:19:35.296327: step 1095, loss 0.0191221, acc 1
2016-09-07T00:19:35.954362: step 1096, loss 0.187773, acc 0.96
2016-09-07T00:19:36.660552: step 1097, loss 0.0412568, acc 1
2016-09-07T00:19:37.383432: step 1098, loss 0.260639, acc 0.9
2016-09-07T00:19:38.083770: step 1099, loss 0.0591297, acc 0.96
2016-09-07T00:19:38.771454: step 1100, loss 0.120904, acc 0.96

Evaluation:
2016-09-07T00:19:41.913926: step 1100, loss 0.883334, acc 0.768293

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-1100

2016-09-07T00:19:43.660971: step 1101, loss 0.106938, acc 0.94
2016-09-07T00:19:44.340424: step 1102, loss 0.0631411, acc 0.96
2016-09-07T00:19:45.038116: step 1103, loss 0.0582828, acc 0.98
2016-09-07T00:19:45.719107: step 1104, loss 0.101245, acc 0.92
2016-09-07T00:19:46.413288: step 1105, loss 0.02594, acc 1
2016-09-07T00:19:47.094688: step 1106, loss 0.131925, acc 0.92
2016-09-07T00:19:47.802268: step 1107, loss 0.10982, acc 0.96
2016-09-07T00:19:48.510553: step 1108, loss 0.107729, acc 0.94
2016-09-07T00:19:49.171898: step 1109, loss 0.0943146, acc 0.96
2016-09-07T00:19:49.883370: step 1110, loss 0.071164, acc 0.96
2016-09-07T00:19:50.606994: step 1111, loss 0.150293, acc 0.94
2016-09-07T00:19:51.294892: step 1112, loss 0.0853882, acc 0.98
2016-09-07T00:19:51.996202: step 1113, loss 0.047053, acc 0.98
2016-09-07T00:19:52.687151: step 1114, loss 0.144947, acc 0.92
2016-09-07T00:19:53.390237: step 1115, loss 0.19431, acc 0.92
2016-09-07T00:19:54.077834: step 1116, loss 0.0481845, acc 0.98
2016-09-07T00:19:54.755757: step 1117, loss 0.129273, acc 0.94
2016-09-07T00:19:55.430455: step 1118, loss 0.119827, acc 0.92
2016-09-07T00:19:56.120302: step 1119, loss 0.0863569, acc 0.94
2016-09-07T00:19:56.809371: step 1120, loss 0.155703, acc 0.96
2016-09-07T00:19:57.495500: step 1121, loss 0.091101, acc 0.96
2016-09-07T00:19:58.194496: step 1122, loss 0.0390894, acc 0.98
2016-09-07T00:19:58.864555: step 1123, loss 0.0941465, acc 0.96
2016-09-07T00:19:59.542166: step 1124, loss 0.0532371, acc 1
2016-09-07T00:20:00.264392: step 1125, loss 0.223782, acc 0.88
2016-09-07T00:20:00.935702: step 1126, loss 0.0720691, acc 0.98
2016-09-07T00:20:01.636669: step 1127, loss 0.20857, acc 0.9
2016-09-07T00:20:02.335603: step 1128, loss 0.164312, acc 0.96
2016-09-07T00:20:03.043008: step 1129, loss 0.209628, acc 0.94
2016-09-07T00:20:03.720979: step 1130, loss 0.114217, acc 0.92
2016-09-07T00:20:04.402625: step 1131, loss 0.0639562, acc 0.98
2016-09-07T00:20:05.082724: step 1132, loss 0.10084, acc 0.96
2016-09-07T00:20:05.775645: step 1133, loss 0.0507653, acc 1
2016-09-07T00:20:06.463659: step 1134, loss 0.0813726, acc 0.96
2016-09-07T00:20:07.114472: step 1135, loss 0.156354, acc 0.94
2016-09-07T00:20:07.817543: step 1136, loss 0.158106, acc 0.88
2016-09-07T00:20:08.482262: step 1137, loss 0.0455506, acc 0.98
2016-09-07T00:20:09.132900: step 1138, loss 0.180971, acc 0.94
2016-09-07T00:20:09.809280: step 1139, loss 0.15262, acc 0.92
2016-09-07T00:20:10.516118: step 1140, loss 0.0805454, acc 0.96
2016-09-07T00:20:11.210391: step 1141, loss 0.104387, acc 0.92
2016-09-07T00:20:11.944987: step 1142, loss 0.0413851, acc 0.98
2016-09-07T00:20:12.668994: step 1143, loss 0.0768358, acc 0.92
2016-09-07T00:20:13.344722: step 1144, loss 0.161593, acc 0.92
2016-09-07T00:20:14.017740: step 1145, loss 0.148059, acc 0.94
2016-09-07T00:20:14.696802: step 1146, loss 0.067629, acc 0.96
2016-09-07T00:20:15.376778: step 1147, loss 0.0987727, acc 0.96
2016-09-07T00:20:16.089638: step 1148, loss 0.19031, acc 0.9
2016-09-07T00:20:16.765954: step 1149, loss 0.272281, acc 0.88
2016-09-07T00:20:17.471222: step 1150, loss 0.0952968, acc 0.96
2016-09-07T00:20:18.137688: step 1151, loss 0.0657658, acc 0.96
2016-09-07T00:20:18.749490: step 1152, loss 0.0857644, acc 0.977273
2016-09-07T00:20:19.459888: step 1153, loss 0.0539066, acc 0.98
2016-09-07T00:20:20.164124: step 1154, loss 0.040969, acc 0.98
2016-09-07T00:20:20.865072: step 1155, loss 0.0927839, acc 0.96
2016-09-07T00:20:21.543614: step 1156, loss 0.0447516, acc 0.96
2016-09-07T00:20:22.250812: step 1157, loss 0.0416717, acc 0.98
2016-09-07T00:20:22.924964: step 1158, loss 0.0643621, acc 0.96
2016-09-07T00:20:23.615661: step 1159, loss 0.194642, acc 0.88
2016-09-07T00:20:24.306540: step 1160, loss 0.0389623, acc 0.98
2016-09-07T00:20:24.996143: step 1161, loss 0.032169, acc 1
2016-09-07T00:20:25.679314: step 1162, loss 0.0647843, acc 0.96
2016-09-07T00:20:26.379656: step 1163, loss 0.040563, acc 0.98
2016-09-07T00:20:27.090121: step 1164, loss 0.0506685, acc 0.98
2016-09-07T00:20:27.780177: step 1165, loss 0.0724035, acc 1
2016-09-07T00:20:28.451883: step 1166, loss 0.0542781, acc 0.96
2016-09-07T00:20:29.124804: step 1167, loss 0.0410684, acc 0.98
2016-09-07T00:20:29.806321: step 1168, loss 0.0307541, acc 1
2016-09-07T00:20:30.491098: step 1169, loss 0.0547923, acc 0.98
2016-09-07T00:20:31.156971: step 1170, loss 0.0347082, acc 1
2016-09-07T00:20:31.846965: step 1171, loss 0.0542046, acc 0.98
2016-09-07T00:20:32.508670: step 1172, loss 0.0443746, acc 0.98
2016-09-07T00:20:33.188893: step 1173, loss 0.100838, acc 0.96
2016-09-07T00:20:33.883098: step 1174, loss 0.0748859, acc 0.96
2016-09-07T00:20:34.574715: step 1175, loss 0.0888037, acc 0.96
2016-09-07T00:20:35.263607: step 1176, loss 0.0148444, acc 1
2016-09-07T00:20:35.951473: step 1177, loss 0.130252, acc 0.98
2016-09-07T00:20:36.646127: step 1178, loss 0.0227451, acc 1
2016-09-07T00:20:37.286954: step 1179, loss 0.0862505, acc 0.94
2016-09-07T00:20:37.984917: step 1180, loss 0.0179759, acc 1
2016-09-07T00:20:38.663189: step 1181, loss 0.0794156, acc 0.94
2016-09-07T00:20:39.349268: step 1182, loss 0.105285, acc 0.94
2016-09-07T00:20:40.019153: step 1183, loss 0.010654, acc 1
2016-09-07T00:20:40.693820: step 1184, loss 0.074807, acc 0.96
2016-09-07T00:20:41.382411: step 1185, loss 0.0641893, acc 0.98
2016-09-07T00:20:42.050389: step 1186, loss 0.0803929, acc 0.96
2016-09-07T00:20:42.746117: step 1187, loss 0.0188963, acc 1
2016-09-07T00:20:43.425788: step 1188, loss 0.0605188, acc 0.98
2016-09-07T00:20:44.141613: step 1189, loss 0.0225472, acc 1
2016-09-07T00:20:44.819298: step 1190, loss 0.398437, acc 0.9
2016-09-07T00:20:45.496371: step 1191, loss 0.137323, acc 0.96
2016-09-07T00:20:46.205560: step 1192, loss 0.00268656, acc 1
2016-09-07T00:20:46.880642: step 1193, loss 0.0662441, acc 0.98
2016-09-07T00:20:47.589375: step 1194, loss 0.195643, acc 0.94
2016-09-07T00:20:48.287626: step 1195, loss 0.0906874, acc 0.96
2016-09-07T00:20:48.977055: step 1196, loss 0.0430283, acc 0.98
2016-09-07T00:20:49.675544: step 1197, loss 0.0488218, acc 1
2016-09-07T00:20:50.371211: step 1198, loss 0.0834185, acc 0.94
2016-09-07T00:20:51.088804: step 1199, loss 0.0771796, acc 0.94
2016-09-07T00:20:51.775087: step 1200, loss 0.118859, acc 0.92

Evaluation:
2016-09-07T00:20:54.919316: step 1200, loss 0.869485, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-1200

2016-09-07T00:20:56.597493: step 1201, loss 0.0373537, acc 0.98
2016-09-07T00:20:57.310323: step 1202, loss 0.08669, acc 0.98
2016-09-07T00:20:57.991790: step 1203, loss 0.0989896, acc 0.94
2016-09-07T00:20:58.676741: step 1204, loss 0.0348624, acc 1
2016-09-07T00:20:59.347453: step 1205, loss 0.0645922, acc 1
2016-09-07T00:21:00.030324: step 1206, loss 0.0568591, acc 0.96
2016-09-07T00:21:00.754232: step 1207, loss 0.0489994, acc 0.98
2016-09-07T00:21:01.423069: step 1208, loss 0.0988249, acc 0.98
2016-09-07T00:21:02.140245: step 1209, loss 0.0714739, acc 1
2016-09-07T00:21:02.817678: step 1210, loss 0.13952, acc 0.98
2016-09-07T00:21:03.517374: step 1211, loss 0.122079, acc 0.96
2016-09-07T00:21:04.204857: step 1212, loss 0.161806, acc 0.94
2016-09-07T00:21:04.869586: step 1213, loss 0.0782457, acc 0.96
2016-09-07T00:21:05.579241: step 1214, loss 0.0759181, acc 0.96
2016-09-07T00:21:06.237800: step 1215, loss 0.0614477, acc 0.98
2016-09-07T00:21:06.955208: step 1216, loss 0.0418408, acc 0.98
2016-09-07T00:21:07.637430: step 1217, loss 0.115421, acc 0.94
2016-09-07T00:21:08.330995: step 1218, loss 0.028915, acc 1
2016-09-07T00:21:09.013776: step 1219, loss 0.018441, acc 1
2016-09-07T00:21:09.665687: step 1220, loss 0.055722, acc 0.98
2016-09-07T00:21:10.355955: step 1221, loss 0.0872108, acc 0.98
2016-09-07T00:21:11.022284: step 1222, loss 0.187609, acc 0.9
2016-09-07T00:21:11.696426: step 1223, loss 0.0222704, acc 1
2016-09-07T00:21:12.382275: step 1224, loss 0.0553626, acc 0.98
2016-09-07T00:21:13.066018: step 1225, loss 0.0774736, acc 0.98
2016-09-07T00:21:13.748109: step 1226, loss 0.0232504, acc 1
2016-09-07T00:21:14.433744: step 1227, loss 0.0741453, acc 0.94
2016-09-07T00:21:15.141459: step 1228, loss 0.216258, acc 0.92
2016-09-07T00:21:15.807394: step 1229, loss 0.0769735, acc 0.96
2016-09-07T00:21:16.496610: step 1230, loss 0.0353164, acc 1
2016-09-07T00:21:17.210513: step 1231, loss 0.050552, acc 0.98
2016-09-07T00:21:17.918742: step 1232, loss 0.0545641, acc 0.98
2016-09-07T00:21:18.601995: step 1233, loss 0.0652814, acc 0.98
2016-09-07T00:21:19.283542: step 1234, loss 0.0961059, acc 0.96
2016-09-07T00:21:20.006810: step 1235, loss 0.0702139, acc 0.96
2016-09-07T00:21:20.694140: step 1236, loss 0.086987, acc 0.98
2016-09-07T00:21:21.393214: step 1237, loss 0.0938105, acc 0.96
2016-09-07T00:21:22.074232: step 1238, loss 0.0391171, acc 1
2016-09-07T00:21:22.773749: step 1239, loss 0.0708466, acc 0.96
2016-09-07T00:21:23.462215: step 1240, loss 0.0273536, acc 1
2016-09-07T00:21:24.130189: step 1241, loss 0.0798002, acc 0.98
2016-09-07T00:21:24.811522: step 1242, loss 0.0769988, acc 0.96
2016-09-07T00:21:25.479497: step 1243, loss 0.0630908, acc 0.96
2016-09-07T00:21:26.160076: step 1244, loss 0.0167, acc 1
2016-09-07T00:21:26.834744: step 1245, loss 0.0599389, acc 0.98
2016-09-07T00:21:27.528074: step 1246, loss 0.113492, acc 0.92
2016-09-07T00:21:28.223693: step 1247, loss 0.167245, acc 0.94
2016-09-07T00:21:28.884305: step 1248, loss 0.0834352, acc 0.94
2016-09-07T00:21:29.583840: step 1249, loss 0.0172998, acc 1
2016-09-07T00:21:30.252274: step 1250, loss 0.06015, acc 0.98
2016-09-07T00:21:30.921718: step 1251, loss 0.0175496, acc 1
2016-09-07T00:21:31.598286: step 1252, loss 0.0543789, acc 0.96
2016-09-07T00:21:32.291815: step 1253, loss 0.032953, acc 0.98
2016-09-07T00:21:32.981815: step 1254, loss 0.0687118, acc 0.98
2016-09-07T00:21:33.678169: step 1255, loss 0.0828542, acc 0.96
2016-09-07T00:21:34.393568: step 1256, loss 0.0534189, acc 0.98
2016-09-07T00:21:35.069676: step 1257, loss 0.233512, acc 0.96
2016-09-07T00:21:35.739654: step 1258, loss 0.139036, acc 0.94
2016-09-07T00:21:36.416678: step 1259, loss 0.0480495, acc 0.98
2016-09-07T00:21:37.099968: step 1260, loss 0.0708181, acc 0.96
2016-09-07T00:21:37.790953: step 1261, loss 0.0784637, acc 0.94
2016-09-07T00:21:38.484091: step 1262, loss 0.054461, acc 0.98
2016-09-07T00:21:39.196281: step 1263, loss 0.252791, acc 0.88
2016-09-07T00:21:39.879917: step 1264, loss 0.148524, acc 0.94
2016-09-07T00:21:40.571764: step 1265, loss 0.150087, acc 0.94
2016-09-07T00:21:41.258017: step 1266, loss 0.123652, acc 0.96
2016-09-07T00:21:41.955455: step 1267, loss 0.102657, acc 0.92
2016-09-07T00:21:42.663685: step 1268, loss 0.0942342, acc 0.94
2016-09-07T00:21:43.309081: step 1269, loss 0.0762829, acc 1
2016-09-07T00:21:44.021155: step 1270, loss 0.0505652, acc 1
2016-09-07T00:21:44.701014: step 1271, loss 0.274225, acc 0.9
2016-09-07T00:21:45.376825: step 1272, loss 0.103908, acc 0.98
2016-09-07T00:21:46.056695: step 1273, loss 0.11975, acc 0.94
2016-09-07T00:21:46.735998: step 1274, loss 0.0496755, acc 0.98
2016-09-07T00:21:47.430751: step 1275, loss 0.107724, acc 0.94
2016-09-07T00:21:48.130302: step 1276, loss 0.102819, acc 0.94
2016-09-07T00:21:48.812432: step 1277, loss 0.109002, acc 0.94
2016-09-07T00:21:49.510016: step 1278, loss 0.113027, acc 0.94
2016-09-07T00:21:50.240309: step 1279, loss 0.342176, acc 0.94
2016-09-07T00:21:50.918958: step 1280, loss 0.0858359, acc 0.96
2016-09-07T00:21:51.639890: step 1281, loss 0.0701634, acc 0.96
2016-09-07T00:21:52.349077: step 1282, loss 0.0540744, acc 0.98
2016-09-07T00:21:53.019741: step 1283, loss 0.0852776, acc 0.98
2016-09-07T00:21:53.711345: step 1284, loss 0.0524995, acc 0.96
2016-09-07T00:21:54.388655: step 1285, loss 0.0353681, acc 1
2016-09-07T00:21:55.087022: step 1286, loss 0.118266, acc 0.96
2016-09-07T00:21:55.791921: step 1287, loss 0.129664, acc 0.98
2016-09-07T00:21:56.473974: step 1288, loss 0.0761928, acc 0.96
2016-09-07T00:21:57.148433: step 1289, loss 0.0634061, acc 0.96
2016-09-07T00:21:57.795174: step 1290, loss 0.0777563, acc 0.98
2016-09-07T00:21:58.486045: step 1291, loss 0.0725351, acc 0.96
2016-09-07T00:21:59.155748: step 1292, loss 0.0691309, acc 0.96
2016-09-07T00:21:59.838623: step 1293, loss 0.0827777, acc 0.94
2016-09-07T00:22:00.556408: step 1294, loss 0.123835, acc 0.98
2016-09-07T00:22:01.246863: step 1295, loss 0.122785, acc 0.96
2016-09-07T00:22:01.946258: step 1296, loss 0.0421196, acc 0.98
2016-09-07T00:22:02.602840: step 1297, loss 0.132549, acc 0.96
2016-09-07T00:22:03.323174: step 1298, loss 0.110004, acc 0.96
2016-09-07T00:22:04.011580: step 1299, loss 0.0305666, acc 0.98
2016-09-07T00:22:04.684589: step 1300, loss 0.102423, acc 0.92

Evaluation:
2016-09-07T00:22:07.831505: step 1300, loss 1.07384, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-1300

2016-09-07T00:22:09.504220: step 1301, loss 0.136803, acc 0.94
2016-09-07T00:22:10.189487: step 1302, loss 0.0524692, acc 0.98
2016-09-07T00:22:10.857510: step 1303, loss 0.0909595, acc 0.94
2016-09-07T00:22:11.541565: step 1304, loss 0.0389378, acc 0.98
2016-09-07T00:22:12.201374: step 1305, loss 0.0305775, acc 1
2016-09-07T00:22:12.891570: step 1306, loss 0.185299, acc 0.88
2016-09-07T00:22:13.559448: step 1307, loss 0.175297, acc 0.9
2016-09-07T00:22:14.235198: step 1308, loss 0.0763057, acc 0.98
2016-09-07T00:22:14.921504: step 1309, loss 0.0621722, acc 0.98
2016-09-07T00:22:15.607134: step 1310, loss 0.0756704, acc 0.96
2016-09-07T00:22:16.302681: step 1311, loss 0.0349079, acc 1
2016-09-07T00:22:16.996494: step 1312, loss 0.0474332, acc 0.98
2016-09-07T00:22:17.698912: step 1313, loss 0.0390854, acc 0.96
2016-09-07T00:22:18.380601: step 1314, loss 0.101025, acc 0.94
2016-09-07T00:22:19.049181: step 1315, loss 0.16935, acc 0.96
2016-09-07T00:22:19.732869: step 1316, loss 0.0549042, acc 0.96
2016-09-07T00:22:20.419564: step 1317, loss 0.119561, acc 0.94
2016-09-07T00:22:21.117542: step 1318, loss 0.0588747, acc 0.98
2016-09-07T00:22:21.802688: step 1319, loss 0.099269, acc 0.96
2016-09-07T00:22:22.519219: step 1320, loss 0.149744, acc 0.94
2016-09-07T00:22:23.206732: step 1321, loss 0.056571, acc 0.98
2016-09-07T00:22:23.887095: step 1322, loss 0.0316933, acc 0.98
2016-09-07T00:22:24.587663: step 1323, loss 0.0771608, acc 0.96
2016-09-07T00:22:25.268864: step 1324, loss 0.0274353, acc 1
2016-09-07T00:22:25.946964: step 1325, loss 0.0404339, acc 1
2016-09-07T00:22:26.620560: step 1326, loss 0.0589016, acc 1
2016-09-07T00:22:27.324409: step 1327, loss 0.0926705, acc 0.96
2016-09-07T00:22:28.004896: step 1328, loss 0.120559, acc 0.96
2016-09-07T00:22:28.708860: step 1329, loss 0.0863243, acc 0.94
2016-09-07T00:22:29.373193: step 1330, loss 0.0796219, acc 0.92
2016-09-07T00:22:30.045607: step 1331, loss 0.162027, acc 0.94
2016-09-07T00:22:30.733924: step 1332, loss 0.0834569, acc 0.94
2016-09-07T00:22:31.418260: step 1333, loss 0.0267415, acc 0.98
2016-09-07T00:22:32.107615: step 1334, loss 0.157581, acc 0.96
2016-09-07T00:22:32.796114: step 1335, loss 0.0952706, acc 0.96
2016-09-07T00:22:33.490994: step 1336, loss 0.138065, acc 0.92
2016-09-07T00:22:34.188071: step 1337, loss 0.14598, acc 0.94
2016-09-07T00:22:34.873354: step 1338, loss 0.0564102, acc 0.96
2016-09-07T00:22:35.565436: step 1339, loss 0.0891684, acc 0.96
2016-09-07T00:22:36.251670: step 1340, loss 0.0479485, acc 0.96
2016-09-07T00:22:36.955621: step 1341, loss 0.0707893, acc 0.98
2016-09-07T00:22:37.635796: step 1342, loss 0.0842208, acc 0.96
2016-09-07T00:22:38.349554: step 1343, loss 0.0640566, acc 0.98
2016-09-07T00:22:38.990217: step 1344, loss 0.133879, acc 0.977273
2016-09-07T00:22:39.704993: step 1345, loss 0.102035, acc 0.94
2016-09-07T00:22:40.389428: step 1346, loss 0.0850011, acc 0.98
2016-09-07T00:22:41.057091: step 1347, loss 0.0689678, acc 0.98
2016-09-07T00:22:41.757874: step 1348, loss 0.0861572, acc 0.96
2016-09-07T00:22:42.436092: step 1349, loss 0.0279654, acc 1
2016-09-07T00:22:43.116694: step 1350, loss 0.0827304, acc 0.98
2016-09-07T00:22:43.811074: step 1351, loss 0.0293758, acc 1
2016-09-07T00:22:44.520200: step 1352, loss 0.177474, acc 0.96
2016-09-07T00:22:45.222320: step 1353, loss 0.0324356, acc 1
2016-09-07T00:22:45.876724: step 1354, loss 0.0223356, acc 1
2016-09-07T00:22:46.565986: step 1355, loss 0.0171359, acc 1
2016-09-07T00:22:47.244633: step 1356, loss 0.0618642, acc 0.98
2016-09-07T00:22:47.927166: step 1357, loss 0.105583, acc 0.92
2016-09-07T00:22:48.614787: step 1358, loss 0.127784, acc 0.88
2016-09-07T00:22:49.295947: step 1359, loss 0.0173596, acc 1
2016-09-07T00:22:49.978729: step 1360, loss 0.0482299, acc 0.96
2016-09-07T00:22:50.643560: step 1361, loss 0.0409589, acc 1
2016-09-07T00:22:51.362846: step 1362, loss 0.0758851, acc 0.94
2016-09-07T00:22:52.047805: step 1363, loss 0.123933, acc 0.94
2016-09-07T00:22:52.718125: step 1364, loss 0.0386651, acc 0.98
2016-09-07T00:22:53.385823: step 1365, loss 0.112151, acc 0.94
2016-09-07T00:22:54.079191: step 1366, loss 0.040461, acc 0.96
2016-09-07T00:22:54.763889: step 1367, loss 0.0630531, acc 0.96
2016-09-07T00:22:55.441044: step 1368, loss 0.0723612, acc 0.96
2016-09-07T00:22:56.165861: step 1369, loss 0.215784, acc 0.92
2016-09-07T00:22:56.833652: step 1370, loss 0.0156967, acc 1
2016-09-07T00:22:57.517359: step 1371, loss 0.0903944, acc 0.94
2016-09-07T00:22:58.211804: step 1372, loss 0.029678, acc 0.98
2016-09-07T00:22:58.894998: step 1373, loss 0.0505023, acc 0.98
2016-09-07T00:22:59.597272: step 1374, loss 0.0684537, acc 0.98
2016-09-07T00:23:00.261900: step 1375, loss 0.0482168, acc 0.96
2016-09-07T00:23:00.935423: step 1376, loss 0.18275, acc 0.94
2016-09-07T00:23:01.598975: step 1377, loss 0.0216789, acc 1
2016-09-07T00:23:02.311897: step 1378, loss 0.0346391, acc 1
2016-09-07T00:23:03.002359: step 1379, loss 0.0488388, acc 0.96
2016-09-07T00:23:03.685483: step 1380, loss 0.126806, acc 0.96
2016-09-07T00:23:04.357034: step 1381, loss 0.102323, acc 0.96
2016-09-07T00:23:05.051750: step 1382, loss 0.0775999, acc 0.98
2016-09-07T00:23:05.769992: step 1383, loss 0.0431089, acc 0.98
2016-09-07T00:23:06.460330: step 1384, loss 0.0162893, acc 1
2016-09-07T00:23:07.133441: step 1385, loss 0.0317383, acc 0.98
2016-09-07T00:23:07.819093: step 1386, loss 0.162221, acc 0.88
2016-09-07T00:23:08.527015: step 1387, loss 0.0476627, acc 0.98
2016-09-07T00:23:09.226584: step 1388, loss 0.0701202, acc 0.96
2016-09-07T00:23:09.913234: step 1389, loss 0.0622243, acc 0.96
2016-09-07T00:23:10.608333: step 1390, loss 0.0429223, acc 1
2016-09-07T00:23:11.291587: step 1391, loss 0.0734975, acc 0.98
2016-09-07T00:23:11.974800: step 1392, loss 0.0575674, acc 0.98
2016-09-07T00:23:12.655084: step 1393, loss 0.100582, acc 0.98
2016-09-07T00:23:13.322384: step 1394, loss 0.0978483, acc 0.96
2016-09-07T00:23:14.009611: step 1395, loss 0.143364, acc 0.96
2016-09-07T00:23:14.687666: step 1396, loss 0.010614, acc 1
2016-09-07T00:23:15.409703: step 1397, loss 0.0159596, acc 1
2016-09-07T00:23:16.089080: step 1398, loss 0.0831446, acc 0.96
2016-09-07T00:23:16.777767: step 1399, loss 0.181199, acc 0.92
2016-09-07T00:23:17.455395: step 1400, loss 0.109736, acc 0.96

Evaluation:
2016-09-07T00:23:20.600925: step 1400, loss 1.2143, acc 0.741088

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-1400

2016-09-07T00:23:22.263465: step 1401, loss 0.198582, acc 0.88
2016-09-07T00:23:22.960183: step 1402, loss 0.0407974, acc 0.98
2016-09-07T00:23:23.666832: step 1403, loss 0.0596097, acc 0.94
2016-09-07T00:23:24.334850: step 1404, loss 0.103651, acc 0.96
2016-09-07T00:23:25.072186: step 1405, loss 0.127415, acc 0.94
2016-09-07T00:23:25.751110: step 1406, loss 0.123775, acc 0.96
2016-09-07T00:23:26.434485: step 1407, loss 0.0620568, acc 0.96
2016-09-07T00:23:27.120634: step 1408, loss 0.0339326, acc 1
2016-09-07T00:23:27.794582: step 1409, loss 0.0586716, acc 1
2016-09-07T00:23:28.487970: step 1410, loss 0.126377, acc 0.94
2016-09-07T00:23:29.153051: step 1411, loss 0.0816378, acc 0.94
2016-09-07T00:23:29.840799: step 1412, loss 0.0793492, acc 0.96
2016-09-07T00:23:30.504972: step 1413, loss 0.109241, acc 0.98
2016-09-07T00:23:31.164902: step 1414, loss 0.0988838, acc 0.94
2016-09-07T00:23:31.886096: step 1415, loss 0.115298, acc 0.96
2016-09-07T00:23:32.591486: step 1416, loss 0.116551, acc 0.92
2016-09-07T00:23:33.290961: step 1417, loss 0.0168511, acc 1
2016-09-07T00:23:33.978543: step 1418, loss 0.0522098, acc 0.98
2016-09-07T00:23:34.686004: step 1419, loss 0.0729354, acc 0.98
2016-09-07T00:23:35.393343: step 1420, loss 0.0356622, acc 1
2016-09-07T00:23:36.089974: step 1421, loss 0.103644, acc 0.98
2016-09-07T00:23:36.768405: step 1422, loss 0.0247061, acc 1
2016-09-07T00:23:37.455837: step 1423, loss 0.0750611, acc 0.96
2016-09-07T00:23:38.149447: step 1424, loss 0.0464508, acc 1
2016-09-07T00:23:38.811833: step 1425, loss 0.0481366, acc 0.98
2016-09-07T00:23:39.522218: step 1426, loss 0.127917, acc 0.92
2016-09-07T00:23:40.205267: step 1427, loss 0.0332418, acc 0.98
2016-09-07T00:23:40.888995: step 1428, loss 0.00896957, acc 1
2016-09-07T00:23:41.595172: step 1429, loss 0.0552477, acc 0.98
2016-09-07T00:23:42.291341: step 1430, loss 0.0117506, acc 1
2016-09-07T00:23:42.984758: step 1431, loss 0.0226173, acc 1
2016-09-07T00:23:43.665014: step 1432, loss 0.0315168, acc 0.98
2016-09-07T00:23:44.360487: step 1433, loss 0.0447345, acc 0.98
2016-09-07T00:23:45.053907: step 1434, loss 0.0775671, acc 0.96
2016-09-07T00:23:45.760298: step 1435, loss 0.0563578, acc 0.98
2016-09-07T00:23:46.450592: step 1436, loss 0.0920084, acc 0.98
2016-09-07T00:23:47.126439: step 1437, loss 0.0542646, acc 0.98
2016-09-07T00:23:47.822622: step 1438, loss 0.0260062, acc 1
2016-09-07T00:23:48.496770: step 1439, loss 0.0686373, acc 0.94
2016-09-07T00:23:49.188197: step 1440, loss 0.00455429, acc 1
2016-09-07T00:23:49.863299: step 1441, loss 0.0552887, acc 0.96
2016-09-07T00:23:50.557010: step 1442, loss 0.186626, acc 0.9
2016-09-07T00:23:51.234012: step 1443, loss 0.0867134, acc 0.92
2016-09-07T00:23:51.923886: step 1444, loss 0.0483074, acc 0.96
2016-09-07T00:23:52.625526: step 1445, loss 0.0257647, acc 1
2016-09-07T00:23:53.308390: step 1446, loss 0.133026, acc 0.96
2016-09-07T00:23:54.012252: step 1447, loss 0.0648408, acc 0.96
2016-09-07T00:23:54.706346: step 1448, loss 0.0882238, acc 0.94
2016-09-07T00:23:55.374586: step 1449, loss 0.0372335, acc 0.98
2016-09-07T00:23:56.047922: step 1450, loss 0.0501211, acc 0.96
2016-09-07T00:23:56.724261: step 1451, loss 0.0852404, acc 0.98
2016-09-07T00:23:57.431464: step 1452, loss 0.0217786, acc 0.98
2016-09-07T00:23:58.100222: step 1453, loss 0.0977702, acc 0.92
2016-09-07T00:23:58.811330: step 1454, loss 0.0918681, acc 0.96
2016-09-07T00:23:59.499082: step 1455, loss 0.0882103, acc 0.94
2016-09-07T00:24:00.187787: step 1456, loss 0.036644, acc 0.98
2016-09-07T00:24:00.908859: step 1457, loss 0.108917, acc 0.98
2016-09-07T00:24:01.613578: step 1458, loss 0.116083, acc 0.96
2016-09-07T00:24:02.312950: step 1459, loss 0.0365857, acc 1
2016-09-07T00:24:03.008317: step 1460, loss 0.04582, acc 0.98
2016-09-07T00:24:03.702544: step 1461, loss 0.0757186, acc 0.98
2016-09-07T00:24:04.395006: step 1462, loss 0.0237196, acc 1
2016-09-07T00:24:05.084461: step 1463, loss 0.109415, acc 0.98
2016-09-07T00:24:05.787787: step 1464, loss 0.0858902, acc 0.96
2016-09-07T00:24:06.460736: step 1465, loss 0.0940289, acc 0.94
2016-09-07T00:24:07.165521: step 1466, loss 0.0691058, acc 0.96
2016-09-07T00:24:07.863967: step 1467, loss 0.0253818, acc 1
2016-09-07T00:24:08.540858: step 1468, loss 0.512234, acc 0.88
2016-09-07T00:24:09.232143: step 1469, loss 0.0564819, acc 0.98
2016-09-07T00:24:09.952389: step 1470, loss 0.0909699, acc 0.92
2016-09-07T00:24:10.644316: step 1471, loss 0.0661568, acc 0.96
2016-09-07T00:24:11.302552: step 1472, loss 0.0307295, acc 1
2016-09-07T00:24:11.998308: step 1473, loss 0.0549509, acc 0.98
2016-09-07T00:24:12.690386: step 1474, loss 0.0748602, acc 0.98
2016-09-07T00:24:13.370365: step 1475, loss 0.117486, acc 0.94
2016-09-07T00:24:14.064643: step 1476, loss 0.136584, acc 0.96
2016-09-07T00:24:14.734509: step 1477, loss 0.0396416, acc 0.98
2016-09-07T00:24:15.413237: step 1478, loss 0.022557, acc 0.98
2016-09-07T00:24:16.091823: step 1479, loss 0.0706106, acc 0.96
2016-09-07T00:24:16.804680: step 1480, loss 0.0343214, acc 1
2016-09-07T00:24:17.484595: step 1481, loss 0.0709528, acc 0.98
2016-09-07T00:24:18.186206: step 1482, loss 0.0491205, acc 0.96
2016-09-07T00:24:18.875594: step 1483, loss 0.0449994, acc 0.96
2016-09-07T00:24:19.572407: step 1484, loss 0.0155751, acc 1
2016-09-07T00:24:20.274026: step 1485, loss 0.0358766, acc 0.98
2016-09-07T00:24:20.946050: step 1486, loss 0.0326447, acc 1
2016-09-07T00:24:21.656472: step 1487, loss 0.0809462, acc 0.98
2016-09-07T00:24:22.360943: step 1488, loss 0.0571064, acc 0.98
2016-09-07T00:24:23.055756: step 1489, loss 0.070842, acc 0.98
2016-09-07T00:24:23.734286: step 1490, loss 0.134042, acc 0.96
2016-09-07T00:24:24.416523: step 1491, loss 0.0159858, acc 1
2016-09-07T00:24:25.131488: step 1492, loss 0.165892, acc 0.9
2016-09-07T00:24:25.809072: step 1493, loss 0.0545116, acc 0.98
2016-09-07T00:24:26.502425: step 1494, loss 0.0565649, acc 0.98
2016-09-07T00:24:27.190708: step 1495, loss 0.0331732, acc 0.98
2016-09-07T00:24:27.872488: step 1496, loss 0.0408942, acc 0.98
2016-09-07T00:24:28.566965: step 1497, loss 0.0456388, acc 0.96
2016-09-07T00:24:29.240285: step 1498, loss 0.0654178, acc 0.98
2016-09-07T00:24:29.946385: step 1499, loss 0.23235, acc 0.92
2016-09-07T00:24:30.589989: step 1500, loss 0.0266428, acc 1

Evaluation:
2016-09-07T00:24:33.752056: step 1500, loss 1.27018, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-1500

2016-09-07T00:24:35.522868: step 1501, loss 0.0911925, acc 0.96
2016-09-07T00:24:36.203690: step 1502, loss 0.0908014, acc 0.96
2016-09-07T00:24:36.891957: step 1503, loss 0.0209499, acc 1
2016-09-07T00:24:37.574450: step 1504, loss 0.0382015, acc 1
2016-09-07T00:24:38.258745: step 1505, loss 0.0597513, acc 0.96
2016-09-07T00:24:38.917841: step 1506, loss 0.0306574, acc 1
2016-09-07T00:24:39.616407: step 1507, loss 0.103706, acc 0.98
2016-09-07T00:24:40.306742: step 1508, loss 0.0661797, acc 0.98
2016-09-07T00:24:40.993723: step 1509, loss 0.109195, acc 0.96
2016-09-07T00:24:41.671159: step 1510, loss 0.0657743, acc 0.98
2016-09-07T00:24:42.379950: step 1511, loss 0.0392712, acc 0.98
2016-09-07T00:24:43.064050: step 1512, loss 0.204592, acc 0.92
2016-09-07T00:24:43.734523: step 1513, loss 0.0379342, acc 1
2016-09-07T00:24:44.434336: step 1514, loss 0.0332973, acc 1
2016-09-07T00:24:45.109010: step 1515, loss 0.02205, acc 1
2016-09-07T00:24:45.789468: step 1516, loss 0.217819, acc 0.88
2016-09-07T00:24:46.485965: step 1517, loss 0.0296794, acc 1
2016-09-07T00:24:47.187005: step 1518, loss 0.0840376, acc 0.96
2016-09-07T00:24:47.895529: step 1519, loss 0.0583544, acc 0.98
2016-09-07T00:24:48.608042: step 1520, loss 0.0287615, acc 0.98
2016-09-07T00:24:49.319645: step 1521, loss 0.129706, acc 0.96
2016-09-07T00:24:50.007323: step 1522, loss 0.114033, acc 0.92
2016-09-07T00:24:50.680664: step 1523, loss 0.0668789, acc 0.96
2016-09-07T00:24:51.373920: step 1524, loss 0.116189, acc 0.94
2016-09-07T00:24:52.047823: step 1525, loss 0.0366569, acc 0.98
2016-09-07T00:24:52.762463: step 1526, loss 0.0372059, acc 1
2016-09-07T00:24:53.425727: step 1527, loss 0.0452901, acc 0.98
2016-09-07T00:24:54.134588: step 1528, loss 0.0448258, acc 0.98
2016-09-07T00:24:54.836562: step 1529, loss 0.106036, acc 0.94
2016-09-07T00:24:55.546355: step 1530, loss 0.0350077, acc 1
2016-09-07T00:24:56.245853: step 1531, loss 0.165121, acc 0.96
2016-09-07T00:24:56.926668: step 1532, loss 0.107782, acc 0.96
2016-09-07T00:24:57.646331: step 1533, loss 0.103195, acc 0.98
2016-09-07T00:24:58.327585: step 1534, loss 0.0419958, acc 0.96
2016-09-07T00:24:59.012849: step 1535, loss 0.108216, acc 0.96
2016-09-07T00:24:59.658422: step 1536, loss 0.102228, acc 0.977273
2016-09-07T00:25:00.377208: step 1537, loss 0.0409315, acc 0.98
2016-09-07T00:25:01.054943: step 1538, loss 0.0307226, acc 0.98
2016-09-07T00:25:01.734048: step 1539, loss 0.143282, acc 0.92
2016-09-07T00:25:02.416998: step 1540, loss 0.0362658, acc 0.98
2016-09-07T00:25:03.086387: step 1541, loss 0.0323658, acc 0.98
2016-09-07T00:25:03.790185: step 1542, loss 0.0553863, acc 0.96
2016-09-07T00:25:04.484327: step 1543, loss 0.0972871, acc 0.96
2016-09-07T00:25:05.170794: step 1544, loss 0.0483764, acc 0.98
2016-09-07T00:25:05.848201: step 1545, loss 0.100784, acc 0.96
2016-09-07T00:25:06.523366: step 1546, loss 0.0645159, acc 0.98
2016-09-07T00:25:07.223764: step 1547, loss 0.0583908, acc 0.98
2016-09-07T00:25:07.916963: step 1548, loss 0.139742, acc 0.96
2016-09-07T00:25:08.590440: step 1549, loss 0.0686447, acc 0.98
2016-09-07T00:25:09.266127: step 1550, loss 0.0326802, acc 0.98
2016-09-07T00:25:09.950972: step 1551, loss 0.076927, acc 0.96
2016-09-07T00:25:10.632909: step 1552, loss 0.104862, acc 0.94
2016-09-07T00:25:11.318410: step 1553, loss 0.109647, acc 0.94
2016-09-07T00:25:12.026114: step 1554, loss 0.0359787, acc 0.98
2016-09-07T00:25:12.679389: step 1555, loss 0.077584, acc 0.94
2016-09-07T00:25:13.369384: step 1556, loss 0.0233028, acc 0.98
2016-09-07T00:25:14.053067: step 1557, loss 0.0903792, acc 0.96
2016-09-07T00:25:14.741525: step 1558, loss 0.0434763, acc 0.98
2016-09-07T00:25:15.429208: step 1559, loss 0.0332229, acc 0.98
2016-09-07T00:25:16.117699: step 1560, loss 0.0653963, acc 0.96
2016-09-07T00:25:16.815687: step 1561, loss 0.0352866, acc 0.98
2016-09-07T00:25:17.498076: step 1562, loss 0.0393673, acc 1
2016-09-07T00:25:18.199975: step 1563, loss 0.0139309, acc 1
2016-09-07T00:25:18.873586: step 1564, loss 0.0744684, acc 0.94
2016-09-07T00:25:19.571681: step 1565, loss 0.0278823, acc 1
2016-09-07T00:25:20.259656: step 1566, loss 0.0475275, acc 0.98
2016-09-07T00:25:20.947010: step 1567, loss 0.053965, acc 0.98
2016-09-07T00:25:21.656942: step 1568, loss 0.0723252, acc 0.94
2016-09-07T00:25:22.339322: step 1569, loss 0.112441, acc 0.94
2016-09-07T00:25:23.033199: step 1570, loss 0.0367476, acc 1
2016-09-07T00:25:23.713596: step 1571, loss 0.0388381, acc 1
2016-09-07T00:25:24.394681: step 1572, loss 0.164682, acc 0.92
2016-09-07T00:25:25.107299: step 1573, loss 0.0709562, acc 0.98
2016-09-07T00:25:25.801270: step 1574, loss 0.114504, acc 0.96
2016-09-07T00:25:26.508058: step 1575, loss 0.0226173, acc 0.98
2016-09-07T00:25:27.181766: step 1576, loss 0.0128953, acc 1
2016-09-07T00:25:27.883871: step 1577, loss 0.025917, acc 1
2016-09-07T00:25:28.578770: step 1578, loss 0.0292228, acc 1
2016-09-07T00:25:29.269526: step 1579, loss 0.0502042, acc 0.98
2016-09-07T00:25:29.972722: step 1580, loss 0.0804653, acc 0.96
2016-09-07T00:25:30.657831: step 1581, loss 0.0103866, acc 1
2016-09-07T00:25:31.357887: step 1582, loss 0.0747398, acc 0.98
2016-09-07T00:25:32.024291: step 1583, loss 0.057665, acc 0.96
2016-09-07T00:25:32.700062: step 1584, loss 0.131982, acc 0.92
2016-09-07T00:25:33.377794: step 1585, loss 0.0889239, acc 0.96
2016-09-07T00:25:34.060266: step 1586, loss 0.0319569, acc 0.98
2016-09-07T00:25:34.742967: step 1587, loss 0.0125986, acc 1
2016-09-07T00:25:35.425725: step 1588, loss 0.0881173, acc 0.98
2016-09-07T00:25:36.172517: step 1589, loss 0.0271038, acc 0.98
2016-09-07T00:25:36.859411: step 1590, loss 0.0172143, acc 1
2016-09-07T00:25:37.552092: step 1591, loss 0.064945, acc 0.98
2016-09-07T00:25:38.250731: step 1592, loss 0.037446, acc 0.98
2016-09-07T00:25:38.929450: step 1593, loss 0.10423, acc 0.94
2016-09-07T00:25:39.589912: step 1594, loss 0.0214929, acc 0.98
2016-09-07T00:25:40.243625: step 1595, loss 0.0797706, acc 0.94
2016-09-07T00:25:40.960692: step 1596, loss 0.0591064, acc 0.96
2016-09-07T00:25:41.684577: step 1597, loss 0.0434549, acc 0.98
2016-09-07T00:25:42.392097: step 1598, loss 0.0925995, acc 0.96
2016-09-07T00:25:43.060306: step 1599, loss 0.0499621, acc 1
2016-09-07T00:25:43.749276: step 1600, loss 0.0489888, acc 0.98

Evaluation:
2016-09-07T00:25:46.927083: step 1600, loss 1.07056, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-1600

2016-09-07T00:25:48.637361: step 1601, loss 0.0320552, acc 1
2016-09-07T00:25:49.359117: step 1602, loss 0.0610611, acc 0.98
2016-09-07T00:25:50.040756: step 1603, loss 0.0424812, acc 0.98
2016-09-07T00:25:50.706940: step 1604, loss 0.20653, acc 0.94
2016-09-07T00:25:51.389330: step 1605, loss 0.0303081, acc 1
2016-09-07T00:25:52.067948: step 1606, loss 0.0926262, acc 0.98
2016-09-07T00:25:52.762389: step 1607, loss 0.0585523, acc 0.96
2016-09-07T00:25:53.455306: step 1608, loss 0.0415472, acc 1
2016-09-07T00:25:54.151510: step 1609, loss 0.0129717, acc 1
2016-09-07T00:25:54.834887: step 1610, loss 0.0397503, acc 0.98
2016-09-07T00:25:55.526679: step 1611, loss 0.0575934, acc 0.96
2016-09-07T00:25:56.198884: step 1612, loss 0.05949, acc 0.98
2016-09-07T00:25:56.897139: step 1613, loss 0.094445, acc 0.94
2016-09-07T00:25:57.585664: step 1614, loss 0.138267, acc 0.94
2016-09-07T00:25:58.270361: step 1615, loss 0.201563, acc 0.92
2016-09-07T00:25:58.987539: step 1616, loss 0.109324, acc 0.94
2016-09-07T00:25:59.653961: step 1617, loss 0.0246996, acc 1
2016-09-07T00:26:00.359171: step 1618, loss 0.129834, acc 0.94
2016-09-07T00:26:01.021514: step 1619, loss 0.0766365, acc 0.94
2016-09-07T00:26:01.708459: step 1620, loss 0.0473165, acc 0.98
2016-09-07T00:26:02.386745: step 1621, loss 0.136458, acc 0.94
2016-09-07T00:26:03.069951: step 1622, loss 0.0663734, acc 0.96
2016-09-07T00:26:03.778145: step 1623, loss 0.0382211, acc 0.98
2016-09-07T00:26:04.436264: step 1624, loss 0.0873578, acc 0.96
2016-09-07T00:26:05.117775: step 1625, loss 0.105922, acc 0.94
2016-09-07T00:26:05.783464: step 1626, loss 0.151309, acc 0.92
2016-09-07T00:26:06.470244: step 1627, loss 0.0683943, acc 0.96
2016-09-07T00:26:07.167968: step 1628, loss 0.0832079, acc 0.96
2016-09-07T00:26:07.842300: step 1629, loss 0.0936316, acc 0.96
2016-09-07T00:26:08.534335: step 1630, loss 0.0375742, acc 0.98
2016-09-07T00:26:09.187852: step 1631, loss 0.0606205, acc 0.98
2016-09-07T00:26:09.895838: step 1632, loss 0.02466, acc 1
2016-09-07T00:26:10.588008: step 1633, loss 0.0349455, acc 0.98
2016-09-07T00:26:11.250293: step 1634, loss 0.0657468, acc 0.98
2016-09-07T00:26:11.910191: step 1635, loss 0.0415339, acc 1
2016-09-07T00:26:12.609543: step 1636, loss 0.027852, acc 1
2016-09-07T00:26:13.302223: step 1637, loss 0.0557233, acc 0.98
2016-09-07T00:26:13.964363: step 1638, loss 0.0354543, acc 0.98
2016-09-07T00:26:14.673683: step 1639, loss 0.055003, acc 0.96
2016-09-07T00:26:15.340543: step 1640, loss 0.0852977, acc 0.96
2016-09-07T00:26:16.027094: step 1641, loss 0.0507055, acc 0.98
2016-09-07T00:26:16.720395: step 1642, loss 0.089111, acc 0.96
2016-09-07T00:26:17.401751: step 1643, loss 0.0258827, acc 1
2016-09-07T00:26:18.083843: step 1644, loss 0.0752549, acc 0.96
2016-09-07T00:26:18.755800: step 1645, loss 0.0391261, acc 0.98
2016-09-07T00:26:19.443438: step 1646, loss 0.0475149, acc 0.98
2016-09-07T00:26:20.114094: step 1647, loss 0.0515987, acc 0.98
2016-09-07T00:26:20.800029: step 1648, loss 0.179141, acc 0.94
2016-09-07T00:26:21.498979: step 1649, loss 0.0649792, acc 0.96
2016-09-07T00:26:22.185556: step 1650, loss 0.039404, acc 0.98
2016-09-07T00:26:22.861378: step 1651, loss 0.0995971, acc 0.96
2016-09-07T00:26:23.518049: step 1652, loss 0.0591869, acc 0.98
2016-09-07T00:26:24.227863: step 1653, loss 0.0153564, acc 1
2016-09-07T00:26:24.910253: step 1654, loss 0.103982, acc 0.94
2016-09-07T00:26:25.584604: step 1655, loss 0.0674113, acc 0.96
2016-09-07T00:26:26.268891: step 1656, loss 0.034402, acc 0.98
2016-09-07T00:26:26.980255: step 1657, loss 0.0169376, acc 0.98
2016-09-07T00:26:27.686275: step 1658, loss 0.0735206, acc 0.96
2016-09-07T00:26:28.400667: step 1659, loss 0.036328, acc 1
2016-09-07T00:26:29.105371: step 1660, loss 0.0544231, acc 0.96
2016-09-07T00:26:29.793124: step 1661, loss 0.0999095, acc 0.98
2016-09-07T00:26:30.460058: step 1662, loss 0.010655, acc 1
2016-09-07T00:26:31.143026: step 1663, loss 0.0695546, acc 0.94
2016-09-07T00:26:31.833943: step 1664, loss 0.00381459, acc 1
2016-09-07T00:26:32.535696: step 1665, loss 0.00271928, acc 1
2016-09-07T00:26:33.209897: step 1666, loss 0.0525204, acc 0.96
2016-09-07T00:26:33.907583: step 1667, loss 0.183874, acc 0.94
2016-09-07T00:26:34.590626: step 1668, loss 0.0310255, acc 1
2016-09-07T00:26:35.280250: step 1669, loss 0.0603857, acc 0.98
2016-09-07T00:26:35.983033: step 1670, loss 0.0165055, acc 1
2016-09-07T00:26:36.672023: step 1671, loss 0.041215, acc 0.98
2016-09-07T00:26:37.368634: step 1672, loss 0.00579188, acc 1
2016-09-07T00:26:38.052014: step 1673, loss 0.00922637, acc 1
2016-09-07T00:26:38.749390: step 1674, loss 0.00390936, acc 1
2016-09-07T00:26:39.452011: step 1675, loss 0.0391289, acc 0.98
2016-09-07T00:26:40.129657: step 1676, loss 0.00585825, acc 1
2016-09-07T00:26:40.818967: step 1677, loss 0.02544, acc 1
2016-09-07T00:26:41.500496: step 1678, loss 0.0215702, acc 0.98
2016-09-07T00:26:42.185130: step 1679, loss 0.0470169, acc 0.98
2016-09-07T00:26:42.847691: step 1680, loss 0.064168, acc 0.98
2016-09-07T00:26:43.550757: step 1681, loss 0.148438, acc 0.92
2016-09-07T00:26:44.232960: step 1682, loss 0.044213, acc 0.98
2016-09-07T00:26:44.918888: step 1683, loss 0.0131964, acc 1
2016-09-07T00:26:45.611594: step 1684, loss 0.059938, acc 0.98
2016-09-07T00:26:46.293007: step 1685, loss 0.112265, acc 0.92
2016-09-07T00:26:46.996176: step 1686, loss 0.0506908, acc 0.98
2016-09-07T00:26:47.678405: step 1687, loss 0.176376, acc 0.96
2016-09-07T00:26:48.356351: step 1688, loss 0.175096, acc 0.96
2016-09-07T00:26:49.046840: step 1689, loss 0.00150233, acc 1
2016-09-07T00:26:49.735585: step 1690, loss 0.0587653, acc 0.96
2016-09-07T00:26:50.426243: step 1691, loss 0.0548538, acc 0.96
2016-09-07T00:26:51.102971: step 1692, loss 0.0294771, acc 1
2016-09-07T00:26:51.796895: step 1693, loss 0.100028, acc 0.96
2016-09-07T00:26:52.478024: step 1694, loss 0.0934945, acc 0.94
2016-09-07T00:26:53.194711: step 1695, loss 0.107798, acc 0.96
2016-09-07T00:26:53.883023: step 1696, loss 0.10793, acc 0.96
2016-09-07T00:26:54.572477: step 1697, loss 0.0123753, acc 1
2016-09-07T00:26:55.256738: step 1698, loss 0.126849, acc 0.96
2016-09-07T00:26:55.944795: step 1699, loss 0.0962486, acc 0.98
2016-09-07T00:26:56.652213: step 1700, loss 0.0289522, acc 0.98

Evaluation:
2016-09-07T00:26:59.819775: step 1700, loss 1.20924, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-1700

2016-09-07T00:27:01.552635: step 1701, loss 0.0183729, acc 1
2016-09-07T00:27:02.256010: step 1702, loss 0.0770451, acc 0.94
2016-09-07T00:27:02.947822: step 1703, loss 0.0551446, acc 0.98
2016-09-07T00:27:03.654812: step 1704, loss 0.182387, acc 0.88
2016-09-07T00:27:04.348814: step 1705, loss 0.0477159, acc 0.98
2016-09-07T00:27:05.060322: step 1706, loss 0.214235, acc 0.94
2016-09-07T00:27:05.737751: step 1707, loss 0.068518, acc 0.96
2016-09-07T00:27:06.424069: step 1708, loss 0.067402, acc 0.96
2016-09-07T00:27:07.134749: step 1709, loss 0.0334488, acc 0.98
2016-09-07T00:27:07.813129: step 1710, loss 0.0640731, acc 0.96
2016-09-07T00:27:08.497058: step 1711, loss 0.0444522, acc 0.98
2016-09-07T00:27:09.195652: step 1712, loss 0.130101, acc 0.96
2016-09-07T00:27:09.881871: step 1713, loss 0.0498812, acc 0.98
2016-09-07T00:27:10.554964: step 1714, loss 0.109037, acc 0.92
2016-09-07T00:27:11.257936: step 1715, loss 0.0275694, acc 1
2016-09-07T00:27:11.942468: step 1716, loss 0.0960846, acc 0.94
2016-09-07T00:27:12.635771: step 1717, loss 0.0437176, acc 0.98
2016-09-07T00:27:13.332068: step 1718, loss 0.0617328, acc 0.98
2016-09-07T00:27:13.991622: step 1719, loss 0.0285686, acc 0.98
2016-09-07T00:27:14.702378: step 1720, loss 0.115481, acc 0.96
2016-09-07T00:27:15.417204: step 1721, loss 0.0391546, acc 0.98
2016-09-07T00:27:16.094893: step 1722, loss 0.0108745, acc 1
2016-09-07T00:27:16.782219: step 1723, loss 0.14318, acc 0.9
2016-09-07T00:27:17.464077: step 1724, loss 0.0159167, acc 1
2016-09-07T00:27:18.186286: step 1725, loss 0.0579187, acc 0.98
2016-09-07T00:27:18.853678: step 1726, loss 0.0850568, acc 0.96
2016-09-07T00:27:19.567457: step 1727, loss 0.0845748, acc 0.96
2016-09-07T00:27:20.176657: step 1728, loss 0.0306238, acc 1
2016-09-07T00:27:20.874517: step 1729, loss 0.0530343, acc 0.96
2016-09-07T00:27:21.567047: step 1730, loss 0.100631, acc 0.96
2016-09-07T00:27:22.244296: step 1731, loss 0.0392135, acc 0.98
2016-09-07T00:27:22.930948: step 1732, loss 0.0108602, acc 1
2016-09-07T00:27:23.607983: step 1733, loss 0.078733, acc 0.96
2016-09-07T00:27:24.302499: step 1734, loss 0.109768, acc 0.94
2016-09-07T00:27:24.969031: step 1735, loss 0.00845229, acc 1
2016-09-07T00:27:25.665624: step 1736, loss 0.0735636, acc 0.94
2016-09-07T00:27:26.360075: step 1737, loss 0.00412008, acc 1
2016-09-07T00:27:27.054848: step 1738, loss 0.0293056, acc 1
2016-09-07T00:27:27.733824: step 1739, loss 0.110557, acc 0.94
2016-09-07T00:27:28.430436: step 1740, loss 0.0358483, acc 0.98
2016-09-07T00:27:29.136830: step 1741, loss 0.152151, acc 0.94
2016-09-07T00:27:29.812108: step 1742, loss 0.0495395, acc 1
2016-09-07T00:27:30.490045: step 1743, loss 0.00579325, acc 1
2016-09-07T00:27:31.180442: step 1744, loss 0.0500659, acc 0.96
2016-09-07T00:27:31.873582: step 1745, loss 0.00825107, acc 1
2016-09-07T00:27:32.569386: step 1746, loss 0.0681356, acc 0.98
2016-09-07T00:27:33.246851: step 1747, loss 0.045753, acc 0.98
2016-09-07T00:27:33.939014: step 1748, loss 0.0141942, acc 1
2016-09-07T00:27:34.613419: step 1749, loss 0.0165363, acc 1
2016-09-07T00:27:35.312556: step 1750, loss 0.0215253, acc 1
2016-09-07T00:27:35.987927: step 1751, loss 0.0100594, acc 1
2016-09-07T00:27:36.691870: step 1752, loss 0.0754366, acc 0.96
2016-09-07T00:27:37.371930: step 1753, loss 0.11792, acc 0.94
2016-09-07T00:27:38.068113: step 1754, loss 0.0140247, acc 1
2016-09-07T00:27:38.766393: step 1755, loss 0.0565031, acc 0.98
2016-09-07T00:27:39.461031: step 1756, loss 0.0283667, acc 0.98
2016-09-07T00:27:40.139420: step 1757, loss 0.0415264, acc 0.96
2016-09-07T00:27:40.835718: step 1758, loss 0.0132598, acc 1
2016-09-07T00:27:41.522210: step 1759, loss 0.130002, acc 0.96
2016-09-07T00:27:42.226469: step 1760, loss 0.328826, acc 0.88
2016-09-07T00:27:42.926981: step 1761, loss 0.0171248, acc 1
2016-09-07T00:27:43.627334: step 1762, loss 0.0512869, acc 0.98
2016-09-07T00:27:44.299313: step 1763, loss 0.0696943, acc 0.94
2016-09-07T00:27:44.971385: step 1764, loss 0.0368668, acc 1
2016-09-07T00:27:45.653581: step 1765, loss 0.0579211, acc 0.96
2016-09-07T00:27:46.337541: step 1766, loss 0.0559702, acc 0.98
2016-09-07T00:27:47.044482: step 1767, loss 0.0421308, acc 0.98
2016-09-07T00:27:47.754028: step 1768, loss 0.00732766, acc 1
2016-09-07T00:27:48.477933: step 1769, loss 0.0466629, acc 0.98
2016-09-07T00:27:49.181249: step 1770, loss 0.0239224, acc 0.98
2016-09-07T00:27:49.871418: step 1771, loss 0.0700332, acc 0.98
2016-09-07T00:27:50.580998: step 1772, loss 0.00158429, acc 1
2016-09-07T00:27:51.277770: step 1773, loss 0.210939, acc 0.96
2016-09-07T00:27:51.975384: step 1774, loss 0.135086, acc 0.94
2016-09-07T00:27:52.664953: step 1775, loss 0.035184, acc 0.98
2016-09-07T00:27:53.356073: step 1776, loss 0.124181, acc 0.98
2016-09-07T00:27:54.033397: step 1777, loss 0.136975, acc 0.96
2016-09-07T00:27:54.743340: step 1778, loss 0.118037, acc 0.92
2016-09-07T00:27:55.395534: step 1779, loss 0.0429895, acc 0.98
2016-09-07T00:27:56.084624: step 1780, loss 0.280774, acc 0.96
2016-09-07T00:27:56.772064: step 1781, loss 0.0578441, acc 0.98
2016-09-07T00:27:57.445765: step 1782, loss 0.0541261, acc 0.96
2016-09-07T00:27:58.147848: step 1783, loss 0.0880106, acc 0.94
2016-09-07T00:27:58.854286: step 1784, loss 0.0373773, acc 1
2016-09-07T00:27:59.532707: step 1785, loss 0.0371772, acc 0.98
2016-09-07T00:28:00.224376: step 1786, loss 0.153739, acc 0.98
2016-09-07T00:28:00.917705: step 1787, loss 0.0491253, acc 0.98
2016-09-07T00:28:01.622145: step 1788, loss 0.017324, acc 1
2016-09-07T00:28:02.286449: step 1789, loss 0.0627459, acc 0.98
2016-09-07T00:28:02.968714: step 1790, loss 0.0206908, acc 1
2016-09-07T00:28:03.669217: step 1791, loss 0.0549471, acc 0.98
2016-09-07T00:28:04.356855: step 1792, loss 0.0432493, acc 0.98
2016-09-07T00:28:05.075747: step 1793, loss 0.013574, acc 1
2016-09-07T00:28:05.748171: step 1794, loss 0.0553418, acc 0.96
2016-09-07T00:28:06.456709: step 1795, loss 0.112468, acc 0.96
2016-09-07T00:28:07.172303: step 1796, loss 0.0547456, acc 1
2016-09-07T00:28:07.879879: step 1797, loss 0.0220876, acc 0.98
2016-09-07T00:28:08.551132: step 1798, loss 0.0534662, acc 0.98
2016-09-07T00:28:09.230369: step 1799, loss 0.0109462, acc 1
2016-09-07T00:28:09.930124: step 1800, loss 0.0884286, acc 0.96

Evaluation:
2016-09-07T00:28:13.069748: step 1800, loss 1.20541, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-1800

2016-09-07T00:28:14.717294: step 1801, loss 0.00541871, acc 1
2016-09-07T00:28:15.378231: step 1802, loss 0.0319213, acc 0.98
2016-09-07T00:28:16.076314: step 1803, loss 0.0372049, acc 1
2016-09-07T00:28:16.764449: step 1804, loss 0.0601353, acc 0.96
2016-09-07T00:28:17.448317: step 1805, loss 0.0919799, acc 0.94
2016-09-07T00:28:18.130479: step 1806, loss 0.0918137, acc 0.98
2016-09-07T00:28:18.814431: step 1807, loss 0.0555598, acc 0.98
2016-09-07T00:28:19.499443: step 1808, loss 0.0587068, acc 0.98
2016-09-07T00:28:20.175408: step 1809, loss 0.00402308, acc 1
2016-09-07T00:28:20.878196: step 1810, loss 0.0284933, acc 0.98
2016-09-07T00:28:21.554220: step 1811, loss 0.10817, acc 0.96
2016-09-07T00:28:22.231522: step 1812, loss 0.0391343, acc 0.98
2016-09-07T00:28:22.937639: step 1813, loss 0.0253105, acc 0.98
2016-09-07T00:28:23.617190: step 1814, loss 0.0570874, acc 0.94
2016-09-07T00:28:24.316578: step 1815, loss 0.0475532, acc 0.98
2016-09-07T00:28:25.008915: step 1816, loss 0.023001, acc 0.98
2016-09-07T00:28:25.746324: step 1817, loss 0.0499167, acc 0.98
2016-09-07T00:28:26.433875: step 1818, loss 0.0857433, acc 0.96
2016-09-07T00:28:27.118244: step 1819, loss 0.0652314, acc 0.98
2016-09-07T00:28:27.824997: step 1820, loss 0.0566032, acc 1
2016-09-07T00:28:28.525881: step 1821, loss 0.0155132, acc 1
2016-09-07T00:28:29.232416: step 1822, loss 0.0573685, acc 0.98
2016-09-07T00:28:29.941781: step 1823, loss 0.0470257, acc 0.98
2016-09-07T00:28:30.618943: step 1824, loss 0.201502, acc 0.92
2016-09-07T00:28:31.313009: step 1825, loss 0.0353386, acc 0.98
2016-09-07T00:28:31.985904: step 1826, loss 0.0329554, acc 0.98
2016-09-07T00:28:32.657937: step 1827, loss 0.0384658, acc 0.98
2016-09-07T00:28:33.333984: step 1828, loss 0.0731544, acc 0.96
2016-09-07T00:28:34.032670: step 1829, loss 0.0303764, acc 1
2016-09-07T00:28:34.693896: step 1830, loss 0.014823, acc 1
2016-09-07T00:28:35.388190: step 1831, loss 0.0445756, acc 0.98
2016-09-07T00:28:36.078427: step 1832, loss 0.11476, acc 0.92
2016-09-07T00:28:36.771172: step 1833, loss 0.049346, acc 0.98
2016-09-07T00:28:37.454491: step 1834, loss 0.0210227, acc 1
2016-09-07T00:28:38.138883: step 1835, loss 0.021901, acc 1
2016-09-07T00:28:38.832693: step 1836, loss 0.0242222, acc 1
2016-09-07T00:28:39.515847: step 1837, loss 0.0456877, acc 0.96
2016-09-07T00:28:40.203346: step 1838, loss 0.0768285, acc 0.98
2016-09-07T00:28:40.889036: step 1839, loss 0.0921243, acc 0.96
2016-09-07T00:28:41.564739: step 1840, loss 0.0113242, acc 1
2016-09-07T00:28:42.240357: step 1841, loss 0.0203324, acc 1
2016-09-07T00:28:42.909620: step 1842, loss 0.0656194, acc 0.98
2016-09-07T00:28:43.593277: step 1843, loss 0.0569014, acc 0.98
2016-09-07T00:28:44.246491: step 1844, loss 0.017386, acc 1
2016-09-07T00:28:44.943778: step 1845, loss 0.0080405, acc 1
2016-09-07T00:28:45.594749: step 1846, loss 0.084141, acc 0.94
2016-09-07T00:28:46.279487: step 1847, loss 0.0880202, acc 0.92
2016-09-07T00:28:46.949068: step 1848, loss 0.0536115, acc 0.98
2016-09-07T00:28:47.649964: step 1849, loss 0.0251653, acc 1
2016-09-07T00:28:48.354312: step 1850, loss 0.0659105, acc 0.96
2016-09-07T00:28:49.031103: step 1851, loss 0.024294, acc 1
2016-09-07T00:28:49.752453: step 1852, loss 0.127789, acc 0.96
2016-09-07T00:28:50.454983: step 1853, loss 0.0687922, acc 0.96
2016-09-07T00:28:51.144396: step 1854, loss 0.115086, acc 0.94
2016-09-07T00:28:51.816052: step 1855, loss 0.0334673, acc 1
2016-09-07T00:28:52.500750: step 1856, loss 0.142533, acc 0.94
2016-09-07T00:28:53.182961: step 1857, loss 0.0207729, acc 1
2016-09-07T00:28:53.875520: step 1858, loss 0.125443, acc 0.96
2016-09-07T00:28:54.578098: step 1859, loss 0.0833238, acc 0.98
2016-09-07T00:28:55.249986: step 1860, loss 0.145906, acc 0.96
2016-09-07T00:28:55.926397: step 1861, loss 0.136077, acc 0.9
2016-09-07T00:28:56.610809: step 1862, loss 0.0403218, acc 0.98
2016-09-07T00:28:57.295229: step 1863, loss 0.0187163, acc 0.98
2016-09-07T00:28:57.973196: step 1864, loss 0.0497684, acc 0.96
2016-09-07T00:28:58.649123: step 1865, loss 0.0333816, acc 0.98
2016-09-07T00:28:59.324522: step 1866, loss 0.0125354, acc 1
2016-09-07T00:28:59.993842: step 1867, loss 0.0814175, acc 0.98
2016-09-07T00:29:00.702242: step 1868, loss 0.0120137, acc 1
2016-09-07T00:29:01.382715: step 1869, loss 0.0365631, acc 1
2016-09-07T00:29:02.060400: step 1870, loss 0.041151, acc 0.98
2016-09-07T00:29:02.733236: step 1871, loss 0.0347248, acc 0.98
2016-09-07T00:29:03.427214: step 1872, loss 0.116429, acc 0.94
2016-09-07T00:29:04.141230: step 1873, loss 0.0597994, acc 0.98
2016-09-07T00:29:04.818095: step 1874, loss 0.0780351, acc 0.96
2016-09-07T00:29:05.511288: step 1875, loss 0.0159877, acc 0.98
2016-09-07T00:29:06.198218: step 1876, loss 0.138244, acc 0.94
2016-09-07T00:29:06.875657: step 1877, loss 0.0503734, acc 1
2016-09-07T00:29:07.561070: step 1878, loss 0.17739, acc 0.98
2016-09-07T00:29:08.252871: step 1879, loss 0.0751378, acc 0.96
2016-09-07T00:29:08.958570: step 1880, loss 0.0291171, acc 0.98
2016-09-07T00:29:09.620375: step 1881, loss 0.0246913, acc 0.98
2016-09-07T00:29:10.340874: step 1882, loss 0.0359045, acc 1
2016-09-07T00:29:11.046318: step 1883, loss 0.198993, acc 0.92
2016-09-07T00:29:11.734616: step 1884, loss 0.0350616, acc 0.98
2016-09-07T00:29:12.423812: step 1885, loss 0.0464524, acc 0.96
2016-09-07T00:29:13.069931: step 1886, loss 0.0904691, acc 0.96
2016-09-07T00:29:13.757701: step 1887, loss 0.0252038, acc 1
2016-09-07T00:29:14.422871: step 1888, loss 0.048157, acc 0.96
2016-09-07T00:29:15.098870: step 1889, loss 0.0162348, acc 1
2016-09-07T00:29:15.798582: step 1890, loss 0.0527657, acc 0.98
2016-09-07T00:29:16.480874: step 1891, loss 0.0461252, acc 0.98
2016-09-07T00:29:17.154133: step 1892, loss 0.0255746, acc 1
2016-09-07T00:29:17.818391: step 1893, loss 0.0359063, acc 0.98
2016-09-07T00:29:18.511913: step 1894, loss 0.117725, acc 0.96
2016-09-07T00:29:19.164598: step 1895, loss 0.104927, acc 0.94
2016-09-07T00:29:19.888083: step 1896, loss 0.106363, acc 0.94
2016-09-07T00:29:20.575042: step 1897, loss 0.0622452, acc 0.96
2016-09-07T00:29:21.262473: step 1898, loss 0.182849, acc 0.94
2016-09-07T00:29:21.934970: step 1899, loss 0.0450948, acc 0.98
2016-09-07T00:29:22.635670: step 1900, loss 0.0145821, acc 1

Evaluation:
2016-09-07T00:29:25.768822: step 1900, loss 1.20944, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-1900

2016-09-07T00:29:27.477546: step 1901, loss 0.0663633, acc 0.96
2016-09-07T00:29:28.178536: step 1902, loss 0.10639, acc 0.98
2016-09-07T00:29:28.853078: step 1903, loss 0.058203, acc 0.98
2016-09-07T00:29:29.546438: step 1904, loss 0.0683157, acc 0.96
2016-09-07T00:29:30.235223: step 1905, loss 0.0718403, acc 0.96
2016-09-07T00:29:30.909715: step 1906, loss 0.107523, acc 0.94
2016-09-07T00:29:31.602243: step 1907, loss 0.0312306, acc 0.98
2016-09-07T00:29:32.298162: step 1908, loss 0.0477643, acc 0.98
2016-09-07T00:29:33.004859: step 1909, loss 0.0804109, acc 0.96
2016-09-07T00:29:33.708794: step 1910, loss 0.0352374, acc 0.98
2016-09-07T00:29:34.418437: step 1911, loss 0.0384773, acc 0.98
2016-09-07T00:29:35.110148: step 1912, loss 0.148077, acc 0.98
2016-09-07T00:29:35.819617: step 1913, loss 0.0319967, acc 0.98
2016-09-07T00:29:36.521460: step 1914, loss 0.13291, acc 0.92
2016-09-07T00:29:37.191102: step 1915, loss 0.0200914, acc 1
2016-09-07T00:29:37.918739: step 1916, loss 0.0494129, acc 0.98
2016-09-07T00:29:38.606872: step 1917, loss 0.0491492, acc 0.96
2016-09-07T00:29:39.298522: step 1918, loss 0.0777619, acc 0.98
2016-09-07T00:29:39.992567: step 1919, loss 0.0284601, acc 0.98
2016-09-07T00:29:40.647435: step 1920, loss 0.017818, acc 1
2016-09-07T00:29:41.352968: step 1921, loss 0.0340277, acc 1
2016-09-07T00:29:42.006553: step 1922, loss 0.026887, acc 0.98
2016-09-07T00:29:42.692762: step 1923, loss 0.214547, acc 0.92
2016-09-07T00:29:43.363920: step 1924, loss 0.0696082, acc 0.96
2016-09-07T00:29:44.039533: step 1925, loss 0.0451595, acc 0.96
2016-09-07T00:29:44.736139: step 1926, loss 0.0727393, acc 0.96
2016-09-07T00:29:45.439017: step 1927, loss 0.167247, acc 0.96
2016-09-07T00:29:46.147527: step 1928, loss 0.0708741, acc 0.96
2016-09-07T00:29:46.816047: step 1929, loss 0.0471843, acc 0.98
2016-09-07T00:29:47.510124: step 1930, loss 0.0775967, acc 0.96
2016-09-07T00:29:48.196863: step 1931, loss 0.028331, acc 0.98
2016-09-07T00:29:48.865880: step 1932, loss 0.0490891, acc 1
2016-09-07T00:29:49.540986: step 1933, loss 0.115185, acc 0.96
2016-09-07T00:29:50.230164: step 1934, loss 0.0417978, acc 0.98
2016-09-07T00:29:50.912845: step 1935, loss 0.0567547, acc 0.98
2016-09-07T00:29:51.573555: step 1936, loss 0.0855036, acc 0.96
2016-09-07T00:29:52.283005: step 1937, loss 0.0319113, acc 1
2016-09-07T00:29:52.956671: step 1938, loss 0.0312185, acc 1
2016-09-07T00:29:53.635955: step 1939, loss 0.0230797, acc 1
2016-09-07T00:29:54.317544: step 1940, loss 0.134652, acc 0.96
2016-09-07T00:29:55.001568: step 1941, loss 0.0477531, acc 0.98
2016-09-07T00:29:55.705513: step 1942, loss 0.188624, acc 0.96
2016-09-07T00:29:56.389259: step 1943, loss 0.0356239, acc 1
2016-09-07T00:29:57.103169: step 1944, loss 0.121121, acc 0.96
2016-09-07T00:29:57.785295: step 1945, loss 0.0353519, acc 0.98
2016-09-07T00:29:58.476994: step 1946, loss 0.0102374, acc 1
2016-09-07T00:29:59.160617: step 1947, loss 0.0146373, acc 1
2016-09-07T00:29:59.839500: step 1948, loss 0.0553993, acc 0.98
2016-09-07T00:30:00.574093: step 1949, loss 0.073029, acc 0.96
2016-09-07T00:30:01.235892: step 1950, loss 0.0279376, acc 0.98
2016-09-07T00:30:01.928355: step 1951, loss 0.0490075, acc 0.96
2016-09-07T00:30:02.626887: step 1952, loss 0.0969242, acc 0.94
2016-09-07T00:30:03.299242: step 1953, loss 0.0230469, acc 1
2016-09-07T00:30:03.991184: step 1954, loss 0.0431793, acc 0.98
2016-09-07T00:30:04.699183: step 1955, loss 0.0158622, acc 1
2016-09-07T00:30:05.391761: step 1956, loss 0.0518727, acc 0.98
2016-09-07T00:30:06.061179: step 1957, loss 0.014278, acc 1
2016-09-07T00:30:06.768466: step 1958, loss 0.0475328, acc 0.96
2016-09-07T00:30:07.470670: step 1959, loss 0.0369429, acc 0.98
2016-09-07T00:30:08.135503: step 1960, loss 0.0222366, acc 1
2016-09-07T00:30:08.812930: step 1961, loss 0.0354817, acc 0.98
2016-09-07T00:30:09.518202: step 1962, loss 0.0499123, acc 0.96
2016-09-07T00:30:10.222371: step 1963, loss 0.124008, acc 0.96
2016-09-07T00:30:10.896267: step 1964, loss 0.0877437, acc 0.98
2016-09-07T00:30:11.593030: step 1965, loss 0.11704, acc 0.92
2016-09-07T00:30:12.274318: step 1966, loss 0.0320519, acc 0.98
2016-09-07T00:30:12.946058: step 1967, loss 0.0457188, acc 0.96
2016-09-07T00:30:13.652517: step 1968, loss 0.0234715, acc 1
2016-09-07T00:30:14.345166: step 1969, loss 0.0652724, acc 0.98
2016-09-07T00:30:15.038523: step 1970, loss 0.0658989, acc 0.94
2016-09-07T00:30:15.701426: step 1971, loss 0.106176, acc 0.96
2016-09-07T00:30:16.376330: step 1972, loss 0.0359076, acc 1
2016-09-07T00:30:17.093435: step 1973, loss 0.0134983, acc 1
2016-09-07T00:30:17.769777: step 1974, loss 0.0725317, acc 0.96
2016-09-07T00:30:18.466451: step 1975, loss 0.0200077, acc 1
2016-09-07T00:30:19.175247: step 1976, loss 0.0488428, acc 0.96
2016-09-07T00:30:19.860117: step 1977, loss 0.063444, acc 0.98
2016-09-07T00:30:20.516965: step 1978, loss 0.0433955, acc 0.98
2016-09-07T00:30:21.227377: step 1979, loss 0.0769464, acc 0.98
2016-09-07T00:30:21.925389: step 1980, loss 0.0610013, acc 0.98
2016-09-07T00:30:22.618565: step 1981, loss 0.0837353, acc 0.98
2016-09-07T00:30:23.302536: step 1982, loss 0.0434272, acc 0.98
2016-09-07T00:30:23.984422: step 1983, loss 0.022493, acc 0.98
2016-09-07T00:30:24.687752: step 1984, loss 0.0335312, acc 1
2016-09-07T00:30:25.346497: step 1985, loss 0.0522998, acc 0.96
2016-09-07T00:30:26.050012: step 1986, loss 0.0677967, acc 0.98
2016-09-07T00:30:26.731273: step 1987, loss 0.0201112, acc 1
2016-09-07T00:30:27.415960: step 1988, loss 0.0186327, acc 0.98
2016-09-07T00:30:28.098788: step 1989, loss 0.028231, acc 1
2016-09-07T00:30:28.802462: step 1990, loss 0.0323309, acc 1
2016-09-07T00:30:29.509205: step 1991, loss 0.0412642, acc 0.98
2016-09-07T00:30:30.196992: step 1992, loss 0.0222741, acc 1
2016-09-07T00:30:30.871865: step 1993, loss 0.0236351, acc 0.98
2016-09-07T00:30:31.569673: step 1994, loss 0.0213234, acc 0.98
2016-09-07T00:30:32.265250: step 1995, loss 0.0454679, acc 0.98
2016-09-07T00:30:32.955375: step 1996, loss 0.036699, acc 1
2016-09-07T00:30:33.634494: step 1997, loss 0.0497861, acc 0.98
2016-09-07T00:30:34.357474: step 1998, loss 0.039007, acc 0.98
2016-09-07T00:30:35.032024: step 1999, loss 0.0865595, acc 0.96
2016-09-07T00:30:35.716930: step 2000, loss 0.0368337, acc 0.98

Evaluation:
2016-09-07T00:30:38.874222: step 2000, loss 1.49922, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-2000

2016-09-07T00:30:40.539496: step 2001, loss 0.0123298, acc 1
2016-09-07T00:30:41.210383: step 2002, loss 0.0174164, acc 0.98
2016-09-07T00:30:41.897963: step 2003, loss 0.0136874, acc 1
2016-09-07T00:30:42.578366: step 2004, loss 0.00592113, acc 1
2016-09-07T00:30:43.254384: step 2005, loss 0.0115887, acc 1
2016-09-07T00:30:43.956255: step 2006, loss 0.0913931, acc 0.96
2016-09-07T00:30:44.630028: step 2007, loss 0.097875, acc 0.94
2016-09-07T00:30:45.335146: step 2008, loss 0.0601504, acc 0.96
2016-09-07T00:30:46.024966: step 2009, loss 0.0132613, acc 1
2016-09-07T00:30:46.693151: step 2010, loss 0.0740721, acc 0.96
2016-09-07T00:30:47.381494: step 2011, loss 0.0506322, acc 0.98
2016-09-07T00:30:48.068973: step 2012, loss 0.0725703, acc 0.94
2016-09-07T00:30:48.763947: step 2013, loss 0.136203, acc 0.98
2016-09-07T00:30:49.432039: step 2014, loss 0.0375638, acc 0.98
2016-09-07T00:30:50.121855: step 2015, loss 0.0704176, acc 0.98
2016-09-07T00:30:50.818770: step 2016, loss 0.160162, acc 0.94
2016-09-07T00:30:51.532165: step 2017, loss 0.0448217, acc 0.98
2016-09-07T00:30:52.224777: step 2018, loss 0.130721, acc 0.94
2016-09-07T00:30:52.897719: step 2019, loss 0.0877318, acc 0.96
2016-09-07T00:30:53.609843: step 2020, loss 0.0129196, acc 1
2016-09-07T00:30:54.279345: step 2021, loss 0.00122717, acc 1
2016-09-07T00:30:54.947145: step 2022, loss 0.0529344, acc 1
2016-09-07T00:30:55.627115: step 2023, loss 0.0847546, acc 0.96
2016-09-07T00:30:56.315634: step 2024, loss 0.0531568, acc 0.98
2016-09-07T00:30:57.009666: step 2025, loss 0.0319497, acc 0.98
2016-09-07T00:30:57.681362: step 2026, loss 0.0214265, acc 0.98
2016-09-07T00:30:58.368179: step 2027, loss 0.0791066, acc 0.94
2016-09-07T00:30:59.044004: step 2028, loss 0.0968986, acc 0.96
2016-09-07T00:30:59.736314: step 2029, loss 0.0900479, acc 0.94
2016-09-07T00:31:00.456086: step 2030, loss 0.0509523, acc 1
2016-09-07T00:31:01.149709: step 2031, loss 0.0539464, acc 0.98
2016-09-07T00:31:01.828414: step 2032, loss 0.0563723, acc 0.96
2016-09-07T00:31:02.527122: step 2033, loss 0.020149, acc 1
2016-09-07T00:31:03.233803: step 2034, loss 0.0311423, acc 1
2016-09-07T00:31:03.899375: step 2035, loss 0.0335926, acc 0.98
2016-09-07T00:31:04.581381: step 2036, loss 0.0589007, acc 0.96
2016-09-07T00:31:05.280406: step 2037, loss 0.0107389, acc 1
2016-09-07T00:31:05.986269: step 2038, loss 0.0188875, acc 1
2016-09-07T00:31:06.663199: step 2039, loss 0.0619798, acc 0.96
2016-09-07T00:31:07.356302: step 2040, loss 0.0221387, acc 1
2016-09-07T00:31:08.053894: step 2041, loss 0.0241444, acc 1
2016-09-07T00:31:08.746856: step 2042, loss 0.0526489, acc 0.98
2016-09-07T00:31:09.410002: step 2043, loss 0.045046, acc 0.96
2016-09-07T00:31:10.079392: step 2044, loss 0.0274756, acc 1
2016-09-07T00:31:10.735803: step 2045, loss 0.0395644, acc 0.98
2016-09-07T00:31:11.403548: step 2046, loss 0.161249, acc 0.96
2016-09-07T00:31:12.077632: step 2047, loss 0.0418648, acc 0.98
2016-09-07T00:31:12.773223: step 2048, loss 0.0232275, acc 1
2016-09-07T00:31:13.428962: step 2049, loss 0.0699236, acc 0.96
2016-09-07T00:31:14.111347: step 2050, loss 0.0877356, acc 0.96
2016-09-07T00:31:14.793521: step 2051, loss 0.210597, acc 0.96
2016-09-07T00:31:15.473365: step 2052, loss 0.0866228, acc 0.96
2016-09-07T00:31:16.162704: step 2053, loss 0.0655618, acc 0.98
2016-09-07T00:31:16.845247: step 2054, loss 0.0962304, acc 0.94
2016-09-07T00:31:17.551400: step 2055, loss 0.0354714, acc 0.98
2016-09-07T00:31:18.207371: step 2056, loss 0.042962, acc 1
2016-09-07T00:31:18.907251: step 2057, loss 0.00998514, acc 1
2016-09-07T00:31:19.580171: step 2058, loss 0.0249651, acc 0.98
2016-09-07T00:31:20.245815: step 2059, loss 0.114308, acc 0.98
2016-09-07T00:31:20.928555: step 2060, loss 0.0627992, acc 0.98
2016-09-07T00:31:21.612973: step 2061, loss 0.0622791, acc 0.96
2016-09-07T00:31:22.279797: step 2062, loss 0.0590837, acc 0.96
2016-09-07T00:31:22.977983: step 2063, loss 0.00492713, acc 1
2016-09-07T00:31:23.666388: step 2064, loss 0.0301011, acc 0.98
2016-09-07T00:31:24.340823: step 2065, loss 0.0339563, acc 1
2016-09-07T00:31:25.034210: step 2066, loss 0.0159698, acc 1
2016-09-07T00:31:25.719467: step 2067, loss 0.00674039, acc 1
2016-09-07T00:31:26.424196: step 2068, loss 0.0424798, acc 0.98
2016-09-07T00:31:27.135193: step 2069, loss 0.04346, acc 0.98
2016-09-07T00:31:27.802097: step 2070, loss 0.0353141, acc 0.98
2016-09-07T00:31:28.503065: step 2071, loss 0.00935639, acc 1
2016-09-07T00:31:29.179400: step 2072, loss 0.0218236, acc 0.98
2016-09-07T00:31:29.868288: step 2073, loss 0.0236757, acc 1
2016-09-07T00:31:30.552030: step 2074, loss 0.0492846, acc 0.98
2016-09-07T00:31:31.249912: step 2075, loss 0.0505423, acc 0.98
2016-09-07T00:31:31.943983: step 2076, loss 0.0536269, acc 0.96
2016-09-07T00:31:32.618877: step 2077, loss 0.0593758, acc 0.98
2016-09-07T00:31:33.345945: step 2078, loss 0.0696445, acc 0.96
2016-09-07T00:31:34.015705: step 2079, loss 0.0170247, acc 1
2016-09-07T00:31:34.695940: step 2080, loss 0.0239734, acc 1
2016-09-07T00:31:35.408348: step 2081, loss 0.0196381, acc 1
2016-09-07T00:31:36.119431: step 2082, loss 0.0868843, acc 0.96
2016-09-07T00:31:36.798248: step 2083, loss 0.03072, acc 1
2016-09-07T00:31:37.451421: step 2084, loss 0.0419478, acc 0.98
2016-09-07T00:31:38.164813: step 2085, loss 0.0751371, acc 0.94
2016-09-07T00:31:38.855503: step 2086, loss 0.194797, acc 0.98
2016-09-07T00:31:39.552576: step 2087, loss 0.013153, acc 1
2016-09-07T00:31:40.228370: step 2088, loss 0.219122, acc 0.92
2016-09-07T00:31:40.907937: step 2089, loss 0.0091285, acc 1
2016-09-07T00:31:41.613971: step 2090, loss 0.0340451, acc 1
2016-09-07T00:31:42.286583: step 2091, loss 0.0693103, acc 0.98
2016-09-07T00:31:42.990579: step 2092, loss 0.0277185, acc 0.98
2016-09-07T00:31:43.680457: step 2093, loss 0.156547, acc 0.94
2016-09-07T00:31:44.360917: step 2094, loss 0.0481949, acc 1
2016-09-07T00:31:45.051667: step 2095, loss 0.207913, acc 0.92
2016-09-07T00:31:45.728200: step 2096, loss 0.0183212, acc 1
2016-09-07T00:31:46.412660: step 2097, loss 0.124424, acc 0.94
2016-09-07T00:31:47.083680: step 2098, loss 0.251269, acc 0.94
2016-09-07T00:31:47.783864: step 2099, loss 0.0957519, acc 0.94
2016-09-07T00:31:48.471600: step 2100, loss 0.0383011, acc 0.98

Evaluation:
2016-09-07T00:31:51.645821: step 2100, loss 1.45551, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-2100

2016-09-07T00:31:53.404843: step 2101, loss 0.139707, acc 0.98
2016-09-07T00:31:54.117103: step 2102, loss 0.0815792, acc 0.96
2016-09-07T00:31:54.820564: step 2103, loss 0.0486543, acc 0.98
2016-09-07T00:31:55.498868: step 2104, loss 0.174159, acc 0.94
2016-09-07T00:31:56.208918: step 2105, loss 0.0504776, acc 1
2016-09-07T00:31:56.892093: step 2106, loss 0.0755541, acc 0.96
2016-09-07T00:31:57.557761: step 2107, loss 0.0672621, acc 0.98
2016-09-07T00:31:58.227549: step 2108, loss 0.00774727, acc 1
2016-09-07T00:31:58.923641: step 2109, loss 0.0778609, acc 0.96
2016-09-07T00:31:59.613728: step 2110, loss 0.0201075, acc 0.98
2016-09-07T00:32:00.293826: step 2111, loss 0.0465617, acc 0.96
2016-09-07T00:32:00.946162: step 2112, loss 0.0160941, acc 1
2016-09-07T00:32:01.621447: step 2113, loss 0.114867, acc 0.94
2016-09-07T00:32:02.314181: step 2114, loss 0.0458557, acc 1
2016-09-07T00:32:03.022084: step 2115, loss 0.0621544, acc 0.96
2016-09-07T00:32:03.692954: step 2116, loss 0.080877, acc 0.96
2016-09-07T00:32:04.374200: step 2117, loss 0.0367928, acc 0.98
2016-09-07T00:32:05.081311: step 2118, loss 0.0805653, acc 0.94
2016-09-07T00:32:05.803216: step 2119, loss 0.0952426, acc 0.96
2016-09-07T00:32:06.474952: step 2120, loss 0.126007, acc 0.96
2016-09-07T00:32:07.171894: step 2121, loss 0.0359788, acc 0.98
2016-09-07T00:32:07.860431: step 2122, loss 0.0423039, acc 1
2016-09-07T00:32:08.556753: step 2123, loss 0.0377112, acc 0.98
2016-09-07T00:32:09.219799: step 2124, loss 0.0960336, acc 0.94
2016-09-07T00:32:09.878228: step 2125, loss 0.10276, acc 0.96
2016-09-07T00:32:10.596790: step 2126, loss 0.0554863, acc 0.98
2016-09-07T00:32:11.276002: step 2127, loss 0.0493669, acc 0.98
2016-09-07T00:32:11.954031: step 2128, loss 0.053186, acc 0.96
2016-09-07T00:32:12.640189: step 2129, loss 0.0591239, acc 0.96
2016-09-07T00:32:13.341644: step 2130, loss 0.141784, acc 0.98
2016-09-07T00:32:14.051114: step 2131, loss 0.027192, acc 1
2016-09-07T00:32:14.729801: step 2132, loss 0.021665, acc 1
2016-09-07T00:32:15.433116: step 2133, loss 0.0624705, acc 1
2016-09-07T00:32:16.129380: step 2134, loss 0.065103, acc 0.94
2016-09-07T00:32:16.815608: step 2135, loss 0.0104369, acc 1
2016-09-07T00:32:17.507526: step 2136, loss 0.0522828, acc 1
2016-09-07T00:32:18.192118: step 2137, loss 0.0239332, acc 1
2016-09-07T00:32:18.874065: step 2138, loss 0.00965661, acc 1
2016-09-07T00:32:19.535089: step 2139, loss 0.00196767, acc 1
2016-09-07T00:32:20.271111: step 2140, loss 0.0258942, acc 1
2016-09-07T00:32:20.986570: step 2141, loss 0.0575081, acc 0.98
2016-09-07T00:32:21.689852: step 2142, loss 0.105869, acc 0.96
2016-09-07T00:32:22.375882: step 2143, loss 0.0333917, acc 1
2016-09-07T00:32:23.048750: step 2144, loss 0.099729, acc 0.94
2016-09-07T00:32:23.761020: step 2145, loss 0.0356853, acc 1
2016-09-07T00:32:24.431039: step 2146, loss 0.00713146, acc 1
2016-09-07T00:32:25.126120: step 2147, loss 0.00657935, acc 1
2016-09-07T00:32:25.824867: step 2148, loss 0.0300653, acc 1
2016-09-07T00:32:26.525153: step 2149, loss 0.0247653, acc 1
2016-09-07T00:32:27.202959: step 2150, loss 0.0841298, acc 0.94
2016-09-07T00:32:27.880737: step 2151, loss 0.0117617, acc 1
2016-09-07T00:32:28.590996: step 2152, loss 0.0645372, acc 0.98
2016-09-07T00:32:29.251881: step 2153, loss 0.10778, acc 0.94
2016-09-07T00:32:29.935055: step 2154, loss 0.028227, acc 1
2016-09-07T00:32:30.636593: step 2155, loss 0.0368831, acc 0.98
2016-09-07T00:32:31.329480: step 2156, loss 0.0202158, acc 0.98
2016-09-07T00:32:32.035534: step 2157, loss 0.18574, acc 0.92
2016-09-07T00:32:32.719775: step 2158, loss 0.0809688, acc 0.98
2016-09-07T00:32:33.438485: step 2159, loss 0.0374485, acc 0.98
2016-09-07T00:32:34.130676: step 2160, loss 0.0637308, acc 0.96
2016-09-07T00:32:34.807566: step 2161, loss 0.142512, acc 0.96
2016-09-07T00:32:35.514037: step 2162, loss 0.0227406, acc 1
2016-09-07T00:32:36.203891: step 2163, loss 0.0459108, acc 0.96
2016-09-07T00:32:36.901847: step 2164, loss 0.0606764, acc 0.96
2016-09-07T00:32:37.578184: step 2165, loss 0.0848038, acc 0.94
2016-09-07T00:32:38.300260: step 2166, loss 0.00119679, acc 1
2016-09-07T00:32:38.964471: step 2167, loss 0.0295795, acc 1
2016-09-07T00:32:39.653902: step 2168, loss 0.0219558, acc 1
2016-09-07T00:32:40.323088: step 2169, loss 0.0271372, acc 0.98
2016-09-07T00:32:41.023486: step 2170, loss 0.0157427, acc 1
2016-09-07T00:32:41.712931: step 2171, loss 0.024903, acc 1
2016-09-07T00:32:42.393733: step 2172, loss 0.0333136, acc 1
2016-09-07T00:32:43.095219: step 2173, loss 0.0713592, acc 0.96
2016-09-07T00:32:43.777720: step 2174, loss 0.0464469, acc 0.98
2016-09-07T00:32:44.456670: step 2175, loss 0.0647905, acc 0.98
2016-09-07T00:32:45.155321: step 2176, loss 0.0538259, acc 0.98
2016-09-07T00:32:45.844095: step 2177, loss 0.0292656, acc 0.98
2016-09-07T00:32:46.528106: step 2178, loss 0.0705665, acc 0.96
2016-09-07T00:32:47.200744: step 2179, loss 0.0515765, acc 0.98
2016-09-07T00:32:47.893464: step 2180, loss 0.0240843, acc 1
2016-09-07T00:32:48.574358: step 2181, loss 0.0178713, acc 1
2016-09-07T00:32:49.270268: step 2182, loss 0.0194729, acc 1
2016-09-07T00:32:49.989211: step 2183, loss 0.0866983, acc 0.96
2016-09-07T00:32:50.673379: step 2184, loss 0.0534637, acc 0.98
2016-09-07T00:32:51.351856: step 2185, loss 0.14029, acc 0.98
2016-09-07T00:32:52.023409: step 2186, loss 0.0700986, acc 0.96
2016-09-07T00:32:52.727678: step 2187, loss 0.169964, acc 0.96
2016-09-07T00:32:53.402668: step 2188, loss 0.0367898, acc 1
2016-09-07T00:32:54.070326: step 2189, loss 0.0778609, acc 0.98
2016-09-07T00:32:54.751948: step 2190, loss 0.0712483, acc 0.96
2016-09-07T00:32:55.450037: step 2191, loss 0.0608706, acc 1
2016-09-07T00:32:56.127307: step 2192, loss 0.0665265, acc 0.98
2016-09-07T00:32:56.781562: step 2193, loss 0.0273981, acc 1
2016-09-07T00:32:57.487726: step 2194, loss 0.026549, acc 0.98
2016-09-07T00:32:58.149810: step 2195, loss 0.0742476, acc 0.96
2016-09-07T00:32:58.865440: step 2196, loss 0.0405158, acc 0.98
2016-09-07T00:32:59.541143: step 2197, loss 0.0726152, acc 0.96
2016-09-07T00:33:00.255327: step 2198, loss 0.05519, acc 0.96
2016-09-07T00:33:00.950898: step 2199, loss 0.00772103, acc 1
2016-09-07T00:33:01.630809: step 2200, loss 0.0499635, acc 0.98

Evaluation:
2016-09-07T00:33:04.776670: step 2200, loss 1.5855, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-2200

2016-09-07T00:33:06.499440: step 2201, loss 0.0215687, acc 1
2016-09-07T00:33:07.196754: step 2202, loss 0.00154345, acc 1
2016-09-07T00:33:07.886578: step 2203, loss 0.0672574, acc 0.98
2016-09-07T00:33:08.571672: step 2204, loss 0.0581276, acc 0.98
2016-09-07T00:33:09.256391: step 2205, loss 0.152748, acc 0.96
2016-09-07T00:33:09.949702: step 2206, loss 0.0307797, acc 0.98
2016-09-07T00:33:10.661213: step 2207, loss 0.0448549, acc 0.98
2016-09-07T00:33:11.316339: step 2208, loss 0.0163241, acc 1
2016-09-07T00:33:12.043493: step 2209, loss 0.0172756, acc 0.98
2016-09-07T00:33:12.732253: step 2210, loss 0.0715989, acc 0.96
2016-09-07T00:33:13.417382: step 2211, loss 0.0303287, acc 0.98
2016-09-07T00:33:14.094519: step 2212, loss 0.0485187, acc 0.98
2016-09-07T00:33:14.796064: step 2213, loss 0.0478724, acc 0.98
2016-09-07T00:33:15.526130: step 2214, loss 0.149, acc 0.98
2016-09-07T00:33:16.215800: step 2215, loss 0.0439757, acc 0.98
2016-09-07T00:33:16.890367: step 2216, loss 0.154503, acc 0.96
2016-09-07T00:33:17.584973: step 2217, loss 0.0296988, acc 0.98
2016-09-07T00:33:18.270956: step 2218, loss 0.0292305, acc 1
2016-09-07T00:33:18.963823: step 2219, loss 0.266553, acc 0.9
2016-09-07T00:33:19.643481: step 2220, loss 0.0118019, acc 1
2016-09-07T00:33:20.340070: step 2221, loss 0.150915, acc 0.96
2016-09-07T00:33:21.022043: step 2222, loss 0.0265717, acc 1
2016-09-07T00:33:21.705394: step 2223, loss 0.0354102, acc 1
2016-09-07T00:33:22.403579: step 2224, loss 0.055441, acc 0.94
2016-09-07T00:33:23.112497: step 2225, loss 0.0711941, acc 0.98
2016-09-07T00:33:23.800691: step 2226, loss 0.0412831, acc 0.98
2016-09-07T00:33:24.500129: step 2227, loss 0.033144, acc 0.98
2016-09-07T00:33:25.242553: step 2228, loss 0.0140145, acc 1
2016-09-07T00:33:25.926734: step 2229, loss 0.0165357, acc 1
2016-09-07T00:33:26.623666: step 2230, loss 0.0399531, acc 0.98
2016-09-07T00:33:27.305857: step 2231, loss 0.092283, acc 0.94
2016-09-07T00:33:27.989440: step 2232, loss 0.0847465, acc 0.94
2016-09-07T00:33:28.690505: step 2233, loss 0.0572203, acc 0.96
2016-09-07T00:33:29.354838: step 2234, loss 0.0213591, acc 1
2016-09-07T00:33:30.072444: step 2235, loss 0.0169185, acc 0.98
2016-09-07T00:33:30.770765: step 2236, loss 0.099888, acc 0.94
2016-09-07T00:33:31.462023: step 2237, loss 0.0563951, acc 0.96
2016-09-07T00:33:32.132695: step 2238, loss 0.0218302, acc 1
2016-09-07T00:33:32.830758: step 2239, loss 0.135763, acc 0.94
2016-09-07T00:33:33.525760: step 2240, loss 0.0256866, acc 0.98
2016-09-07T00:33:34.197899: step 2241, loss 0.199792, acc 0.98
2016-09-07T00:33:34.901184: step 2242, loss 0.0295395, acc 0.98
2016-09-07T00:33:35.586928: step 2243, loss 0.115049, acc 0.96
2016-09-07T00:33:36.266353: step 2244, loss 0.127146, acc 0.96
2016-09-07T00:33:36.967597: step 2245, loss 0.0312429, acc 0.98
2016-09-07T00:33:37.664282: step 2246, loss 0.0903062, acc 0.96
2016-09-07T00:33:38.372652: step 2247, loss 0.065926, acc 0.98
2016-09-07T00:33:39.050231: step 2248, loss 0.0172276, acc 1
2016-09-07T00:33:39.746037: step 2249, loss 0.0540269, acc 0.98
2016-09-07T00:33:40.432233: step 2250, loss 0.027195, acc 1
2016-09-07T00:33:41.106105: step 2251, loss 0.0475006, acc 0.98
2016-09-07T00:33:41.783792: step 2252, loss 0.0133027, acc 1
2016-09-07T00:33:42.506586: step 2253, loss 0.0444843, acc 0.98
2016-09-07T00:33:43.197256: step 2254, loss 0.015705, acc 1
2016-09-07T00:33:43.878844: step 2255, loss 0.0359652, acc 0.98
2016-09-07T00:33:44.548751: step 2256, loss 0.0763598, acc 0.96
2016-09-07T00:33:45.245246: step 2257, loss 0.0204235, acc 1
2016-09-07T00:33:45.927544: step 2258, loss 0.0249707, acc 1
2016-09-07T00:33:46.634423: step 2259, loss 0.0480188, acc 0.98
2016-09-07T00:33:47.289755: step 2260, loss 0.0898772, acc 0.94
2016-09-07T00:33:48.003902: step 2261, loss 0.0373171, acc 0.98
2016-09-07T00:33:48.666704: step 2262, loss 0.0696201, acc 0.98
2016-09-07T00:33:49.363084: step 2263, loss 0.0642832, acc 0.96
2016-09-07T00:33:50.059046: step 2264, loss 0.0250378, acc 1
2016-09-07T00:33:50.747911: step 2265, loss 0.0137935, acc 1
2016-09-07T00:33:51.446171: step 2266, loss 0.0739856, acc 0.98
2016-09-07T00:33:52.116807: step 2267, loss 0.119631, acc 0.96
2016-09-07T00:33:52.818298: step 2268, loss 0.00608844, acc 1
2016-09-07T00:33:53.494873: step 2269, loss 0.00652426, acc 1
2016-09-07T00:33:54.194310: step 2270, loss 0.0232052, acc 1
2016-09-07T00:33:54.888394: step 2271, loss 0.00575542, acc 1
2016-09-07T00:33:55.590045: step 2272, loss 0.0545753, acc 0.98
2016-09-07T00:33:56.268036: step 2273, loss 0.00442005, acc 1
2016-09-07T00:33:56.929746: step 2274, loss 0.0418304, acc 0.98
2016-09-07T00:33:57.617987: step 2275, loss 0.0100726, acc 1
2016-09-07T00:33:58.299783: step 2276, loss 0.00984535, acc 1
2016-09-07T00:33:58.970778: step 2277, loss 0.0692988, acc 0.96
2016-09-07T00:33:59.668148: step 2278, loss 0.0363731, acc 0.98
2016-09-07T00:34:00.386354: step 2279, loss 0.0595819, acc 0.98
2016-09-07T00:34:01.069472: step 2280, loss 0.0388474, acc 0.98
2016-09-07T00:34:01.756013: step 2281, loss 0.0481661, acc 0.98
2016-09-07T00:34:02.488396: step 2282, loss 0.056072, acc 0.98
2016-09-07T00:34:03.179138: step 2283, loss 0.0451621, acc 0.98
2016-09-07T00:34:03.852185: step 2284, loss 0.0285526, acc 0.98
2016-09-07T00:34:04.538862: step 2285, loss 0.096661, acc 0.96
2016-09-07T00:34:05.225177: step 2286, loss 0.0850007, acc 0.94
2016-09-07T00:34:05.913419: step 2287, loss 0.0706582, acc 0.94
2016-09-07T00:34:06.609528: step 2288, loss 0.0353283, acc 0.96
2016-09-07T00:34:07.323195: step 2289, loss 0.0103637, acc 1
2016-09-07T00:34:08.006818: step 2290, loss 0.0240936, acc 1
2016-09-07T00:34:08.682417: step 2291, loss 0.0100953, acc 1
2016-09-07T00:34:09.374977: step 2292, loss 0.0309898, acc 0.98
2016-09-07T00:34:10.093647: step 2293, loss 0.0448864, acc 0.96
2016-09-07T00:34:10.782883: step 2294, loss 0.0137097, acc 1
2016-09-07T00:34:11.446355: step 2295, loss 0.0256233, acc 0.98
2016-09-07T00:34:12.149868: step 2296, loss 0.0488276, acc 0.98
2016-09-07T00:34:12.819176: step 2297, loss 0.0866229, acc 0.96
2016-09-07T00:34:13.514500: step 2298, loss 0.0279212, acc 1
2016-09-07T00:34:14.196741: step 2299, loss 0.0427238, acc 0.98
2016-09-07T00:34:14.865720: step 2300, loss 0.0108253, acc 1

Evaluation:
2016-09-07T00:34:18.032797: step 2300, loss 1.35351, acc 0.765478

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-2300

2016-09-07T00:34:19.678895: step 2301, loss 0.0667527, acc 0.96
2016-09-07T00:34:20.371825: step 2302, loss 0.0374878, acc 0.98
2016-09-07T00:34:21.047103: step 2303, loss 0.00215312, acc 1
2016-09-07T00:34:21.704235: step 2304, loss 0.0268368, acc 0.977273
2016-09-07T00:34:22.385835: step 2305, loss 0.0144981, acc 1
2016-09-07T00:34:23.060845: step 2306, loss 0.160293, acc 0.94
2016-09-07T00:34:23.740652: step 2307, loss 0.048635, acc 0.98
2016-09-07T00:34:24.443363: step 2308, loss 0.00614007, acc 1
2016-09-07T00:34:25.123596: step 2309, loss 0.058473, acc 0.98
2016-09-07T00:34:25.791440: step 2310, loss 0.0134695, acc 1
2016-09-07T00:34:26.491319: step 2311, loss 0.0141448, acc 1
2016-09-07T00:34:27.171117: step 2312, loss 0.0340199, acc 0.98
2016-09-07T00:34:27.849034: step 2313, loss 0.0308669, acc 1
2016-09-07T00:34:28.544932: step 2314, loss 0.0402299, acc 1
2016-09-07T00:34:29.236096: step 2315, loss 0.139591, acc 0.94
2016-09-07T00:34:29.917996: step 2316, loss 0.0390684, acc 0.98
2016-09-07T00:34:30.607646: step 2317, loss 0.0621759, acc 0.98
2016-09-07T00:34:31.307615: step 2318, loss 0.0448336, acc 0.98
2016-09-07T00:34:32.007716: step 2319, loss 0.0317992, acc 0.98
2016-09-07T00:34:32.701671: step 2320, loss 0.0300026, acc 0.98
2016-09-07T00:34:33.384813: step 2321, loss 0.031198, acc 1
2016-09-07T00:34:34.066332: step 2322, loss 0.0767444, acc 0.98
2016-09-07T00:34:34.759885: step 2323, loss 0.0625717, acc 0.96
2016-09-07T00:34:35.420017: step 2324, loss 0.0572324, acc 0.96
2016-09-07T00:34:36.109728: step 2325, loss 0.058604, acc 0.98
2016-09-07T00:34:36.788738: step 2326, loss 0.0184753, acc 0.98
2016-09-07T00:34:37.459330: step 2327, loss 0.0361165, acc 1
2016-09-07T00:34:38.174409: step 2328, loss 0.0673654, acc 0.96
2016-09-07T00:34:38.870865: step 2329, loss 0.0254441, acc 1
2016-09-07T00:34:39.540755: step 2330, loss 0.0826717, acc 0.98
2016-09-07T00:34:40.218115: step 2331, loss 0.0184779, acc 1
2016-09-07T00:34:40.951948: step 2332, loss 0.040211, acc 0.98
2016-09-07T00:34:41.666738: step 2333, loss 0.121712, acc 0.94
2016-09-07T00:34:42.342038: step 2334, loss 0.0539349, acc 0.96
2016-09-07T00:34:43.005826: step 2335, loss 0.032823, acc 1
2016-09-07T00:34:43.694688: step 2336, loss 0.105935, acc 0.94
2016-09-07T00:34:44.383631: step 2337, loss 0.0290582, acc 0.98
2016-09-07T00:34:45.062695: step 2338, loss 0.0306501, acc 1
2016-09-07T00:34:45.763250: step 2339, loss 0.0407547, acc 0.98
2016-09-07T00:34:46.454340: step 2340, loss 0.0130287, acc 1
2016-09-07T00:34:47.134584: step 2341, loss 0.0162926, acc 1
2016-09-07T00:34:47.822201: step 2342, loss 0.0710985, acc 0.96
2016-09-07T00:34:48.517205: step 2343, loss 0.0791392, acc 0.96
2016-09-07T00:34:49.214919: step 2344, loss 0.00339863, acc 1
2016-09-07T00:34:49.878458: step 2345, loss 0.0421415, acc 0.98
2016-09-07T00:34:50.600946: step 2346, loss 0.0671653, acc 0.96
2016-09-07T00:34:51.289830: step 2347, loss 0.0202594, acc 0.98
2016-09-07T00:34:51.988109: step 2348, loss 0.0316761, acc 0.98
2016-09-07T00:34:52.687535: step 2349, loss 0.0831184, acc 0.98
2016-09-07T00:34:53.372332: step 2350, loss 0.0304412, acc 0.98
2016-09-07T00:34:54.081367: step 2351, loss 0.0525287, acc 0.96
2016-09-07T00:34:54.726866: step 2352, loss 0.0545929, acc 0.94
2016-09-07T00:34:55.416021: step 2353, loss 0.039513, acc 0.98
2016-09-07T00:34:56.113285: step 2354, loss 0.15355, acc 0.94
2016-09-07T00:34:56.796431: step 2355, loss 0.0359717, acc 0.98
2016-09-07T00:34:57.521726: step 2356, loss 0.0302076, acc 0.98
2016-09-07T00:34:58.203048: step 2357, loss 0.104582, acc 0.96
2016-09-07T00:34:58.902358: step 2358, loss 0.0831779, acc 0.96
2016-09-07T00:34:59.559389: step 2359, loss 0.00771264, acc 1
2016-09-07T00:35:00.267073: step 2360, loss 0.0390875, acc 1
2016-09-07T00:35:00.968597: step 2361, loss 0.0443799, acc 0.98
2016-09-07T00:35:01.647834: step 2362, loss 0.0432617, acc 0.96
2016-09-07T00:35:02.346427: step 2363, loss 0.0741597, acc 0.98
2016-09-07T00:35:03.035240: step 2364, loss 0.0216642, acc 1
2016-09-07T00:35:03.730094: step 2365, loss 0.0575948, acc 0.98
2016-09-07T00:35:04.422001: step 2366, loss 0.042353, acc 0.96
2016-09-07T00:35:05.089239: step 2367, loss 0.0950731, acc 0.94
2016-09-07T00:35:05.759862: step 2368, loss 0.0603553, acc 0.96
2016-09-07T00:35:06.429872: step 2369, loss 0.0617054, acc 0.98
2016-09-07T00:35:07.108821: step 2370, loss 0.0524408, acc 0.98
2016-09-07T00:35:07.806485: step 2371, loss 0.0829933, acc 0.98
2016-09-07T00:35:08.528567: step 2372, loss 0.0025709, acc 1
2016-09-07T00:35:09.198373: step 2373, loss 0.142368, acc 0.98
2016-09-07T00:35:09.903085: step 2374, loss 0.0261703, acc 1
2016-09-07T00:35:10.588711: step 2375, loss 0.052132, acc 0.98
2016-09-07T00:35:11.272464: step 2376, loss 0.0402034, acc 0.98
2016-09-07T00:35:11.976140: step 2377, loss 0.0127138, acc 1
2016-09-07T00:35:12.652630: step 2378, loss 0.0181103, acc 0.98
2016-09-07T00:35:13.365392: step 2379, loss 0.0741172, acc 0.96
2016-09-07T00:35:14.055155: step 2380, loss 0.0457322, acc 0.98
2016-09-07T00:35:14.733827: step 2381, loss 0.0251319, acc 1
2016-09-07T00:35:15.429162: step 2382, loss 0.0687956, acc 0.98
2016-09-07T00:35:16.119496: step 2383, loss 0.0101814, acc 1
2016-09-07T00:35:16.800555: step 2384, loss 0.0628062, acc 0.96
2016-09-07T00:35:17.475614: step 2385, loss 0.206058, acc 0.92
2016-09-07T00:35:18.186119: step 2386, loss 0.029545, acc 1
2016-09-07T00:35:18.872064: step 2387, loss 0.0290175, acc 0.98
2016-09-07T00:35:19.556862: step 2388, loss 0.0109962, acc 1
2016-09-07T00:35:20.239079: step 2389, loss 0.0256391, acc 1
2016-09-07T00:35:20.908769: step 2390, loss 0.0254899, acc 1
2016-09-07T00:35:21.587423: step 2391, loss 0.026034, acc 0.98
2016-09-07T00:35:22.268016: step 2392, loss 0.0908552, acc 0.94
2016-09-07T00:35:22.974259: step 2393, loss 0.0951434, acc 0.98
2016-09-07T00:35:23.658653: step 2394, loss 0.0561446, acc 0.98
2016-09-07T00:35:24.344781: step 2395, loss 0.14817, acc 0.98
2016-09-07T00:35:25.033060: step 2396, loss 0.0134669, acc 1
2016-09-07T00:35:25.724281: step 2397, loss 0.0323419, acc 1
2016-09-07T00:35:26.408053: step 2398, loss 0.050537, acc 0.96
2016-09-07T00:35:27.085710: step 2399, loss 0.0584099, acc 0.98
2016-09-07T00:35:27.802644: step 2400, loss 0.0772686, acc 0.96

Evaluation:
2016-09-07T00:35:30.965709: step 2400, loss 1.28327, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-2400

2016-09-07T00:35:32.670269: step 2401, loss 0.0727763, acc 0.96
2016-09-07T00:35:33.350073: step 2402, loss 0.0945531, acc 0.98
2016-09-07T00:35:34.041903: step 2403, loss 0.00632452, acc 1
2016-09-07T00:35:34.726295: step 2404, loss 0.154455, acc 0.96
2016-09-07T00:35:35.409321: step 2405, loss 0.0623432, acc 0.96
2016-09-07T00:35:36.078930: step 2406, loss 0.0122708, acc 1
2016-09-07T00:35:36.736184: step 2407, loss 0.03072, acc 0.98
2016-09-07T00:35:37.459475: step 2408, loss 0.10837, acc 0.98
2016-09-07T00:35:38.149800: step 2409, loss 0.0645603, acc 0.98
2016-09-07T00:35:38.836321: step 2410, loss 0.030584, acc 0.98
2016-09-07T00:35:39.521182: step 2411, loss 0.0184159, acc 1
2016-09-07T00:35:40.204020: step 2412, loss 0.0482636, acc 0.98
2016-09-07T00:35:40.897351: step 2413, loss 0.0737618, acc 0.96
2016-09-07T00:35:41.594095: step 2414, loss 0.0397606, acc 1
2016-09-07T00:35:42.311222: step 2415, loss 0.0877028, acc 0.98
2016-09-07T00:35:43.000981: step 2416, loss 0.0443157, acc 0.98
2016-09-07T00:35:43.693783: step 2417, loss 0.0105536, acc 1
2016-09-07T00:35:44.395901: step 2418, loss 0.0184188, acc 1
2016-09-07T00:35:45.104656: step 2419, loss 0.0529763, acc 0.98
2016-09-07T00:35:45.828226: step 2420, loss 0.0653124, acc 0.96
2016-09-07T00:35:46.518728: step 2421, loss 0.0579919, acc 0.96
2016-09-07T00:35:47.210831: step 2422, loss 0.00800483, acc 1
2016-09-07T00:35:47.894833: step 2423, loss 0.0245288, acc 1
2016-09-07T00:35:48.591196: step 2424, loss 0.0456945, acc 0.98
2016-09-07T00:35:49.276668: step 2425, loss 0.0432918, acc 1
2016-09-07T00:35:49.960072: step 2426, loss 0.0681631, acc 0.96
2016-09-07T00:35:50.665299: step 2427, loss 0.0147891, acc 1
2016-09-07T00:35:51.349381: step 2428, loss 0.0631723, acc 0.96
2016-09-07T00:35:52.032823: step 2429, loss 0.0748894, acc 0.96
2016-09-07T00:35:52.707108: step 2430, loss 0.0377155, acc 0.98
2016-09-07T00:35:53.379673: step 2431, loss 0.0817709, acc 0.96
2016-09-07T00:35:54.065763: step 2432, loss 0.0710749, acc 0.96
2016-09-07T00:35:54.739466: step 2433, loss 0.0289105, acc 0.98
2016-09-07T00:35:55.440567: step 2434, loss 0.0499401, acc 0.98
2016-09-07T00:35:56.125760: step 2435, loss 0.134544, acc 0.9
2016-09-07T00:35:56.851469: step 2436, loss 0.0532401, acc 0.98
2016-09-07T00:35:57.536901: step 2437, loss 0.0810087, acc 0.96
2016-09-07T00:35:58.237114: step 2438, loss 0.0643542, acc 0.98
2016-09-07T00:35:58.941595: step 2439, loss 0.00934681, acc 1
2016-09-07T00:35:59.595308: step 2440, loss 0.129673, acc 0.96
2016-09-07T00:36:00.327816: step 2441, loss 0.0335769, acc 0.98
2016-09-07T00:36:00.987256: step 2442, loss 0.0188989, acc 1
2016-09-07T00:36:01.659624: step 2443, loss 0.0262121, acc 0.98
2016-09-07T00:36:02.353336: step 2444, loss 0.0408879, acc 0.98
2016-09-07T00:36:03.027598: step 2445, loss 0.0344993, acc 1
2016-09-07T00:36:03.741299: step 2446, loss 0.0352113, acc 0.98
2016-09-07T00:36:04.420117: step 2447, loss 0.0286966, acc 1
2016-09-07T00:36:05.119024: step 2448, loss 0.0113276, acc 1
2016-09-07T00:36:05.781369: step 2449, loss 0.0602537, acc 0.96
2016-09-07T00:36:06.465726: step 2450, loss 0.0156408, acc 1
2016-09-07T00:36:07.146738: step 2451, loss 0.0166346, acc 0.98
2016-09-07T00:36:07.813946: step 2452, loss 0.108875, acc 0.92
2016-09-07T00:36:08.491273: step 2453, loss 0.0419686, acc 0.98
2016-09-07T00:36:09.174008: step 2454, loss 0.0330017, acc 0.98
2016-09-07T00:36:09.876022: step 2455, loss 0.0922743, acc 0.92
2016-09-07T00:36:10.544249: step 2456, loss 0.0287047, acc 1
2016-09-07T00:36:11.219127: step 2457, loss 0.00846103, acc 1
2016-09-07T00:36:11.902840: step 2458, loss 0.0253435, acc 0.98
2016-09-07T00:36:12.579405: step 2459, loss 0.177759, acc 0.94
2016-09-07T00:36:13.256088: step 2460, loss 0.0171047, acc 1
2016-09-07T00:36:13.942282: step 2461, loss 0.00549534, acc 1
2016-09-07T00:36:14.622018: step 2462, loss 0.0710332, acc 0.98
2016-09-07T00:36:15.301277: step 2463, loss 0.00422689, acc 1
2016-09-07T00:36:16.002547: step 2464, loss 0.0618211, acc 0.96
2016-09-07T00:36:16.665157: step 2465, loss 0.011417, acc 1
2016-09-07T00:36:17.345128: step 2466, loss 0.0198049, acc 0.98
2016-09-07T00:36:18.036158: step 2467, loss 0.0229756, acc 1
2016-09-07T00:36:18.724614: step 2468, loss 0.0319142, acc 0.98
2016-09-07T00:36:19.447387: step 2469, loss 0.0288191, acc 1
2016-09-07T00:36:20.132871: step 2470, loss 0.0562834, acc 0.96
2016-09-07T00:36:20.835812: step 2471, loss 0.00423092, acc 1
2016-09-07T00:36:21.519947: step 2472, loss 0.0221121, acc 1
2016-09-07T00:36:22.218451: step 2473, loss 0.0239653, acc 0.98
2016-09-07T00:36:22.889541: step 2474, loss 0.00773128, acc 1
2016-09-07T00:36:23.575819: step 2475, loss 0.00958654, acc 1
2016-09-07T00:36:24.266995: step 2476, loss 0.0137536, acc 1
2016-09-07T00:36:24.935827: step 2477, loss 0.00752821, acc 1
2016-09-07T00:36:25.654869: step 2478, loss 0.146282, acc 0.92
2016-09-07T00:36:26.348726: step 2479, loss 0.030614, acc 0.98
2016-09-07T00:36:27.036180: step 2480, loss 0.165979, acc 0.96
2016-09-07T00:36:27.724795: step 2481, loss 0.00757122, acc 1
2016-09-07T00:36:28.418352: step 2482, loss 0.0994743, acc 0.96
2016-09-07T00:36:29.166658: step 2483, loss 0.267644, acc 0.96
2016-09-07T00:36:29.843927: step 2484, loss 0.0231242, acc 1
2016-09-07T00:36:30.509847: step 2485, loss 0.0289005, acc 0.98
2016-09-07T00:36:31.195725: step 2486, loss 0.0892265, acc 0.96
2016-09-07T00:36:31.891789: step 2487, loss 0.00140656, acc 1
2016-09-07T00:36:32.580872: step 2488, loss 0.0236849, acc 1
2016-09-07T00:36:33.236842: step 2489, loss 0.00906412, acc 1
2016-09-07T00:36:33.925862: step 2490, loss 0.0996636, acc 0.94
2016-09-07T00:36:34.606276: step 2491, loss 0.0746794, acc 0.96
2016-09-07T00:36:35.279514: step 2492, loss 0.0465803, acc 0.98
2016-09-07T00:36:35.948806: step 2493, loss 0.0616633, acc 0.96
2016-09-07T00:36:36.641803: step 2494, loss 0.0435511, acc 0.98
2016-09-07T00:36:37.310011: step 2495, loss 0.0303891, acc 0.98
2016-09-07T00:36:37.952546: step 2496, loss 0.0193353, acc 1
2016-09-07T00:36:38.652496: step 2497, loss 0.0549137, acc 0.96
2016-09-07T00:36:39.332047: step 2498, loss 0.0303343, acc 0.98
2016-09-07T00:36:40.023464: step 2499, loss 0.0657937, acc 0.96
2016-09-07T00:36:40.709052: step 2500, loss 0.120774, acc 0.96

Evaluation:
2016-09-07T00:36:43.855102: step 2500, loss 1.40426, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-2500

2016-09-07T00:36:45.523327: step 2501, loss 0.0966034, acc 0.94
2016-09-07T00:36:46.215742: step 2502, loss 0.0424723, acc 0.98
2016-09-07T00:36:46.897112: step 2503, loss 0.0148112, acc 1
2016-09-07T00:36:47.583791: step 2504, loss 0.0111439, acc 1
2016-09-07T00:36:48.298024: step 2505, loss 0.0789967, acc 0.94
2016-09-07T00:36:48.964186: step 2506, loss 0.0233995, acc 0.98
2016-09-07T00:36:49.657676: step 2507, loss 0.0162502, acc 1
2016-09-07T00:36:50.346915: step 2508, loss 0.0157189, acc 1
2016-09-07T00:36:51.048156: step 2509, loss 0.0308606, acc 1
2016-09-07T00:36:51.722432: step 2510, loss 0.0217526, acc 1
2016-09-07T00:36:52.415808: step 2511, loss 0.0279022, acc 0.98
2016-09-07T00:36:53.119958: step 2512, loss 0.0336925, acc 0.98
2016-09-07T00:36:53.781852: step 2513, loss 0.0142231, acc 1
2016-09-07T00:36:54.475317: step 2514, loss 0.0144493, acc 1
2016-09-07T00:36:55.164488: step 2515, loss 0.0482446, acc 0.98
2016-09-07T00:36:55.853194: step 2516, loss 0.0609827, acc 0.94
2016-09-07T00:36:56.551893: step 2517, loss 0.0332941, acc 0.98
2016-09-07T00:36:57.230771: step 2518, loss 0.0239189, acc 1
2016-09-07T00:36:57.931799: step 2519, loss 0.0223955, acc 0.98
2016-09-07T00:36:58.620013: step 2520, loss 0.098588, acc 0.98
2016-09-07T00:36:59.313841: step 2521, loss 0.0155428, acc 1
2016-09-07T00:37:00.002450: step 2522, loss 0.00392002, acc 1
2016-09-07T00:37:00.720762: step 2523, loss 0.0117937, acc 1
2016-09-07T00:37:01.393133: step 2524, loss 0.0134139, acc 1
2016-09-07T00:37:02.081866: step 2525, loss 0.0104672, acc 1
2016-09-07T00:37:02.801604: step 2526, loss 0.126793, acc 0.96
2016-09-07T00:37:03.507014: step 2527, loss 0.0570793, acc 0.98
2016-09-07T00:37:04.207992: step 2528, loss 0.0873463, acc 0.96
2016-09-07T00:37:04.895181: step 2529, loss 0.0197172, acc 1
2016-09-07T00:37:05.574076: step 2530, loss 0.0899415, acc 0.96
2016-09-07T00:37:06.258096: step 2531, loss 0.0141997, acc 1
2016-09-07T00:37:06.940020: step 2532, loss 0.0755047, acc 0.98
2016-09-07T00:37:07.664792: step 2533, loss 0.0236643, acc 1
2016-09-07T00:37:08.346672: step 2534, loss 0.0322101, acc 1
2016-09-07T00:37:09.032870: step 2535, loss 0.036332, acc 0.98
2016-09-07T00:37:09.733241: step 2536, loss 0.0162305, acc 1
2016-09-07T00:37:10.403726: step 2537, loss 0.0700351, acc 0.98
2016-09-07T00:37:11.097265: step 2538, loss 0.0208358, acc 1
2016-09-07T00:37:11.801418: step 2539, loss 0.0228505, acc 0.98
2016-09-07T00:37:12.496770: step 2540, loss 0.00790406, acc 1
2016-09-07T00:37:13.171890: step 2541, loss 0.0851614, acc 0.96
2016-09-07T00:37:13.849113: step 2542, loss 0.0202191, acc 1
2016-09-07T00:37:14.538515: step 2543, loss 0.00252679, acc 1
2016-09-07T00:37:15.222103: step 2544, loss 0.0860965, acc 0.96
2016-09-07T00:37:15.923578: step 2545, loss 0.0230456, acc 0.98
2016-09-07T00:37:16.607867: step 2546, loss 0.0342636, acc 0.98
2016-09-07T00:37:17.314335: step 2547, loss 0.0244447, acc 1
2016-09-07T00:37:18.001846: step 2548, loss 0.0249201, acc 0.98
2016-09-07T00:37:18.699707: step 2549, loss 0.00812858, acc 1
2016-09-07T00:37:19.389285: step 2550, loss 0.0136338, acc 1
2016-09-07T00:37:20.073711: step 2551, loss 0.09862, acc 0.94
2016-09-07T00:37:20.751288: step 2552, loss 0.0547419, acc 0.98
2016-09-07T00:37:21.420588: step 2553, loss 0.0436328, acc 0.98
2016-09-07T00:37:22.105240: step 2554, loss 0.0233197, acc 1
2016-09-07T00:37:22.777119: step 2555, loss 0.0276208, acc 0.98
2016-09-07T00:37:23.464180: step 2556, loss 0.0624553, acc 0.98
2016-09-07T00:37:24.145870: step 2557, loss 0.106153, acc 0.92
2016-09-07T00:37:24.830005: step 2558, loss 0.0477266, acc 0.98
2016-09-07T00:37:25.544137: step 2559, loss 0.0617322, acc 0.94
2016-09-07T00:37:26.198868: step 2560, loss 0.0348715, acc 0.98
2016-09-07T00:37:26.897031: step 2561, loss 0.0212784, acc 1
2016-09-07T00:37:27.567547: step 2562, loss 0.0500718, acc 0.98
2016-09-07T00:37:28.247984: step 2563, loss 0.0898296, acc 0.96
2016-09-07T00:37:28.915092: step 2564, loss 0.00650395, acc 1
2016-09-07T00:37:29.629716: step 2565, loss 0.0187077, acc 1
2016-09-07T00:37:30.333058: step 2566, loss 0.00704981, acc 1
2016-09-07T00:37:31.010890: step 2567, loss 0.00889344, acc 1
2016-09-07T00:37:31.719097: step 2568, loss 0.00811252, acc 1
2016-09-07T00:37:32.382410: step 2569, loss 0.00159366, acc 1
2016-09-07T00:37:33.060396: step 2570, loss 0.015025, acc 1
2016-09-07T00:37:33.763260: step 2571, loss 0.116359, acc 0.96
2016-09-07T00:37:34.449675: step 2572, loss 0.0648952, acc 0.98
2016-09-07T00:37:35.138168: step 2573, loss 0.113224, acc 0.94
2016-09-07T00:37:35.810564: step 2574, loss 0.086232, acc 0.94
2016-09-07T00:37:36.534374: step 2575, loss 0.111098, acc 0.96
2016-09-07T00:37:37.218262: step 2576, loss 0.074075, acc 0.94
2016-09-07T00:37:37.922611: step 2577, loss 0.0546449, acc 0.96
2016-09-07T00:37:38.617597: step 2578, loss 0.0103689, acc 1
2016-09-07T00:37:39.311903: step 2579, loss 0.0505844, acc 0.96
2016-09-07T00:37:40.029360: step 2580, loss 0.0241711, acc 1
2016-09-07T00:37:40.700330: step 2581, loss 0.0121488, acc 1
2016-09-07T00:37:41.398310: step 2582, loss 0.0642434, acc 0.96
2016-09-07T00:37:42.074183: step 2583, loss 0.0241644, acc 1
2016-09-07T00:37:42.770949: step 2584, loss 0.0224021, acc 0.98
2016-09-07T00:37:43.464706: step 2585, loss 0.0818775, acc 0.96
2016-09-07T00:37:44.177840: step 2586, loss 0.0370681, acc 0.98
2016-09-07T00:37:44.881388: step 2587, loss 0.0114855, acc 1
2016-09-07T00:37:45.556251: step 2588, loss 0.0441539, acc 0.98
2016-09-07T00:37:46.228753: step 2589, loss 0.147762, acc 0.98
2016-09-07T00:37:46.900278: step 2590, loss 0.00962853, acc 1
2016-09-07T00:37:47.590388: step 2591, loss 0.0883314, acc 0.96
2016-09-07T00:37:48.300563: step 2592, loss 0.0140191, acc 1
2016-09-07T00:37:48.981412: step 2593, loss 0.0481993, acc 1
2016-09-07T00:37:49.679689: step 2594, loss 0.0525083, acc 0.96
2016-09-07T00:37:50.350642: step 2595, loss 0.0202184, acc 0.98
2016-09-07T00:37:51.035793: step 2596, loss 0.0368173, acc 0.98
2016-09-07T00:37:51.703909: step 2597, loss 0.0624484, acc 0.98
2016-09-07T00:37:52.397958: step 2598, loss 0.0823993, acc 0.98
2016-09-07T00:37:53.086120: step 2599, loss 0.032538, acc 0.98
2016-09-07T00:37:53.789027: step 2600, loss 0.0380282, acc 1

Evaluation:
2016-09-07T00:37:56.924084: step 2600, loss 1.39811, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-2600

2016-09-07T00:37:58.622367: step 2601, loss 0.0452362, acc 0.98
2016-09-07T00:37:59.328756: step 2602, loss 0.0203191, acc 1
2016-09-07T00:38:00.000676: step 2603, loss 0.0224814, acc 1
2016-09-07T00:38:00.708987: step 2604, loss 0.0498258, acc 0.98
2016-09-07T00:38:01.392083: step 2605, loss 0.0412855, acc 0.98
2016-09-07T00:38:02.070411: step 2606, loss 0.0362744, acc 0.96
2016-09-07T00:38:02.759720: step 2607, loss 0.123635, acc 0.96
2016-09-07T00:38:03.447223: step 2608, loss 0.0324849, acc 0.98
2016-09-07T00:38:04.148014: step 2609, loss 0.0374522, acc 0.98
2016-09-07T00:38:04.822987: step 2610, loss 0.0511084, acc 0.96
2016-09-07T00:38:05.511124: step 2611, loss 0.033313, acc 0.98
2016-09-07T00:38:06.218401: step 2612, loss 0.0786176, acc 0.98
2016-09-07T00:38:06.917687: step 2613, loss 0.0277495, acc 1
2016-09-07T00:38:07.603712: step 2614, loss 0.0711815, acc 0.98
2016-09-07T00:38:08.278612: step 2615, loss 0.0328237, acc 1
2016-09-07T00:38:08.977568: step 2616, loss 0.0262078, acc 1
2016-09-07T00:38:09.670763: step 2617, loss 0.0261907, acc 1
2016-09-07T00:38:10.341065: step 2618, loss 0.00714311, acc 1
2016-09-07T00:38:11.016788: step 2619, loss 0.0240701, acc 1
2016-09-07T00:38:11.716153: step 2620, loss 0.038309, acc 0.98
2016-09-07T00:38:12.423086: step 2621, loss 0.021062, acc 1
2016-09-07T00:38:13.082533: step 2622, loss 0.148782, acc 0.92
2016-09-07T00:38:13.773814: step 2623, loss 0.0257559, acc 0.98
2016-09-07T00:38:14.468457: step 2624, loss 0.108645, acc 0.96
2016-09-07T00:38:15.147846: step 2625, loss 0.0137326, acc 1
2016-09-07T00:38:15.833639: step 2626, loss 0.0441632, acc 0.98
2016-09-07T00:38:16.509988: step 2627, loss 0.121325, acc 0.94
2016-09-07T00:38:17.217063: step 2628, loss 0.0453295, acc 0.98
2016-09-07T00:38:17.864985: step 2629, loss 0.0788344, acc 0.98
2016-09-07T00:38:18.576530: step 2630, loss 0.0523764, acc 0.98
2016-09-07T00:38:19.254381: step 2631, loss 0.00413579, acc 1
2016-09-07T00:38:19.942851: step 2632, loss 0.000453241, acc 1
2016-09-07T00:38:20.638635: step 2633, loss 0.0482969, acc 0.98
2016-09-07T00:38:21.330694: step 2634, loss 0.0928989, acc 0.96
2016-09-07T00:38:22.013456: step 2635, loss 0.0182218, acc 1
2016-09-07T00:38:22.675447: step 2636, loss 0.00420419, acc 1
2016-09-07T00:38:23.387206: step 2637, loss 0.0516732, acc 0.98
2016-09-07T00:38:24.052184: step 2638, loss 0.0116368, acc 1
2016-09-07T00:38:24.722738: step 2639, loss 0.0195907, acc 0.98
2016-09-07T00:38:25.426280: step 2640, loss 0.0425491, acc 0.98
2016-09-07T00:38:26.108324: step 2641, loss 0.0308995, acc 1
2016-09-07T00:38:26.795392: step 2642, loss 0.00985302, acc 1
2016-09-07T00:38:27.492238: step 2643, loss 0.0335202, acc 0.98
2016-09-07T00:38:28.212566: step 2644, loss 0.0994348, acc 0.94
2016-09-07T00:38:28.912556: step 2645, loss 0.0394213, acc 1
2016-09-07T00:38:29.610590: step 2646, loss 0.0702221, acc 0.96
2016-09-07T00:38:30.295111: step 2647, loss 0.00723991, acc 1
2016-09-07T00:38:30.984670: step 2648, loss 0.0743119, acc 0.98
2016-09-07T00:38:31.671971: step 2649, loss 0.0549105, acc 0.98
2016-09-07T00:38:32.337025: step 2650, loss 0.0375722, acc 1
2016-09-07T00:38:33.049941: step 2651, loss 0.0461448, acc 0.98
2016-09-07T00:38:33.746079: step 2652, loss 0.080022, acc 0.96
2016-09-07T00:38:34.428611: step 2653, loss 0.150411, acc 0.96
2016-09-07T00:38:35.112452: step 2654, loss 0.0782446, acc 0.98
2016-09-07T00:38:35.816184: step 2655, loss 0.00716585, acc 1
2016-09-07T00:38:36.523974: step 2656, loss 0.0389244, acc 0.98
2016-09-07T00:38:37.206602: step 2657, loss 0.0248565, acc 0.98
2016-09-07T00:38:37.913609: step 2658, loss 0.042409, acc 0.96
2016-09-07T00:38:38.597959: step 2659, loss 0.0610328, acc 0.96
2016-09-07T00:38:39.292720: step 2660, loss 0.00652085, acc 1
2016-09-07T00:38:39.991478: step 2661, loss 0.185046, acc 0.94
2016-09-07T00:38:40.691989: step 2662, loss 0.163333, acc 0.96
2016-09-07T00:38:41.396573: step 2663, loss 0.00575418, acc 1
2016-09-07T00:38:42.076870: step 2664, loss 0.0158731, acc 1
2016-09-07T00:38:42.826347: step 2665, loss 0.0217295, acc 1
2016-09-07T00:38:43.522813: step 2666, loss 0.0884272, acc 0.96
2016-09-07T00:38:44.197257: step 2667, loss 0.0504521, acc 0.98
2016-09-07T00:38:44.888890: step 2668, loss 0.0582989, acc 0.98
2016-09-07T00:38:45.547833: step 2669, loss 0.0393218, acc 0.98
2016-09-07T00:38:46.242379: step 2670, loss 0.0591427, acc 1
2016-09-07T00:38:46.919182: step 2671, loss 0.0244712, acc 0.98
2016-09-07T00:38:47.591497: step 2672, loss 0.0886921, acc 0.98
2016-09-07T00:38:48.315368: step 2673, loss 0.046085, acc 0.98
2016-09-07T00:38:49.000457: step 2674, loss 0.121028, acc 0.96
2016-09-07T00:38:49.686094: step 2675, loss 0.036357, acc 0.98
2016-09-07T00:38:50.362462: step 2676, loss 0.0362386, acc 0.98
2016-09-07T00:38:51.084572: step 2677, loss 0.0435303, acc 0.96
2016-09-07T00:38:51.769086: step 2678, loss 0.0393992, acc 0.98
2016-09-07T00:38:52.446177: step 2679, loss 0.0318316, acc 0.98
2016-09-07T00:38:53.132970: step 2680, loss 0.108562, acc 0.96
2016-09-07T00:38:53.842683: step 2681, loss 0.0200749, acc 0.98
2016-09-07T00:38:54.541276: step 2682, loss 0.0832292, acc 0.96
2016-09-07T00:38:55.223036: step 2683, loss 0.0428821, acc 0.98
2016-09-07T00:38:55.927873: step 2684, loss 0.0108875, acc 1
2016-09-07T00:38:56.640422: step 2685, loss 0.10097, acc 0.98
2016-09-07T00:38:57.328857: step 2686, loss 0.0357815, acc 1
2016-09-07T00:38:58.010969: step 2687, loss 0.0289124, acc 1
2016-09-07T00:38:58.668570: step 2688, loss 0.0136977, acc 1
2016-09-07T00:38:59.363994: step 2689, loss 0.126397, acc 0.94
2016-09-07T00:39:00.032350: step 2690, loss 0.0727747, acc 0.98
2016-09-07T00:39:00.767926: step 2691, loss 0.0178303, acc 1
2016-09-07T00:39:01.473427: step 2692, loss 0.020443, acc 1
2016-09-07T00:39:02.170643: step 2693, loss 0.00277324, acc 1
2016-09-07T00:39:02.859395: step 2694, loss 0.0658117, acc 0.96
2016-09-07T00:39:03.589401: step 2695, loss 0.0665248, acc 0.98
2016-09-07T00:39:04.283882: step 2696, loss 0.0408461, acc 0.96
2016-09-07T00:39:04.962761: step 2697, loss 0.0104881, acc 1
2016-09-07T00:39:05.652879: step 2698, loss 0.0558547, acc 0.96
2016-09-07T00:39:06.338569: step 2699, loss 0.0424293, acc 0.98
2016-09-07T00:39:07.011925: step 2700, loss 0.0115788, acc 1

Evaluation:
2016-09-07T00:39:10.154199: step 2700, loss 1.43261, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-2700

2016-09-07T00:39:11.806371: step 2701, loss 0.0593191, acc 0.98
2016-09-07T00:39:12.495694: step 2702, loss 0.0356236, acc 0.96
2016-09-07T00:39:13.198451: step 2703, loss 0.0129456, acc 1
2016-09-07T00:39:13.911282: step 2704, loss 0.0180916, acc 1
2016-09-07T00:39:14.617459: step 2705, loss 0.0392512, acc 0.98
2016-09-07T00:39:15.314743: step 2706, loss 0.0354011, acc 0.98
2016-09-07T00:39:16.016432: step 2707, loss 0.0763039, acc 0.96
2016-09-07T00:39:16.708510: step 2708, loss 0.0367674, acc 0.98
2016-09-07T00:39:17.388748: step 2709, loss 0.0137264, acc 1
2016-09-07T00:39:18.063963: step 2710, loss 0.0684917, acc 0.98
2016-09-07T00:39:18.767893: step 2711, loss 0.0409566, acc 0.96
2016-09-07T00:39:19.437507: step 2712, loss 0.0419721, acc 0.98
2016-09-07T00:39:20.105022: step 2713, loss 0.0729864, acc 0.96
2016-09-07T00:39:20.787632: step 2714, loss 0.0581559, acc 0.96
2016-09-07T00:39:21.469339: step 2715, loss 0.00697049, acc 1
2016-09-07T00:39:22.158382: step 2716, loss 0.00733102, acc 1
2016-09-07T00:39:22.815162: step 2717, loss 0.2214, acc 0.96
2016-09-07T00:39:23.524615: step 2718, loss 0.0351867, acc 0.98
2016-09-07T00:39:24.209949: step 2719, loss 0.0748576, acc 0.98
2016-09-07T00:39:24.876666: step 2720, loss 0.095325, acc 0.94
2016-09-07T00:39:25.575253: step 2721, loss 0.0482015, acc 0.98
2016-09-07T00:39:26.250105: step 2722, loss 0.0811862, acc 0.94
2016-09-07T00:39:26.932228: step 2723, loss 0.0185687, acc 1
2016-09-07T00:39:27.599327: step 2724, loss 0.0481234, acc 1
2016-09-07T00:39:28.302822: step 2725, loss 0.0193846, acc 1
2016-09-07T00:39:28.996046: step 2726, loss 0.0118692, acc 1
2016-09-07T00:39:29.687296: step 2727, loss 0.0462881, acc 0.96
2016-09-07T00:39:30.377587: step 2728, loss 0.00926568, acc 1
2016-09-07T00:39:31.064168: step 2729, loss 0.0430937, acc 0.96
2016-09-07T00:39:31.744249: step 2730, loss 0.0345535, acc 1
2016-09-07T00:39:32.421933: step 2731, loss 0.00698384, acc 1
2016-09-07T00:39:33.123124: step 2732, loss 0.0209645, acc 1
2016-09-07T00:39:33.785283: step 2733, loss 0.0754666, acc 0.98
2016-09-07T00:39:34.499376: step 2734, loss 0.0359989, acc 1
2016-09-07T00:39:35.186955: step 2735, loss 0.0495311, acc 0.98
2016-09-07T00:39:35.912723: step 2736, loss 0.104233, acc 0.96
2016-09-07T00:39:36.611403: step 2737, loss 0.00837461, acc 1
2016-09-07T00:39:37.284758: step 2738, loss 0.0550794, acc 0.96
2016-09-07T00:39:38.005871: step 2739, loss 0.0858345, acc 0.98
2016-09-07T00:39:38.698226: step 2740, loss 0.0188814, acc 1
2016-09-07T00:39:39.377835: step 2741, loss 0.0151829, acc 1
2016-09-07T00:39:40.066748: step 2742, loss 0.00174228, acc 1
2016-09-07T00:39:40.745077: step 2743, loss 0.0756264, acc 0.98
2016-09-07T00:39:41.458246: step 2744, loss 0.0290096, acc 1
2016-09-07T00:39:42.132312: step 2745, loss 0.0812068, acc 0.94
2016-09-07T00:39:42.850110: step 2746, loss 0.0210372, acc 1
2016-09-07T00:39:43.539129: step 2747, loss 0.0212737, acc 0.98
2016-09-07T00:39:44.228654: step 2748, loss 0.0380334, acc 0.98
2016-09-07T00:39:44.914915: step 2749, loss 0.0464476, acc 0.96
2016-09-07T00:39:45.616140: step 2750, loss 0.0672352, acc 0.98
2016-09-07T00:39:46.314987: step 2751, loss 0.0351904, acc 0.98
2016-09-07T00:39:46.985692: step 2752, loss 0.0386701, acc 0.98
2016-09-07T00:39:47.694697: step 2753, loss 0.0446098, acc 0.98
2016-09-07T00:39:48.390403: step 2754, loss 0.228921, acc 0.94
2016-09-07T00:39:49.089179: step 2755, loss 0.020303, acc 0.98
2016-09-07T00:39:49.795502: step 2756, loss 0.283506, acc 0.98
2016-09-07T00:39:50.455940: step 2757, loss 0.0848388, acc 0.98
2016-09-07T00:39:51.162845: step 2758, loss 0.0466805, acc 0.98
2016-09-07T00:39:51.811597: step 2759, loss 0.00302568, acc 1
2016-09-07T00:39:52.482372: step 2760, loss 0.0472973, acc 0.98
2016-09-07T00:39:53.142546: step 2761, loss 0.0073554, acc 1
2016-09-07T00:39:53.814355: step 2762, loss 0.00306824, acc 1
2016-09-07T00:39:54.480104: step 2763, loss 0.0364017, acc 0.98
2016-09-07T00:39:55.158980: step 2764, loss 0.0408688, acc 0.98
2016-09-07T00:39:55.864483: step 2765, loss 0.0308964, acc 0.98
2016-09-07T00:39:56.538981: step 2766, loss 0.0050862, acc 1
2016-09-07T00:39:57.235370: step 2767, loss 0.0720637, acc 0.98
2016-09-07T00:39:57.905316: step 2768, loss 0.0233663, acc 1
2016-09-07T00:39:58.579686: step 2769, loss 0.130314, acc 0.98
2016-09-07T00:39:59.268129: step 2770, loss 0.0274695, acc 1
2016-09-07T00:39:59.969243: step 2771, loss 0.00330751, acc 1
2016-09-07T00:40:00.689671: step 2772, loss 0.0143332, acc 1
2016-09-07T00:40:01.357761: step 2773, loss 0.00672421, acc 1
2016-09-07T00:40:02.048473: step 2774, loss 0.0613988, acc 0.96
2016-09-07T00:40:02.737445: step 2775, loss 0.0136754, acc 1
2016-09-07T00:40:03.418103: step 2776, loss 0.0214764, acc 0.98
2016-09-07T00:40:04.097718: step 2777, loss 0.0823125, acc 0.96
2016-09-07T00:40:04.775673: step 2778, loss 0.0954798, acc 0.96
2016-09-07T00:40:05.463020: step 2779, loss 0.0664259, acc 0.98
2016-09-07T00:40:06.141722: step 2780, loss 0.0242307, acc 1
2016-09-07T00:40:06.834905: step 2781, loss 0.148756, acc 0.96
2016-09-07T00:40:07.507334: step 2782, loss 0.0446066, acc 0.98
2016-09-07T00:40:08.171426: step 2783, loss 0.0620747, acc 0.96
2016-09-07T00:40:08.843101: step 2784, loss 0.0451823, acc 0.98
2016-09-07T00:40:09.517928: step 2785, loss 0.037987, acc 0.98
2016-09-07T00:40:10.195728: step 2786, loss 0.0817447, acc 0.96
2016-09-07T00:40:10.899269: step 2787, loss 0.0326686, acc 0.98
2016-09-07T00:40:11.611309: step 2788, loss 0.0274089, acc 0.98
2016-09-07T00:40:12.301286: step 2789, loss 0.0252221, acc 1
2016-09-07T00:40:12.975228: step 2790, loss 0.0321343, acc 0.98
2016-09-07T00:40:13.665240: step 2791, loss 0.0105784, acc 1
2016-09-07T00:40:14.362448: step 2792, loss 0.199511, acc 0.9
2016-09-07T00:40:15.052255: step 2793, loss 0.0296922, acc 0.98
2016-09-07T00:40:15.755406: step 2794, loss 0.0883279, acc 0.98
2016-09-07T00:40:16.452501: step 2795, loss 0.0862891, acc 0.98
2016-09-07T00:40:17.117931: step 2796, loss 0.0494203, acc 0.96
2016-09-07T00:40:17.802217: step 2797, loss 0.00529289, acc 1
2016-09-07T00:40:18.475161: step 2798, loss 0.105176, acc 0.94
2016-09-07T00:40:19.143044: step 2799, loss 0.132013, acc 0.92
2016-09-07T00:40:19.833517: step 2800, loss 0.00786533, acc 1

Evaluation:
2016-09-07T00:40:23.002471: step 2800, loss 1.57646, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-2800

2016-09-07T00:40:24.647160: step 2801, loss 0.0567465, acc 0.96
2016-09-07T00:40:25.338501: step 2802, loss 0.0173566, acc 1
2016-09-07T00:40:26.044432: step 2803, loss 0.0189267, acc 1
2016-09-07T00:40:26.710905: step 2804, loss 0.0323306, acc 0.98
2016-09-07T00:40:27.420896: step 2805, loss 0.00292895, acc 1
2016-09-07T00:40:28.107178: step 2806, loss 0.00966554, acc 1
2016-09-07T00:40:28.794321: step 2807, loss 0.0277885, acc 1
2016-09-07T00:40:29.480780: step 2808, loss 0.0712174, acc 0.98
2016-09-07T00:40:30.185115: step 2809, loss 0.0349985, acc 1
2016-09-07T00:40:30.891183: step 2810, loss 0.0288264, acc 1
2016-09-07T00:40:31.565643: step 2811, loss 0.0173294, acc 0.98
2016-09-07T00:40:32.268898: step 2812, loss 0.0291274, acc 0.98
2016-09-07T00:40:32.954419: step 2813, loss 0.0186685, acc 0.98
2016-09-07T00:40:33.640490: step 2814, loss 0.0209734, acc 1
2016-09-07T00:40:34.319669: step 2815, loss 0.0791055, acc 0.96
2016-09-07T00:40:35.012113: step 2816, loss 0.0450113, acc 0.98
2016-09-07T00:40:35.708816: step 2817, loss 0.0400792, acc 0.98
2016-09-07T00:40:36.408106: step 2818, loss 0.0686906, acc 0.96
2016-09-07T00:40:37.100828: step 2819, loss 0.0518409, acc 0.98
2016-09-07T00:40:37.791677: step 2820, loss 0.0203519, acc 1
2016-09-07T00:40:38.509920: step 2821, loss 0.0398081, acc 0.98
2016-09-07T00:40:39.216314: step 2822, loss 0.025257, acc 1
2016-09-07T00:40:39.877970: step 2823, loss 0.00688267, acc 1
2016-09-07T00:40:40.582171: step 2824, loss 0.092498, acc 0.96
2016-09-07T00:40:41.284001: step 2825, loss 0.268086, acc 0.9
2016-09-07T00:40:41.977202: step 2826, loss 0.00135768, acc 1
2016-09-07T00:40:42.651780: step 2827, loss 0.0213323, acc 0.98
2016-09-07T00:40:43.344009: step 2828, loss 0.0758884, acc 0.98
2016-09-07T00:40:44.037798: step 2829, loss 0.131419, acc 0.92
2016-09-07T00:40:44.699041: step 2830, loss 0.00696095, acc 1
2016-09-07T00:40:45.401222: step 2831, loss 0.034289, acc 0.98
2016-09-07T00:40:46.064931: step 2832, loss 0.0280171, acc 0.98
2016-09-07T00:40:46.763692: step 2833, loss 0.0267161, acc 1
2016-09-07T00:40:47.449676: step 2834, loss 0.0341772, acc 1
2016-09-07T00:40:48.132090: step 2835, loss 0.0178383, acc 0.98
2016-09-07T00:40:48.807049: step 2836, loss 0.0761037, acc 0.94
2016-09-07T00:40:49.480092: step 2837, loss 0.0513349, acc 0.98
2016-09-07T00:40:50.191113: step 2838, loss 0.0223449, acc 0.98
2016-09-07T00:40:50.875165: step 2839, loss 0.137814, acc 0.94
2016-09-07T00:40:51.563101: step 2840, loss 0.0109771, acc 1
2016-09-07T00:40:52.271751: step 2841, loss 0.0247508, acc 0.98
2016-09-07T00:40:52.981811: step 2842, loss 0.0435685, acc 1
2016-09-07T00:40:53.697628: step 2843, loss 0.0271176, acc 0.98
2016-09-07T00:40:54.362702: step 2844, loss 0.00653499, acc 1
2016-09-07T00:40:55.073543: step 2845, loss 0.0366176, acc 0.98
2016-09-07T00:40:55.760165: step 2846, loss 0.014584, acc 1
2016-09-07T00:40:56.426488: step 2847, loss 0.0307085, acc 0.98
2016-09-07T00:40:57.121947: step 2848, loss 0.0894304, acc 0.96
2016-09-07T00:40:57.819892: step 2849, loss 0.122784, acc 0.96
2016-09-07T00:40:58.530159: step 2850, loss 0.111926, acc 0.98
2016-09-07T00:40:59.214394: step 2851, loss 0.0905869, acc 0.94
2016-09-07T00:40:59.915791: step 2852, loss 0.0107794, acc 1
2016-09-07T00:41:00.645884: step 2853, loss 0.0637149, acc 0.96
2016-09-07T00:41:01.331832: step 2854, loss 0.0442859, acc 0.98
2016-09-07T00:41:02.023289: step 2855, loss 0.101768, acc 0.96
2016-09-07T00:41:02.680764: step 2856, loss 0.0227583, acc 1
2016-09-07T00:41:03.387690: step 2857, loss 0.0263788, acc 1
2016-09-07T00:41:04.053385: step 2858, loss 0.0453629, acc 1
2016-09-07T00:41:04.734755: step 2859, loss 0.0803074, acc 0.96
2016-09-07T00:41:05.437391: step 2860, loss 0.0322213, acc 0.98
2016-09-07T00:41:06.118110: step 2861, loss 0.0551853, acc 0.98
2016-09-07T00:41:06.815840: step 2862, loss 0.0621591, acc 0.98
2016-09-07T00:41:07.540508: step 2863, loss 0.0251053, acc 0.98
2016-09-07T00:41:08.255685: step 2864, loss 0.0407168, acc 0.98
2016-09-07T00:41:08.957933: step 2865, loss 0.0174995, acc 1
2016-09-07T00:41:09.665744: step 2866, loss 0.0774128, acc 0.98
2016-09-07T00:41:10.355426: step 2867, loss 0.0459487, acc 0.98
2016-09-07T00:41:11.030432: step 2868, loss 0.0409532, acc 0.98
2016-09-07T00:41:11.719179: step 2869, loss 0.0927472, acc 0.98
2016-09-07T00:41:12.380214: step 2870, loss 0.03726, acc 0.98
2016-09-07T00:41:13.085988: step 2871, loss 0.0518892, acc 0.98
2016-09-07T00:41:13.754568: step 2872, loss 0.0794888, acc 0.92
2016-09-07T00:41:14.445084: step 2873, loss 0.0892766, acc 0.98
2016-09-07T00:41:15.151279: step 2874, loss 0.108122, acc 0.96
2016-09-07T00:41:15.880523: step 2875, loss 0.0434197, acc 0.98
2016-09-07T00:41:16.607387: step 2876, loss 0.0346822, acc 0.98
2016-09-07T00:41:17.279370: step 2877, loss 0.0203455, acc 0.98
2016-09-07T00:41:17.977833: step 2878, loss 0.0643578, acc 0.96
2016-09-07T00:41:18.665258: step 2879, loss 0.0903943, acc 0.96
2016-09-07T00:41:19.314519: step 2880, loss 0.0238609, acc 1
2016-09-07T00:41:20.017222: step 2881, loss 0.019119, acc 0.98
2016-09-07T00:41:20.690758: step 2882, loss 0.0856725, acc 0.98
2016-09-07T00:41:21.397021: step 2883, loss 0.0488404, acc 0.98
2016-09-07T00:41:22.068558: step 2884, loss 0.0175794, acc 0.98
2016-09-07T00:41:22.739614: step 2885, loss 0.0470949, acc 0.98
2016-09-07T00:41:23.449684: step 2886, loss 0.0718591, acc 0.94
2016-09-07T00:41:24.127613: step 2887, loss 0.0146927, acc 1
2016-09-07T00:41:24.820113: step 2888, loss 0.0672513, acc 0.98
2016-09-07T00:41:25.509954: step 2889, loss 0.0551502, acc 0.98
2016-09-07T00:41:26.207314: step 2890, loss 0.0379203, acc 0.98
2016-09-07T00:41:26.870673: step 2891, loss 0.0658316, acc 0.96
2016-09-07T00:41:27.566009: step 2892, loss 0.132459, acc 0.92
2016-09-07T00:41:28.242181: step 2893, loss 0.151944, acc 0.94
2016-09-07T00:41:28.945194: step 2894, loss 0.0178951, acc 1
2016-09-07T00:41:29.625900: step 2895, loss 0.0819147, acc 0.96
2016-09-07T00:41:30.309731: step 2896, loss 0.00942619, acc 1
2016-09-07T00:41:31.001893: step 2897, loss 0.0455334, acc 0.96
2016-09-07T00:41:31.659947: step 2898, loss 0.0278721, acc 1
2016-09-07T00:41:32.354748: step 2899, loss 0.0201674, acc 1
2016-09-07T00:41:33.037050: step 2900, loss 0.0308907, acc 1

Evaluation:
2016-09-07T00:41:36.199219: step 2900, loss 1.31409, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-2900

2016-09-07T00:41:37.997053: step 2901, loss 0.0823195, acc 0.96
2016-09-07T00:41:38.681974: step 2902, loss 0.0311589, acc 0.98
2016-09-07T00:41:39.365126: step 2903, loss 0.0073523, acc 1
2016-09-07T00:41:40.044090: step 2904, loss 0.0345975, acc 1
2016-09-07T00:41:40.749226: step 2905, loss 0.0888803, acc 0.96
2016-09-07T00:41:41.440828: step 2906, loss 0.0374556, acc 0.98
2016-09-07T00:41:42.152466: step 2907, loss 0.0374137, acc 0.98
2016-09-07T00:41:42.841611: step 2908, loss 0.00954258, acc 1
2016-09-07T00:41:43.533322: step 2909, loss 0.036964, acc 1
2016-09-07T00:41:44.229271: step 2910, loss 0.0270154, acc 1
2016-09-07T00:41:44.922376: step 2911, loss 0.0140642, acc 1
2016-09-07T00:41:45.630664: step 2912, loss 0.067291, acc 0.98
2016-09-07T00:41:46.329370: step 2913, loss 0.0510855, acc 0.98
2016-09-07T00:41:47.040637: step 2914, loss 0.0546946, acc 0.98
2016-09-07T00:41:47.745579: step 2915, loss 0.0422392, acc 0.98
2016-09-07T00:41:48.410758: step 2916, loss 0.026236, acc 0.98
2016-09-07T00:41:49.126051: step 2917, loss 0.0508318, acc 0.98
2016-09-07T00:41:49.803967: step 2918, loss 0.0212731, acc 0.98
2016-09-07T00:41:50.502480: step 2919, loss 0.00355589, acc 1
2016-09-07T00:41:51.186238: step 2920, loss 0.0230442, acc 0.98
2016-09-07T00:41:51.877793: step 2921, loss 0.0248203, acc 1
2016-09-07T00:41:52.561541: step 2922, loss 0.050467, acc 0.98
2016-09-07T00:41:53.240669: step 2923, loss 0.175433, acc 0.96
2016-09-07T00:41:53.962647: step 2924, loss 0.0407246, acc 0.98
2016-09-07T00:41:54.682792: step 2925, loss 0.0326368, acc 1
2016-09-07T00:41:55.363768: step 2926, loss 0.0370898, acc 0.98
2016-09-07T00:41:56.067626: step 2927, loss 0.036432, acc 0.98
2016-09-07T00:41:56.758733: step 2928, loss 0.00661749, acc 1
2016-09-07T00:41:57.477310: step 2929, loss 0.00414186, acc 1
2016-09-07T00:41:58.168549: step 2930, loss 0.0429832, acc 0.96
2016-09-07T00:41:58.847772: step 2931, loss 0.0281786, acc 1
2016-09-07T00:41:59.532663: step 2932, loss 0.0695316, acc 0.94
2016-09-07T00:42:00.233924: step 2933, loss 0.0740358, acc 0.98
2016-09-07T00:42:00.935434: step 2934, loss 0.0689576, acc 0.98
2016-09-07T00:42:01.600609: step 2935, loss 0.00772393, acc 1
2016-09-07T00:42:02.302127: step 2936, loss 0.0278486, acc 0.98
2016-09-07T00:42:02.980838: step 2937, loss 0.0264365, acc 0.98
2016-09-07T00:42:03.653531: step 2938, loss 0.00446101, acc 1
2016-09-07T00:42:04.329067: step 2939, loss 0.00185657, acc 1
2016-09-07T00:42:05.004934: step 2940, loss 0.00421456, acc 1
2016-09-07T00:42:05.681338: step 2941, loss 0.117835, acc 0.94
2016-09-07T00:42:06.364224: step 2942, loss 0.0130172, acc 1
2016-09-07T00:42:07.061655: step 2943, loss 0.116807, acc 0.96
2016-09-07T00:42:07.727040: step 2944, loss 0.0200964, acc 0.98
2016-09-07T00:42:08.421339: step 2945, loss 0.0289947, acc 0.98
2016-09-07T00:42:09.095273: step 2946, loss 0.124553, acc 0.98
2016-09-07T00:42:09.768393: step 2947, loss 0.00714831, acc 1
2016-09-07T00:42:10.454167: step 2948, loss 0.0260167, acc 1
2016-09-07T00:42:11.134502: step 2949, loss 0.0281789, acc 0.98
2016-09-07T00:42:11.816230: step 2950, loss 0.0309531, acc 0.98
2016-09-07T00:42:12.481814: step 2951, loss 0.0140245, acc 1
2016-09-07T00:42:13.167850: step 2952, loss 0.0543374, acc 0.98
2016-09-07T00:42:13.853808: step 2953, loss 0.00645651, acc 1
2016-09-07T00:42:14.524807: step 2954, loss 0.016629, acc 1
2016-09-07T00:42:15.199305: step 2955, loss 0.0233005, acc 1
2016-09-07T00:42:15.892521: step 2956, loss 0.0389037, acc 0.98
2016-09-07T00:42:16.577947: step 2957, loss 0.0388074, acc 1
2016-09-07T00:42:17.235146: step 2958, loss 0.0255927, acc 0.98
2016-09-07T00:42:17.945139: step 2959, loss 0.0239444, acc 1
2016-09-07T00:42:18.626324: step 2960, loss 0.0252921, acc 0.98
2016-09-07T00:42:19.298741: step 2961, loss 0.0373702, acc 0.98
2016-09-07T00:42:19.980326: step 2962, loss 0.0223026, acc 1
2016-09-07T00:42:20.671254: step 2963, loss 0.0117894, acc 1
2016-09-07T00:42:21.372095: step 2964, loss 0.0300366, acc 0.98
2016-09-07T00:42:22.043036: step 2965, loss 0.0206088, acc 0.98
2016-09-07T00:42:22.752809: step 2966, loss 0.0417768, acc 0.96
2016-09-07T00:42:23.429147: step 2967, loss 0.00103122, acc 1
2016-09-07T00:42:24.099355: step 2968, loss 0.202734, acc 0.98
2016-09-07T00:42:24.786169: step 2969, loss 0.00881017, acc 1
2016-09-07T00:42:25.489726: step 2970, loss 0.0184003, acc 0.98
2016-09-07T00:42:26.175880: step 2971, loss 0.0726243, acc 0.96
2016-09-07T00:42:26.848020: step 2972, loss 0.0965906, acc 0.94
2016-09-07T00:42:27.554969: step 2973, loss 0.117271, acc 0.96
2016-09-07T00:42:28.227687: step 2974, loss 0.0373087, acc 0.96
2016-09-07T00:42:28.911865: step 2975, loss 0.0262019, acc 0.98
2016-09-07T00:42:29.619970: step 2976, loss 0.104844, acc 0.96
2016-09-07T00:42:30.314031: step 2977, loss 0.0593146, acc 0.96
2016-09-07T00:42:31.020519: step 2978, loss 0.0585872, acc 0.96
2016-09-07T00:42:31.710767: step 2979, loss 0.0705154, acc 0.94
2016-09-07T00:42:32.436755: step 2980, loss 0.0725909, acc 0.96
2016-09-07T00:42:33.129785: step 2981, loss 0.0168759, acc 1
2016-09-07T00:42:33.812351: step 2982, loss 0.0325267, acc 0.98
2016-09-07T00:42:34.493351: step 2983, loss 0.0683904, acc 0.96
2016-09-07T00:42:35.180688: step 2984, loss 0.0623219, acc 0.96
2016-09-07T00:42:35.907889: step 2985, loss 0.0382448, acc 0.98
2016-09-07T00:42:36.573871: step 2986, loss 0.0319479, acc 1
2016-09-07T00:42:37.301538: step 2987, loss 0.0662478, acc 0.96
2016-09-07T00:42:37.986244: step 2988, loss 0.0841374, acc 0.94
2016-09-07T00:42:38.678468: step 2989, loss 0.0830884, acc 0.98
2016-09-07T00:42:39.398955: step 2990, loss 0.281017, acc 0.92
2016-09-07T00:42:40.093640: step 2991, loss 0.0232028, acc 0.98
2016-09-07T00:42:40.784802: step 2992, loss 0.0558201, acc 0.96
2016-09-07T00:42:41.468061: step 2993, loss 0.0133574, acc 1
2016-09-07T00:42:42.148314: step 2994, loss 0.0467685, acc 0.98
2016-09-07T00:42:42.827082: step 2995, loss 0.0514909, acc 0.98
2016-09-07T00:42:43.522644: step 2996, loss 0.0197042, acc 0.98
2016-09-07T00:42:44.212131: step 2997, loss 0.0667516, acc 0.96
2016-09-07T00:42:44.901121: step 2998, loss 0.0439941, acc 0.98
2016-09-07T00:42:45.626703: step 2999, loss 0.0986736, acc 0.96
2016-09-07T00:42:46.303392: step 3000, loss 0.031803, acc 0.98

Evaluation:
2016-09-07T00:42:49.447867: step 3000, loss 1.39722, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-3000

2016-09-07T00:42:51.162641: step 3001, loss 0.0404807, acc 0.98
2016-09-07T00:42:51.850967: step 3002, loss 0.0251578, acc 1
2016-09-07T00:42:52.525224: step 3003, loss 0.0036385, acc 1
2016-09-07T00:42:53.189292: step 3004, loss 0.0535039, acc 0.96
2016-09-07T00:42:53.880051: step 3005, loss 0.127741, acc 0.96
2016-09-07T00:42:54.529022: step 3006, loss 0.043557, acc 0.98
2016-09-07T00:42:55.235618: step 3007, loss 0.126538, acc 0.94
2016-09-07T00:42:55.904835: step 3008, loss 0.0887862, acc 0.94
2016-09-07T00:42:56.575842: step 3009, loss 0.0300326, acc 0.98
2016-09-07T00:42:57.260651: step 3010, loss 0.0557201, acc 0.98
2016-09-07T00:42:57.938438: step 3011, loss 0.0524826, acc 0.98
2016-09-07T00:42:58.641761: step 3012, loss 0.0701071, acc 0.98
2016-09-07T00:42:59.306298: step 3013, loss 0.0684427, acc 0.96
2016-09-07T00:43:00.013983: step 3014, loss 0.0630252, acc 0.96
2016-09-07T00:43:00.727464: step 3015, loss 0.00479178, acc 1
2016-09-07T00:43:01.420465: step 3016, loss 0.0471621, acc 0.98
2016-09-07T00:43:02.115917: step 3017, loss 0.0229896, acc 1
2016-09-07T00:43:02.809273: step 3018, loss 0.0148711, acc 1
2016-09-07T00:43:03.513712: step 3019, loss 0.113869, acc 0.9
2016-09-07T00:43:04.178764: step 3020, loss 0.0136329, acc 1
2016-09-07T00:43:04.865617: step 3021, loss 0.0297728, acc 1
2016-09-07T00:43:05.540595: step 3022, loss 0.038391, acc 0.98
2016-09-07T00:43:06.219144: step 3023, loss 0.0222133, acc 1
2016-09-07T00:43:06.893865: step 3024, loss 0.0537546, acc 0.98
2016-09-07T00:43:07.593814: step 3025, loss 0.0268625, acc 0.98
2016-09-07T00:43:08.273417: step 3026, loss 0.144752, acc 0.96
2016-09-07T00:43:08.935072: step 3027, loss 0.012415, acc 1
2016-09-07T00:43:09.638405: step 3028, loss 0.0611813, acc 0.98
2016-09-07T00:43:10.317004: step 3029, loss 0.0940372, acc 0.98
2016-09-07T00:43:10.990603: step 3030, loss 0.00452165, acc 1
2016-09-07T00:43:11.689732: step 3031, loss 0.055631, acc 0.96
2016-09-07T00:43:12.385266: step 3032, loss 0.00174591, acc 1
2016-09-07T00:43:13.074907: step 3033, loss 0.0353075, acc 0.98
2016-09-07T00:43:13.747398: step 3034, loss 0.0628675, acc 0.96
2016-09-07T00:43:14.437781: step 3035, loss 0.0344403, acc 0.98
2016-09-07T00:43:15.118501: step 3036, loss 0.0266098, acc 0.98
2016-09-07T00:43:15.793932: step 3037, loss 0.0304414, acc 0.98
2016-09-07T00:43:16.474484: step 3038, loss 0.0332145, acc 0.98
2016-09-07T00:43:17.167417: step 3039, loss 0.0158845, acc 1
2016-09-07T00:43:17.845776: step 3040, loss 0.0420086, acc 0.96
2016-09-07T00:43:18.534989: step 3041, loss 0.00214628, acc 1
2016-09-07T00:43:19.252145: step 3042, loss 0.0125431, acc 1
2016-09-07T00:43:19.921414: step 3043, loss 0.0360724, acc 0.98
2016-09-07T00:43:20.590564: step 3044, loss 0.0211086, acc 0.98
2016-09-07T00:43:21.277280: step 3045, loss 0.0468049, acc 0.96
2016-09-07T00:43:21.979007: step 3046, loss 0.0237106, acc 1
2016-09-07T00:43:22.660610: step 3047, loss 0.0259989, acc 0.98
2016-09-07T00:43:23.335946: step 3048, loss 0.0417127, acc 0.96
2016-09-07T00:43:24.048530: step 3049, loss 0.0336263, acc 0.98
2016-09-07T00:43:24.726532: step 3050, loss 0.0911344, acc 0.98
2016-09-07T00:43:25.393413: step 3051, loss 0.0307605, acc 0.98
2016-09-07T00:43:26.119585: step 3052, loss 0.0457818, acc 0.98
2016-09-07T00:43:26.803306: step 3053, loss 0.0244442, acc 1
2016-09-07T00:43:27.499470: step 3054, loss 0.101752, acc 0.96
2016-09-07T00:43:28.184007: step 3055, loss 0.0108563, acc 1
2016-09-07T00:43:28.879249: step 3056, loss 0.0410282, acc 0.98
2016-09-07T00:43:29.554061: step 3057, loss 0.0925737, acc 0.96
2016-09-07T00:43:30.234815: step 3058, loss 0.148011, acc 0.96
2016-09-07T00:43:30.912033: step 3059, loss 0.00804768, acc 1
2016-09-07T00:43:31.601527: step 3060, loss 0.0290166, acc 0.98
2016-09-07T00:43:32.292839: step 3061, loss 0.0301813, acc 0.98
2016-09-07T00:43:32.988899: step 3062, loss 0.00944846, acc 1
2016-09-07T00:43:33.689702: step 3063, loss 0.0112125, acc 1
2016-09-07T00:43:34.381364: step 3064, loss 0.0800144, acc 0.94
2016-09-07T00:43:35.065655: step 3065, loss 0.0419654, acc 1
2016-09-07T00:43:35.768420: step 3066, loss 0.0191044, acc 0.98
2016-09-07T00:43:36.455829: step 3067, loss 0.025663, acc 1
2016-09-07T00:43:37.162460: step 3068, loss 0.00608822, acc 1
2016-09-07T00:43:37.830947: step 3069, loss 0.0631634, acc 0.98
2016-09-07T00:43:38.528499: step 3070, loss 0.0135874, acc 1
2016-09-07T00:43:39.208700: step 3071, loss 0.0187259, acc 1
2016-09-07T00:43:39.844026: step 3072, loss 0.016467, acc 1
2016-09-07T00:43:40.525705: step 3073, loss 0.0134127, acc 1
2016-09-07T00:43:41.201403: step 3074, loss 0.0287234, acc 1
2016-09-07T00:43:41.883975: step 3075, loss 0.0428189, acc 1
2016-09-07T00:43:42.569772: step 3076, loss 0.0135123, acc 1
2016-09-07T00:43:43.272334: step 3077, loss 0.0227847, acc 0.98
2016-09-07T00:43:43.959179: step 3078, loss 0.0604071, acc 0.98
2016-09-07T00:43:44.636443: step 3079, loss 0.038355, acc 0.96
2016-09-07T00:43:45.328047: step 3080, loss 0.0239997, acc 1
2016-09-07T00:43:46.007510: step 3081, loss 0.0164883, acc 0.98
2016-09-07T00:43:46.708217: step 3082, loss 0.0245309, acc 0.98
2016-09-07T00:43:47.384376: step 3083, loss 0.0569328, acc 0.98
2016-09-07T00:43:48.085000: step 3084, loss 0.033242, acc 0.98
2016-09-07T00:43:48.736706: step 3085, loss 0.0659434, acc 0.96
2016-09-07T00:43:49.432291: step 3086, loss 0.0592992, acc 0.98
2016-09-07T00:43:50.113497: step 3087, loss 0.0337723, acc 0.98
2016-09-07T00:43:50.792267: step 3088, loss 0.00268287, acc 1
2016-09-07T00:43:51.464440: step 3089, loss 0.0222514, acc 1
2016-09-07T00:43:52.147423: step 3090, loss 0.00359745, acc 1
2016-09-07T00:43:52.845382: step 3091, loss 0.000693528, acc 1
2016-09-07T00:43:53.527816: step 3092, loss 0.064957, acc 0.96
2016-09-07T00:43:54.252064: step 3093, loss 0.104149, acc 0.96
2016-09-07T00:43:54.946959: step 3094, loss 0.0275068, acc 0.98
2016-09-07T00:43:55.631373: step 3095, loss 0.0134227, acc 1
2016-09-07T00:43:56.314373: step 3096, loss 0.068155, acc 0.98
2016-09-07T00:43:57.004599: step 3097, loss 0.0630369, acc 0.96
2016-09-07T00:43:57.701104: step 3098, loss 0.0482615, acc 0.98
2016-09-07T00:43:58.365349: step 3099, loss 0.0251878, acc 0.98
2016-09-07T00:43:59.057943: step 3100, loss 0.0278205, acc 0.98

Evaluation:
2016-09-07T00:44:02.246150: step 3100, loss 1.73914, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-3100

2016-09-07T00:44:03.912398: step 3101, loss 0.0398639, acc 0.96
2016-09-07T00:44:04.595925: step 3102, loss 0.0120421, acc 1
2016-09-07T00:44:05.271350: step 3103, loss 0.0185171, acc 0.98
2016-09-07T00:44:05.951784: step 3104, loss 0.128555, acc 0.98
2016-09-07T00:44:06.625415: step 3105, loss 0.0198148, acc 0.98
2016-09-07T00:44:07.319980: step 3106, loss 0.0325394, acc 0.98
2016-09-07T00:44:07.996454: step 3107, loss 0.012652, acc 1
2016-09-07T00:44:08.677085: step 3108, loss 0.0508107, acc 0.98
2016-09-07T00:44:09.372627: step 3109, loss 0.010283, acc 1
2016-09-07T00:44:10.051448: step 3110, loss 0.146231, acc 0.96
2016-09-07T00:44:10.751156: step 3111, loss 0.00366199, acc 1
2016-09-07T00:44:11.434306: step 3112, loss 0.00132006, acc 1
2016-09-07T00:44:12.153733: step 3113, loss 0.0327404, acc 1
2016-09-07T00:44:12.823408: step 3114, loss 0.167927, acc 0.96
2016-09-07T00:44:13.523942: step 3115, loss 0.0519165, acc 0.96
2016-09-07T00:44:14.215776: step 3116, loss 0.0869775, acc 0.98
2016-09-07T00:44:14.898426: step 3117, loss 0.0308023, acc 1
2016-09-07T00:44:15.599999: step 3118, loss 0.123331, acc 0.94
2016-09-07T00:44:16.279921: step 3119, loss 0.00483073, acc 1
2016-09-07T00:44:17.000031: step 3120, loss 0.0522205, acc 0.98
2016-09-07T00:44:17.676623: step 3121, loss 0.00245608, acc 1
2016-09-07T00:44:18.353599: step 3122, loss 0.13673, acc 0.94
2016-09-07T00:44:19.033731: step 3123, loss 0.0573594, acc 0.98
2016-09-07T00:44:19.734522: step 3124, loss 0.0241016, acc 0.98
2016-09-07T00:44:20.435803: step 3125, loss 0.00589303, acc 1
2016-09-07T00:44:21.116202: step 3126, loss 0.0665728, acc 0.92
2016-09-07T00:44:21.845960: step 3127, loss 0.0431739, acc 0.98
2016-09-07T00:44:22.525107: step 3128, loss 0.0629199, acc 0.94
2016-09-07T00:44:23.203857: step 3129, loss 0.0600626, acc 0.96
2016-09-07T00:44:23.906376: step 3130, loss 0.0143229, acc 1
2016-09-07T00:44:24.588829: step 3131, loss 0.074826, acc 0.94
2016-09-07T00:44:25.282863: step 3132, loss 0.0498898, acc 0.98
2016-09-07T00:44:25.945735: step 3133, loss 0.109865, acc 0.94
2016-09-07T00:44:26.644502: step 3134, loss 0.0850614, acc 0.96
2016-09-07T00:44:27.323157: step 3135, loss 0.0837164, acc 0.96
2016-09-07T00:44:27.990485: step 3136, loss 0.0538145, acc 0.98
2016-09-07T00:44:28.690348: step 3137, loss 0.0140478, acc 1
2016-09-07T00:44:29.401130: step 3138, loss 0.0697601, acc 0.98
2016-09-07T00:44:30.081165: step 3139, loss 0.00835545, acc 1
2016-09-07T00:44:30.739623: step 3140, loss 0.0254158, acc 1
2016-09-07T00:44:31.433782: step 3141, loss 0.0182539, acc 1
2016-09-07T00:44:32.104528: step 3142, loss 0.0855908, acc 0.94
2016-09-07T00:44:32.801933: step 3143, loss 0.0171825, acc 0.98
2016-09-07T00:44:33.483293: step 3144, loss 0.0255093, acc 0.98
2016-09-07T00:44:34.162118: step 3145, loss 0.00410709, acc 1
2016-09-07T00:44:34.823018: step 3146, loss 0.0251802, acc 0.98
2016-09-07T00:44:35.502428: step 3147, loss 0.0258925, acc 0.98
2016-09-07T00:44:36.198912: step 3148, loss 0.0448251, acc 0.98
2016-09-07T00:44:36.874527: step 3149, loss 0.0269963, acc 1
2016-09-07T00:44:37.574969: step 3150, loss 0.00453882, acc 1
2016-09-07T00:44:38.250316: step 3151, loss 0.0285257, acc 0.98
2016-09-07T00:44:38.940833: step 3152, loss 0.00439757, acc 1
2016-09-07T00:44:39.637715: step 3153, loss 0.296958, acc 0.94
2016-09-07T00:44:40.337851: step 3154, loss 0.0240563, acc 1
2016-09-07T00:44:41.044925: step 3155, loss 0.0117783, acc 1
2016-09-07T00:44:41.763262: step 3156, loss 0.0192334, acc 1
2016-09-07T00:44:42.424364: step 3157, loss 0.00741037, acc 1
2016-09-07T00:44:43.141406: step 3158, loss 0.0188679, acc 0.98
2016-09-07T00:44:43.840053: step 3159, loss 0.131841, acc 0.96
2016-09-07T00:44:44.543868: step 3160, loss 0.0648217, acc 0.98
2016-09-07T00:44:45.200652: step 3161, loss 0.0329569, acc 0.98
2016-09-07T00:44:45.918194: step 3162, loss 0.014458, acc 1
2016-09-07T00:44:46.605508: step 3163, loss 0.0100376, acc 1
2016-09-07T00:44:47.284053: step 3164, loss 0.0381281, acc 0.98
2016-09-07T00:44:47.967291: step 3165, loss 0.0913508, acc 0.96
2016-09-07T00:44:48.665239: step 3166, loss 0.0214351, acc 0.98
2016-09-07T00:44:49.364295: step 3167, loss 0.0212034, acc 1
2016-09-07T00:44:50.019437: step 3168, loss 0.0405349, acc 0.96
2016-09-07T00:44:50.726176: step 3169, loss 0.0182348, acc 0.98
2016-09-07T00:44:51.430026: step 3170, loss 0.110748, acc 0.94
2016-09-07T00:44:52.100553: step 3171, loss 0.0421555, acc 0.96
2016-09-07T00:44:52.797815: step 3172, loss 0.0193005, acc 1
2016-09-07T00:44:53.505984: step 3173, loss 0.0188898, acc 0.98
2016-09-07T00:44:54.213142: step 3174, loss 0.0584696, acc 0.96
2016-09-07T00:44:54.889515: step 3175, loss 0.0395954, acc 0.98
2016-09-07T00:44:55.578486: step 3176, loss 0.0769999, acc 0.92
2016-09-07T00:44:56.276582: step 3177, loss 0.0379904, acc 0.98
2016-09-07T00:44:56.964354: step 3178, loss 0.13794, acc 0.94
2016-09-07T00:44:57.655710: step 3179, loss 0.016669, acc 1
2016-09-07T00:44:58.352435: step 3180, loss 0.0474113, acc 0.98
2016-09-07T00:44:59.053599: step 3181, loss 0.0331697, acc 0.98
2016-09-07T00:44:59.736474: step 3182, loss 0.0550502, acc 0.96
2016-09-07T00:45:00.463501: step 3183, loss 0.0433478, acc 0.98
2016-09-07T00:45:01.162744: step 3184, loss 0.00560468, acc 1
2016-09-07T00:45:01.856434: step 3185, loss 0.0819742, acc 0.96
2016-09-07T00:45:02.543207: step 3186, loss 0.0949118, acc 0.94
2016-09-07T00:45:03.209982: step 3187, loss 0.0793809, acc 0.96
2016-09-07T00:45:03.905453: step 3188, loss 0.00576267, acc 1
2016-09-07T00:45:04.585883: step 3189, loss 0.082659, acc 0.98
2016-09-07T00:45:05.271156: step 3190, loss 0.033791, acc 0.98
2016-09-07T00:45:05.951199: step 3191, loss 0.101851, acc 0.96
2016-09-07T00:45:06.628732: step 3192, loss 0.0670389, acc 0.98
2016-09-07T00:45:07.348652: step 3193, loss 0.0283789, acc 1
2016-09-07T00:45:08.026953: step 3194, loss 0.0181408, acc 1
2016-09-07T00:45:08.737422: step 3195, loss 0.0124807, acc 1
2016-09-07T00:45:09.400387: step 3196, loss 0.0322667, acc 1
2016-09-07T00:45:10.086407: step 3197, loss 0.0370641, acc 0.98
2016-09-07T00:45:10.767671: step 3198, loss 0.0678028, acc 0.98
2016-09-07T00:45:11.438932: step 3199, loss 0.0169065, acc 1
2016-09-07T00:45:12.118829: step 3200, loss 0.0171208, acc 1

Evaluation:
2016-09-07T00:45:15.249036: step 3200, loss 1.49252, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-3200

2016-09-07T00:45:16.884020: step 3201, loss 0.0106554, acc 1
2016-09-07T00:45:17.573503: step 3202, loss 0.0637716, acc 0.94
2016-09-07T00:45:18.285636: step 3203, loss 0.0957708, acc 0.96
2016-09-07T00:45:18.934201: step 3204, loss 0.0309991, acc 0.98
2016-09-07T00:45:19.621654: step 3205, loss 0.0280968, acc 1
2016-09-07T00:45:20.297364: step 3206, loss 0.207578, acc 0.98
2016-09-07T00:45:20.983320: step 3207, loss 0.00756842, acc 1
2016-09-07T00:45:21.673020: step 3208, loss 0.144989, acc 0.92
2016-09-07T00:45:22.369831: step 3209, loss 0.0157999, acc 1
2016-09-07T00:45:23.092451: step 3210, loss 0.0384439, acc 0.98
2016-09-07T00:45:23.764752: step 3211, loss 0.0495568, acc 0.98
2016-09-07T00:45:24.463150: step 3212, loss 0.0228641, acc 0.98
2016-09-07T00:45:25.173978: step 3213, loss 0.0558628, acc 0.96
2016-09-07T00:45:25.855599: step 3214, loss 0.0188796, acc 1
2016-09-07T00:45:26.554279: step 3215, loss 0.0156021, acc 1
2016-09-07T00:45:27.249822: step 3216, loss 0.0458047, acc 0.98
2016-09-07T00:45:27.965375: step 3217, loss 0.0314068, acc 0.98
2016-09-07T00:45:28.631758: step 3218, loss 0.203184, acc 0.98
2016-09-07T00:45:29.322554: step 3219, loss 0.0249551, acc 1
2016-09-07T00:45:30.003419: step 3220, loss 0.0328773, acc 0.98
2016-09-07T00:45:30.682360: step 3221, loss 0.0432162, acc 0.98
2016-09-07T00:45:31.371745: step 3222, loss 0.240818, acc 0.94
2016-09-07T00:45:32.054527: step 3223, loss 0.0289447, acc 0.98
2016-09-07T00:45:32.758663: step 3224, loss 0.0257712, acc 1
2016-09-07T00:45:33.422075: step 3225, loss 0.0221507, acc 0.98
2016-09-07T00:45:34.122145: step 3226, loss 0.00646606, acc 1
2016-09-07T00:45:34.801266: step 3227, loss 0.0574727, acc 0.98
2016-09-07T00:45:35.496763: step 3228, loss 0.0293718, acc 0.98
2016-09-07T00:45:36.189430: step 3229, loss 0.00972465, acc 1
2016-09-07T00:45:36.858961: step 3230, loss 0.0417525, acc 0.98
2016-09-07T00:45:37.549861: step 3231, loss 0.0570698, acc 0.98
2016-09-07T00:45:38.224911: step 3232, loss 0.0267992, acc 1
2016-09-07T00:45:38.918151: step 3233, loss 0.00105351, acc 1
2016-09-07T00:45:39.594505: step 3234, loss 0.0189702, acc 1
2016-09-07T00:45:40.282276: step 3235, loss 0.0168455, acc 1
2016-09-07T00:45:40.975338: step 3236, loss 0.00612412, acc 1
2016-09-07T00:45:41.664259: step 3237, loss 0.00475667, acc 1
2016-09-07T00:45:42.375186: step 3238, loss 0.0326198, acc 0.98
2016-09-07T00:45:43.050123: step 3239, loss 0.00605106, acc 1
2016-09-07T00:45:43.742083: step 3240, loss 0.0596876, acc 0.96
2016-09-07T00:45:44.411767: step 3241, loss 0.0757553, acc 0.94
2016-09-07T00:45:45.120780: step 3242, loss 0.0535919, acc 0.96
2016-09-07T00:45:45.829744: step 3243, loss 0.0121115, acc 1
2016-09-07T00:45:46.488215: step 3244, loss 0.0272501, acc 0.98
2016-09-07T00:45:47.188448: step 3245, loss 0.00985814, acc 1
2016-09-07T00:45:47.874374: step 3246, loss 0.0405841, acc 1
2016-09-07T00:45:48.574382: step 3247, loss 0.0628063, acc 0.98
2016-09-07T00:45:49.245596: step 3248, loss 0.0234455, acc 1
2016-09-07T00:45:49.930348: step 3249, loss 0.025744, acc 1
2016-09-07T00:45:50.605966: step 3250, loss 0.0501269, acc 0.98
2016-09-07T00:45:51.296031: step 3251, loss 0.00999254, acc 1
2016-09-07T00:45:52.011690: step 3252, loss 0.0126071, acc 1
2016-09-07T00:45:52.691432: step 3253, loss 0.015575, acc 1
2016-09-07T00:45:53.382385: step 3254, loss 0.00949987, acc 1
2016-09-07T00:45:54.072645: step 3255, loss 0.099021, acc 0.96
2016-09-07T00:45:54.763273: step 3256, loss 0.0268338, acc 1
2016-09-07T00:45:55.467533: step 3257, loss 0.0239437, acc 0.98
2016-09-07T00:45:56.163489: step 3258, loss 0.0428177, acc 0.98
2016-09-07T00:45:56.866663: step 3259, loss 0.0470736, acc 0.96
2016-09-07T00:45:57.538812: step 3260, loss 0.190968, acc 0.96
2016-09-07T00:45:58.221213: step 3261, loss 0.0267509, acc 0.98
2016-09-07T00:45:58.909335: step 3262, loss 0.0968916, acc 0.96
2016-09-07T00:45:59.588724: step 3263, loss 0.0764193, acc 0.98
2016-09-07T00:46:00.213964: step 3264, loss 0.0750493, acc 0.977273
2016-09-07T00:46:00.902490: step 3265, loss 0.0226509, acc 1
2016-09-07T00:46:01.584720: step 3266, loss 0.0433567, acc 0.96
2016-09-07T00:46:02.257088: step 3267, loss 0.111587, acc 0.96
2016-09-07T00:46:02.932716: step 3268, loss 0.0100081, acc 1
2016-09-07T00:46:03.624645: step 3269, loss 0.0244708, acc 0.98
2016-09-07T00:46:04.309257: step 3270, loss 0.0231652, acc 1
2016-09-07T00:46:04.984664: step 3271, loss 0.0180419, acc 1
2016-09-07T00:46:05.681400: step 3272, loss 0.0337617, acc 0.98
2016-09-07T00:46:06.374987: step 3273, loss 0.0254788, acc 1
2016-09-07T00:46:07.048204: step 3274, loss 0.0807017, acc 0.96
2016-09-07T00:46:07.734973: step 3275, loss 0.026323, acc 0.98
2016-09-07T00:46:08.408197: step 3276, loss 0.0516136, acc 0.98
2016-09-07T00:46:09.094698: step 3277, loss 0.045494, acc 0.98
2016-09-07T00:46:09.796528: step 3278, loss 0.0407599, acc 0.98
2016-09-07T00:46:10.477322: step 3279, loss 0.0615258, acc 0.96
2016-09-07T00:46:11.179574: step 3280, loss 0.0212199, acc 0.98
2016-09-07T00:46:11.838312: step 3281, loss 0.0348318, acc 0.98
2016-09-07T00:46:12.532402: step 3282, loss 0.0112732, acc 1
2016-09-07T00:46:13.213394: step 3283, loss 0.0108861, acc 1
2016-09-07T00:46:13.907054: step 3284, loss 0.000607704, acc 1
2016-09-07T00:46:14.594768: step 3285, loss 0.0501549, acc 0.98
2016-09-07T00:46:15.299680: step 3286, loss 0.0242985, acc 1
2016-09-07T00:46:16.026668: step 3287, loss 0.0116496, acc 1
2016-09-07T00:46:16.707453: step 3288, loss 0.136148, acc 0.96
2016-09-07T00:46:17.383985: step 3289, loss 0.0124369, acc 1
2016-09-07T00:46:18.069213: step 3290, loss 0.00649729, acc 1
2016-09-07T00:46:18.778121: step 3291, loss 0.0315793, acc 0.98
2016-09-07T00:46:19.476171: step 3292, loss 0.00791075, acc 1
2016-09-07T00:46:20.147191: step 3293, loss 0.0769915, acc 0.98
2016-09-07T00:46:20.862538: step 3294, loss 0.00714189, acc 1
2016-09-07T00:46:21.540029: step 3295, loss 0.0262597, acc 1
2016-09-07T00:46:22.226343: step 3296, loss 0.0635145, acc 0.98
2016-09-07T00:46:22.934584: step 3297, loss 0.0404945, acc 0.98
2016-09-07T00:46:23.626085: step 3298, loss 0.0657318, acc 0.98
2016-09-07T00:46:24.322952: step 3299, loss 0.0195241, acc 1
2016-09-07T00:46:24.987256: step 3300, loss 0.022486, acc 1

Evaluation:
2016-09-07T00:46:28.143570: step 3300, loss 1.64202, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-3300

2016-09-07T00:46:29.855158: step 3301, loss 0.0216132, acc 0.98
2016-09-07T00:46:30.545405: step 3302, loss 0.0375361, acc 0.96
2016-09-07T00:46:31.245440: step 3303, loss 0.0468693, acc 0.98
2016-09-07T00:46:31.974752: step 3304, loss 0.000643679, acc 1
2016-09-07T00:46:32.665899: step 3305, loss 0.0175068, acc 0.98
2016-09-07T00:46:33.351677: step 3306, loss 0.0224117, acc 1
2016-09-07T00:46:34.063401: step 3307, loss 0.0399886, acc 1
2016-09-07T00:46:34.743513: step 3308, loss 0.0330019, acc 0.98
2016-09-07T00:46:35.417267: step 3309, loss 0.0354465, acc 0.98
2016-09-07T00:46:36.116407: step 3310, loss 0.0340474, acc 0.98
2016-09-07T00:46:36.809156: step 3311, loss 0.0255661, acc 1
2016-09-07T00:46:37.507091: step 3312, loss 0.0243107, acc 0.98
2016-09-07T00:46:38.191393: step 3313, loss 0.00101639, acc 1
2016-09-07T00:46:38.894242: step 3314, loss 0.0776848, acc 0.96
2016-09-07T00:46:39.567015: step 3315, loss 0.0581945, acc 0.96
2016-09-07T00:46:40.255804: step 3316, loss 0.0197233, acc 1
2016-09-07T00:46:40.942431: step 3317, loss 0.0244147, acc 0.98
2016-09-07T00:46:41.618299: step 3318, loss 0.0212482, acc 1
2016-09-07T00:46:42.303844: step 3319, loss 0.0132326, acc 1
2016-09-07T00:46:42.975241: step 3320, loss 0.0860665, acc 0.94
2016-09-07T00:46:43.688802: step 3321, loss 0.0544929, acc 0.98
2016-09-07T00:46:44.358265: step 3322, loss 0.013613, acc 1
2016-09-07T00:46:45.049548: step 3323, loss 0.0251765, acc 1
2016-09-07T00:46:45.745361: step 3324, loss 0.00066194, acc 1
2016-09-07T00:46:46.415141: step 3325, loss 0.0854482, acc 0.98
2016-09-07T00:46:47.089413: step 3326, loss 0.0362413, acc 1
2016-09-07T00:46:47.774422: step 3327, loss 0.0459841, acc 0.98
2016-09-07T00:46:48.470768: step 3328, loss 0.0198032, acc 1
2016-09-07T00:46:49.144279: step 3329, loss 0.0288055, acc 0.98
2016-09-07T00:46:49.818011: step 3330, loss 0.0524415, acc 0.96
2016-09-07T00:46:50.513647: step 3331, loss 0.0131628, acc 1
2016-09-07T00:46:51.224042: step 3332, loss 0.000237022, acc 1
2016-09-07T00:46:51.926536: step 3333, loss 0.145557, acc 0.98
2016-09-07T00:46:52.602783: step 3334, loss 0.0285674, acc 0.98
2016-09-07T00:46:53.315014: step 3335, loss 0.00313136, acc 1
2016-09-07T00:46:54.003560: step 3336, loss 0.123027, acc 0.96
2016-09-07T00:46:54.693851: step 3337, loss 0.0138156, acc 1
2016-09-07T00:46:55.395426: step 3338, loss 0.119556, acc 0.96
2016-09-07T00:46:56.112442: step 3339, loss 0.0244287, acc 1
2016-09-07T00:46:56.802475: step 3340, loss 0.0160429, acc 1
2016-09-07T00:46:57.467117: step 3341, loss 0.010678, acc 1
2016-09-07T00:46:58.168349: step 3342, loss 0.0123517, acc 1
2016-09-07T00:46:58.856065: step 3343, loss 0.0203392, acc 1
2016-09-07T00:46:59.532180: step 3344, loss 0.0266979, acc 1
2016-09-07T00:47:00.209534: step 3345, loss 0.033728, acc 0.98
2016-09-07T00:47:00.928602: step 3346, loss 0.0883912, acc 0.96
2016-09-07T00:47:01.610924: step 3347, loss 0.0668515, acc 0.96
2016-09-07T00:47:02.276101: step 3348, loss 0.0548539, acc 0.98
2016-09-07T00:47:02.982391: step 3349, loss 0.0342344, acc 0.98
2016-09-07T00:47:03.675738: step 3350, loss 0.0305779, acc 0.98
2016-09-07T00:47:04.372155: step 3351, loss 0.0106931, acc 1
2016-09-07T00:47:05.069415: step 3352, loss 0.000784966, acc 1
2016-09-07T00:47:05.767529: step 3353, loss 0.103762, acc 0.96
2016-09-07T00:47:06.482131: step 3354, loss 0.0307817, acc 0.98
2016-09-07T00:47:07.166439: step 3355, loss 0.0708895, acc 0.94
2016-09-07T00:47:07.853747: step 3356, loss 0.0500272, acc 0.98
2016-09-07T00:47:08.534906: step 3357, loss 0.0567535, acc 0.96
2016-09-07T00:47:09.212481: step 3358, loss 0.0261157, acc 1
2016-09-07T00:47:09.892716: step 3359, loss 0.0280857, acc 0.98
2016-09-07T00:47:10.577964: step 3360, loss 0.0589843, acc 0.98
2016-09-07T00:47:11.296419: step 3361, loss 0.00820606, acc 1
2016-09-07T00:47:11.968026: step 3362, loss 0.0733898, acc 0.96
2016-09-07T00:47:12.669257: step 3363, loss 0.00932275, acc 1
2016-09-07T00:47:13.369894: step 3364, loss 0.0489331, acc 0.98
2016-09-07T00:47:14.053787: step 3365, loss 0.0336197, acc 0.98
2016-09-07T00:47:14.752716: step 3366, loss 0.031681, acc 0.98
2016-09-07T00:47:15.474215: step 3367, loss 0.0168857, acc 1
2016-09-07T00:47:16.197663: step 3368, loss 0.077488, acc 0.96
2016-09-07T00:47:16.889413: step 3369, loss 0.00333339, acc 1
2016-09-07T00:47:17.603802: step 3370, loss 0.0436326, acc 0.98
2016-09-07T00:47:18.284330: step 3371, loss 0.00680209, acc 1
2016-09-07T00:47:18.954465: step 3372, loss 0.00176869, acc 1
2016-09-07T00:47:19.659708: step 3373, loss 0.187061, acc 0.94
2016-09-07T00:47:20.343239: step 3374, loss 0.0525927, acc 0.96
2016-09-07T00:47:21.043495: step 3375, loss 0.00369195, acc 1
2016-09-07T00:47:21.717012: step 3376, loss 0.0157443, acc 1
2016-09-07T00:47:22.399161: step 3377, loss 0.0430165, acc 0.98
2016-09-07T00:47:23.088141: step 3378, loss 0.028209, acc 1
2016-09-07T00:47:23.784358: step 3379, loss 0.0718966, acc 0.98
2016-09-07T00:47:24.503235: step 3380, loss 0.00234936, acc 1
2016-09-07T00:47:25.167143: step 3381, loss 0.00925819, acc 1
2016-09-07T00:47:25.877552: step 3382, loss 0.00905329, acc 1
2016-09-07T00:47:26.586966: step 3383, loss 0.0748144, acc 0.96
2016-09-07T00:47:27.270417: step 3384, loss 0.00839669, acc 1
2016-09-07T00:47:27.950467: step 3385, loss 0.109583, acc 0.96
2016-09-07T00:47:28.641982: step 3386, loss 0.0371404, acc 0.98
2016-09-07T00:47:29.338870: step 3387, loss 0.0165463, acc 0.98
2016-09-07T00:47:30.011823: step 3388, loss 0.0017806, acc 1
2016-09-07T00:47:30.689616: step 3389, loss 0.00128341, acc 1
2016-09-07T00:47:31.362163: step 3390, loss 0.0484164, acc 0.96
2016-09-07T00:47:32.046042: step 3391, loss 0.00307354, acc 1
2016-09-07T00:47:32.714353: step 3392, loss 0.0215478, acc 0.98
2016-09-07T00:47:33.391656: step 3393, loss 0.133474, acc 0.96
2016-09-07T00:47:34.080270: step 3394, loss 0.0427002, acc 0.98
2016-09-07T00:47:34.745445: step 3395, loss 0.0418763, acc 0.98
2016-09-07T00:47:35.456853: step 3396, loss 0.171362, acc 0.96
2016-09-07T00:47:36.149947: step 3397, loss 0.0817757, acc 0.96
2016-09-07T00:47:36.849911: step 3398, loss 0.0949873, acc 0.98
2016-09-07T00:47:37.544715: step 3399, loss 0.00132064, acc 1
2016-09-07T00:47:38.228065: step 3400, loss 0.0258518, acc 0.98

Evaluation:
2016-09-07T00:47:41.361467: step 3400, loss 1.46092, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-3400

2016-09-07T00:47:43.114097: step 3401, loss 0.0695801, acc 0.96
2016-09-07T00:47:43.817468: step 3402, loss 0.0634633, acc 0.96
2016-09-07T00:47:44.494291: step 3403, loss 0.02727, acc 0.98
2016-09-07T00:47:45.179827: step 3404, loss 0.00914513, acc 1
2016-09-07T00:47:45.877470: step 3405, loss 0.0123201, acc 1
2016-09-07T00:47:46.583049: step 3406, loss 0.14616, acc 0.96
2016-09-07T00:47:47.280321: step 3407, loss 0.0432609, acc 1
2016-09-07T00:47:47.947467: step 3408, loss 0.0282857, acc 1
2016-09-07T00:47:48.657032: step 3409, loss 0.0331606, acc 0.98
2016-09-07T00:47:49.301850: step 3410, loss 0.0411002, acc 0.98
2016-09-07T00:47:49.982836: step 3411, loss 0.012953, acc 1
2016-09-07T00:47:50.685571: step 3412, loss 0.0235945, acc 1
2016-09-07T00:47:51.379924: step 3413, loss 0.0112425, acc 1
2016-09-07T00:47:52.049604: step 3414, loss 0.00705395, acc 1
2016-09-07T00:47:52.726617: step 3415, loss 0.0155165, acc 1
2016-09-07T00:47:53.436927: step 3416, loss 0.147083, acc 0.96
2016-09-07T00:47:54.124208: step 3417, loss 0.0508542, acc 0.98
2016-09-07T00:47:54.815591: step 3418, loss 0.0246469, acc 1
2016-09-07T00:47:55.487864: step 3419, loss 0.0346735, acc 0.98
2016-09-07T00:47:56.182138: step 3420, loss 0.0416323, acc 0.98
2016-09-07T00:47:56.882214: step 3421, loss 0.192308, acc 0.96
2016-09-07T00:47:57.542998: step 3422, loss 0.0641855, acc 0.98
2016-09-07T00:47:58.258561: step 3423, loss 0.00904118, acc 1
2016-09-07T00:47:58.944398: step 3424, loss 0.0309014, acc 1
2016-09-07T00:47:59.621755: step 3425, loss 0.0386527, acc 0.98
2016-09-07T00:48:00.333048: step 3426, loss 0.0279844, acc 0.98
2016-09-07T00:48:01.033851: step 3427, loss 0.0703333, acc 0.96
2016-09-07T00:48:01.740004: step 3428, loss 0.0143914, acc 1
2016-09-07T00:48:02.395165: step 3429, loss 0.0827364, acc 0.96
2016-09-07T00:48:03.100483: step 3430, loss 0.0746388, acc 0.94
2016-09-07T00:48:03.786766: step 3431, loss 0.120615, acc 0.98
2016-09-07T00:48:04.512243: step 3432, loss 0.0444308, acc 1
2016-09-07T00:48:05.194552: step 3433, loss 0.0170035, acc 0.98
2016-09-07T00:48:05.883530: step 3434, loss 0.0796915, acc 0.94
2016-09-07T00:48:06.556725: step 3435, loss 0.00863533, acc 1
2016-09-07T00:48:07.227833: step 3436, loss 0.0459035, acc 0.98
2016-09-07T00:48:07.927050: step 3437, loss 0.0778339, acc 0.98
2016-09-07T00:48:08.626527: step 3438, loss 0.0334785, acc 1
2016-09-07T00:48:09.312660: step 3439, loss 0.088478, acc 0.96
2016-09-07T00:48:10.025840: step 3440, loss 0.0244663, acc 1
2016-09-07T00:48:10.713040: step 3441, loss 0.156571, acc 0.96
2016-09-07T00:48:11.419699: step 3442, loss 0.0484357, acc 1
2016-09-07T00:48:12.099366: step 3443, loss 0.0350767, acc 1
2016-09-07T00:48:12.764777: step 3444, loss 0.0813326, acc 0.98
2016-09-07T00:48:13.443458: step 3445, loss 0.0230148, acc 1
2016-09-07T00:48:14.132020: step 3446, loss 0.0454513, acc 0.98
2016-09-07T00:48:14.831579: step 3447, loss 0.0405904, acc 0.98
2016-09-07T00:48:15.519697: step 3448, loss 0.0569069, acc 0.98
2016-09-07T00:48:16.191945: step 3449, loss 0.144119, acc 0.98
2016-09-07T00:48:16.865828: step 3450, loss 0.0145005, acc 1
2016-09-07T00:48:17.566493: step 3451, loss 0.0482837, acc 1
2016-09-07T00:48:18.242804: step 3452, loss 0.0155851, acc 1
2016-09-07T00:48:18.943871: step 3453, loss 0.0437454, acc 0.98
2016-09-07T00:48:19.635691: step 3454, loss 0.0343215, acc 0.98
2016-09-07T00:48:20.322814: step 3455, loss 0.00475666, acc 1
2016-09-07T00:48:20.977463: step 3456, loss 0.00989371, acc 1
2016-09-07T00:48:21.640210: step 3457, loss 0.00734304, acc 1
2016-09-07T00:48:22.343861: step 3458, loss 0.0384812, acc 0.98
2016-09-07T00:48:23.025001: step 3459, loss 0.0451028, acc 0.98
2016-09-07T00:48:23.705267: step 3460, loss 0.0129454, acc 1
2016-09-07T00:48:24.403319: step 3461, loss 0.0120986, acc 1
2016-09-07T00:48:25.084929: step 3462, loss 0.0810308, acc 0.96
2016-09-07T00:48:25.761394: step 3463, loss 0.0215702, acc 1
2016-09-07T00:48:26.424237: step 3464, loss 0.0282891, acc 0.98
2016-09-07T00:48:27.144208: step 3465, loss 0.0476776, acc 0.98
2016-09-07T00:48:27.813590: step 3466, loss 0.0140765, acc 1
2016-09-07T00:48:28.486166: step 3467, loss 0.00278971, acc 1
2016-09-07T00:48:29.170591: step 3468, loss 0.0119302, acc 1
2016-09-07T00:48:29.853237: step 3469, loss 0.0693403, acc 0.96
2016-09-07T00:48:30.537510: step 3470, loss 0.181226, acc 0.98
2016-09-07T00:48:31.190251: step 3471, loss 0.0107186, acc 1
2016-09-07T00:48:31.895049: step 3472, loss 0.000799727, acc 1
2016-09-07T00:48:32.564122: step 3473, loss 0.0715093, acc 0.98
2016-09-07T00:48:33.233374: step 3474, loss 0.0221766, acc 1
2016-09-07T00:48:33.928017: step 3475, loss 0.0205283, acc 0.98
2016-09-07T00:48:34.624015: step 3476, loss 0.0180619, acc 0.98
2016-09-07T00:48:35.320250: step 3477, loss 0.0412246, acc 0.96
2016-09-07T00:48:36.006851: step 3478, loss 0.0388491, acc 0.96
2016-09-07T00:48:36.699918: step 3479, loss 0.0151744, acc 0.98
2016-09-07T00:48:37.369330: step 3480, loss 0.00541545, acc 1
2016-09-07T00:48:38.060509: step 3481, loss 0.0189189, acc 0.98
2016-09-07T00:48:38.749452: step 3482, loss 0.00602322, acc 1
2016-09-07T00:48:39.445163: step 3483, loss 0.00239315, acc 1
2016-09-07T00:48:40.125244: step 3484, loss 0.0117796, acc 1
2016-09-07T00:48:40.821847: step 3485, loss 0.0020721, acc 1
2016-09-07T00:48:41.543133: step 3486, loss 0.0123696, acc 1
2016-09-07T00:48:42.237236: step 3487, loss 0.0129682, acc 1
2016-09-07T00:48:42.916759: step 3488, loss 0.0481069, acc 0.98
2016-09-07T00:48:43.585001: step 3489, loss 0.0208505, acc 1
2016-09-07T00:48:44.276883: step 3490, loss 0.0245758, acc 0.98
2016-09-07T00:48:44.964676: step 3491, loss 0.0281299, acc 0.98
2016-09-07T00:48:45.624800: step 3492, loss 0.00257894, acc 1
2016-09-07T00:48:46.313336: step 3493, loss 0.00813669, acc 1
2016-09-07T00:48:46.982378: step 3494, loss 0.0126248, acc 1
2016-09-07T00:48:47.667235: step 3495, loss 0.000400543, acc 1
2016-09-07T00:48:48.349993: step 3496, loss 0.000485838, acc 1
2016-09-07T00:48:49.059039: step 3497, loss 0.0272652, acc 0.98
2016-09-07T00:48:49.770623: step 3498, loss 0.0279495, acc 0.98
2016-09-07T00:48:50.444210: step 3499, loss 0.0211701, acc 1
2016-09-07T00:48:51.149992: step 3500, loss 0.00718983, acc 1

Evaluation:
2016-09-07T00:48:54.279311: step 3500, loss 1.73696, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-3500

2016-09-07T00:48:55.940797: step 3501, loss 0.0507468, acc 0.96
2016-09-07T00:48:56.595194: step 3502, loss 0.00828623, acc 1
2016-09-07T00:48:57.265151: step 3503, loss 0.00367785, acc 1
2016-09-07T00:48:57.940663: step 3504, loss 0.0523724, acc 0.96
2016-09-07T00:48:58.649737: step 3505, loss 0.015749, acc 1
2016-09-07T00:48:59.335117: step 3506, loss 0.0870615, acc 0.96
2016-09-07T00:49:00.010569: step 3507, loss 0.0430608, acc 0.96
2016-09-07T00:49:00.748079: step 3508, loss 0.000953911, acc 1
2016-09-07T00:49:01.411926: step 3509, loss 0.0392816, acc 0.98
2016-09-07T00:49:02.104252: step 3510, loss 0.0603691, acc 0.96
2016-09-07T00:49:02.798649: step 3511, loss 0.0566335, acc 0.98
2016-09-07T00:49:03.483475: step 3512, loss 0.0354024, acc 0.98
2016-09-07T00:49:04.162617: step 3513, loss 0.0101223, acc 1
2016-09-07T00:49:04.843089: step 3514, loss 0.012995, acc 1
2016-09-07T00:49:05.543698: step 3515, loss 0.0186852, acc 0.98
2016-09-07T00:49:06.206812: step 3516, loss 0.00393944, acc 1
2016-09-07T00:49:06.889847: step 3517, loss 0.031448, acc 1
2016-09-07T00:49:07.587508: step 3518, loss 0.012805, acc 1
2016-09-07T00:49:08.280455: step 3519, loss 0.0161601, acc 1
2016-09-07T00:49:08.963079: step 3520, loss 0.023063, acc 0.98
2016-09-07T00:49:09.654604: step 3521, loss 0.0452059, acc 0.96
2016-09-07T00:49:10.364738: step 3522, loss 0.0788249, acc 0.96
2016-09-07T00:49:11.063272: step 3523, loss 0.241161, acc 0.94
2016-09-07T00:49:11.748245: step 3524, loss 0.00116747, acc 1
2016-09-07T00:49:12.424049: step 3525, loss 0.0564603, acc 0.98
2016-09-07T00:49:13.122100: step 3526, loss 0.00293135, acc 1
2016-09-07T00:49:13.799409: step 3527, loss 0.0324279, acc 0.98
2016-09-07T00:49:14.493868: step 3528, loss 0.0108758, acc 1
2016-09-07T00:49:15.184456: step 3529, loss 0.0056643, acc 1
2016-09-07T00:49:15.862624: step 3530, loss 0.0424644, acc 0.96
2016-09-07T00:49:16.546815: step 3531, loss 0.0343643, acc 0.98
2016-09-07T00:49:17.210109: step 3532, loss 0.0441411, acc 0.98
2016-09-07T00:49:17.898853: step 3533, loss 0.0765854, acc 0.98
2016-09-07T00:49:18.583165: step 3534, loss 0.00698118, acc 1
2016-09-07T00:49:19.260248: step 3535, loss 0.0264342, acc 1
2016-09-07T00:49:19.950644: step 3536, loss 0.0125991, acc 1
2016-09-07T00:49:20.635773: step 3537, loss 0.0717087, acc 0.96
2016-09-07T00:49:21.310140: step 3538, loss 0.0460296, acc 0.98
2016-09-07T00:49:21.988700: step 3539, loss 0.0203643, acc 1
2016-09-07T00:49:22.675095: step 3540, loss 0.0149474, acc 1
2016-09-07T00:49:23.362024: step 3541, loss 0.0257598, acc 0.98
2016-09-07T00:49:24.062601: step 3542, loss 0.0265707, acc 0.98
2016-09-07T00:49:24.797728: step 3543, loss 0.0197939, acc 1
2016-09-07T00:49:25.468845: step 3544, loss 0.0051469, acc 1
2016-09-07T00:49:26.157252: step 3545, loss 0.0305056, acc 0.98
2016-09-07T00:49:26.845840: step 3546, loss 0.0331113, acc 1
2016-09-07T00:49:27.531096: step 3547, loss 0.028884, acc 0.98
2016-09-07T00:49:28.221889: step 3548, loss 0.025035, acc 1
2016-09-07T00:49:28.899332: step 3549, loss 0.00367857, acc 1
2016-09-07T00:49:29.619348: step 3550, loss 0.0636537, acc 0.98
2016-09-07T00:49:30.309609: step 3551, loss 0.0147027, acc 1
2016-09-07T00:49:30.975409: step 3552, loss 0.0243013, acc 0.98
2016-09-07T00:49:31.666830: step 3553, loss 0.0750767, acc 0.96
2016-09-07T00:49:32.360612: step 3554, loss 0.041101, acc 0.98
2016-09-07T00:49:33.050600: step 3555, loss 0.0538394, acc 0.98
2016-09-07T00:49:33.719204: step 3556, loss 0.0149806, acc 1
2016-09-07T00:49:34.433459: step 3557, loss 0.025139, acc 1
2016-09-07T00:49:35.097674: step 3558, loss 0.00260273, acc 1
2016-09-07T00:49:35.774345: step 3559, loss 0.0308889, acc 1
2016-09-07T00:49:36.477584: step 3560, loss 0.0469155, acc 0.98
2016-09-07T00:49:37.144669: step 3561, loss 0.0262681, acc 0.98
2016-09-07T00:49:37.815515: step 3562, loss 0.0379777, acc 0.96
2016-09-07T00:49:38.500352: step 3563, loss 0.00891114, acc 1
2016-09-07T00:49:39.197314: step 3564, loss 0.0304702, acc 0.98
2016-09-07T00:49:39.869618: step 3565, loss 0.0229643, acc 0.98
2016-09-07T00:49:40.568680: step 3566, loss 0.0291167, acc 1
2016-09-07T00:49:41.248853: step 3567, loss 0.00777178, acc 1
2016-09-07T00:49:41.931912: step 3568, loss 0.156858, acc 0.98
2016-09-07T00:49:42.617535: step 3569, loss 0.00849727, acc 1
2016-09-07T00:49:43.286556: step 3570, loss 0.131361, acc 0.94
2016-09-07T00:49:43.995382: step 3571, loss 0.0152739, acc 1
2016-09-07T00:49:44.676777: step 3572, loss 0.00613544, acc 1
2016-09-07T00:49:45.366879: step 3573, loss 0.0013039, acc 1
2016-09-07T00:49:46.047975: step 3574, loss 0.0232933, acc 0.98
2016-09-07T00:49:46.750980: step 3575, loss 0.0177189, acc 1
2016-09-07T00:49:47.443646: step 3576, loss 0.028841, acc 0.98
2016-09-07T00:49:48.125570: step 3577, loss 0.0472818, acc 1
2016-09-07T00:49:48.842726: step 3578, loss 0.0567517, acc 0.98
2016-09-07T00:49:49.510597: step 3579, loss 0.00131396, acc 1
2016-09-07T00:49:50.178024: step 3580, loss 0.063165, acc 0.96
2016-09-07T00:49:50.862292: step 3581, loss 0.115698, acc 0.94
2016-09-07T00:49:51.553637: step 3582, loss 0.054338, acc 0.96
2016-09-07T00:49:52.231986: step 3583, loss 0.00819873, acc 1
2016-09-07T00:49:52.930763: step 3584, loss 0.0828894, acc 0.94
2016-09-07T00:49:53.628105: step 3585, loss 0.0141713, acc 1
2016-09-07T00:49:54.294555: step 3586, loss 0.0436864, acc 0.96
2016-09-07T00:49:54.998302: step 3587, loss 0.0865136, acc 0.96
2016-09-07T00:49:55.693443: step 3588, loss 0.154907, acc 0.94
2016-09-07T00:49:56.375218: step 3589, loss 0.0332942, acc 1
2016-09-07T00:49:57.073260: step 3590, loss 0.0701627, acc 0.98
2016-09-07T00:49:57.751405: step 3591, loss 0.0248637, acc 0.98
2016-09-07T00:49:58.467883: step 3592, loss 0.0739323, acc 0.98
2016-09-07T00:49:59.141804: step 3593, loss 0.0361014, acc 0.96
2016-09-07T00:49:59.877782: step 3594, loss 5.53631e-05, acc 1
2016-09-07T00:50:00.608237: step 3595, loss 0.0135943, acc 1
2016-09-07T00:50:01.330538: step 3596, loss 0.00388939, acc 1
2016-09-07T00:50:02.043787: step 3597, loss 0.0466007, acc 0.98
2016-09-07T00:50:02.702783: step 3598, loss 0.0122269, acc 1
2016-09-07T00:50:03.408649: step 3599, loss 0.00819548, acc 1
2016-09-07T00:50:04.096949: step 3600, loss 0.0178933, acc 1

Evaluation:
2016-09-07T00:50:07.277227: step 3600, loss 1.6881, acc 0.741088

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-3600

2016-09-07T00:50:08.975451: step 3601, loss 0.0138801, acc 1
2016-09-07T00:50:09.683100: step 3602, loss 0.0718029, acc 0.96
2016-09-07T00:50:10.368170: step 3603, loss 0.0563423, acc 0.96
2016-09-07T00:50:11.045949: step 3604, loss 0.0990206, acc 0.96
2016-09-07T00:50:11.758568: step 3605, loss 0.0663304, acc 0.94
2016-09-07T00:50:12.435181: step 3606, loss 0.0289945, acc 0.98
2016-09-07T00:50:13.114219: step 3607, loss 0.0236108, acc 0.98
2016-09-07T00:50:13.820799: step 3608, loss 0.0218186, acc 0.98
2016-09-07T00:50:14.518355: step 3609, loss 0.00101305, acc 1
2016-09-07T00:50:15.212343: step 3610, loss 0.0263148, acc 1
2016-09-07T00:50:15.881256: step 3611, loss 0.0373399, acc 0.96
2016-09-07T00:50:16.579178: step 3612, loss 0.00311102, acc 1
2016-09-07T00:50:17.286295: step 3613, loss 0.0186939, acc 0.98
2016-09-07T00:50:17.968090: step 3614, loss 0.0390286, acc 0.96
2016-09-07T00:50:18.669507: step 3615, loss 0.104644, acc 0.94
2016-09-07T00:50:19.359743: step 3616, loss 0.00594497, acc 1
2016-09-07T00:50:20.075045: step 3617, loss 0.0038594, acc 1
2016-09-07T00:50:20.770621: step 3618, loss 0.0498015, acc 0.96
2016-09-07T00:50:21.452026: step 3619, loss 0.0535522, acc 0.96
2016-09-07T00:50:22.140127: step 3620, loss 0.0152358, acc 1
2016-09-07T00:50:22.834714: step 3621, loss 0.0530568, acc 0.98
2016-09-07T00:50:23.502053: step 3622, loss 0.0380716, acc 0.96
2016-09-07T00:50:24.204922: step 3623, loss 0.0146665, acc 1
2016-09-07T00:50:24.913611: step 3624, loss 0.0544672, acc 0.96
2016-09-07T00:50:25.586711: step 3625, loss 0.0275187, acc 1
2016-09-07T00:50:26.273834: step 3626, loss 0.0437926, acc 1
2016-09-07T00:50:26.964665: step 3627, loss 0.0370803, acc 0.96
2016-09-07T00:50:27.650616: step 3628, loss 0.0695155, acc 0.94
2016-09-07T00:50:28.330338: step 3629, loss 0.0552694, acc 0.98
2016-09-07T00:50:29.019426: step 3630, loss 0.0339069, acc 0.98
2016-09-07T00:50:29.709473: step 3631, loss 0.00548511, acc 1
2016-09-07T00:50:30.377405: step 3632, loss 0.036727, acc 0.98
2016-09-07T00:50:31.075962: step 3633, loss 0.00187794, acc 1
2016-09-07T00:50:31.771410: step 3634, loss 0.184839, acc 0.96
2016-09-07T00:50:32.478036: step 3635, loss 0.09783, acc 0.96
2016-09-07T00:50:33.170911: step 3636, loss 0.0414224, acc 0.98
2016-09-07T00:50:33.864803: step 3637, loss 0.0571086, acc 0.96
2016-09-07T00:50:34.554640: step 3638, loss 0.0248242, acc 0.98
2016-09-07T00:50:35.230171: step 3639, loss 0.032313, acc 0.98
2016-09-07T00:50:35.912082: step 3640, loss 0.0301902, acc 1
2016-09-07T00:50:36.595720: step 3641, loss 0.0105731, acc 1
2016-09-07T00:50:37.291898: step 3642, loss 0.00660214, acc 1
2016-09-07T00:50:37.987099: step 3643, loss 0.0244547, acc 1
2016-09-07T00:50:38.691120: step 3644, loss 0.0155429, acc 1
2016-09-07T00:50:39.389067: step 3645, loss 0.0405296, acc 1
2016-09-07T00:50:40.080733: step 3646, loss 0.0135871, acc 1
2016-09-07T00:50:40.767660: step 3647, loss 0.0342711, acc 0.98
2016-09-07T00:50:41.423308: step 3648, loss 0.00899376, acc 1
2016-09-07T00:50:42.103316: step 3649, loss 0.011608, acc 1
2016-09-07T00:50:42.801413: step 3650, loss 0.0518523, acc 0.98
2016-09-07T00:50:43.485702: step 3651, loss 0.037612, acc 0.98
2016-09-07T00:50:44.175197: step 3652, loss 0.00854374, acc 1
2016-09-07T00:50:44.840924: step 3653, loss 0.0108948, acc 1
2016-09-07T00:50:45.529761: step 3654, loss 0.0416892, acc 0.96
2016-09-07T00:50:46.211735: step 3655, loss 0.00647065, acc 1
2016-09-07T00:50:46.896773: step 3656, loss 0.0453272, acc 0.96
2016-09-07T00:50:47.582585: step 3657, loss 0.0192695, acc 0.98
2016-09-07T00:50:48.277449: step 3658, loss 0.0160028, acc 1
2016-09-07T00:50:48.990695: step 3659, loss 0.0096324, acc 1
2016-09-07T00:50:49.674500: step 3660, loss 0.0190371, acc 0.98
2016-09-07T00:50:50.350995: step 3661, loss 0.0677765, acc 0.94
2016-09-07T00:50:51.045686: step 3662, loss 0.0231061, acc 1
2016-09-07T00:50:51.735687: step 3663, loss 0.0306445, acc 0.98
2016-09-07T00:50:52.445041: step 3664, loss 0.00844078, acc 1
2016-09-07T00:50:53.125142: step 3665, loss 0.0370929, acc 0.96
2016-09-07T00:50:53.829182: step 3666, loss 0.0213079, acc 0.98
2016-09-07T00:50:54.514514: step 3667, loss 0.0142657, acc 1
2016-09-07T00:50:55.183583: step 3668, loss 0.00317172, acc 1
2016-09-07T00:50:55.896528: step 3669, loss 0.0475903, acc 0.98
2016-09-07T00:50:56.576341: step 3670, loss 0.0373701, acc 0.96
2016-09-07T00:50:57.270287: step 3671, loss 0.0188102, acc 0.98
2016-09-07T00:50:57.936280: step 3672, loss 0.00675614, acc 1
2016-09-07T00:50:58.654893: step 3673, loss 0.00139778, acc 1
2016-09-07T00:50:59.346194: step 3674, loss 0.0060776, acc 1
2016-09-07T00:51:00.027916: step 3675, loss 0.000625131, acc 1
2016-09-07T00:51:00.741509: step 3676, loss 0.0261903, acc 1
2016-09-07T00:51:01.411307: step 3677, loss 0.0288772, acc 0.98
2016-09-07T00:51:02.091110: step 3678, loss 0.0225984, acc 0.98
2016-09-07T00:51:02.768107: step 3679, loss 0.0219413, acc 0.98
2016-09-07T00:51:03.460277: step 3680, loss 0.0436888, acc 0.96
2016-09-07T00:51:04.132387: step 3681, loss 0.133086, acc 0.96
2016-09-07T00:51:04.828027: step 3682, loss 0.0275637, acc 0.98
2016-09-07T00:51:05.517500: step 3683, loss 0.00109897, acc 1
2016-09-07T00:51:06.199471: step 3684, loss 0.053205, acc 0.96
2016-09-07T00:51:06.899286: step 3685, loss 0.0506831, acc 0.98
2016-09-07T00:51:07.555237: step 3686, loss 0.0197117, acc 0.98
2016-09-07T00:51:08.278130: step 3687, loss 0.00371964, acc 1
2016-09-07T00:51:09.004731: step 3688, loss 0.120897, acc 0.98
2016-09-07T00:51:09.684040: step 3689, loss 0.0382594, acc 0.96
2016-09-07T00:51:10.353062: step 3690, loss 0.0147041, acc 1
2016-09-07T00:51:11.050472: step 3691, loss 0.023356, acc 1
2016-09-07T00:51:11.732944: step 3692, loss 0.0240246, acc 1
2016-09-07T00:51:12.393294: step 3693, loss 0.0183719, acc 1
2016-09-07T00:51:13.093400: step 3694, loss 0.0933943, acc 0.96
2016-09-07T00:51:13.786096: step 3695, loss 0.0293484, acc 1
2016-09-07T00:51:14.479574: step 3696, loss 0.0530935, acc 0.98
2016-09-07T00:51:15.167841: step 3697, loss 0.00318471, acc 1
2016-09-07T00:51:15.853477: step 3698, loss 0.0174904, acc 1
2016-09-07T00:51:16.521870: step 3699, loss 0.0272096, acc 0.98
2016-09-07T00:51:17.191728: step 3700, loss 0.00366464, acc 1

Evaluation:
2016-09-07T00:51:20.348322: step 3700, loss 1.48651, acc 0.728893

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-3700

2016-09-07T00:51:22.111278: step 3701, loss 0.0415465, acc 0.98
2016-09-07T00:51:22.780811: step 3702, loss 0.0199743, acc 0.98
2016-09-07T00:51:23.446933: step 3703, loss 0.0101929, acc 1
2016-09-07T00:51:24.110239: step 3704, loss 0.0414132, acc 0.98
2016-09-07T00:51:24.779871: step 3705, loss 0.0356076, acc 1
2016-09-07T00:51:25.467557: step 3706, loss 0.0215608, acc 0.98
2016-09-07T00:51:26.197627: step 3707, loss 0.014732, acc 1
2016-09-07T00:51:26.872572: step 3708, loss 0.00736425, acc 1
2016-09-07T00:51:27.562661: step 3709, loss 0.0237564, acc 0.98
2016-09-07T00:51:28.243849: step 3710, loss 0.00364482, acc 1
2016-09-07T00:51:28.942653: step 3711, loss 0.0282759, acc 1
2016-09-07T00:51:29.636045: step 3712, loss 0.026904, acc 0.98
2016-09-07T00:51:30.319668: step 3713, loss 0.049623, acc 0.98
2016-09-07T00:51:31.023197: step 3714, loss 0.0728204, acc 0.96
2016-09-07T00:51:31.716233: step 3715, loss 0.0262614, acc 0.98
2016-09-07T00:51:32.416908: step 3716, loss 0.00397576, acc 1
2016-09-07T00:51:33.111362: step 3717, loss 0.00456801, acc 1
2016-09-07T00:51:33.792374: step 3718, loss 0.0401655, acc 0.98
2016-09-07T00:51:34.474175: step 3719, loss 0.00412599, acc 1
2016-09-07T00:51:35.162918: step 3720, loss 0.0496816, acc 0.98
2016-09-07T00:51:35.874571: step 3721, loss 0.00101869, acc 1
2016-09-07T00:51:36.541837: step 3722, loss 0.00538686, acc 1
2016-09-07T00:51:37.242027: step 3723, loss 0.0249256, acc 0.98
2016-09-07T00:51:37.932145: step 3724, loss 0.0209583, acc 0.98
2016-09-07T00:51:38.614626: step 3725, loss 0.0178613, acc 0.98
2016-09-07T00:51:39.304777: step 3726, loss 0.0951591, acc 0.98
2016-09-07T00:51:39.985137: step 3727, loss 0.0208988, acc 0.98
2016-09-07T00:51:40.698266: step 3728, loss 0.0130142, acc 1
2016-09-07T00:51:41.372542: step 3729, loss 0.0313734, acc 0.98
2016-09-07T00:51:42.046849: step 3730, loss 0.0232363, acc 0.98
2016-09-07T00:51:42.740872: step 3731, loss 0.0151464, acc 1
2016-09-07T00:51:43.411595: step 3732, loss 0.00128278, acc 1
2016-09-07T00:51:44.110458: step 3733, loss 0.0314947, acc 0.98
2016-09-07T00:51:44.794680: step 3734, loss 0.0368943, acc 0.98
2016-09-07T00:51:45.503662: step 3735, loss 0.0069872, acc 1
2016-09-07T00:51:46.193731: step 3736, loss 0.0323915, acc 0.98
2016-09-07T00:51:46.900542: step 3737, loss 0.000850993, acc 1
2016-09-07T00:51:47.605908: step 3738, loss 0.0429893, acc 0.98
2016-09-07T00:51:48.287632: step 3739, loss 0.0761264, acc 0.96
2016-09-07T00:51:48.979974: step 3740, loss 0.0307809, acc 1
2016-09-07T00:51:49.645652: step 3741, loss 0.00646484, acc 1
2016-09-07T00:51:50.348529: step 3742, loss 0.00422774, acc 1
2016-09-07T00:51:51.033886: step 3743, loss 0.0377587, acc 0.96
2016-09-07T00:51:51.710704: step 3744, loss 0.029812, acc 0.98
2016-09-07T00:51:52.398362: step 3745, loss 0.031712, acc 0.98
2016-09-07T00:51:53.106272: step 3746, loss 0.031671, acc 0.98
2016-09-07T00:51:53.775848: step 3747, loss 0.122869, acc 0.96
2016-09-07T00:51:54.458240: step 3748, loss 0.0296432, acc 0.98
2016-09-07T00:51:55.179637: step 3749, loss 0.00392479, acc 1
2016-09-07T00:51:55.897396: step 3750, loss 0.190342, acc 0.94
2016-09-07T00:51:56.589457: step 3751, loss 0.0249483, acc 0.98
2016-09-07T00:51:57.287970: step 3752, loss 0.0389986, acc 0.98
2016-09-07T00:51:57.969995: step 3753, loss 0.00886739, acc 1
2016-09-07T00:51:58.682573: step 3754, loss 0.00061132, acc 1
2016-09-07T00:51:59.351025: step 3755, loss 0.0172524, acc 1
2016-09-07T00:52:00.054740: step 3756, loss 0.0522881, acc 0.96
2016-09-07T00:52:00.793640: step 3757, loss 0.0520287, acc 0.98
2016-09-07T00:52:01.488298: step 3758, loss 0.0148812, acc 1
2016-09-07T00:52:02.193045: step 3759, loss 0.0607976, acc 0.94
2016-09-07T00:52:02.886255: step 3760, loss 0.0299819, acc 1
2016-09-07T00:52:03.577631: step 3761, loss 0.0172808, acc 1
2016-09-07T00:52:04.271766: step 3762, loss 0.016091, acc 0.98
2016-09-07T00:52:04.967410: step 3763, loss 0.0281851, acc 1
2016-09-07T00:52:05.659043: step 3764, loss 0.030177, acc 0.98
2016-09-07T00:52:06.364507: step 3765, loss 0.0100609, acc 1
2016-09-07T00:52:07.054227: step 3766, loss 0.0166871, acc 1
2016-09-07T00:52:07.713698: step 3767, loss 0.0405384, acc 0.98
2016-09-07T00:52:08.416616: step 3768, loss 0.0584646, acc 0.96
2016-09-07T00:52:09.110644: step 3769, loss 0.0215119, acc 1
2016-09-07T00:52:09.798472: step 3770, loss 0.0547927, acc 0.98
2016-09-07T00:52:10.474720: step 3771, loss 0.0849181, acc 0.96
2016-09-07T00:52:11.172383: step 3772, loss 0.0229331, acc 1
2016-09-07T00:52:11.883070: step 3773, loss 0.003731, acc 1
2016-09-07T00:52:12.543100: step 3774, loss 0.0284437, acc 0.98
2016-09-07T00:52:13.245408: step 3775, loss 0.0249312, acc 0.98
2016-09-07T00:52:13.938183: step 3776, loss 0.0850351, acc 0.96
2016-09-07T00:52:14.620980: step 3777, loss 0.0210565, acc 0.98
2016-09-07T00:52:15.289439: step 3778, loss 0.0481618, acc 0.98
2016-09-07T00:52:15.989006: step 3779, loss 0.0471979, acc 0.98
2016-09-07T00:52:16.685700: step 3780, loss 0.104593, acc 0.94
2016-09-07T00:52:17.349200: step 3781, loss 0.0408064, acc 0.98
2016-09-07T00:52:18.046591: step 3782, loss 0.0202566, acc 0.98
2016-09-07T00:52:18.731408: step 3783, loss 0.00421881, acc 1
2016-09-07T00:52:19.412702: step 3784, loss 0.026422, acc 0.98
2016-09-07T00:52:20.106575: step 3785, loss 0.00455231, acc 1
2016-09-07T00:52:20.790285: step 3786, loss 0.00299985, acc 1
2016-09-07T00:52:21.479583: step 3787, loss 0.0491684, acc 0.98
2016-09-07T00:52:22.147718: step 3788, loss 0.0158778, acc 1
2016-09-07T00:52:22.861996: step 3789, loss 0.0191753, acc 0.98
2016-09-07T00:52:23.536973: step 3790, loss 0.0540961, acc 0.96
2016-09-07T00:52:24.210453: step 3791, loss 0.00428206, acc 1
2016-09-07T00:52:24.875820: step 3792, loss 0.0232515, acc 0.98
2016-09-07T00:52:25.589226: step 3793, loss 0.040302, acc 0.98
2016-09-07T00:52:26.311302: step 3794, loss 0.00482251, acc 1
2016-09-07T00:52:26.993236: step 3795, loss 0.00196231, acc 1
2016-09-07T00:52:27.688182: step 3796, loss 0.100119, acc 0.98
2016-09-07T00:52:28.390852: step 3797, loss 0.0529267, acc 0.98
2016-09-07T00:52:29.073839: step 3798, loss 0.0386118, acc 0.98
2016-09-07T00:52:29.763553: step 3799, loss 0.0131868, acc 1
2016-09-07T00:52:30.476213: step 3800, loss 0.00840661, acc 1

Evaluation:
2016-09-07T00:52:33.621307: step 3800, loss 1.59554, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-3800

2016-09-07T00:52:35.262388: step 3801, loss 0.00199632, acc 1
2016-09-07T00:52:35.975215: step 3802, loss 0.0236434, acc 0.98
2016-09-07T00:52:36.665651: step 3803, loss 0.0459309, acc 0.98
2016-09-07T00:52:37.365883: step 3804, loss 0.0167038, acc 0.98
2016-09-07T00:52:38.057050: step 3805, loss 0.0673783, acc 0.98
2016-09-07T00:52:38.777811: step 3806, loss 0.0344935, acc 0.98
2016-09-07T00:52:39.464950: step 3807, loss 0.0681686, acc 0.98
2016-09-07T00:52:40.132563: step 3808, loss 0.016628, acc 0.98
2016-09-07T00:52:40.806383: step 3809, loss 0.00131679, acc 1
2016-09-07T00:52:41.471694: step 3810, loss 0.0353474, acc 0.98
2016-09-07T00:52:42.142551: step 3811, loss 0.0183235, acc 0.98
2016-09-07T00:52:42.843769: step 3812, loss 0.0783262, acc 0.96
2016-09-07T00:52:43.572543: step 3813, loss 0.0054469, acc 1
2016-09-07T00:52:44.271795: step 3814, loss 0.0318108, acc 0.98
2016-09-07T00:52:45.021198: step 3815, loss 0.175817, acc 0.98
2016-09-07T00:52:45.737423: step 3816, loss 0.0483065, acc 0.98
2016-09-07T00:52:46.438028: step 3817, loss 0.0186505, acc 1
2016-09-07T00:52:47.120875: step 3818, loss 0.114057, acc 0.92
2016-09-07T00:52:47.783061: step 3819, loss 0.00440218, acc 1
2016-09-07T00:52:48.456670: step 3820, loss 0.0259254, acc 0.98
2016-09-07T00:52:49.149530: step 3821, loss 0.032988, acc 0.98
2016-09-07T00:52:49.829370: step 3822, loss 0.0205839, acc 0.98
2016-09-07T00:52:50.528667: step 3823, loss 0.0036541, acc 1
2016-09-07T00:52:51.212386: step 3824, loss 0.0208436, acc 1
2016-09-07T00:52:51.910843: step 3825, loss 0.102554, acc 0.92
2016-09-07T00:52:52.604999: step 3826, loss 0.0748613, acc 0.96
2016-09-07T00:52:53.298479: step 3827, loss 0.0181335, acc 1
2016-09-07T00:52:54.000015: step 3828, loss 0.0198531, acc 0.98
2016-09-07T00:52:54.677026: step 3829, loss 0.00333223, acc 1
2016-09-07T00:52:55.357738: step 3830, loss 0.00698088, acc 1
2016-09-07T00:52:56.039888: step 3831, loss 0.0656403, acc 0.96
2016-09-07T00:52:56.724221: step 3832, loss 0.0290059, acc 1
2016-09-07T00:52:57.430594: step 3833, loss 0.0459178, acc 0.98
2016-09-07T00:52:58.122779: step 3834, loss 0.0189402, acc 0.98
2016-09-07T00:52:58.830304: step 3835, loss 0.0715701, acc 0.94
2016-09-07T00:52:59.518927: step 3836, loss 0.00125272, acc 1
2016-09-07T00:53:00.211584: step 3837, loss 0.00564096, acc 1
2016-09-07T00:53:00.917681: step 3838, loss 0.0367962, acc 0.98
2016-09-07T00:53:01.592864: step 3839, loss 0.00335829, acc 1
2016-09-07T00:53:02.218557: step 3840, loss 0.0774638, acc 0.977273
2016-09-07T00:53:02.898877: step 3841, loss 0.0578785, acc 0.98
2016-09-07T00:53:03.580309: step 3842, loss 0.0203924, acc 0.98
2016-09-07T00:53:04.250169: step 3843, loss 0.0251596, acc 0.98
2016-09-07T00:53:04.950751: step 3844, loss 0.0190093, acc 0.98
2016-09-07T00:53:05.656529: step 3845, loss 0.0279961, acc 0.98
2016-09-07T00:53:06.338107: step 3846, loss 0.0194902, acc 0.98
2016-09-07T00:53:07.015871: step 3847, loss 0.00510524, acc 1
2016-09-07T00:53:07.726877: step 3848, loss 0.0210801, acc 0.98
2016-09-07T00:53:08.419059: step 3849, loss 0.0678896, acc 0.98
2016-09-07T00:53:09.105575: step 3850, loss 0.057917, acc 0.96
2016-09-07T00:53:09.798585: step 3851, loss 0.00854308, acc 1
2016-09-07T00:53:10.499319: step 3852, loss 0.0548542, acc 0.98
2016-09-07T00:53:11.202806: step 3853, loss 0.0301705, acc 0.98
2016-09-07T00:53:11.892944: step 3854, loss 0.00299984, acc 1
2016-09-07T00:53:12.559183: step 3855, loss 0.000802502, acc 1
2016-09-07T00:53:13.243682: step 3856, loss 0.021215, acc 0.98
2016-09-07T00:53:13.914535: step 3857, loss 0.15713, acc 0.98
2016-09-07T00:53:14.590857: step 3858, loss 0.0164554, acc 1
2016-09-07T00:53:15.295294: step 3859, loss 0.0174299, acc 0.98
2016-09-07T00:53:16.002438: step 3860, loss 0.0391165, acc 0.98
2016-09-07T00:53:16.683412: step 3861, loss 0.0283824, acc 0.98
2016-09-07T00:53:17.365506: step 3862, loss 0.0361205, acc 0.98
2016-09-07T00:53:18.064775: step 3863, loss 0.00975929, acc 1
2016-09-07T00:53:18.734091: step 3864, loss 0.0686475, acc 0.98
2016-09-07T00:53:19.452842: step 3865, loss 0.00858057, acc 1
2016-09-07T00:53:20.163243: step 3866, loss 0.0233725, acc 0.98
2016-09-07T00:53:20.845868: step 3867, loss 0.0365341, acc 0.98
2016-09-07T00:53:21.546581: step 3868, loss 0.0374863, acc 0.98
2016-09-07T00:53:22.205886: step 3869, loss 0.0554672, acc 0.96
2016-09-07T00:53:22.899739: step 3870, loss 0.00846291, acc 1
2016-09-07T00:53:23.586576: step 3871, loss 0.0563716, acc 0.98
2016-09-07T00:53:24.269500: step 3872, loss 0.0511503, acc 0.98
2016-09-07T00:53:24.956168: step 3873, loss 0.0630186, acc 0.98
2016-09-07T00:53:25.639117: step 3874, loss 0.0265394, acc 1
2016-09-07T00:53:26.351261: step 3875, loss 0.0487548, acc 0.96
2016-09-07T00:53:27.038977: step 3876, loss 0.0351547, acc 0.98
2016-09-07T00:53:27.770454: step 3877, loss 0.0304461, acc 0.98
2016-09-07T00:53:28.453651: step 3878, loss 0.0531819, acc 0.98
2016-09-07T00:53:29.123790: step 3879, loss 0.0513613, acc 0.98
2016-09-07T00:53:29.826367: step 3880, loss 0.0158298, acc 1
2016-09-07T00:53:30.515534: step 3881, loss 0.056599, acc 0.98
2016-09-07T00:53:31.209801: step 3882, loss 0.0264052, acc 1
2016-09-07T00:53:31.859696: step 3883, loss 0.0160004, acc 0.98
2016-09-07T00:53:32.554727: step 3884, loss 0.0180461, acc 0.98
2016-09-07T00:53:33.244117: step 3885, loss 0.0346688, acc 0.98
2016-09-07T00:53:33.912259: step 3886, loss 0.0727394, acc 0.96
2016-09-07T00:53:34.594969: step 3887, loss 0.0206372, acc 1
2016-09-07T00:53:35.261826: step 3888, loss 0.0656057, acc 0.98
2016-09-07T00:53:35.949959: step 3889, loss 0.108617, acc 0.98
2016-09-07T00:53:36.622394: step 3890, loss 0.0115701, acc 1
2016-09-07T00:53:37.319034: step 3891, loss 0.0492815, acc 0.96
2016-09-07T00:53:37.977453: step 3892, loss 0.00892996, acc 1
2016-09-07T00:53:38.669500: step 3893, loss 0.0140952, acc 1
2016-09-07T00:53:39.354583: step 3894, loss 0.0311441, acc 1
2016-09-07T00:53:40.040277: step 3895, loss 0.0705059, acc 0.94
2016-09-07T00:53:40.728686: step 3896, loss 0.0165291, acc 1
2016-09-07T00:53:41.409395: step 3897, loss 0.0150824, acc 1
2016-09-07T00:53:42.111678: step 3898, loss 0.00753526, acc 1
2016-09-07T00:53:42.788568: step 3899, loss 0.0096028, acc 1
2016-09-07T00:53:43.469166: step 3900, loss 0.012606, acc 1

Evaluation:
2016-09-07T00:53:46.580397: step 3900, loss 1.89118, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-3900

2016-09-07T00:53:48.224053: step 3901, loss 0.0510957, acc 0.96
2016-09-07T00:53:48.924895: step 3902, loss 0.0524068, acc 0.98
2016-09-07T00:53:49.613378: step 3903, loss 0.0604026, acc 0.96
2016-09-07T00:53:50.303377: step 3904, loss 0.0289532, acc 0.98
2016-09-07T00:53:50.972750: step 3905, loss 0.0151222, acc 1
2016-09-07T00:53:51.688159: step 3906, loss 0.013236, acc 1
2016-09-07T00:53:52.362458: step 3907, loss 0.0251755, acc 1
2016-09-07T00:53:53.057748: step 3908, loss 0.011986, acc 1
2016-09-07T00:53:53.739261: step 3909, loss 0.0238063, acc 1
2016-09-07T00:53:54.416858: step 3910, loss 0.0121464, acc 1
2016-09-07T00:53:55.120506: step 3911, loss 0.0192633, acc 1
2016-09-07T00:53:55.816045: step 3912, loss 0.0204481, acc 1
2016-09-07T00:53:56.514890: step 3913, loss 0.00514673, acc 1
2016-09-07T00:53:57.176918: step 3914, loss 0.0167498, acc 0.98
2016-09-07T00:53:57.855233: step 3915, loss 0.00239058, acc 1
2016-09-07T00:53:58.538002: step 3916, loss 0.0290591, acc 0.98
2016-09-07T00:53:59.216762: step 3917, loss 0.0195946, acc 1
2016-09-07T00:53:59.901917: step 3918, loss 0.0010818, acc 1
2016-09-07T00:54:00.641747: step 3919, loss 0.0122303, acc 1
2016-09-07T00:54:01.366299: step 3920, loss 0.0046852, acc 1
2016-09-07T00:54:02.027843: step 3921, loss 0.0672951, acc 0.98
2016-09-07T00:54:02.701009: step 3922, loss 0.00438014, acc 1
2016-09-07T00:54:03.386712: step 3923, loss 0.0198124, acc 1
2016-09-07T00:54:04.070578: step 3924, loss 0.0501046, acc 0.98
2016-09-07T00:54:04.750168: step 3925, loss 0.0149722, acc 1
2016-09-07T00:54:05.445735: step 3926, loss 0.000834525, acc 1
2016-09-07T00:54:06.164162: step 3927, loss 0.011042, acc 1
2016-09-07T00:54:06.844060: step 3928, loss 0.0334957, acc 0.98
2016-09-07T00:54:07.534675: step 3929, loss 0.100354, acc 0.98
2016-09-07T00:54:08.212696: step 3930, loss 0.0780414, acc 0.96
2016-09-07T00:54:08.897771: step 3931, loss 0.128223, acc 0.94
2016-09-07T00:54:09.594347: step 3932, loss 0.0770439, acc 0.98
2016-09-07T00:54:10.284649: step 3933, loss 0.037112, acc 0.98
2016-09-07T00:54:11.004830: step 3934, loss 0.0628569, acc 0.96
2016-09-07T00:54:11.676913: step 3935, loss 0.0584467, acc 0.96
2016-09-07T00:54:12.351510: step 3936, loss 0.05121, acc 0.96
2016-09-07T00:54:13.039201: step 3937, loss 0.0205337, acc 1
2016-09-07T00:54:13.730232: step 3938, loss 0.0291086, acc 1
2016-09-07T00:54:14.424070: step 3939, loss 0.060312, acc 0.98
2016-09-07T00:54:15.117384: step 3940, loss 0.0349593, acc 0.98
2016-09-07T00:54:15.811151: step 3941, loss 0.0146533, acc 1
2016-09-07T00:54:16.478588: step 3942, loss 0.0504141, acc 0.98
2016-09-07T00:54:17.140630: step 3943, loss 0.0407985, acc 0.98
2016-09-07T00:54:17.830683: step 3944, loss 0.0190378, acc 0.98
2016-09-07T00:54:18.525159: step 3945, loss 0.00218595, acc 1
2016-09-07T00:54:19.222117: step 3946, loss 0.0501537, acc 0.96
2016-09-07T00:54:19.908027: step 3947, loss 0.00234785, acc 1
2016-09-07T00:54:20.612519: step 3948, loss 0.0272743, acc 0.98
2016-09-07T00:54:21.301022: step 3949, loss 0.0293527, acc 0.98
2016-09-07T00:54:21.979782: step 3950, loss 0.0316758, acc 0.98
2016-09-07T00:54:22.665278: step 3951, loss 0.0153551, acc 1
2016-09-07T00:54:23.364023: step 3952, loss 0.00867684, acc 1
2016-09-07T00:54:24.036306: step 3953, loss 0.0919982, acc 0.96
2016-09-07T00:54:24.730743: step 3954, loss 0.0595984, acc 0.98
2016-09-07T00:54:25.429759: step 3955, loss 0.0813812, acc 0.96
2016-09-07T00:54:26.107897: step 3956, loss 0.0220067, acc 1
2016-09-07T00:54:26.786685: step 3957, loss 0.0045804, acc 1
2016-09-07T00:54:27.486936: step 3958, loss 0.00354046, acc 1
2016-09-07T00:54:28.166611: step 3959, loss 0.0472469, acc 0.96
2016-09-07T00:54:28.872966: step 3960, loss 0.0288622, acc 0.98
2016-09-07T00:54:29.554157: step 3961, loss 0.0571254, acc 0.98
2016-09-07T00:54:30.248465: step 3962, loss 0.00431973, acc 1
2016-09-07T00:54:30.945796: step 3963, loss 0.00959663, acc 1
2016-09-07T00:54:31.647191: step 3964, loss 0.0239629, acc 1
2016-09-07T00:54:32.344736: step 3965, loss 0.00231237, acc 1
2016-09-07T00:54:33.040206: step 3966, loss 0.0503134, acc 0.98
2016-09-07T00:54:33.745396: step 3967, loss 0.00860424, acc 1
2016-09-07T00:54:34.412037: step 3968, loss 0.0306493, acc 0.98
2016-09-07T00:54:35.122274: step 3969, loss 0.00291411, acc 1
2016-09-07T00:54:35.823962: step 3970, loss 0.0532279, acc 0.96
2016-09-07T00:54:36.507995: step 3971, loss 0.0238776, acc 1
2016-09-07T00:54:37.198451: step 3972, loss 0.0700158, acc 0.96
2016-09-07T00:54:37.898003: step 3973, loss 0.0253691, acc 0.98
2016-09-07T00:54:38.609636: step 3974, loss 0.0167461, acc 1
2016-09-07T00:54:39.267093: step 3975, loss 0.00426857, acc 1
2016-09-07T00:54:39.977095: step 3976, loss 0.0248006, acc 0.98
2016-09-07T00:54:40.647012: step 3977, loss 0.0546625, acc 0.98
2016-09-07T00:54:41.337250: step 3978, loss 0.00301984, acc 1
2016-09-07T00:54:42.021263: step 3979, loss 0.0700992, acc 0.94
2016-09-07T00:54:42.730170: step 3980, loss 0.020565, acc 1
2016-09-07T00:54:43.419498: step 3981, loss 0.0559921, acc 0.98
2016-09-07T00:54:44.079852: step 3982, loss 0.0541721, acc 0.98
2016-09-07T00:54:44.783851: step 3983, loss 0.130589, acc 0.94
2016-09-07T00:54:45.481419: step 3984, loss 0.0158255, acc 1
2016-09-07T00:54:46.163692: step 3985, loss 0.0509352, acc 0.98
2016-09-07T00:54:46.853541: step 3986, loss 0.0364434, acc 0.98
2016-09-07T00:54:47.547774: step 3987, loss 0.00581268, acc 1
2016-09-07T00:54:48.260679: step 3988, loss 0.0266877, acc 1
2016-09-07T00:54:48.933608: step 3989, loss 0.0221285, acc 1
2016-09-07T00:54:49.618173: step 3990, loss 0.0541795, acc 0.96
2016-09-07T00:54:50.300825: step 3991, loss 0.0123423, acc 1
2016-09-07T00:54:50.991828: step 3992, loss 0.0358301, acc 0.98
2016-09-07T00:54:51.671963: step 3993, loss 0.000460812, acc 1
2016-09-07T00:54:52.346471: step 3994, loss 0.0620559, acc 0.98
2016-09-07T00:54:53.060768: step 3995, loss 0.00643885, acc 1
2016-09-07T00:54:53.734190: step 3996, loss 0.00530577, acc 1
2016-09-07T00:54:54.404118: step 3997, loss 0.0200567, acc 1
2016-09-07T00:54:55.110536: step 3998, loss 0.0231509, acc 0.98
2016-09-07T00:54:55.808359: step 3999, loss 0.0311982, acc 1
2016-09-07T00:54:56.506921: step 4000, loss 0.017685, acc 1

Evaluation:
2016-09-07T00:54:59.635008: step 4000, loss 1.73945, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-4000

2016-09-07T00:55:01.408681: step 4001, loss 0.0095168, acc 1
2016-09-07T00:55:02.066007: step 4002, loss 0.0520326, acc 0.98
2016-09-07T00:55:02.778194: step 4003, loss 0.241125, acc 0.94
2016-09-07T00:55:03.466423: step 4004, loss 0.0451751, acc 0.98
2016-09-07T00:55:04.148802: step 4005, loss 0.0222461, acc 0.98
2016-09-07T00:55:04.840744: step 4006, loss 0.0160676, acc 1
2016-09-07T00:55:05.538613: step 4007, loss 0.0182244, acc 1
2016-09-07T00:55:06.226690: step 4008, loss 0.0230333, acc 0.98
2016-09-07T00:55:06.895639: step 4009, loss 0.017168, acc 1
2016-09-07T00:55:07.596931: step 4010, loss 0.0190613, acc 1
2016-09-07T00:55:08.278762: step 4011, loss 0.00470224, acc 1
2016-09-07T00:55:08.959925: step 4012, loss 0.0340337, acc 0.98
2016-09-07T00:55:09.642960: step 4013, loss 0.0693353, acc 0.96
2016-09-07T00:55:10.334029: step 4014, loss 0.0624104, acc 0.98
2016-09-07T00:55:11.033882: step 4015, loss 0.00386063, acc 1
2016-09-07T00:55:11.695336: step 4016, loss 0.0254226, acc 1
2016-09-07T00:55:12.397797: step 4017, loss 0.0130172, acc 1
2016-09-07T00:55:13.093984: step 4018, loss 0.0589655, acc 0.98
2016-09-07T00:55:13.771196: step 4019, loss 0.00302973, acc 1
2016-09-07T00:55:14.455509: step 4020, loss 0.097295, acc 0.98
2016-09-07T00:55:15.132606: step 4021, loss 0.00517103, acc 1
2016-09-07T00:55:15.837888: step 4022, loss 0.0192388, acc 1
2016-09-07T00:55:16.509881: step 4023, loss 0.00451906, acc 1
2016-09-07T00:55:17.238052: step 4024, loss 0.0295115, acc 1
2016-09-07T00:55:17.919325: step 4025, loss 0.066723, acc 0.98
2016-09-07T00:55:18.620129: step 4026, loss 0.0171862, acc 0.98
2016-09-07T00:55:19.306938: step 4027, loss 0.0248991, acc 0.98
2016-09-07T00:55:19.999856: step 4028, loss 0.00285725, acc 1
2016-09-07T00:55:20.698840: step 4029, loss 0.03766, acc 0.98
2016-09-07T00:55:21.349404: step 4030, loss 0.0400004, acc 0.96
2016-09-07T00:55:22.069360: step 4031, loss 0.00676157, acc 1
2016-09-07T00:55:22.701213: step 4032, loss 0.000224444, acc 1
2016-09-07T00:55:23.396324: step 4033, loss 0.0193977, acc 1
2016-09-07T00:55:24.100598: step 4034, loss 0.0436604, acc 0.98
2016-09-07T00:55:24.795196: step 4035, loss 0.0376765, acc 0.98
2016-09-07T00:55:25.508314: step 4036, loss 0.0415673, acc 0.98
2016-09-07T00:55:26.173393: step 4037, loss 0.00499156, acc 1
2016-09-07T00:55:26.876332: step 4038, loss 0.0891856, acc 0.98
2016-09-07T00:55:27.572855: step 4039, loss 0.0275984, acc 1
2016-09-07T00:55:28.256652: step 4040, loss 0.0988536, acc 0.94
2016-09-07T00:55:28.939113: step 4041, loss 0.0137012, acc 1
2016-09-07T00:55:29.619788: step 4042, loss 0.0313446, acc 0.96
2016-09-07T00:55:30.347427: step 4043, loss 0.0935714, acc 0.96
2016-09-07T00:55:31.030542: step 4044, loss 0.015665, acc 0.98
2016-09-07T00:55:31.712306: step 4045, loss 0.0434512, acc 0.96
2016-09-07T00:55:32.396822: step 4046, loss 0.0193423, acc 0.98
2016-09-07T00:55:33.062178: step 4047, loss 0.00977075, acc 1
2016-09-07T00:55:33.762217: step 4048, loss 0.00270646, acc 1
2016-09-07T00:55:34.445412: step 4049, loss 0.00452426, acc 1
2016-09-07T00:55:35.145848: step 4050, loss 0.0314432, acc 0.98
2016-09-07T00:55:35.811773: step 4051, loss 0.0213781, acc 1
2016-09-07T00:55:36.503333: step 4052, loss 0.0263878, acc 1
2016-09-07T00:55:37.188750: step 4053, loss 0.0452408, acc 0.98
2016-09-07T00:55:37.862924: step 4054, loss 0.0333905, acc 0.98
2016-09-07T00:55:38.543775: step 4055, loss 0.000116846, acc 1
2016-09-07T00:55:39.217690: step 4056, loss 0.00131537, acc 1
2016-09-07T00:55:39.917644: step 4057, loss 0.0376946, acc 0.98
2016-09-07T00:55:40.571475: step 4058, loss 0.0173257, acc 0.98
2016-09-07T00:55:41.287953: step 4059, loss 0.00601363, acc 1
2016-09-07T00:55:41.973474: step 4060, loss 0.0415227, acc 0.98
2016-09-07T00:55:42.660173: step 4061, loss 0.0180799, acc 0.98
2016-09-07T00:55:43.354589: step 4062, loss 0.0020594, acc 1
2016-09-07T00:55:44.042897: step 4063, loss 0.0483587, acc 0.96
2016-09-07T00:55:44.742852: step 4064, loss 0.00689201, acc 1
2016-09-07T00:55:45.420213: step 4065, loss 0.0100155, acc 1
2016-09-07T00:55:46.117017: step 4066, loss 0.0453061, acc 0.98
2016-09-07T00:55:46.805166: step 4067, loss 0.00417805, acc 1
2016-09-07T00:55:47.518312: step 4068, loss 0.140213, acc 0.96
2016-09-07T00:55:48.189647: step 4069, loss 0.0519996, acc 0.96
2016-09-07T00:55:48.877917: step 4070, loss 0.0163951, acc 1
2016-09-07T00:55:49.609238: step 4071, loss 0.00036996, acc 1
2016-09-07T00:55:50.275624: step 4072, loss 0.00968663, acc 1
2016-09-07T00:55:50.959736: step 4073, loss 0.0180805, acc 0.98
2016-09-07T00:55:51.653308: step 4074, loss 0.00295882, acc 1
2016-09-07T00:55:52.347230: step 4075, loss 0.0116607, acc 1
2016-09-07T00:55:53.038421: step 4076, loss 0.133768, acc 0.94
2016-09-07T00:55:53.729696: step 4077, loss 0.00143913, acc 1
2016-09-07T00:55:54.436276: step 4078, loss 0.0303877, acc 0.98
2016-09-07T00:55:55.133346: step 4079, loss 0.0476811, acc 0.98
2016-09-07T00:55:55.834310: step 4080, loss 0.0887736, acc 0.94
2016-09-07T00:55:56.526861: step 4081, loss 0.0255232, acc 0.98
2016-09-07T00:55:57.212024: step 4082, loss 0.0311053, acc 0.98
2016-09-07T00:55:57.883338: step 4083, loss 0.0286548, acc 0.98
2016-09-07T00:55:58.558517: step 4084, loss 0.000113028, acc 1
2016-09-07T00:55:59.271758: step 4085, loss 0.00181942, acc 1
2016-09-07T00:55:59.960268: step 4086, loss 0.0271027, acc 0.98
2016-09-07T00:56:00.663099: step 4087, loss 0.0136002, acc 1
2016-09-07T00:56:01.359388: step 4088, loss 0.0104127, acc 1
2016-09-07T00:56:02.062745: step 4089, loss 0.0723369, acc 0.96
2016-09-07T00:56:02.760663: step 4090, loss 0.00189052, acc 1
2016-09-07T00:56:03.419225: step 4091, loss 0.0483146, acc 0.98
2016-09-07T00:56:04.123387: step 4092, loss 0.000682851, acc 1
2016-09-07T00:56:04.802706: step 4093, loss 0.0843451, acc 0.94
2016-09-07T00:56:05.476007: step 4094, loss 0.0213016, acc 1
2016-09-07T00:56:06.172794: step 4095, loss 0.0306134, acc 0.98
2016-09-07T00:56:06.854486: step 4096, loss 0.0831022, acc 0.96
2016-09-07T00:56:07.525536: step 4097, loss 0.0528014, acc 0.96
2016-09-07T00:56:08.198145: step 4098, loss 0.0167829, acc 1
2016-09-07T00:56:08.918455: step 4099, loss 0.0281389, acc 0.98
2016-09-07T00:56:09.600705: step 4100, loss 0.0491056, acc 0.98

Evaluation:
2016-09-07T00:56:12.712352: step 4100, loss 1.7913, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-4100

2016-09-07T00:56:14.507875: step 4101, loss 0.0327168, acc 0.98
2016-09-07T00:56:15.189834: step 4102, loss 0.00329243, acc 1
2016-09-07T00:56:15.868255: step 4103, loss 0.0150135, acc 0.98
2016-09-07T00:56:16.552110: step 4104, loss 0.061637, acc 0.96
2016-09-07T00:56:17.252889: step 4105, loss 0.0134865, acc 1
2016-09-07T00:56:17.933506: step 4106, loss 0.0230761, acc 0.98
2016-09-07T00:56:18.612277: step 4107, loss 0.0213643, acc 1
2016-09-07T00:56:19.302219: step 4108, loss 0.0407332, acc 0.98
2016-09-07T00:56:19.973090: step 4109, loss 0.0142395, acc 1
2016-09-07T00:56:20.645828: step 4110, loss 0.0568033, acc 0.98
2016-09-07T00:56:21.343199: step 4111, loss 0.0216954, acc 0.98
2016-09-07T00:56:22.065005: step 4112, loss 0.0437766, acc 0.98
2016-09-07T00:56:22.725431: step 4113, loss 0.0184185, acc 1
2016-09-07T00:56:23.410001: step 4114, loss 0.0283659, acc 0.98
2016-09-07T00:56:24.100732: step 4115, loss 0.0500552, acc 0.96
2016-09-07T00:56:24.797076: step 4116, loss 0.0140888, acc 1
2016-09-07T00:56:25.463580: step 4117, loss 0.00787614, acc 1
2016-09-07T00:56:26.135584: step 4118, loss 0.0411203, acc 0.96
2016-09-07T00:56:26.839890: step 4119, loss 0.0524391, acc 0.98
2016-09-07T00:56:27.509237: step 4120, loss 0.00542035, acc 1
2016-09-07T00:56:28.206340: step 4121, loss 0.0129334, acc 1
2016-09-07T00:56:28.885424: step 4122, loss 0.0493021, acc 0.96
2016-09-07T00:56:29.568464: step 4123, loss 0.0290326, acc 0.98
2016-09-07T00:56:30.250777: step 4124, loss 0.00924205, acc 1
2016-09-07T00:56:30.919617: step 4125, loss 0.0310687, acc 0.98
2016-09-07T00:56:31.599745: step 4126, loss 0.0678085, acc 0.96
2016-09-07T00:56:32.267383: step 4127, loss 0.0287869, acc 0.98
2016-09-07T00:56:32.958493: step 4128, loss 0.00105343, acc 1
2016-09-07T00:56:33.660713: step 4129, loss 0.000426835, acc 1
2016-09-07T00:56:34.383005: step 4130, loss 0.0949308, acc 0.9
2016-09-07T00:56:35.068025: step 4131, loss 0.0817385, acc 0.98
2016-09-07T00:56:35.754630: step 4132, loss 0.00944263, acc 1
2016-09-07T00:56:36.457929: step 4133, loss 0.0633606, acc 0.96
2016-09-07T00:56:37.123582: step 4134, loss 0.0300714, acc 0.98
2016-09-07T00:56:37.809774: step 4135, loss 0.119854, acc 0.94
2016-09-07T00:56:38.501944: step 4136, loss 0.036701, acc 0.98
2016-09-07T00:56:39.201392: step 4137, loss 0.00575096, acc 1
2016-09-07T00:56:39.871215: step 4138, loss 0.011475, acc 1
2016-09-07T00:56:40.546169: step 4139, loss 0.0902615, acc 0.96
2016-09-07T00:56:41.246836: step 4140, loss 0.00604144, acc 1
2016-09-07T00:56:41.924663: step 4141, loss 0.106033, acc 0.94
2016-09-07T00:56:42.631519: step 4142, loss 0.0269478, acc 1
2016-09-07T00:56:43.321427: step 4143, loss 0.0303662, acc 1
2016-09-07T00:56:44.016958: step 4144, loss 0.0126053, acc 1
2016-09-07T00:56:44.703154: step 4145, loss 0.0212447, acc 1
2016-09-07T00:56:45.381003: step 4146, loss 0.0364015, acc 0.98
2016-09-07T00:56:46.090320: step 4147, loss 0.0231827, acc 1
2016-09-07T00:56:46.765885: step 4148, loss 0.0335853, acc 0.98
2016-09-07T00:56:47.436008: step 4149, loss 0.00332981, acc 1
2016-09-07T00:56:48.142360: step 4150, loss 0.0132891, acc 1
2016-09-07T00:56:48.837625: step 4151, loss 0.0411095, acc 0.98
2016-09-07T00:56:49.581079: step 4152, loss 0.0663089, acc 0.98
2016-09-07T00:56:50.246348: step 4153, loss 0.00741974, acc 1
2016-09-07T00:56:50.937615: step 4154, loss 0.0367195, acc 0.98
2016-09-07T00:56:51.612824: step 4155, loss 0.00975836, acc 1
2016-09-07T00:56:52.295033: step 4156, loss 0.0914324, acc 0.96
2016-09-07T00:56:52.981006: step 4157, loss 0.0452081, acc 0.96
2016-09-07T00:56:53.671811: step 4158, loss 0.0666732, acc 0.96
2016-09-07T00:56:54.346939: step 4159, loss 0.00533758, acc 1
2016-09-07T00:56:55.045678: step 4160, loss 0.0154987, acc 1
2016-09-07T00:56:55.760935: step 4161, loss 0.001755, acc 1
2016-09-07T00:56:56.449030: step 4162, loss 0.0193058, acc 1
2016-09-07T00:56:57.142814: step 4163, loss 0.0770485, acc 0.96
2016-09-07T00:56:57.847358: step 4164, loss 0.116939, acc 0.96
2016-09-07T00:56:58.541694: step 4165, loss 0.0135108, acc 1
2016-09-07T00:56:59.238820: step 4166, loss 0.0216761, acc 1
2016-09-07T00:56:59.914064: step 4167, loss 0.0203868, acc 1
2016-09-07T00:57:00.635785: step 4168, loss 0.0117452, acc 1
2016-09-07T00:57:01.322974: step 4169, loss 0.0290531, acc 0.98
2016-09-07T00:57:02.003090: step 4170, loss 0.00298632, acc 1
2016-09-07T00:57:02.683794: step 4171, loss 0.0206729, acc 0.98
2016-09-07T00:57:03.379895: step 4172, loss 0.0158141, acc 0.98
2016-09-07T00:57:04.083345: step 4173, loss 0.0588305, acc 0.96
2016-09-07T00:57:04.764675: step 4174, loss 0.0318401, acc 1
2016-09-07T00:57:05.479493: step 4175, loss 0.00130102, acc 1
2016-09-07T00:57:06.156053: step 4176, loss 0.000358889, acc 1
2016-09-07T00:57:06.849834: step 4177, loss 0.0286623, acc 0.98
2016-09-07T00:57:07.544661: step 4178, loss 0.00864742, acc 1
2016-09-07T00:57:08.225918: step 4179, loss 0.0394286, acc 0.98
2016-09-07T00:57:08.913499: step 4180, loss 0.0719416, acc 0.96
2016-09-07T00:57:09.585290: step 4181, loss 0.0910105, acc 0.98
2016-09-07T00:57:10.283887: step 4182, loss 0.00668519, acc 1
2016-09-07T00:57:10.973416: step 4183, loss 0.0211502, acc 0.98
2016-09-07T00:57:11.650235: step 4184, loss 0.0230286, acc 1
2016-09-07T00:57:12.351634: step 4185, loss 0.0247956, acc 0.98
2016-09-07T00:57:13.040733: step 4186, loss 0.15741, acc 0.94
2016-09-07T00:57:13.743567: step 4187, loss 0.0242308, acc 1
2016-09-07T00:57:14.433824: step 4188, loss 0.0613053, acc 0.98
2016-09-07T00:57:15.127228: step 4189, loss 0.0213467, acc 0.98
2016-09-07T00:57:15.813136: step 4190, loss 0.0874575, acc 0.96
2016-09-07T00:57:16.506793: step 4191, loss 0.00110215, acc 1
2016-09-07T00:57:17.230555: step 4192, loss 0.10124, acc 0.96
2016-09-07T00:57:17.892497: step 4193, loss 0.0437804, acc 0.98
2016-09-07T00:57:18.590027: step 4194, loss 0.0613711, acc 0.96
2016-09-07T00:57:19.272089: step 4195, loss 0.0116392, acc 1
2016-09-07T00:57:19.985166: step 4196, loss 0.0150688, acc 1
2016-09-07T00:57:20.663818: step 4197, loss 0.000199965, acc 1
2016-09-07T00:57:21.348784: step 4198, loss 0.0453534, acc 0.96
2016-09-07T00:57:22.026030: step 4199, loss 0.0174927, acc 1
2016-09-07T00:57:22.701983: step 4200, loss 0.0758841, acc 0.94

Evaluation:
2016-09-07T00:57:25.877314: step 4200, loss 1.90784, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-4200

2016-09-07T00:57:27.605960: step 4201, loss 0.00321631, acc 1
2016-09-07T00:57:28.325097: step 4202, loss 0.0247276, acc 1
2016-09-07T00:57:29.007735: step 4203, loss 0.0337489, acc 0.98
2016-09-07T00:57:29.697464: step 4204, loss 0.0301802, acc 0.98
2016-09-07T00:57:30.382025: step 4205, loss 0.0227852, acc 1
2016-09-07T00:57:31.065849: step 4206, loss 0.100043, acc 0.96
2016-09-07T00:57:31.774962: step 4207, loss 0.0057098, acc 1
2016-09-07T00:57:32.470684: step 4208, loss 0.0189996, acc 1
2016-09-07T00:57:33.153055: step 4209, loss 0.0366705, acc 0.96
2016-09-07T00:57:33.838903: step 4210, loss 0.0846576, acc 0.92
2016-09-07T00:57:34.520529: step 4211, loss 5.2326e-05, acc 1
2016-09-07T00:57:35.201037: step 4212, loss 0.000436714, acc 1
2016-09-07T00:57:35.886047: step 4213, loss 0.0344594, acc 0.98
2016-09-07T00:57:36.603769: step 4214, loss 0.0867095, acc 0.98
2016-09-07T00:57:37.277815: step 4215, loss 0.0142926, acc 1
2016-09-07T00:57:37.942792: step 4216, loss 0.055295, acc 0.96
2016-09-07T00:57:38.628884: step 4217, loss 0.0119329, acc 1
2016-09-07T00:57:39.343964: step 4218, loss 0.0172351, acc 0.98
2016-09-07T00:57:40.016324: step 4219, loss 0.0239099, acc 0.98
2016-09-07T00:57:40.694913: step 4220, loss 0.0102943, acc 1
2016-09-07T00:57:41.394194: step 4221, loss 0.0186816, acc 1
2016-09-07T00:57:42.076557: step 4222, loss 0.0251652, acc 0.98
2016-09-07T00:57:42.759643: step 4223, loss 0.0336412, acc 1
2016-09-07T00:57:43.396193: step 4224, loss 0.00214853, acc 1
2016-09-07T00:57:44.064895: step 4225, loss 0.0680894, acc 0.96
2016-09-07T00:57:44.753888: step 4226, loss 0.0579443, acc 0.98
2016-09-07T00:57:45.433667: step 4227, loss 0.14433, acc 0.96
2016-09-07T00:57:46.123331: step 4228, loss 0.0433477, acc 0.98
2016-09-07T00:57:46.791482: step 4229, loss 0.0142763, acc 1
2016-09-07T00:57:47.497750: step 4230, loss 0.191929, acc 0.96
2016-09-07T00:57:48.189910: step 4231, loss 0.0168191, acc 1
2016-09-07T00:57:48.886763: step 4232, loss 0.0115608, acc 1
2016-09-07T00:57:49.590933: step 4233, loss 0.0239709, acc 0.98
2016-09-07T00:57:50.277639: step 4234, loss 0.00781179, acc 1
2016-09-07T00:57:50.964501: step 4235, loss 0.00277417, acc 1
2016-09-07T00:57:51.640032: step 4236, loss 0.116401, acc 0.98
2016-09-07T00:57:52.350155: step 4237, loss 0.0441785, acc 1
2016-09-07T00:57:53.039937: step 4238, loss 0.0598782, acc 0.96
2016-09-07T00:57:53.740040: step 4239, loss 0.0138646, acc 1
2016-09-07T00:57:54.447709: step 4240, loss 0.000201472, acc 1
2016-09-07T00:57:55.137290: step 4241, loss 0.0772699, acc 0.96
2016-09-07T00:57:55.832398: step 4242, loss 0.00501444, acc 1
2016-09-07T00:57:56.508706: step 4243, loss 0.0223378, acc 0.98
2016-09-07T00:57:57.193623: step 4244, loss 0.0234493, acc 0.98
2016-09-07T00:57:57.882608: step 4245, loss 0.00547479, acc 1
2016-09-07T00:57:58.586349: step 4246, loss 0.00931102, acc 1
2016-09-07T00:57:59.287839: step 4247, loss 0.0262049, acc 0.98
2016-09-07T00:57:59.987496: step 4248, loss 0.0215492, acc 0.98
2016-09-07T00:58:00.731219: step 4249, loss 0.00511101, acc 1
2016-09-07T00:58:01.413538: step 4250, loss 0.00104812, acc 1
2016-09-07T00:58:02.087614: step 4251, loss 0.0208435, acc 0.98
2016-09-07T00:58:02.785644: step 4252, loss 0.00970375, acc 1
2016-09-07T00:58:03.476232: step 4253, loss 0.00744407, acc 1
2016-09-07T00:58:04.173784: step 4254, loss 0.0661835, acc 0.98
2016-09-07T00:58:04.845754: step 4255, loss 0.0199625, acc 0.98
2016-09-07T00:58:05.571108: step 4256, loss 0.065796, acc 0.96
2016-09-07T00:58:06.257356: step 4257, loss 0.0366831, acc 0.98
2016-09-07T00:58:06.972531: step 4258, loss 0.0171242, acc 1
2016-09-07T00:58:07.674020: step 4259, loss 0.0204204, acc 1
2016-09-07T00:58:08.352667: step 4260, loss 0.0235597, acc 1
2016-09-07T00:58:09.056247: step 4261, loss 0.102297, acc 0.98
2016-09-07T00:58:09.739215: step 4262, loss 0.0956085, acc 0.96
2016-09-07T00:58:10.424150: step 4263, loss 0.0232015, acc 0.98
2016-09-07T00:58:11.110430: step 4264, loss 0.00602426, acc 1
2016-09-07T00:58:11.796329: step 4265, loss 0.0199584, acc 1
2016-09-07T00:58:12.484461: step 4266, loss 0.00992927, acc 1
2016-09-07T00:58:13.179245: step 4267, loss 0.0220869, acc 0.98
2016-09-07T00:58:13.884226: step 4268, loss 0.0194352, acc 1
2016-09-07T00:58:14.576810: step 4269, loss 0.0367372, acc 0.98
2016-09-07T00:58:15.293251: step 4270, loss 0.0261927, acc 1
2016-09-07T00:58:15.988852: step 4271, loss 0.0165479, acc 1
2016-09-07T00:58:16.672359: step 4272, loss 0.0153588, acc 1
2016-09-07T00:58:17.357552: step 4273, loss 0.0368693, acc 0.98
2016-09-07T00:58:18.028528: step 4274, loss 0.0322394, acc 0.98
2016-09-07T00:58:18.744713: step 4275, loss 0.0427598, acc 0.98
2016-09-07T00:58:19.431080: step 4276, loss 0.00900725, acc 1
2016-09-07T00:58:20.128957: step 4277, loss 0.0080064, acc 1
2016-09-07T00:58:20.835512: step 4278, loss 0.0163169, acc 1
2016-09-07T00:58:21.525569: step 4279, loss 0.000759264, acc 1
2016-09-07T00:58:22.209473: step 4280, loss 0.0370964, acc 0.98
2016-09-07T00:58:22.879470: step 4281, loss 0.0361356, acc 0.98
2016-09-07T00:58:23.584686: step 4282, loss 0.0190095, acc 0.98
2016-09-07T00:58:24.282929: step 4283, loss 0.00626964, acc 1
2016-09-07T00:58:24.987638: step 4284, loss 0.00132882, acc 1
2016-09-07T00:58:25.689841: step 4285, loss 0.0451497, acc 0.98
2016-09-07T00:58:26.373264: step 4286, loss 0.00635878, acc 1
2016-09-07T00:58:27.076475: step 4287, loss 0.0471634, acc 0.98
2016-09-07T00:58:27.748353: step 4288, loss 0.0168209, acc 0.98
2016-09-07T00:58:28.439992: step 4289, loss 0.0484073, acc 0.96
2016-09-07T00:58:29.133830: step 4290, loss 0.0922972, acc 0.98
2016-09-07T00:58:29.832209: step 4291, loss 0.0375481, acc 0.98
2016-09-07T00:58:30.537317: step 4292, loss 0.00757336, acc 1
2016-09-07T00:58:31.221861: step 4293, loss 0.0301047, acc 0.98
2016-09-07T00:58:31.928167: step 4294, loss 0.080765, acc 0.92
2016-09-07T00:58:32.609674: step 4295, loss 0.0505059, acc 0.98
2016-09-07T00:58:33.301468: step 4296, loss 0.040373, acc 0.98
2016-09-07T00:58:33.985991: step 4297, loss 0.00071119, acc 1
2016-09-07T00:58:34.646331: step 4298, loss 0.0221379, acc 0.98
2016-09-07T00:58:35.325188: step 4299, loss 0.00971275, acc 1
2016-09-07T00:58:35.979568: step 4300, loss 0.0434206, acc 0.98

Evaluation:
2016-09-07T00:58:39.114222: step 4300, loss 2.04148, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-4300

2016-09-07T00:58:40.878013: step 4301, loss 0.0551367, acc 0.96
2016-09-07T00:58:41.576474: step 4302, loss 0.000642191, acc 1
2016-09-07T00:58:42.252019: step 4303, loss 0.00276368, acc 1
2016-09-07T00:58:42.957306: step 4304, loss 0.0168373, acc 0.98
2016-09-07T00:58:43.651694: step 4305, loss 0.0057012, acc 1
2016-09-07T00:58:44.338919: step 4306, loss 0.00102314, acc 1
2016-09-07T00:58:45.028830: step 4307, loss 0.0530448, acc 0.98
2016-09-07T00:58:45.674206: step 4308, loss 0.000929251, acc 1
2016-09-07T00:58:46.365422: step 4309, loss 0.0481012, acc 0.98
2016-09-07T00:58:47.023847: step 4310, loss 0.00908403, acc 1
2016-09-07T00:58:47.753926: step 4311, loss 0.00109935, acc 1
2016-09-07T00:58:48.423117: step 4312, loss 0.00845779, acc 1
2016-09-07T00:58:49.095771: step 4313, loss 0.00752719, acc 1
2016-09-07T00:58:49.769116: step 4314, loss 0.0201758, acc 0.98
2016-09-07T00:58:50.454086: step 4315, loss 0.0248604, acc 1
2016-09-07T00:58:51.192240: step 4316, loss 0.00130416, acc 1
2016-09-07T00:58:51.883734: step 4317, loss 0.0418499, acc 0.98
2016-09-07T00:58:52.556816: step 4318, loss 0.0175664, acc 0.98
2016-09-07T00:58:53.219711: step 4319, loss 0.0630685, acc 0.94
2016-09-07T00:58:53.917885: step 4320, loss 0.00417505, acc 1
2016-09-07T00:58:54.583242: step 4321, loss 0.0524602, acc 0.98
2016-09-07T00:58:55.268275: step 4322, loss 0.0378839, acc 0.98
2016-09-07T00:58:55.978277: step 4323, loss 0.00843769, acc 1
2016-09-07T00:58:56.648935: step 4324, loss 0.0173029, acc 0.98
2016-09-07T00:58:57.340064: step 4325, loss 0.00520105, acc 1
2016-09-07T00:58:58.045198: step 4326, loss 0.0381969, acc 0.98
2016-09-07T00:58:58.730123: step 4327, loss 0.0118976, acc 1
2016-09-07T00:58:59.412960: step 4328, loss 0.0541114, acc 0.96
2016-09-07T00:59:00.073898: step 4329, loss 0.0190644, acc 0.98
2016-09-07T00:59:00.786456: step 4330, loss 0.0599932, acc 0.98
2016-09-07T00:59:01.469531: step 4331, loss 0.0123913, acc 1
2016-09-07T00:59:02.148470: step 4332, loss 0.00567772, acc 1
2016-09-07T00:59:02.812065: step 4333, loss 0.084007, acc 0.96
2016-09-07T00:59:03.507765: step 4334, loss 0.0253969, acc 0.98
2016-09-07T00:59:04.202133: step 4335, loss 0.06179, acc 0.96
2016-09-07T00:59:04.880912: step 4336, loss 0.0150178, acc 1
2016-09-07T00:59:05.570623: step 4337, loss 0.0148816, acc 1
2016-09-07T00:59:06.245841: step 4338, loss 0.0253375, acc 0.98
2016-09-07T00:59:06.950859: step 4339, loss 0.0852753, acc 0.98
2016-09-07T00:59:07.641035: step 4340, loss 0.000525916, acc 1
2016-09-07T00:59:08.322872: step 4341, loss 0.00177773, acc 1
2016-09-07T00:59:09.011764: step 4342, loss 0.0214605, acc 0.98
2016-09-07T00:59:09.701339: step 4343, loss 0.0151893, acc 1
2016-09-07T00:59:10.394307: step 4344, loss 0.0946722, acc 0.98
2016-09-07T00:59:11.073380: step 4345, loss 0.100846, acc 0.98
2016-09-07T00:59:11.770367: step 4346, loss 0.100539, acc 0.96
2016-09-07T00:59:12.452385: step 4347, loss 0.0167175, acc 1
2016-09-07T00:59:13.139753: step 4348, loss 0.021514, acc 0.98
2016-09-07T00:59:13.825958: step 4349, loss 0.0291072, acc 0.98
2016-09-07T00:59:14.507421: step 4350, loss 0.0101048, acc 1
2016-09-07T00:59:15.217398: step 4351, loss 0.00622756, acc 1
2016-09-07T00:59:15.926841: step 4352, loss 0.00884975, acc 1
2016-09-07T00:59:16.610219: step 4353, loss 0.0135479, acc 1
2016-09-07T00:59:17.310210: step 4354, loss 0.0520962, acc 0.98
2016-09-07T00:59:17.986417: step 4355, loss 0.062147, acc 0.96
2016-09-07T00:59:18.657600: step 4356, loss 0.050067, acc 0.96
2016-09-07T00:59:19.351906: step 4357, loss 0.0257681, acc 0.98
2016-09-07T00:59:20.071890: step 4358, loss 0.0173235, acc 0.98
2016-09-07T00:59:20.759876: step 4359, loss 0.00274641, acc 1
2016-09-07T00:59:21.428453: step 4360, loss 0.0269141, acc 0.98
2016-09-07T00:59:22.122711: step 4361, loss 0.135857, acc 0.96
2016-09-07T00:59:22.806630: step 4362, loss 0.00268564, acc 1
2016-09-07T00:59:23.487434: step 4363, loss 0.0322123, acc 0.98
2016-09-07T00:59:24.174694: step 4364, loss 0.0254096, acc 0.98
2016-09-07T00:59:24.864048: step 4365, loss 0.0186254, acc 1
2016-09-07T00:59:25.538526: step 4366, loss 0.0231297, acc 0.98
2016-09-07T00:59:26.225959: step 4367, loss 0.132396, acc 0.96
2016-09-07T00:59:26.921245: step 4368, loss 0.115654, acc 0.94
2016-09-07T00:59:27.603785: step 4369, loss 0.0194095, acc 1
2016-09-07T00:59:28.295197: step 4370, loss 0.00114238, acc 1
2016-09-07T00:59:28.974354: step 4371, loss 0.00265584, acc 1
2016-09-07T00:59:29.683114: step 4372, loss 0.0523325, acc 0.96
2016-09-07T00:59:30.353314: step 4373, loss 0.0378795, acc 0.96
2016-09-07T00:59:31.049946: step 4374, loss 0.000638641, acc 1
2016-09-07T00:59:31.737427: step 4375, loss 0.0339563, acc 0.98
2016-09-07T00:59:32.437650: step 4376, loss 0.0159207, acc 1
2016-09-07T00:59:33.130508: step 4377, loss 0.00222319, acc 1
2016-09-07T00:59:33.791199: step 4378, loss 0.076334, acc 0.96
2016-09-07T00:59:34.510312: step 4379, loss 0.0308829, acc 1
2016-09-07T00:59:35.187562: step 4380, loss 0.00888945, acc 1
2016-09-07T00:59:35.866140: step 4381, loss 0.00129247, acc 1
2016-09-07T00:59:36.575007: step 4382, loss 0.112157, acc 0.96
2016-09-07T00:59:37.261287: step 4383, loss 0.0269239, acc 0.98
2016-09-07T00:59:37.956211: step 4384, loss 0.0378044, acc 1
2016-09-07T00:59:38.616935: step 4385, loss 0.00136883, acc 1
2016-09-07T00:59:39.357392: step 4386, loss 0.0101604, acc 1
2016-09-07T00:59:40.037912: step 4387, loss 0.0141842, acc 1
2016-09-07T00:59:40.722219: step 4388, loss 0.0106736, acc 1
2016-09-07T00:59:41.410535: step 4389, loss 0.0133214, acc 1
2016-09-07T00:59:42.094885: step 4390, loss 0.020677, acc 1
2016-09-07T00:59:42.769349: step 4391, loss 0.00537674, acc 1
2016-09-07T00:59:43.415848: step 4392, loss 0.0506406, acc 0.98
2016-09-07T00:59:44.129947: step 4393, loss 0.0443223, acc 0.96
2016-09-07T00:59:44.795010: step 4394, loss 0.0788314, acc 0.96
2016-09-07T00:59:45.484087: step 4395, loss 0.0874565, acc 0.98
2016-09-07T00:59:46.162210: step 4396, loss 0.0072618, acc 1
2016-09-07T00:59:46.850656: step 4397, loss 0.00192316, acc 1
2016-09-07T00:59:47.546754: step 4398, loss 0.021547, acc 1
2016-09-07T00:59:48.226659: step 4399, loss 0.0336848, acc 0.96
2016-09-07T00:59:48.924932: step 4400, loss 0.0797571, acc 0.98

Evaluation:
2016-09-07T00:59:52.074684: step 4400, loss 1.88313, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-4400

2016-09-07T00:59:53.755625: step 4401, loss 0.0666584, acc 0.94
2016-09-07T00:59:54.443258: step 4402, loss 0.0843195, acc 0.98
2016-09-07T00:59:55.129097: step 4403, loss 0.0107858, acc 1
2016-09-07T00:59:55.804804: step 4404, loss 0.103083, acc 0.96
2016-09-07T00:59:56.495548: step 4405, loss 0.00615895, acc 1
2016-09-07T00:59:57.189258: step 4406, loss 0.0103895, acc 1
2016-09-07T00:59:57.858585: step 4407, loss 0.000839541, acc 1
2016-09-07T00:59:58.539407: step 4408, loss 0.0602061, acc 0.98
2016-09-07T00:59:59.235615: step 4409, loss 0.00681528, acc 1
2016-09-07T00:59:59.912191: step 4410, loss 0.00713986, acc 1
2016-09-07T01:00:00.646529: step 4411, loss 0.0248618, acc 1
2016-09-07T01:00:01.347130: step 4412, loss 0.000619882, acc 1
2016-09-07T01:00:02.053493: step 4413, loss 0.038623, acc 0.98
2016-09-07T01:00:02.736464: step 4414, loss 0.00265239, acc 1
2016-09-07T01:00:03.441647: step 4415, loss 0.110072, acc 0.98
2016-09-07T01:00:04.074990: step 4416, loss 0.000546538, acc 1
2016-09-07T01:00:04.755256: step 4417, loss 0.00703272, acc 1
2016-09-07T01:00:05.436529: step 4418, loss 0.0415603, acc 0.98
2016-09-07T01:00:06.125119: step 4419, loss 0.0660714, acc 0.96
2016-09-07T01:00:06.822534: step 4420, loss 0.0124384, acc 1
2016-09-07T01:00:07.478476: step 4421, loss 0.00781705, acc 1
2016-09-07T01:00:08.163217: step 4422, loss 0.0336623, acc 0.98
2016-09-07T01:00:08.829152: step 4423, loss 0.011945, acc 1
2016-09-07T01:00:09.494780: step 4424, loss 0.0344977, acc 0.98
2016-09-07T01:00:10.173458: step 4425, loss 0.0487031, acc 0.96
2016-09-07T01:00:10.851722: step 4426, loss 0.0222833, acc 1
2016-09-07T01:00:11.555258: step 4427, loss 0.00757251, acc 1
2016-09-07T01:00:12.226652: step 4428, loss 0.00443274, acc 1
2016-09-07T01:00:12.897526: step 4429, loss 0.123969, acc 0.98
2016-09-07T01:00:13.581608: step 4430, loss 0.0201034, acc 1
2016-09-07T01:00:14.287370: step 4431, loss 0.0184902, acc 1
2016-09-07T01:00:14.972619: step 4432, loss 0.0120932, acc 1
2016-09-07T01:00:15.658180: step 4433, loss 0.14943, acc 0.92
2016-09-07T01:00:16.338536: step 4434, loss 0.00364285, acc 1
2016-09-07T01:00:17.012429: step 4435, loss 0.0857671, acc 0.96
2016-09-07T01:00:17.711395: step 4436, loss 0.0709659, acc 0.94
2016-09-07T01:00:18.384393: step 4437, loss 0.0143213, acc 1
2016-09-07T01:00:19.082679: step 4438, loss 0.00484143, acc 1
2016-09-07T01:00:19.754189: step 4439, loss 0.00768913, acc 1
2016-09-07T01:00:20.450505: step 4440, loss 0.0174814, acc 1
2016-09-07T01:00:21.150659: step 4441, loss 0.0118351, acc 1
2016-09-07T01:00:21.830412: step 4442, loss 0.0141472, acc 1
2016-09-07T01:00:22.526884: step 4443, loss 0.00724543, acc 1
2016-09-07T01:00:23.197679: step 4444, loss 0.0224578, acc 0.98
2016-09-07T01:00:23.899540: step 4445, loss 0.063892, acc 0.98
2016-09-07T01:00:24.576745: step 4446, loss 0.0797743, acc 0.94
2016-09-07T01:00:25.253251: step 4447, loss 0.0228961, acc 1
2016-09-07T01:00:25.979506: step 4448, loss 0.0843659, acc 0.94
2016-09-07T01:00:26.666607: step 4449, loss 0.0387237, acc 0.98
2016-09-07T01:00:27.368910: step 4450, loss 0.00687286, acc 1
2016-09-07T01:00:28.064283: step 4451, loss 0.017273, acc 1
2016-09-07T01:00:28.758351: step 4452, loss 0.00787296, acc 1
2016-09-07T01:00:29.437782: step 4453, loss 0.0108107, acc 1
2016-09-07T01:00:30.108808: step 4454, loss 0.0116239, acc 1
2016-09-07T01:00:30.786530: step 4455, loss 0.0330928, acc 0.98
2016-09-07T01:00:31.454203: step 4456, loss 0.0112805, acc 1
2016-09-07T01:00:32.158443: step 4457, loss 0.103123, acc 0.94
2016-09-07T01:00:32.832147: step 4458, loss 0.000899264, acc 1
2016-09-07T01:00:33.515659: step 4459, loss 0.0136452, acc 1
2016-09-07T01:00:34.205161: step 4460, loss 0.0542218, acc 0.98
2016-09-07T01:00:34.891755: step 4461, loss 0.0196152, acc 0.98
2016-09-07T01:00:35.583523: step 4462, loss 0.0239194, acc 1
2016-09-07T01:00:36.271676: step 4463, loss 0.014128, acc 1
2016-09-07T01:00:36.972431: step 4464, loss 0.0199471, acc 0.98
2016-09-07T01:00:37.661638: step 4465, loss 0.00310763, acc 1
2016-09-07T01:00:38.344517: step 4466, loss 0.0479788, acc 0.98
2016-09-07T01:00:39.041148: step 4467, loss 0.0106338, acc 1
2016-09-07T01:00:39.747763: step 4468, loss 0.0324074, acc 0.98
2016-09-07T01:00:40.430617: step 4469, loss 0.0105306, acc 1
2016-09-07T01:00:41.130307: step 4470, loss 0.0013069, acc 1
2016-09-07T01:00:41.879653: step 4471, loss 0.00909877, acc 1
2016-09-07T01:00:42.579767: step 4472, loss 0.062818, acc 0.98
2016-09-07T01:00:43.272410: step 4473, loss 0.0141674, acc 1
2016-09-07T01:00:43.947677: step 4474, loss 0.00537188, acc 1
2016-09-07T01:00:44.653425: step 4475, loss 0.0226181, acc 1
2016-09-07T01:00:45.391717: step 4476, loss 0.0124871, acc 1
2016-09-07T01:00:46.085733: step 4477, loss 0.0448678, acc 0.98
2016-09-07T01:00:46.769134: step 4478, loss 0.022261, acc 0.98
2016-09-07T01:00:47.457258: step 4479, loss 0.0133735, acc 1
2016-09-07T01:00:48.149257: step 4480, loss 0.00574498, acc 1
2016-09-07T01:00:48.834699: step 4481, loss 0.0523564, acc 0.98
2016-09-07T01:00:49.534764: step 4482, loss 0.0166833, acc 1
2016-09-07T01:00:50.245502: step 4483, loss 0.0312655, acc 0.98
2016-09-07T01:00:50.898316: step 4484, loss 0.00498305, acc 1
2016-09-07T01:00:51.564566: step 4485, loss 0.0381112, acc 0.96
2016-09-07T01:00:52.233326: step 4486, loss 0.0365876, acc 0.98
2016-09-07T01:00:52.916197: step 4487, loss 0.00805766, acc 1
2016-09-07T01:00:53.606189: step 4488, loss 0.0557196, acc 0.98
2016-09-07T01:00:54.299980: step 4489, loss 9.82977e-05, acc 1
2016-09-07T01:00:55.001270: step 4490, loss 5.2091e-06, acc 1
2016-09-07T01:00:55.700666: step 4491, loss 0.0504771, acc 0.98
2016-09-07T01:00:56.394213: step 4492, loss 0.0182346, acc 1
2016-09-07T01:00:57.083609: step 4493, loss 0.0210113, acc 0.98
2016-09-07T01:00:57.769753: step 4494, loss 0.00783476, acc 1
2016-09-07T01:00:58.462145: step 4495, loss 0.0118033, acc 1
2016-09-07T01:00:59.138880: step 4496, loss 0.0191986, acc 1
2016-09-07T01:00:59.849117: step 4497, loss 0.0102354, acc 1
2016-09-07T01:01:00.545550: step 4498, loss 0.0930835, acc 0.96
2016-09-07T01:01:01.237252: step 4499, loss 0.0160379, acc 1
2016-09-07T01:01:01.953404: step 4500, loss 0.0176401, acc 0.98

Evaluation:
2016-09-07T01:01:05.079750: step 4500, loss 2.28307, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-4500

2016-09-07T01:01:06.794018: step 4501, loss 0.00149004, acc 1
2016-09-07T01:01:07.479225: step 4502, loss 0.0499143, acc 0.96
2016-09-07T01:01:08.182684: step 4503, loss 0.00248849, acc 1
2016-09-07T01:01:08.843011: step 4504, loss 0.150225, acc 0.94
2016-09-07T01:01:09.540042: step 4505, loss 0.0197716, acc 1
2016-09-07T01:01:10.220285: step 4506, loss 0.0257094, acc 1
2016-09-07T01:01:10.908384: step 4507, loss 0.0598467, acc 0.96
2016-09-07T01:01:11.621060: step 4508, loss 0.0115276, acc 1
2016-09-07T01:01:12.289679: step 4509, loss 0.108311, acc 0.98
2016-09-07T01:01:12.995705: step 4510, loss 0.0239743, acc 1
2016-09-07T01:01:13.680536: step 4511, loss 0.0101922, acc 1
2016-09-07T01:01:14.413324: step 4512, loss 0.00935738, acc 1
2016-09-07T01:01:15.091444: step 4513, loss 0.012457, acc 1
2016-09-07T01:01:15.792758: step 4514, loss 0.0299371, acc 0.98
2016-09-07T01:01:16.495934: step 4515, loss 0.0366237, acc 0.98
2016-09-07T01:01:17.181893: step 4516, loss 0.0353435, acc 0.98
2016-09-07T01:01:17.884561: step 4517, loss 0.0156273, acc 0.98
2016-09-07T01:01:18.572525: step 4518, loss 0.0481876, acc 0.98
2016-09-07T01:01:19.289305: step 4519, loss 0.0225702, acc 0.98
2016-09-07T01:01:19.985686: step 4520, loss 0.0216846, acc 0.98
2016-09-07T01:01:20.677931: step 4521, loss 0.0210216, acc 0.98
2016-09-07T01:01:21.368806: step 4522, loss 0.0356458, acc 0.98
2016-09-07T01:01:22.036143: step 4523, loss 0.0174561, acc 1
2016-09-07T01:01:22.758134: step 4524, loss 0.0157477, acc 0.98
2016-09-07T01:01:23.457700: step 4525, loss 0.0016822, acc 1
2016-09-07T01:01:24.144370: step 4526, loss 0.0213974, acc 1
2016-09-07T01:01:24.835785: step 4527, loss 0.000214672, acc 1
2016-09-07T01:01:25.506025: step 4528, loss 0.0168422, acc 1
2016-09-07T01:01:26.192652: step 4529, loss 0.0281817, acc 0.98
2016-09-07T01:01:26.865071: step 4530, loss 0.0386479, acc 0.96
2016-09-07T01:01:27.558927: step 4531, loss 0.00167428, acc 1
2016-09-07T01:01:28.249783: step 4532, loss 7.03532e-05, acc 1
2016-09-07T01:01:28.925593: step 4533, loss 0.000684942, acc 1
2016-09-07T01:01:29.600325: step 4534, loss 0.0157432, acc 1
2016-09-07T01:01:30.270645: step 4535, loss 0.0052727, acc 1
2016-09-07T01:01:30.980797: step 4536, loss 0.0122228, acc 1
2016-09-07T01:01:31.657214: step 4537, loss 0.0750871, acc 0.98
2016-09-07T01:01:32.350469: step 4538, loss 0.0178857, acc 0.98
2016-09-07T01:01:33.020324: step 4539, loss 0.000142295, acc 1
2016-09-07T01:01:33.682111: step 4540, loss 0.0171402, acc 1
2016-09-07T01:01:34.390258: step 4541, loss 0.0132707, acc 1
2016-09-07T01:01:35.087413: step 4542, loss 0.0126741, acc 1
2016-09-07T01:01:35.787476: step 4543, loss 0.000480212, acc 1
2016-09-07T01:01:36.458180: step 4544, loss 0.00175089, acc 1
2016-09-07T01:01:37.155910: step 4545, loss 0.000388841, acc 1
2016-09-07T01:01:37.832560: step 4546, loss 0.133727, acc 0.96
2016-09-07T01:01:38.566943: step 4547, loss 0.119368, acc 0.92
2016-09-07T01:01:39.241671: step 4548, loss 0.0334129, acc 0.98
2016-09-07T01:01:39.935071: step 4549, loss 0.0393796, acc 0.98
2016-09-07T01:01:40.619620: step 4550, loss 0.118796, acc 0.98
2016-09-07T01:01:41.282596: step 4551, loss 0.000333636, acc 1
2016-09-07T01:01:41.987337: step 4552, loss 0.00234667, acc 1
2016-09-07T01:01:42.676015: step 4553, loss 0.00376075, acc 1
2016-09-07T01:01:43.372710: step 4554, loss 0.006854, acc 1
2016-09-07T01:01:44.048565: step 4555, loss 0.000546341, acc 1
2016-09-07T01:01:44.738371: step 4556, loss 0.0156846, acc 1
2016-09-07T01:01:45.433867: step 4557, loss 0.03264, acc 1
2016-09-07T01:01:46.096339: step 4558, loss 0.00357658, acc 1
2016-09-07T01:01:46.794251: step 4559, loss 0.000558487, acc 1
2016-09-07T01:01:47.522121: step 4560, loss 0.0331476, acc 0.98
2016-09-07T01:01:48.215154: step 4561, loss 0.021003, acc 0.98
2016-09-07T01:01:48.894547: step 4562, loss 0.00143866, acc 1
2016-09-07T01:01:49.597921: step 4563, loss 0.0099144, acc 1
2016-09-07T01:01:50.301805: step 4564, loss 0.0122315, acc 1
2016-09-07T01:01:50.984115: step 4565, loss 0.0210326, acc 1
2016-09-07T01:01:51.668486: step 4566, loss 0.0766077, acc 0.98
2016-09-07T01:01:52.359491: step 4567, loss 0.0668126, acc 0.98
2016-09-07T01:01:53.047478: step 4568, loss 0.0017533, acc 1
2016-09-07T01:01:53.740033: step 4569, loss 0.00740371, acc 1
2016-09-07T01:01:54.425883: step 4570, loss 0.0449856, acc 0.98
2016-09-07T01:01:55.139676: step 4571, loss 0.00575023, acc 1
2016-09-07T01:01:55.821084: step 4572, loss 0.00163719, acc 1
2016-09-07T01:01:56.493271: step 4573, loss 0.00929129, acc 1
2016-09-07T01:01:57.179089: step 4574, loss 0.0261107, acc 1
2016-09-07T01:01:57.865707: step 4575, loss 0.00892286, acc 1
2016-09-07T01:01:58.546802: step 4576, loss 0.0251529, acc 0.98
2016-09-07T01:01:59.259420: step 4577, loss 0.00101857, acc 1
2016-09-07T01:01:59.964376: step 4578, loss 0.0157289, acc 0.98
2016-09-07T01:02:00.687556: step 4579, loss 0.0792056, acc 0.98
2016-09-07T01:02:01.389651: step 4580, loss 0.0127023, acc 1
2016-09-07T01:02:02.072845: step 4581, loss 0.0620817, acc 0.96
2016-09-07T01:02:02.765147: step 4582, loss 0.0707661, acc 0.98
2016-09-07T01:02:03.460117: step 4583, loss 0.00598259, acc 1
2016-09-07T01:02:04.128495: step 4584, loss 0.050679, acc 0.94
2016-09-07T01:02:04.844677: step 4585, loss 0.0304894, acc 0.98
2016-09-07T01:02:05.505984: step 4586, loss 0.0192188, acc 1
2016-09-07T01:02:06.177610: step 4587, loss 0.0136427, acc 1
2016-09-07T01:02:06.858678: step 4588, loss 0.105612, acc 0.98
2016-09-07T01:02:07.554807: step 4589, loss 0.0131398, acc 1
2016-09-07T01:02:08.235398: step 4590, loss 0.146338, acc 0.96
2016-09-07T01:02:08.928867: step 4591, loss 6.36051e-05, acc 1
2016-09-07T01:02:09.617458: step 4592, loss 0.0526515, acc 0.96
2016-09-07T01:02:10.279303: step 4593, loss 0.020689, acc 1
2016-09-07T01:02:10.975047: step 4594, loss 0.190181, acc 0.96
2016-09-07T01:02:11.684758: step 4595, loss 0.0168955, acc 0.98
2016-09-07T01:02:12.378748: step 4596, loss 0.080804, acc 0.94
2016-09-07T01:02:13.067064: step 4597, loss 0.00870733, acc 1
2016-09-07T01:02:13.742765: step 4598, loss 0.00521461, acc 1
2016-09-07T01:02:14.452434: step 4599, loss 0.0299303, acc 0.98
2016-09-07T01:02:15.122551: step 4600, loss 0.131101, acc 0.92

Evaluation:
2016-09-07T01:02:18.319146: step 4600, loss 1.97064, acc 0.726079

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-4600

2016-09-07T01:02:20.011703: step 4601, loss 0.0374451, acc 0.98
2016-09-07T01:02:20.713747: step 4602, loss 0.0564941, acc 0.96
2016-09-07T01:02:21.397952: step 4603, loss 0.0495549, acc 0.96
2016-09-07T01:02:22.096565: step 4604, loss 0.000600325, acc 1
2016-09-07T01:02:22.811115: step 4605, loss 0.0246411, acc 1
2016-09-07T01:02:23.477020: step 4606, loss 0.00698977, acc 1
2016-09-07T01:02:24.183392: step 4607, loss 0.0190805, acc 1
2016-09-07T01:02:24.800367: step 4608, loss 0.0179825, acc 1
2016-09-07T01:02:25.493885: step 4609, loss 0.0316555, acc 0.96
2016-09-07T01:02:26.183769: step 4610, loss 0.00512047, acc 1
2016-09-07T01:02:26.876658: step 4611, loss 0.0362358, acc 0.98
2016-09-07T01:02:27.576210: step 4612, loss 0.0227945, acc 0.98
2016-09-07T01:02:28.227859: step 4613, loss 0.0269024, acc 1
2016-09-07T01:02:28.915791: step 4614, loss 0.0257913, acc 0.98
2016-09-07T01:02:29.595800: step 4615, loss 0.0222296, acc 0.98
2016-09-07T01:02:30.291057: step 4616, loss 0.0326734, acc 1
2016-09-07T01:02:30.992219: step 4617, loss 0.00697217, acc 1
2016-09-07T01:02:31.684013: step 4618, loss 0.0570164, acc 0.96
2016-09-07T01:02:32.391660: step 4619, loss 0.0230911, acc 0.98
2016-09-07T01:02:33.069887: step 4620, loss 0.00878413, acc 1
2016-09-07T01:02:33.773760: step 4621, loss 0.021326, acc 1
2016-09-07T01:02:34.469399: step 4622, loss 0.0315311, acc 0.98
2016-09-07T01:02:35.156485: step 4623, loss 0.000304068, acc 1
2016-09-07T01:02:35.846552: step 4624, loss 0.00151122, acc 1
2016-09-07T01:02:36.541150: step 4625, loss 0.0421561, acc 0.96
2016-09-07T01:02:37.245760: step 4626, loss 0.049441, acc 0.96
2016-09-07T01:02:37.913427: step 4627, loss 0.0302197, acc 0.98
2016-09-07T01:02:38.620236: step 4628, loss 0.0126208, acc 1
2016-09-07T01:02:39.314378: step 4629, loss 0.0126393, acc 1
2016-09-07T01:02:39.988620: step 4630, loss 0.0700601, acc 0.98
2016-09-07T01:02:40.678415: step 4631, loss 0.0031596, acc 1
2016-09-07T01:02:41.364271: step 4632, loss 0.0310105, acc 0.98
2016-09-07T01:02:42.071876: step 4633, loss 0.00967589, acc 1
2016-09-07T01:02:42.747438: step 4634, loss 0.0143654, acc 1
2016-09-07T01:02:43.437471: step 4635, loss 0.0262319, acc 0.98
2016-09-07T01:02:44.119128: step 4636, loss 0.00840542, acc 1
2016-09-07T01:02:44.805503: step 4637, loss 0.0279663, acc 0.98
2016-09-07T01:02:45.490723: step 4638, loss 0.0982278, acc 0.96
2016-09-07T01:02:46.186779: step 4639, loss 0.00464241, acc 1
2016-09-07T01:02:46.896440: step 4640, loss 0.0515153, acc 0.96
2016-09-07T01:02:47.591311: step 4641, loss 0.0405918, acc 0.98
2016-09-07T01:02:48.307208: step 4642, loss 0.00724402, acc 1
2016-09-07T01:02:48.983691: step 4643, loss 0.00177364, acc 1
2016-09-07T01:02:49.684794: step 4644, loss 0.0736822, acc 0.96
2016-09-07T01:02:50.357546: step 4645, loss 0.0341387, acc 0.98
2016-09-07T01:02:51.042432: step 4646, loss 0.0197328, acc 1
2016-09-07T01:02:51.746431: step 4647, loss 0.00149435, acc 1
2016-09-07T01:02:52.445196: step 4648, loss 2.03704e-05, acc 1
2016-09-07T01:02:53.125692: step 4649, loss 0.0445689, acc 0.98
2016-09-07T01:02:53.809275: step 4650, loss 0.0237618, acc 0.98
2016-09-07T01:02:54.518291: step 4651, loss 0.00215072, acc 1
2016-09-07T01:02:55.208141: step 4652, loss 0.0397178, acc 0.98
2016-09-07T01:02:55.899920: step 4653, loss 0.116984, acc 0.98
2016-09-07T01:02:56.603952: step 4654, loss 0.019935, acc 0.98
2016-09-07T01:02:57.291475: step 4655, loss 0.0481363, acc 0.98
2016-09-07T01:02:57.969093: step 4656, loss 0.0404557, acc 0.96
2016-09-07T01:02:58.643936: step 4657, loss 0.00188708, acc 1
2016-09-07T01:02:59.344876: step 4658, loss 0.055952, acc 0.96
2016-09-07T01:03:00.032864: step 4659, loss 0.0555689, acc 0.98
2016-09-07T01:03:00.741876: step 4660, loss 0.0148056, acc 1
2016-09-07T01:03:01.448211: step 4661, loss 0.00218605, acc 1
2016-09-07T01:03:02.130547: step 4662, loss 0.0228677, acc 0.98
2016-09-07T01:03:02.805409: step 4663, loss 0.00596361, acc 1
2016-09-07T01:03:03.515277: step 4664, loss 0.0627084, acc 0.96
2016-09-07T01:03:04.222582: step 4665, loss 0.04844, acc 0.98
2016-09-07T01:03:04.927517: step 4666, loss 0.00875891, acc 1
2016-09-07T01:03:05.657091: step 4667, loss 0.0263293, acc 0.98
2016-09-07T01:03:06.361654: step 4668, loss 0.00692461, acc 1
2016-09-07T01:03:07.021666: step 4669, loss 0.0101974, acc 1
2016-09-07T01:03:07.711478: step 4670, loss 0.031462, acc 0.98
2016-09-07T01:03:08.404431: step 4671, loss 0.0316834, acc 0.98
2016-09-07T01:03:09.090726: step 4672, loss 0.0703454, acc 0.96
2016-09-07T01:03:09.802874: step 4673, loss 0.0165864, acc 1
2016-09-07T01:03:10.476646: step 4674, loss 0.0931183, acc 0.98
2016-09-07T01:03:11.168234: step 4675, loss 0.0344925, acc 0.98
2016-09-07T01:03:11.866261: step 4676, loss 3.06177e-05, acc 1
2016-09-07T01:03:12.570517: step 4677, loss 0.018656, acc 0.98
2016-09-07T01:03:13.263512: step 4678, loss 0.0128779, acc 1
2016-09-07T01:03:13.922629: step 4679, loss 0.0197907, acc 0.98
2016-09-07T01:03:14.644475: step 4680, loss 0.0651806, acc 0.96
2016-09-07T01:03:15.333993: step 4681, loss 0.000254152, acc 1
2016-09-07T01:03:16.018233: step 4682, loss 0.0191323, acc 1
2016-09-07T01:03:16.709967: step 4683, loss 0.0315281, acc 0.98
2016-09-07T01:03:17.389342: step 4684, loss 0.0195813, acc 1
2016-09-07T01:03:18.081202: step 4685, loss 0.0511847, acc 0.98
2016-09-07T01:03:18.754218: step 4686, loss 0.063774, acc 0.96
2016-09-07T01:03:19.472296: step 4687, loss 0.014241, acc 1
2016-09-07T01:03:20.162211: step 4688, loss 0.010825, acc 1
2016-09-07T01:03:20.846723: step 4689, loss 0.027367, acc 0.98
2016-09-07T01:03:21.523453: step 4690, loss 0.00149654, acc 1
2016-09-07T01:03:22.224108: step 4691, loss 0.0108949, acc 1
2016-09-07T01:03:22.929523: step 4692, loss 0.0637241, acc 0.98
2016-09-07T01:03:23.610942: step 4693, loss 0.00339064, acc 1
2016-09-07T01:03:24.304036: step 4694, loss 0.114169, acc 0.96
2016-09-07T01:03:24.990820: step 4695, loss 0.00914916, acc 1
2016-09-07T01:03:25.655449: step 4696, loss 0.0389072, acc 0.98
2016-09-07T01:03:26.364217: step 4697, loss 0.00395404, acc 1
2016-09-07T01:03:27.040060: step 4698, loss 0.0133085, acc 1
2016-09-07T01:03:27.723289: step 4699, loss 0.049381, acc 0.96
2016-09-07T01:03:28.381877: step 4700, loss 0.0205243, acc 1

Evaluation:
2016-09-07T01:03:31.553241: step 4700, loss 2.24845, acc 0.730769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-4700

2016-09-07T01:03:33.239675: step 4701, loss 0.00326995, acc 1
2016-09-07T01:03:33.965082: step 4702, loss 0.0300227, acc 1
2016-09-07T01:03:34.653787: step 4703, loss 0.0359374, acc 1
2016-09-07T01:03:35.344063: step 4704, loss 0.0126941, acc 1
2016-09-07T01:03:36.043446: step 4705, loss 0.00847709, acc 1
2016-09-07T01:03:36.720297: step 4706, loss 0.0867441, acc 0.96
2016-09-07T01:03:37.417100: step 4707, loss 0.0362932, acc 0.98
2016-09-07T01:03:38.104691: step 4708, loss 0.0291966, acc 0.98
2016-09-07T01:03:38.802445: step 4709, loss 0.00181694, acc 1
2016-09-07T01:03:39.492908: step 4710, loss 0.033727, acc 0.98
2016-09-07T01:03:40.192625: step 4711, loss 0.0784461, acc 0.98
2016-09-07T01:03:40.896208: step 4712, loss 0.01325, acc 1
2016-09-07T01:03:41.551561: step 4713, loss 0.23156, acc 0.92
2016-09-07T01:03:42.255775: step 4714, loss 0.077057, acc 0.96
2016-09-07T01:03:42.937499: step 4715, loss 0.0150684, acc 1
2016-09-07T01:03:43.624225: step 4716, loss 0.0214997, acc 0.98
2016-09-07T01:03:44.312733: step 4717, loss 0.00294233, acc 1
2016-09-07T01:03:45.059148: step 4718, loss 0.0529288, acc 0.96
2016-09-07T01:03:45.767298: step 4719, loss 0.00501348, acc 1
2016-09-07T01:03:46.457352: step 4720, loss 0.0435496, acc 0.96
2016-09-07T01:03:47.154349: step 4721, loss 0.0231201, acc 0.98
2016-09-07T01:03:47.850157: step 4722, loss 0.0496261, acc 0.98
2016-09-07T01:03:48.534099: step 4723, loss 0.0110643, acc 1
2016-09-07T01:03:49.206508: step 4724, loss 0.00907609, acc 1
2016-09-07T01:03:49.862749: step 4725, loss 0.141991, acc 0.98
2016-09-07T01:03:50.549946: step 4726, loss 0.0131789, acc 1
2016-09-07T01:03:51.233298: step 4727, loss 0.0225734, acc 0.98
2016-09-07T01:03:51.932157: step 4728, loss 0.0286791, acc 0.98
2016-09-07T01:03:52.628117: step 4729, loss 0.0851259, acc 0.98
2016-09-07T01:03:53.301433: step 4730, loss 0.0148086, acc 1
2016-09-07T01:03:53.982784: step 4731, loss 0.0465031, acc 1
2016-09-07T01:03:54.647902: step 4732, loss 0.0433634, acc 1
2016-09-07T01:03:55.343614: step 4733, loss 0.0194465, acc 0.98
2016-09-07T01:03:56.009664: step 4734, loss 0.0109334, acc 1
2016-09-07T01:03:56.698398: step 4735, loss 0.0247667, acc 1
2016-09-07T01:03:57.386309: step 4736, loss 0.0490479, acc 0.98
2016-09-07T01:03:58.068790: step 4737, loss 0.0973851, acc 0.98
2016-09-07T01:03:58.756176: step 4738, loss 0.0819032, acc 0.96
2016-09-07T01:03:59.458840: step 4739, loss 0.0766048, acc 0.96
2016-09-07T01:04:00.152967: step 4740, loss 0.0110666, acc 1
2016-09-07T01:04:00.841180: step 4741, loss 0.0389371, acc 0.98
2016-09-07T01:04:01.539162: step 4742, loss 0.0245578, acc 0.98
2016-09-07T01:04:02.242849: step 4743, loss 0.00379454, acc 1
2016-09-07T01:04:02.924200: step 4744, loss 0.0189046, acc 1
2016-09-07T01:04:03.591742: step 4745, loss 0.0138091, acc 1
2016-09-07T01:04:04.288877: step 4746, loss 0.00788409, acc 1
2016-09-07T01:04:05.008509: step 4747, loss 0.0157003, acc 1
2016-09-07T01:04:05.669102: step 4748, loss 0.0336778, acc 1
2016-09-07T01:04:06.387934: step 4749, loss 0.0689501, acc 0.96
2016-09-07T01:04:07.083124: step 4750, loss 0.051428, acc 0.98
2016-09-07T01:04:07.763521: step 4751, loss 0.00212219, acc 1
2016-09-07T01:04:08.450163: step 4752, loss 0.00734084, acc 1
2016-09-07T01:04:09.139076: step 4753, loss 0.0157743, acc 1
2016-09-07T01:04:09.850934: step 4754, loss 0.0103132, acc 1
2016-09-07T01:04:10.536898: step 4755, loss 0.0130544, acc 1
2016-09-07T01:04:11.227302: step 4756, loss 0.0360207, acc 0.98
2016-09-07T01:04:11.940278: step 4757, loss 0.05453, acc 0.96
2016-09-07T01:04:12.626019: step 4758, loss 0.00955688, acc 1
2016-09-07T01:04:13.318671: step 4759, loss 0.0171804, acc 1
2016-09-07T01:04:13.998890: step 4760, loss 0.0153936, acc 1
2016-09-07T01:04:14.704875: step 4761, loss 0.0108538, acc 1
2016-09-07T01:04:15.377501: step 4762, loss 0.000949461, acc 1
2016-09-07T01:04:16.057114: step 4763, loss 0.00326056, acc 1
2016-09-07T01:04:16.745555: step 4764, loss 0.0019445, acc 1
2016-09-07T01:04:17.436151: step 4765, loss 0.00617366, acc 1
2016-09-07T01:04:18.115351: step 4766, loss 0.0184275, acc 1
2016-09-07T01:04:18.789398: step 4767, loss 0.0163115, acc 1
2016-09-07T01:04:19.523952: step 4768, loss 0.0505755, acc 0.96
2016-09-07T01:04:20.193280: step 4769, loss 0.000385388, acc 1
2016-09-07T01:04:20.878486: step 4770, loss 0.0132067, acc 1
2016-09-07T01:04:21.565351: step 4771, loss 0.0312124, acc 0.98
2016-09-07T01:04:22.241036: step 4772, loss 0.0116177, acc 1
2016-09-07T01:04:22.927691: step 4773, loss 0.131076, acc 0.96
2016-09-07T01:04:23.600509: step 4774, loss 0.0447508, acc 0.96
2016-09-07T01:04:24.292140: step 4775, loss 0.0268653, acc 0.98
2016-09-07T01:04:24.968289: step 4776, loss 0.0296947, acc 0.98
2016-09-07T01:04:25.645438: step 4777, loss 0.00898023, acc 1
2016-09-07T01:04:26.320167: step 4778, loss 0.00508233, acc 1
2016-09-07T01:04:26.996951: step 4779, loss 0.00308992, acc 1
2016-09-07T01:04:27.682381: step 4780, loss 0.0637849, acc 0.96
2016-09-07T01:04:28.364643: step 4781, loss 0.0209575, acc 1
2016-09-07T01:04:29.095382: step 4782, loss 0.0451673, acc 0.98
2016-09-07T01:04:29.776877: step 4783, loss 0.0535293, acc 0.98
2016-09-07T01:04:30.477649: step 4784, loss 0.00255252, acc 1
2016-09-07T01:04:31.181625: step 4785, loss 0.0596343, acc 0.98
2016-09-07T01:04:31.863118: step 4786, loss 0.156229, acc 0.96
2016-09-07T01:04:32.575524: step 4787, loss 0.0243031, acc 0.98
2016-09-07T01:04:33.258574: step 4788, loss 0.000277431, acc 1
2016-09-07T01:04:33.957563: step 4789, loss 0.00925668, acc 1
2016-09-07T01:04:34.632603: step 4790, loss 0.10416, acc 0.98
2016-09-07T01:04:35.332527: step 4791, loss 0.0190369, acc 0.98
2016-09-07T01:04:36.011154: step 4792, loss 0.0490624, acc 1
2016-09-07T01:04:36.697686: step 4793, loss 0.0179882, acc 0.98
2016-09-07T01:04:37.381865: step 4794, loss 0.0197766, acc 0.98
2016-09-07T01:04:38.045418: step 4795, loss 0.0225839, acc 1
2016-09-07T01:04:38.723189: step 4796, loss 0.00172242, acc 1
2016-09-07T01:04:39.393128: step 4797, loss 0.0252641, acc 0.98
2016-09-07T01:04:40.071198: step 4798, loss 0.0215365, acc 0.98
2016-09-07T01:04:40.764765: step 4799, loss 0.0409105, acc 0.96
2016-09-07T01:04:41.389758: step 4800, loss 0.000104728, acc 1

Evaluation:
2016-09-07T01:04:44.559091: step 4800, loss 1.8142, acc 0.736398

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-4800

2016-09-07T01:04:46.211564: step 4801, loss 0.0277541, acc 0.98
2016-09-07T01:04:46.885247: step 4802, loss 0.0258282, acc 0.98
2016-09-07T01:04:47.572089: step 4803, loss 0.176377, acc 0.94
2016-09-07T01:04:48.282163: step 4804, loss 0.0204184, acc 1
2016-09-07T01:04:48.962392: step 4805, loss 0.0578989, acc 0.98
2016-09-07T01:04:49.668817: step 4806, loss 0.0433504, acc 0.98
2016-09-07T01:04:50.362387: step 4807, loss 0.10041, acc 0.96
2016-09-07T01:04:51.047893: step 4808, loss 0.0891327, acc 0.98
2016-09-07T01:04:51.770483: step 4809, loss 0.0320834, acc 0.98
2016-09-07T01:04:52.464626: step 4810, loss 0.0409813, acc 0.98
2016-09-07T01:04:53.179760: step 4811, loss 0.0103104, acc 1
2016-09-07T01:04:53.862990: step 4812, loss 0.0110666, acc 1
2016-09-07T01:04:54.536550: step 4813, loss 0.0321722, acc 0.98
2016-09-07T01:04:55.221428: step 4814, loss 0.0706358, acc 0.98
2016-09-07T01:04:55.920818: step 4815, loss 0.0121478, acc 1
2016-09-07T01:04:56.619284: step 4816, loss 0.014047, acc 1
2016-09-07T01:04:57.276286: step 4817, loss 0.0171151, acc 1
2016-09-07T01:04:57.999007: step 4818, loss 0.0176472, acc 1
2016-09-07T01:04:58.674032: step 4819, loss 0.0267982, acc 0.98
2016-09-07T01:04:59.363760: step 4820, loss 0.208727, acc 0.96
2016-09-07T01:05:00.037778: step 4821, loss 0.057678, acc 0.98
2016-09-07T01:05:00.750918: step 4822, loss 0.0307669, acc 0.98
2016-09-07T01:05:01.446327: step 4823, loss 0.00343122, acc 1
2016-09-07T01:05:02.106713: step 4824, loss 0.0207309, acc 0.98
2016-09-07T01:05:02.812832: step 4825, loss 0.0887869, acc 0.96
2016-09-07T01:05:03.489408: step 4826, loss 0.0264161, acc 0.98
2016-09-07T01:05:04.169479: step 4827, loss 0.000428835, acc 1
2016-09-07T01:05:04.862627: step 4828, loss 0.00545886, acc 1
2016-09-07T01:05:05.556473: step 4829, loss 0.0573969, acc 0.96
2016-09-07T01:05:06.282227: step 4830, loss 0.0254634, acc 1
2016-09-07T01:05:06.952371: step 4831, loss 0.031338, acc 0.98
2016-09-07T01:05:07.653487: step 4832, loss 0.0928564, acc 0.94
2016-09-07T01:05:08.340171: step 4833, loss 0.0178159, acc 0.98
2016-09-07T01:05:09.014228: step 4834, loss 0.0244651, acc 0.98
2016-09-07T01:05:09.684385: step 4835, loss 0.0305593, acc 0.98
2016-09-07T01:05:10.365210: step 4836, loss 0.0471584, acc 0.98
2016-09-07T01:05:11.063194: step 4837, loss 0.00990235, acc 1
2016-09-07T01:05:11.737025: step 4838, loss 0.0266157, acc 1
2016-09-07T01:05:12.444937: step 4839, loss 0.00342577, acc 1
2016-09-07T01:05:13.109070: step 4840, loss 0.000125989, acc 1
2016-09-07T01:05:13.785073: step 4841, loss 0.0410584, acc 0.98
2016-09-07T01:05:14.490994: step 4842, loss 0.0396771, acc 0.98
2016-09-07T01:05:15.185068: step 4843, loss 0.137125, acc 0.94
2016-09-07T01:05:15.893066: step 4844, loss 0.0565889, acc 0.96
2016-09-07T01:05:16.558066: step 4845, loss 0.0395469, acc 0.98
2016-09-07T01:05:17.264061: step 4846, loss 0.0245729, acc 0.98
2016-09-07T01:05:17.950040: step 4847, loss 0.00325588, acc 1
2016-09-07T01:05:18.641021: step 4848, loss 0.0107729, acc 1
2016-09-07T01:05:19.334268: step 4849, loss 0.024337, acc 0.98
2016-09-07T01:05:20.021526: step 4850, loss 0.0081523, acc 1
2016-09-07T01:05:20.706490: step 4851, loss 0.0113906, acc 1
2016-09-07T01:05:21.388975: step 4852, loss 0.00418438, acc 1
2016-09-07T01:05:22.079796: step 4853, loss 0.0840563, acc 0.98
2016-09-07T01:05:22.763755: step 4854, loss 0.0300969, acc 0.98
2016-09-07T01:05:23.438557: step 4855, loss 0.0192928, acc 0.98
2016-09-07T01:05:24.127677: step 4856, loss 0.101393, acc 0.98
2016-09-07T01:05:24.810366: step 4857, loss 0.0372071, acc 0.98
2016-09-07T01:05:25.498427: step 4858, loss 0.0037467, acc 1
2016-09-07T01:05:26.159077: step 4859, loss 0.21446, acc 0.94
2016-09-07T01:05:26.873527: step 4860, loss 0.00260548, acc 1
2016-09-07T01:05:27.560526: step 4861, loss 0.0787423, acc 0.96
2016-09-07T01:05:28.244525: step 4862, loss 0.0104075, acc 1
2016-09-07T01:05:28.944935: step 4863, loss 0.0242458, acc 1
2016-09-07T01:05:29.660802: step 4864, loss 0.0060837, acc 1
2016-09-07T01:05:30.360397: step 4865, loss 0.0932568, acc 0.96
2016-09-07T01:05:31.044341: step 4866, loss 0.00798333, acc 1
2016-09-07T01:05:31.758207: step 4867, loss 0.0135957, acc 1
2016-09-07T01:05:32.434250: step 4868, loss 0.00225874, acc 1
2016-09-07T01:05:33.115657: step 4869, loss 0.00673463, acc 1
2016-09-07T01:05:33.805814: step 4870, loss 0.0599582, acc 0.96
2016-09-07T01:05:34.472735: step 4871, loss 0.19676, acc 0.96
2016-09-07T01:05:35.164562: step 4872, loss 0.0150071, acc 0.98
2016-09-07T01:05:35.823873: step 4873, loss 6.61891e-05, acc 1
2016-09-07T01:05:36.525852: step 4874, loss 0.0137862, acc 1
2016-09-07T01:05:37.215003: step 4875, loss 0.000710035, acc 1
2016-09-07T01:05:37.878654: step 4876, loss 0.0285914, acc 1
2016-09-07T01:05:38.553067: step 4877, loss 0.0169103, acc 1
2016-09-07T01:05:39.231232: step 4878, loss 0.0202661, acc 1
2016-09-07T01:05:39.926927: step 4879, loss 0.0521546, acc 0.96
2016-09-07T01:05:40.611551: step 4880, loss 0.0152481, acc 1
2016-09-07T01:05:41.310670: step 4881, loss 0.0388427, acc 0.96
2016-09-07T01:05:41.999382: step 4882, loss 0.0261321, acc 0.98
2016-09-07T01:05:42.690093: step 4883, loss 0.026017, acc 0.98
2016-09-07T01:05:43.385569: step 4884, loss 0.00153739, acc 1
2016-09-07T01:05:44.071901: step 4885, loss 0.003723, acc 1
2016-09-07T01:05:44.758329: step 4886, loss 0.055437, acc 0.98
2016-09-07T01:05:45.414068: step 4887, loss 0.0230863, acc 0.98
2016-09-07T01:05:46.122926: step 4888, loss 0.0477386, acc 0.96
2016-09-07T01:05:46.809613: step 4889, loss 0.0643479, acc 0.96
2016-09-07T01:05:47.496473: step 4890, loss 0.0396901, acc 0.98
2016-09-07T01:05:48.189720: step 4891, loss 0.00496938, acc 1
2016-09-07T01:05:48.870722: step 4892, loss 0.017741, acc 1
2016-09-07T01:05:49.550345: step 4893, loss 0.0273464, acc 0.98
2016-09-07T01:05:50.210439: step 4894, loss 0.0106195, acc 1
2016-09-07T01:05:50.945099: step 4895, loss 0.0110451, acc 1
2016-09-07T01:05:51.651155: step 4896, loss 0.0241941, acc 1
2016-09-07T01:05:52.332106: step 4897, loss 0.01753, acc 0.98
2016-09-07T01:05:53.020321: step 4898, loss 0.0529101, acc 0.96
2016-09-07T01:05:53.702635: step 4899, loss 0.047536, acc 0.98
2016-09-07T01:05:54.397684: step 4900, loss 0.0579201, acc 0.96

Evaluation:
2016-09-07T01:05:57.576833: step 4900, loss 1.83535, acc 0.736398

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-4900

2016-09-07T01:05:59.212036: step 4901, loss 0.0122081, acc 1
2016-09-07T01:05:59.872451: step 4902, loss 0.0110222, acc 1
2016-09-07T01:06:00.601631: step 4903, loss 0.0244152, acc 1
2016-09-07T01:06:01.299226: step 4904, loss 0.0482452, acc 0.96
2016-09-07T01:06:02.013099: step 4905, loss 0.000494123, acc 1
2016-09-07T01:06:02.698414: step 4906, loss 0.00353833, acc 1
2016-09-07T01:06:03.383971: step 4907, loss 0.00239815, acc 1
2016-09-07T01:06:04.068331: step 4908, loss 0.0923422, acc 0.96
2016-09-07T01:06:04.754560: step 4909, loss 0.0196767, acc 0.98
2016-09-07T01:06:05.441316: step 4910, loss 0.0373875, acc 0.98
2016-09-07T01:06:06.124860: step 4911, loss 0.0439227, acc 0.98
2016-09-07T01:06:06.795607: step 4912, loss 0.0163126, acc 0.98
2016-09-07T01:06:07.498609: step 4913, loss 0.00178422, acc 1
2016-09-07T01:06:08.189862: step 4914, loss 0.0105111, acc 1
2016-09-07T01:06:08.906443: step 4915, loss 0.0112325, acc 1
2016-09-07T01:06:09.580106: step 4916, loss 0.0148568, acc 1
2016-09-07T01:06:10.269614: step 4917, loss 0.000300682, acc 1
2016-09-07T01:06:10.943411: step 4918, loss 0.0344486, acc 0.98
2016-09-07T01:06:11.643332: step 4919, loss 0.0455304, acc 0.96
2016-09-07T01:06:12.359943: step 4920, loss 0.0154447, acc 0.98
2016-09-07T01:06:13.030459: step 4921, loss 0.0166279, acc 1
2016-09-07T01:06:13.746327: step 4922, loss 0.0233321, acc 1
2016-09-07T01:06:14.417331: step 4923, loss 0.0255769, acc 0.98
2016-09-07T01:06:15.090715: step 4924, loss 0.0356311, acc 0.96
2016-09-07T01:06:15.779654: step 4925, loss 0.0821042, acc 0.98
2016-09-07T01:06:16.457411: step 4926, loss 0.0438387, acc 0.98
2016-09-07T01:06:17.142268: step 4927, loss 0.00243057, acc 1
2016-09-07T01:06:17.816019: step 4928, loss 0.0325166, acc 1
2016-09-07T01:06:18.516759: step 4929, loss 0.0693441, acc 0.98
2016-09-07T01:06:19.198337: step 4930, loss 0.0516253, acc 0.98
2016-09-07T01:06:19.877738: step 4931, loss 0.0373694, acc 0.98
2016-09-07T01:06:20.568151: step 4932, loss 0.0266444, acc 1
2016-09-07T01:06:21.256376: step 4933, loss 0.0135843, acc 1
2016-09-07T01:06:21.953094: step 4934, loss 0.00313385, acc 1
2016-09-07T01:06:22.636028: step 4935, loss 0.00248587, acc 1
2016-09-07T01:06:23.341554: step 4936, loss 0.0223838, acc 1
2016-09-07T01:06:24.018043: step 4937, loss 0.00757294, acc 1
2016-09-07T01:06:24.705493: step 4938, loss 0.0130576, acc 1
2016-09-07T01:06:25.406029: step 4939, loss 0.00624853, acc 1
2016-09-07T01:06:26.074329: step 4940, loss 0.0167907, acc 1
2016-09-07T01:06:26.753755: step 4941, loss 0.0528848, acc 0.96
2016-09-07T01:06:27.446831: step 4942, loss 0.00107885, acc 1
2016-09-07T01:06:28.141108: step 4943, loss 0.00344675, acc 1
2016-09-07T01:06:28.811701: step 4944, loss 0.000218849, acc 1
2016-09-07T01:06:29.482148: step 4945, loss 0.0732487, acc 0.98
2016-09-07T01:06:30.157973: step 4946, loss 0.0255675, acc 0.98
2016-09-07T01:06:30.830930: step 4947, loss 0.0390227, acc 0.96
2016-09-07T01:06:31.531888: step 4948, loss 0.0388554, acc 0.98
2016-09-07T01:06:32.215006: step 4949, loss 0.0861753, acc 0.96
2016-09-07T01:06:33.056682: step 4950, loss 0.0680493, acc 0.96
2016-09-07T01:06:33.741811: step 4951, loss 0.123654, acc 0.98
2016-09-07T01:06:34.424251: step 4952, loss 0.029643, acc 0.96
2016-09-07T01:06:35.093454: step 4953, loss 0.0609343, acc 0.98
2016-09-07T01:06:35.792259: step 4954, loss 0.0216697, acc 0.98
2016-09-07T01:06:36.466645: step 4955, loss 0.0112502, acc 1
2016-09-07T01:06:37.123143: step 4956, loss 0.0093052, acc 1
2016-09-07T01:06:37.813353: step 4957, loss 0.0475676, acc 0.98
2016-09-07T01:06:38.518233: step 4958, loss 0.0762828, acc 0.94
2016-09-07T01:06:39.192407: step 4959, loss 0.0200581, acc 1
2016-09-07T01:06:39.886735: step 4960, loss 0.039474, acc 0.98
2016-09-07T01:06:40.564855: step 4961, loss 0.000197795, acc 1
2016-09-07T01:06:41.250344: step 4962, loss 0.0186232, acc 1
2016-09-07T01:06:41.909890: step 4963, loss 0.00947578, acc 1
2016-09-07T01:06:42.605181: step 4964, loss 0.0158973, acc 0.98
2016-09-07T01:06:43.279604: step 4965, loss 0.0102085, acc 1
2016-09-07T01:06:43.966643: step 4966, loss 0.0355848, acc 0.96
2016-09-07T01:06:44.645599: step 4967, loss 0.00991113, acc 1
2016-09-07T01:06:45.326992: step 4968, loss 0.031898, acc 1
2016-09-07T01:06:46.005780: step 4969, loss 0.164922, acc 0.98
2016-09-07T01:06:46.687156: step 4970, loss 0.0205712, acc 1
2016-09-07T01:06:47.379298: step 4971, loss 0.0146589, acc 0.98
2016-09-07T01:06:48.070797: step 4972, loss 0.0471213, acc 0.98
2016-09-07T01:06:48.766105: step 4973, loss 0.0173194, acc 0.98
2016-09-07T01:06:49.454034: step 4974, loss 0.0217241, acc 0.98
2016-09-07T01:06:50.139078: step 4975, loss 0.00314741, acc 1
2016-09-07T01:06:50.846021: step 4976, loss 0.00110377, acc 1
2016-09-07T01:06:51.514932: step 4977, loss 0.0175407, acc 0.98
2016-09-07T01:06:52.200373: step 4978, loss 0.0795748, acc 0.94
2016-09-07T01:06:52.877185: step 4979, loss 0.0466543, acc 0.98
2016-09-07T01:06:53.582099: step 4980, loss 0.0467788, acc 0.98
2016-09-07T01:06:54.262900: step 4981, loss 0.00150401, acc 1
2016-09-07T01:06:54.948065: step 4982, loss 0.0230161, acc 0.98
2016-09-07T01:06:55.655325: step 4983, loss 0.00832652, acc 1
2016-09-07T01:06:56.313679: step 4984, loss 0.035097, acc 1
2016-09-07T01:06:57.042767: step 4985, loss 0.041711, acc 0.96
2016-09-07T01:06:57.725005: step 4986, loss 0.000351103, acc 1
2016-09-07T01:06:58.412953: step 4987, loss 0.0849003, acc 0.94
2016-09-07T01:06:59.114781: step 4988, loss 0.0171333, acc 0.98
2016-09-07T01:06:59.827282: step 4989, loss 0.0238158, acc 0.98
2016-09-07T01:07:00.534489: step 4990, loss 0.037322, acc 0.98
2016-09-07T01:07:01.209264: step 4991, loss 0.073779, acc 0.98
2016-09-07T01:07:01.864777: step 4992, loss 0.00254392, acc 1
2016-09-07T01:07:02.553924: step 4993, loss 0.025863, acc 1
2016-09-07T01:07:03.233811: step 4994, loss 0.0239525, acc 0.98
2016-09-07T01:07:03.930558: step 4995, loss 0.0229594, acc 1
2016-09-07T01:07:04.630148: step 4996, loss 0.0143073, acc 1
2016-09-07T01:07:05.311925: step 4997, loss 0.0202588, acc 1
2016-09-07T01:07:05.991069: step 4998, loss 0.0121327, acc 1
2016-09-07T01:07:06.688263: step 4999, loss 0.0266999, acc 0.98
2016-09-07T01:07:07.379803: step 5000, loss 0.0838991, acc 0.96

Evaluation:
2016-09-07T01:07:10.555312: step 5000, loss 1.92632, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-5000

2016-09-07T01:07:12.279577: step 5001, loss 0.00860398, acc 1
2016-09-07T01:07:12.974304: step 5002, loss 0.0359262, acc 0.96
2016-09-07T01:07:13.654861: step 5003, loss 0.00497407, acc 1
2016-09-07T01:07:14.344942: step 5004, loss 0.0195408, acc 1
2016-09-07T01:07:15.046634: step 5005, loss 0.144288, acc 0.94
2016-09-07T01:07:15.727231: step 5006, loss 0.0935703, acc 0.96
2016-09-07T01:07:16.387068: step 5007, loss 0.00358974, acc 1
2016-09-07T01:07:17.068018: step 5008, loss 0.00369034, acc 1
2016-09-07T01:07:17.758702: step 5009, loss 0.0728342, acc 0.94
2016-09-07T01:07:18.436567: step 5010, loss 0.0236172, acc 1
2016-09-07T01:07:19.100992: step 5011, loss 0.00849676, acc 1
2016-09-07T01:07:19.817638: step 5012, loss 0.101643, acc 0.96
2016-09-07T01:07:20.484176: step 5013, loss 0.0075162, acc 1
2016-09-07T01:07:21.171434: step 5014, loss 0.0349815, acc 0.98
2016-09-07T01:07:21.857626: step 5015, loss 0.0159687, acc 1
2016-09-07T01:07:22.573612: step 5016, loss 0.115892, acc 0.98
2016-09-07T01:07:23.265331: step 5017, loss 0.0018844, acc 1
2016-09-07T01:07:23.953450: step 5018, loss 0.0125065, acc 1
2016-09-07T01:07:24.663593: step 5019, loss 0.00932497, acc 1
2016-09-07T01:07:25.369970: step 5020, loss 0.0590181, acc 0.96
2016-09-07T01:07:26.066799: step 5021, loss 0.00301585, acc 1
2016-09-07T01:07:26.768552: step 5022, loss 0.104344, acc 0.94
2016-09-07T01:07:27.455555: step 5023, loss 0.038528, acc 0.98
2016-09-07T01:07:28.148494: step 5024, loss 0.0415632, acc 0.96
2016-09-07T01:07:28.814698: step 5025, loss 0.0600816, acc 0.96
2016-09-07T01:07:29.518463: step 5026, loss 0.0276278, acc 0.98
2016-09-07T01:07:30.181405: step 5027, loss 0.0266044, acc 0.98
2016-09-07T01:07:30.854204: step 5028, loss 0.0616134, acc 0.96
2016-09-07T01:07:31.546845: step 5029, loss 0.122352, acc 0.96
2016-09-07T01:07:32.247309: step 5030, loss 0.0337933, acc 0.98
2016-09-07T01:07:32.941145: step 5031, loss 0.0107792, acc 1
2016-09-07T01:07:33.588293: step 5032, loss 0.00223309, acc 1
2016-09-07T01:07:34.299524: step 5033, loss 0.0157892, acc 1
2016-09-07T01:07:34.971494: step 5034, loss 0.0155872, acc 1
2016-09-07T01:07:35.650707: step 5035, loss 0.0326443, acc 1
2016-09-07T01:07:36.338357: step 5036, loss 0.000938768, acc 1
2016-09-07T01:07:37.026084: step 5037, loss 0.0321289, acc 0.98
2016-09-07T01:07:37.744752: step 5038, loss 0.0103069, acc 1
2016-09-07T01:07:38.395676: step 5039, loss 0.0366502, acc 0.96
2016-09-07T01:07:39.101746: step 5040, loss 0.0146996, acc 1
2016-09-07T01:07:39.778937: step 5041, loss 0.022338, acc 0.98
2016-09-07T01:07:40.462459: step 5042, loss 0.0434358, acc 0.98
2016-09-07T01:07:41.162891: step 5043, loss 0.0301785, acc 0.98
2016-09-07T01:07:41.869713: step 5044, loss 0.0198788, acc 1
2016-09-07T01:07:42.573165: step 5045, loss 0.000867128, acc 1
2016-09-07T01:07:43.260308: step 5046, loss 0.0710612, acc 0.96
2016-09-07T01:07:43.945516: step 5047, loss 0.0234722, acc 0.98
2016-09-07T01:07:44.628900: step 5048, loss 0.0269081, acc 0.98
2016-09-07T01:07:45.319119: step 5049, loss 0.016046, acc 1
2016-09-07T01:07:46.013621: step 5050, loss 0.0863239, acc 0.96
2016-09-07T01:07:46.710632: step 5051, loss 0.0401799, acc 0.98
2016-09-07T01:07:47.396798: step 5052, loss 0.00536479, acc 1
2016-09-07T01:07:48.072134: step 5053, loss 0.0304133, acc 0.98
2016-09-07T01:07:48.796776: step 5054, loss 5.43453e-05, acc 1
2016-09-07T01:07:49.508618: step 5055, loss 0.0398372, acc 0.96
2016-09-07T01:07:50.193046: step 5056, loss 0.0301795, acc 0.98
2016-09-07T01:07:50.874109: step 5057, loss 0.028366, acc 1
2016-09-07T01:07:51.539759: step 5058, loss 0.0428788, acc 0.98
2016-09-07T01:07:52.230188: step 5059, loss 0.0284503, acc 0.98
2016-09-07T01:07:52.904353: step 5060, loss 0.012853, acc 1
2016-09-07T01:07:53.606317: step 5061, loss 0.00198366, acc 1
2016-09-07T01:07:54.282192: step 5062, loss 0.15961, acc 0.96
2016-09-07T01:07:54.962414: step 5063, loss 0.0228228, acc 0.98
2016-09-07T01:07:55.654043: step 5064, loss 0.0188462, acc 0.98
2016-09-07T01:07:56.347025: step 5065, loss 0.00276619, acc 1
2016-09-07T01:07:57.046168: step 5066, loss 0.167035, acc 0.96
2016-09-07T01:07:57.722256: step 5067, loss 0.0084319, acc 1
2016-09-07T01:07:58.418124: step 5068, loss 0.0426604, acc 0.96
2016-09-07T01:07:59.119505: step 5069, loss 0.0338389, acc 0.98
2016-09-07T01:07:59.804556: step 5070, loss 0.0224451, acc 1
2016-09-07T01:08:00.517000: step 5071, loss 0.0043494, acc 1
2016-09-07T01:08:01.197008: step 5072, loss 0.0304696, acc 0.98
2016-09-07T01:08:01.912672: step 5073, loss 0.00468862, acc 1
2016-09-07T01:08:02.605316: step 5074, loss 0.0220669, acc 0.98
2016-09-07T01:08:03.302735: step 5075, loss 0.00961735, acc 1
2016-09-07T01:08:04.007623: step 5076, loss 0.00965478, acc 1
2016-09-07T01:08:04.706764: step 5077, loss 0.0398336, acc 0.98
2016-09-07T01:08:05.412823: step 5078, loss 0.0713505, acc 0.96
2016-09-07T01:08:06.091265: step 5079, loss 0.036501, acc 0.98
2016-09-07T01:08:06.801821: step 5080, loss 0.00866814, acc 1
2016-09-07T01:08:07.500762: step 5081, loss 0.0124084, acc 1
2016-09-07T01:08:08.187476: step 5082, loss 0.106966, acc 0.94
2016-09-07T01:08:08.860880: step 5083, loss 0.0613867, acc 0.96
2016-09-07T01:08:09.562422: step 5084, loss 0.0614887, acc 0.98
2016-09-07T01:08:10.299048: step 5085, loss 0.00420473, acc 1
2016-09-07T01:08:10.972304: step 5086, loss 0.0059989, acc 1
2016-09-07T01:08:11.663309: step 5087, loss 0.0123005, acc 1
2016-09-07T01:08:12.357422: step 5088, loss 0.00857877, acc 1
2016-09-07T01:08:13.051307: step 5089, loss 0.0183753, acc 1
2016-09-07T01:08:13.728332: step 5090, loss 0.0216795, acc 0.98
2016-09-07T01:08:14.428449: step 5091, loss 0.00721378, acc 1
2016-09-07T01:08:15.134419: step 5092, loss 0.020994, acc 0.98
2016-09-07T01:08:15.805424: step 5093, loss 0.00981264, acc 1
2016-09-07T01:08:16.498351: step 5094, loss 0.000754017, acc 1
2016-09-07T01:08:17.192703: step 5095, loss 0.0144253, acc 1
2016-09-07T01:08:17.868817: step 5096, loss 0.148222, acc 0.96
2016-09-07T01:08:18.548219: step 5097, loss 0.0299131, acc 0.98
2016-09-07T01:08:19.231242: step 5098, loss 0.0318519, acc 0.98
2016-09-07T01:08:19.930643: step 5099, loss 0.0246231, acc 0.98
2016-09-07T01:08:20.595587: step 5100, loss 0.00858464, acc 1

Evaluation:
2016-09-07T01:08:23.773371: step 5100, loss 1.78108, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-5100

2016-09-07T01:08:25.456059: step 5101, loss 0.0445291, acc 0.96
2016-09-07T01:08:26.149317: step 5102, loss 0.0228281, acc 1
2016-09-07T01:08:26.862009: step 5103, loss 0.0142087, acc 1
2016-09-07T01:08:27.548064: step 5104, loss 0.0122861, acc 1
2016-09-07T01:08:28.228796: step 5105, loss 0.0619426, acc 0.98
2016-09-07T01:08:28.905000: step 5106, loss 0.0494114, acc 0.98
2016-09-07T01:08:29.610263: step 5107, loss 0.029855, acc 0.98
2016-09-07T01:08:30.281082: step 5108, loss 0.0107125, acc 1
2016-09-07T01:08:30.956675: step 5109, loss 0.0143895, acc 0.98
2016-09-07T01:08:31.641676: step 5110, loss 0.00279787, acc 1
2016-09-07T01:08:32.345913: step 5111, loss 0.00190198, acc 1
2016-09-07T01:08:33.039832: step 5112, loss 0.0269771, acc 0.98
2016-09-07T01:08:33.717930: step 5113, loss 0.0145035, acc 1
2016-09-07T01:08:34.466453: step 5114, loss 0.017897, acc 1
2016-09-07T01:08:35.186178: step 5115, loss 0.00349659, acc 1
2016-09-07T01:08:35.886610: step 5116, loss 0.0320774, acc 0.98
2016-09-07T01:08:36.586587: step 5117, loss 0.0357745, acc 1
2016-09-07T01:08:37.281313: step 5118, loss 0.320089, acc 0.92
2016-09-07T01:08:37.972341: step 5119, loss 0.0288955, acc 0.98
2016-09-07T01:08:38.638510: step 5120, loss 0.102715, acc 0.96
2016-09-07T01:08:39.347076: step 5121, loss 0.00579346, acc 1
2016-09-07T01:08:40.042677: step 5122, loss 0.0339577, acc 0.98
2016-09-07T01:08:40.711520: step 5123, loss 0.0756383, acc 0.98
2016-09-07T01:08:41.384369: step 5124, loss 0.0291562, acc 0.98
2016-09-07T01:08:42.056900: step 5125, loss 0.018734, acc 1
2016-09-07T01:08:42.772531: step 5126, loss 0.00956156, acc 1
2016-09-07T01:08:43.444107: step 5127, loss 0.00393753, acc 1
2016-09-07T01:08:44.142261: step 5128, loss 0.000965715, acc 1
2016-09-07T01:08:44.821130: step 5129, loss 0.0551064, acc 0.98
2016-09-07T01:08:45.508149: step 5130, loss 0.00336332, acc 1
2016-09-07T01:08:46.194097: step 5131, loss 0.00943623, acc 1
2016-09-07T01:08:46.855477: step 5132, loss 0.0133765, acc 1
2016-09-07T01:08:47.567151: step 5133, loss 0.0222797, acc 0.98
2016-09-07T01:08:48.253380: step 5134, loss 0.00596919, acc 1
2016-09-07T01:08:48.951939: step 5135, loss 0.0120798, acc 1
2016-09-07T01:08:49.644650: step 5136, loss 0.0150128, acc 1
2016-09-07T01:08:50.330231: step 5137, loss 0.0189404, acc 1
2016-09-07T01:08:51.022080: step 5138, loss 0.0416559, acc 0.96
2016-09-07T01:08:51.722291: step 5139, loss 0.124753, acc 0.96
2016-09-07T01:08:52.427311: step 5140, loss 0.0840537, acc 0.96
2016-09-07T01:08:53.102596: step 5141, loss 0.0863948, acc 0.96
2016-09-07T01:08:53.796058: step 5142, loss 0.00641577, acc 1
2016-09-07T01:08:54.485858: step 5143, loss 0.0153905, acc 1
2016-09-07T01:08:55.157978: step 5144, loss 0.0298497, acc 0.98
2016-09-07T01:08:55.819175: step 5145, loss 0.0201587, acc 1
2016-09-07T01:08:56.507153: step 5146, loss 0.136311, acc 0.94
2016-09-07T01:08:57.222097: step 5147, loss 0.0064536, acc 1
2016-09-07T01:08:57.948759: step 5148, loss 0.00039131, acc 1
2016-09-07T01:08:58.638100: step 5149, loss 0.00572203, acc 1
2016-09-07T01:08:59.327040: step 5150, loss 0.0287475, acc 1
2016-09-07T01:09:00.024649: step 5151, loss 0.007866, acc 1
2016-09-07T01:09:00.782339: step 5152, loss 0.0458815, acc 0.98
2016-09-07T01:09:01.463985: step 5153, loss 0.00460518, acc 1
2016-09-07T01:09:02.158670: step 5154, loss 0.0288167, acc 0.98
2016-09-07T01:09:02.863395: step 5155, loss 0.0137598, acc 1
2016-09-07T01:09:03.553332: step 5156, loss 0.0138598, acc 1
2016-09-07T01:09:04.246895: step 5157, loss 0.0789201, acc 0.98
2016-09-07T01:09:04.945560: step 5158, loss 0.045319, acc 1
2016-09-07T01:09:05.645762: step 5159, loss 0.00171639, acc 1
2016-09-07T01:09:06.306836: step 5160, loss 0.0277706, acc 0.98
2016-09-07T01:09:06.986364: step 5161, loss 0.150328, acc 0.94
2016-09-07T01:09:07.680634: step 5162, loss 0.00168582, acc 1
2016-09-07T01:09:08.377558: step 5163, loss 0.00957934, acc 1
2016-09-07T01:09:09.062540: step 5164, loss 0.0161857, acc 1
2016-09-07T01:09:09.742404: step 5165, loss 0.0228461, acc 0.98
2016-09-07T01:09:10.444073: step 5166, loss 0.0563559, acc 0.98
2016-09-07T01:09:11.118791: step 5167, loss 0.0958528, acc 0.96
2016-09-07T01:09:11.821197: step 5168, loss 0.0327948, acc 0.98
2016-09-07T01:09:12.502332: step 5169, loss 0.0764545, acc 0.96
2016-09-07T01:09:13.198647: step 5170, loss 0.0351797, acc 0.98
2016-09-07T01:09:13.875283: step 5171, loss 0.00531933, acc 1
2016-09-07T01:09:14.541054: step 5172, loss 0.00379913, acc 1
2016-09-07T01:09:15.261779: step 5173, loss 0.0111746, acc 1
2016-09-07T01:09:15.940221: step 5174, loss 0.0122312, acc 1
2016-09-07T01:09:16.612745: step 5175, loss 0.000661718, acc 1
2016-09-07T01:09:17.306797: step 5176, loss 0.07138, acc 0.98
2016-09-07T01:09:17.995857: step 5177, loss 0.0246136, acc 0.98
2016-09-07T01:09:18.689904: step 5178, loss 0.00564459, acc 1
2016-09-07T01:09:19.382325: step 5179, loss 0.0370848, acc 0.98
2016-09-07T01:09:20.105864: step 5180, loss 0.000935147, acc 1
2016-09-07T01:09:20.799553: step 5181, loss 0.0118036, acc 1
2016-09-07T01:09:21.484286: step 5182, loss 0.0157114, acc 1
2016-09-07T01:09:22.172909: step 5183, loss 0.00252491, acc 1
2016-09-07T01:09:22.809532: step 5184, loss 0.00266968, acc 1
2016-09-07T01:09:23.506962: step 5185, loss 0.0410018, acc 0.98
2016-09-07T01:09:24.181266: step 5186, loss 0.00968078, acc 1
2016-09-07T01:09:24.888622: step 5187, loss 0.0179866, acc 1
2016-09-07T01:09:25.567208: step 5188, loss 0.0131571, acc 1
2016-09-07T01:09:26.258534: step 5189, loss 0.0477268, acc 0.96
2016-09-07T01:09:26.935601: step 5190, loss 0.0479607, acc 0.98
2016-09-07T01:09:27.637495: step 5191, loss 0.0223284, acc 0.98
2016-09-07T01:09:28.363102: step 5192, loss 0.0751254, acc 0.94
2016-09-07T01:09:29.024438: step 5193, loss 0.0183313, acc 0.98
2016-09-07T01:09:29.732597: step 5194, loss 0.000369253, acc 1
2016-09-07T01:09:30.419682: step 5195, loss 0.0941311, acc 0.98
2016-09-07T01:09:31.110197: step 5196, loss 0.0144669, acc 1
2016-09-07T01:09:31.797773: step 5197, loss 0.0271186, acc 0.98
2016-09-07T01:09:32.478845: step 5198, loss 0.0576834, acc 0.98
2016-09-07T01:09:33.161287: step 5199, loss 0.0111989, acc 1
2016-09-07T01:09:33.817361: step 5200, loss 0.019052, acc 1

Evaluation:
2016-09-07T01:09:37.003818: step 5200, loss 1.70596, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-5200

2016-09-07T01:09:38.633220: step 5201, loss 0.0298606, acc 1
2016-09-07T01:09:39.321369: step 5202, loss 0.0473657, acc 0.96
2016-09-07T01:09:39.986112: step 5203, loss 0.0107593, acc 1
2016-09-07T01:09:40.687989: step 5204, loss 0.0306595, acc 0.98
2016-09-07T01:09:41.375572: step 5205, loss 0.0286281, acc 0.98
2016-09-07T01:09:42.054152: step 5206, loss 0.0196417, acc 1
2016-09-07T01:09:42.729641: step 5207, loss 0.0590715, acc 0.98
2016-09-07T01:09:43.409188: step 5208, loss 0.00735548, acc 1
2016-09-07T01:09:44.107958: step 5209, loss 0.0160982, acc 0.98
2016-09-07T01:09:44.778963: step 5210, loss 0.00352105, acc 1
2016-09-07T01:09:45.490275: step 5211, loss 0.00583113, acc 1
2016-09-07T01:09:46.176899: step 5212, loss 0.0585373, acc 0.98
2016-09-07T01:09:46.879578: step 5213, loss 0.00488766, acc 1
2016-09-07T01:09:47.556158: step 5214, loss 0.0200021, acc 0.98
2016-09-07T01:09:48.243361: step 5215, loss 0.0475969, acc 0.96
2016-09-07T01:09:48.951580: step 5216, loss 0.102467, acc 0.92
2016-09-07T01:09:49.625254: step 5217, loss 0.00343733, acc 1
2016-09-07T01:09:50.312021: step 5218, loss 0.0302255, acc 0.98
2016-09-07T01:09:51.024935: step 5219, loss 0.0300244, acc 0.98
2016-09-07T01:09:51.724170: step 5220, loss 0.0148554, acc 1
2016-09-07T01:09:52.423898: step 5221, loss 0.00935647, acc 1
2016-09-07T01:09:53.082046: step 5222, loss 0.0150755, acc 0.98
2016-09-07T01:09:53.776347: step 5223, loss 0.06469, acc 0.98
2016-09-07T01:09:54.458900: step 5224, loss 0.000688672, acc 1
2016-09-07T01:09:55.145318: step 5225, loss 0.0216795, acc 0.98
2016-09-07T01:09:55.845979: step 5226, loss 0.106555, acc 0.96
2016-09-07T01:09:56.537754: step 5227, loss 0.00592816, acc 1
2016-09-07T01:09:57.214877: step 5228, loss 0.021461, acc 0.98
2016-09-07T01:09:57.889704: step 5229, loss 0.0426729, acc 0.96
2016-09-07T01:09:58.588427: step 5230, loss 0.00937834, acc 1
2016-09-07T01:09:59.266856: step 5231, loss 0.00190031, acc 1
2016-09-07T01:09:59.946592: step 5232, loss 0.10756, acc 0.98
2016-09-07T01:10:00.654632: step 5233, loss 0.00866997, acc 1
2016-09-07T01:10:01.338581: step 5234, loss 0.0225474, acc 1
2016-09-07T01:10:02.040877: step 5235, loss 0.0317912, acc 0.98
2016-09-07T01:10:02.709363: step 5236, loss 0.0408479, acc 0.96
2016-09-07T01:10:03.395729: step 5237, loss 0.0150581, acc 1
2016-09-07T01:10:04.099656: step 5238, loss 0.00210031, acc 1
2016-09-07T01:10:04.789024: step 5239, loss 0.000988574, acc 1
2016-09-07T01:10:05.468720: step 5240, loss 0.0283728, acc 1
2016-09-07T01:10:06.163823: step 5241, loss 0.0204967, acc 1
2016-09-07T01:10:06.858358: step 5242, loss 0.000546847, acc 1
2016-09-07T01:10:07.534249: step 5243, loss 0.007251, acc 1
2016-09-07T01:10:08.220501: step 5244, loss 0.0518189, acc 0.98
2016-09-07T01:10:08.902131: step 5245, loss 0.0100767, acc 1
2016-09-07T01:10:09.586481: step 5246, loss 0.00649377, acc 1
2016-09-07T01:10:10.276348: step 5247, loss 0.042314, acc 0.96
2016-09-07T01:10:10.967120: step 5248, loss 0.0307843, acc 0.98
2016-09-07T01:10:11.647521: step 5249, loss 0.101205, acc 0.96
2016-09-07T01:10:12.303782: step 5250, loss 0.028642, acc 1
2016-09-07T01:10:13.038413: step 5251, loss 0.00176996, acc 1
2016-09-07T01:10:13.715153: step 5252, loss 0.00218084, acc 1
2016-09-07T01:10:14.402998: step 5253, loss 0.0431257, acc 0.98
2016-09-07T01:10:15.090710: step 5254, loss 0.0821965, acc 0.96
2016-09-07T01:10:15.782580: step 5255, loss 0.0135861, acc 1
2016-09-07T01:10:16.461876: step 5256, loss 0.0216224, acc 0.98
2016-09-07T01:10:17.128281: step 5257, loss 0.000728134, acc 1
2016-09-07T01:10:17.853605: step 5258, loss 0.0248217, acc 1
2016-09-07T01:10:18.531198: step 5259, loss 0.0194344, acc 0.98
2016-09-07T01:10:19.222206: step 5260, loss 0.0631969, acc 0.96
2016-09-07T01:10:19.902238: step 5261, loss 0.0038538, acc 1
2016-09-07T01:10:20.586471: step 5262, loss 0.0139629, acc 1
2016-09-07T01:10:21.279669: step 5263, loss 0.0198352, acc 0.98
2016-09-07T01:10:21.950737: step 5264, loss 0.0254698, acc 0.98
2016-09-07T01:10:22.645847: step 5265, loss 0.0189833, acc 0.98
2016-09-07T01:10:23.324534: step 5266, loss 0.0651728, acc 0.96
2016-09-07T01:10:23.994044: step 5267, loss 0.0124137, acc 1
2016-09-07T01:10:24.671893: step 5268, loss 0.00394766, acc 1
2016-09-07T01:10:25.341993: step 5269, loss 0.0131938, acc 1
2016-09-07T01:10:26.029569: step 5270, loss 0.0130106, acc 1
2016-09-07T01:10:26.716881: step 5271, loss 0.026593, acc 0.98
2016-09-07T01:10:27.405231: step 5272, loss 0.0118741, acc 1
2016-09-07T01:10:28.075153: step 5273, loss 0.0275521, acc 0.98
2016-09-07T01:10:28.759453: step 5274, loss 0.0347429, acc 0.98
2016-09-07T01:10:29.457767: step 5275, loss 0.039687, acc 0.98
2016-09-07T01:10:30.188263: step 5276, loss 0.0245426, acc 1
2016-09-07T01:10:30.876037: step 5277, loss 0.00963492, acc 1
2016-09-07T01:10:31.536341: step 5278, loss 0.0710217, acc 0.96
2016-09-07T01:10:32.243970: step 5279, loss 0.0201352, acc 1
2016-09-07T01:10:32.928492: step 5280, loss 0.0019618, acc 1
2016-09-07T01:10:33.611967: step 5281, loss 0.0560084, acc 0.96
2016-09-07T01:10:34.303349: step 5282, loss 0.0972862, acc 0.94
2016-09-07T01:10:34.987943: step 5283, loss 0.0191099, acc 1
2016-09-07T01:10:35.688558: step 5284, loss 0.0144184, acc 1
2016-09-07T01:10:36.354133: step 5285, loss 0.00393462, acc 1
2016-09-07T01:10:37.045997: step 5286, loss 0.132237, acc 0.98
2016-09-07T01:10:37.711911: step 5287, loss 0.026804, acc 0.98
2016-09-07T01:10:38.417094: step 5288, loss 0.0162365, acc 1
2016-09-07T01:10:39.091772: step 5289, loss 0.0332379, acc 0.98
2016-09-07T01:10:39.796365: step 5290, loss 0.0243528, acc 0.98
2016-09-07T01:10:40.489715: step 5291, loss 0.00587939, acc 1
2016-09-07T01:10:41.146876: step 5292, loss 0.0653191, acc 0.98
2016-09-07T01:10:41.863846: step 5293, loss 0.0678723, acc 0.98
2016-09-07T01:10:42.550596: step 5294, loss 0.0126128, acc 1
2016-09-07T01:10:43.235390: step 5295, loss 9.28394e-05, acc 1
2016-09-07T01:10:43.912738: step 5296, loss 0.00416788, acc 1
2016-09-07T01:10:44.601932: step 5297, loss 0.0271419, acc 0.98
2016-09-07T01:10:45.274071: step 5298, loss 0.033266, acc 0.98
2016-09-07T01:10:45.962478: step 5299, loss 0.0753387, acc 0.96
2016-09-07T01:10:46.685534: step 5300, loss 0.000465986, acc 1

Evaluation:
2016-09-07T01:10:49.875500: step 5300, loss 1.84737, acc 0.729831

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-5300

2016-09-07T01:10:51.600780: step 5301, loss 0.000596392, acc 1
2016-09-07T01:10:52.297145: step 5302, loss 0.000314476, acc 1
2016-09-07T01:10:52.976401: step 5303, loss 0.00157768, acc 1
2016-09-07T01:10:53.665093: step 5304, loss 0.0382624, acc 0.98
2016-09-07T01:10:54.365327: step 5305, loss 0.0458114, acc 0.96
2016-09-07T01:10:55.055399: step 5306, loss 0.0140857, acc 1
2016-09-07T01:10:55.728343: step 5307, loss 0.044741, acc 0.98
2016-09-07T01:10:56.396830: step 5308, loss 0.0242075, acc 1
2016-09-07T01:10:57.064170: step 5309, loss 0.00235434, acc 1
2016-09-07T01:10:57.754447: step 5310, loss 0.0858027, acc 0.92
2016-09-07T01:10:58.440592: step 5311, loss 0.00689222, acc 1
2016-09-07T01:10:59.095865: step 5312, loss 0.0329642, acc 0.98
2016-09-07T01:10:59.815003: step 5313, loss 0.00515364, acc 1
2016-09-07T01:11:00.535428: step 5314, loss 0.0255108, acc 1
2016-09-07T01:11:01.198935: step 5315, loss 0.0539315, acc 1
2016-09-07T01:11:01.893930: step 5316, loss 0.215913, acc 0.96
2016-09-07T01:11:02.563539: step 5317, loss 0.0720613, acc 0.98
2016-09-07T01:11:03.239312: step 5318, loss 0.023436, acc 0.98
2016-09-07T01:11:03.929189: step 5319, loss 0.013987, acc 1
2016-09-07T01:11:04.621926: step 5320, loss 0.00794914, acc 1
2016-09-07T01:11:05.297958: step 5321, loss 0.0291367, acc 0.98
2016-09-07T01:11:06.018878: step 5322, loss 0.0131496, acc 1
2016-09-07T01:11:06.762306: step 5323, loss 0.0142714, acc 1
2016-09-07T01:11:07.467367: step 5324, loss 0.0109229, acc 1
2016-09-07T01:11:08.147731: step 5325, loss 0.0250304, acc 1
2016-09-07T01:11:08.817988: step 5326, loss 0.0121964, acc 1
2016-09-07T01:11:09.529016: step 5327, loss 0.00921403, acc 1
2016-09-07T01:11:10.223560: step 5328, loss 0.126185, acc 0.98
2016-09-07T01:11:10.897135: step 5329, loss 0.0501286, acc 0.98
2016-09-07T01:11:11.585232: step 5330, loss 0.000985308, acc 1
2016-09-07T01:11:12.284107: step 5331, loss 0.00190069, acc 1
2016-09-07T01:11:12.981608: step 5332, loss 0.137987, acc 0.96
2016-09-07T01:11:13.644101: step 5333, loss 0.106578, acc 0.94
2016-09-07T01:11:14.345731: step 5334, loss 0.0130647, acc 1
2016-09-07T01:11:15.029984: step 5335, loss 0.010833, acc 1
2016-09-07T01:11:15.730329: step 5336, loss 0.0239948, acc 0.98
2016-09-07T01:11:16.389998: step 5337, loss 0.0374687, acc 0.96
2016-09-07T01:11:17.077444: step 5338, loss 0.0202018, acc 1
2016-09-07T01:11:17.815325: step 5339, loss 0.0207127, acc 1
2016-09-07T01:11:18.490844: step 5340, loss 0.00981092, acc 1
2016-09-07T01:11:19.210854: step 5341, loss 0.00151491, acc 1
2016-09-07T01:11:19.916223: step 5342, loss 0.0240445, acc 0.98
2016-09-07T01:11:20.590966: step 5343, loss 0.0470402, acc 0.98
2016-09-07T01:11:21.297261: step 5344, loss 0.0411989, acc 0.98
2016-09-07T01:11:21.984163: step 5345, loss 0.00442846, acc 1
2016-09-07T01:11:22.680317: step 5346, loss 0.0123659, acc 1
2016-09-07T01:11:23.352892: step 5347, loss 0.00583356, acc 1
2016-09-07T01:11:24.030614: step 5348, loss 0.0106784, acc 1
2016-09-07T01:11:24.694852: step 5349, loss 0.0101204, acc 1
2016-09-07T01:11:25.376729: step 5350, loss 0.0443062, acc 0.96
2016-09-07T01:11:26.045198: step 5351, loss 0.00025286, acc 1
2016-09-07T01:11:26.737690: step 5352, loss 0.0274997, acc 0.98
2016-09-07T01:11:27.459071: step 5353, loss 0.0175657, acc 1
2016-09-07T01:11:28.143969: step 5354, loss 0.0143424, acc 1
2016-09-07T01:11:28.853578: step 5355, loss 0.000589285, acc 1
2016-09-07T01:11:29.536548: step 5356, loss 0.0356767, acc 0.98
2016-09-07T01:11:30.217929: step 5357, loss 0.042258, acc 0.96
2016-09-07T01:11:30.902925: step 5358, loss 0.0508556, acc 0.98
2016-09-07T01:11:31.595930: step 5359, loss 0.0747005, acc 0.98
2016-09-07T01:11:32.300259: step 5360, loss 0.0252562, acc 1
2016-09-07T01:11:32.969817: step 5361, loss 0.164052, acc 0.94
2016-09-07T01:11:33.654945: step 5362, loss 0.0318132, acc 0.98
2016-09-07T01:11:34.339051: step 5363, loss 0.0141056, acc 1
2016-09-07T01:11:35.028535: step 5364, loss 0.00825864, acc 1
2016-09-07T01:11:35.729719: step 5365, loss 0.0028303, acc 1
2016-09-07T01:11:36.394935: step 5366, loss 0.00433665, acc 1
2016-09-07T01:11:37.094227: step 5367, loss 0.000371394, acc 1
2016-09-07T01:11:37.791822: step 5368, loss 0.0468345, acc 1
2016-09-07T01:11:38.478161: step 5369, loss 0.00502715, acc 1
2016-09-07T01:11:39.169778: step 5370, loss 0.0316622, acc 0.98
2016-09-07T01:11:39.863648: step 5371, loss 0.0428191, acc 0.98
2016-09-07T01:11:40.564989: step 5372, loss 0.000674986, acc 1
2016-09-07T01:11:41.256066: step 5373, loss 0.0455729, acc 0.96
2016-09-07T01:11:41.982177: step 5374, loss 0.0140104, acc 0.98
2016-09-07T01:11:42.657689: step 5375, loss 0.15668, acc 0.96
2016-09-07T01:11:43.302420: step 5376, loss 0.00418711, acc 1
2016-09-07T01:11:44.002732: step 5377, loss 0.0339573, acc 0.98
2016-09-07T01:11:44.693401: step 5378, loss 0.0461025, acc 0.98
2016-09-07T01:11:45.411340: step 5379, loss 0.119838, acc 0.96
2016-09-07T01:11:46.068817: step 5380, loss 0.0291106, acc 0.98
2016-09-07T01:11:46.765731: step 5381, loss 0.000747066, acc 1
2016-09-07T01:11:47.444991: step 5382, loss 0.0742734, acc 0.98
2016-09-07T01:11:48.136987: step 5383, loss 0.00845576, acc 1
2016-09-07T01:11:48.842235: step 5384, loss 0.10411, acc 0.98
2016-09-07T01:11:49.549028: step 5385, loss 0.00102044, acc 1
2016-09-07T01:11:50.258035: step 5386, loss 0.0105967, acc 1
2016-09-07T01:11:50.917410: step 5387, loss 0.0517748, acc 0.98
2016-09-07T01:11:51.635397: step 5388, loss 0.00773899, acc 1
2016-09-07T01:11:52.321539: step 5389, loss 0.0194975, acc 0.98
2016-09-07T01:11:53.010837: step 5390, loss 0.0159705, acc 1
2016-09-07T01:11:53.701862: step 5391, loss 0.00671228, acc 1
2016-09-07T01:11:54.398478: step 5392, loss 0.00124928, acc 1
2016-09-07T01:11:55.119445: step 5393, loss 0.0202177, acc 0.98
2016-09-07T01:11:55.781649: step 5394, loss 0.00866633, acc 1
2016-09-07T01:11:56.498560: step 5395, loss 0.0435629, acc 0.98
2016-09-07T01:11:57.172333: step 5396, loss 0.0360977, acc 0.98
2016-09-07T01:11:57.852936: step 5397, loss 0.0111223, acc 1
2016-09-07T01:11:58.518460: step 5398, loss 0.0336782, acc 0.98
2016-09-07T01:11:59.211727: step 5399, loss 0.00323442, acc 1
2016-09-07T01:11:59.926854: step 5400, loss 0.000157936, acc 1

Evaluation:
2016-09-07T01:12:03.146205: step 5400, loss 2.07014, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-5400

2016-09-07T01:12:04.839305: step 5401, loss 0.0082648, acc 1
2016-09-07T01:12:05.524901: step 5402, loss 0.028614, acc 0.98
2016-09-07T01:12:06.203957: step 5403, loss 0.0455034, acc 0.98
2016-09-07T01:12:06.896854: step 5404, loss 0.000169551, acc 1
2016-09-07T01:12:07.591674: step 5405, loss 0.141206, acc 0.96
2016-09-07T01:12:08.272786: step 5406, loss 0.00309721, acc 1
2016-09-07T01:12:08.948802: step 5407, loss 0.0244786, acc 1
2016-09-07T01:12:09.666862: step 5408, loss 0.310091, acc 0.9
2016-09-07T01:12:10.355499: step 5409, loss 0.00780389, acc 1
2016-09-07T01:12:11.052622: step 5410, loss 0.00957375, acc 1
2016-09-07T01:12:11.748236: step 5411, loss 0.0169401, acc 0.98
2016-09-07T01:12:12.444938: step 5412, loss 0.0401335, acc 0.98
2016-09-07T01:12:13.140771: step 5413, loss 0.00892784, acc 1
2016-09-07T01:12:13.804138: step 5414, loss 0.0865196, acc 0.96
2016-09-07T01:12:14.499259: step 5415, loss 0.00544015, acc 1
2016-09-07T01:12:15.166062: step 5416, loss 0.00309183, acc 1
2016-09-07T01:12:15.846593: step 5417, loss 0.065135, acc 0.98
2016-09-07T01:12:16.544764: step 5418, loss 0.0179404, acc 1
2016-09-07T01:12:17.243515: step 5419, loss 0.00691931, acc 1
2016-09-07T01:12:17.936912: step 5420, loss 0.0144063, acc 1
2016-09-07T01:12:18.606470: step 5421, loss 0.0193208, acc 1
2016-09-07T01:12:19.303222: step 5422, loss 0.0474235, acc 0.98
2016-09-07T01:12:19.974901: step 5423, loss 0.125876, acc 0.96
2016-09-07T01:12:20.661905: step 5424, loss 0.0323969, acc 0.98
2016-09-07T01:12:21.345752: step 5425, loss 0.00823286, acc 1
2016-09-07T01:12:22.043204: step 5426, loss 0.0263266, acc 0.98
2016-09-07T01:12:22.743396: step 5427, loss 0.00396051, acc 1
2016-09-07T01:12:23.425889: step 5428, loss 0.00186562, acc 1
2016-09-07T01:12:24.118596: step 5429, loss 0.0540844, acc 0.98
2016-09-07T01:12:24.784617: step 5430, loss 0.00125828, acc 1
2016-09-07T01:12:25.480705: step 5431, loss 0.0255772, acc 0.98
2016-09-07T01:12:26.183416: step 5432, loss 0.0258494, acc 0.98
2016-09-07T01:12:26.882557: step 5433, loss 0.0248117, acc 0.98
2016-09-07T01:12:27.593338: step 5434, loss 0.0167612, acc 0.98
2016-09-07T01:12:28.264375: step 5435, loss 0.0648568, acc 0.98
2016-09-07T01:12:28.975678: step 5436, loss 0.0311896, acc 0.98
2016-09-07T01:12:29.671588: step 5437, loss 0.0550015, acc 0.94
2016-09-07T01:12:30.361160: step 5438, loss 0.0159783, acc 1
2016-09-07T01:12:31.053138: step 5439, loss 0.0496479, acc 0.98
2016-09-07T01:12:31.745953: step 5440, loss 0.0365884, acc 0.98
2016-09-07T01:12:32.442493: step 5441, loss 0.0389258, acc 0.96
2016-09-07T01:12:33.115304: step 5442, loss 0.0152001, acc 1
2016-09-07T01:12:33.816627: step 5443, loss 0.00779101, acc 1
2016-09-07T01:12:34.518536: step 5444, loss 0.00627909, acc 1
2016-09-07T01:12:35.199459: step 5445, loss 0.0117977, acc 1
2016-09-07T01:12:35.880862: step 5446, loss 0.0494474, acc 0.98
2016-09-07T01:12:36.560911: step 5447, loss 0.0385383, acc 0.98
2016-09-07T01:12:37.268610: step 5448, loss 0.00457796, acc 1
2016-09-07T01:12:37.952028: step 5449, loss 0.00422353, acc 1
2016-09-07T01:12:38.638873: step 5450, loss 0.0531799, acc 0.98
2016-09-07T01:12:39.322764: step 5451, loss 0.00320081, acc 1
2016-09-07T01:12:40.007212: step 5452, loss 0.02556, acc 1
2016-09-07T01:12:40.691596: step 5453, loss 0.010842, acc 1
2016-09-07T01:12:41.388148: step 5454, loss 0.0111308, acc 1
2016-09-07T01:12:42.098875: step 5455, loss 0.0246073, acc 0.98
2016-09-07T01:12:42.784495: step 5456, loss 0.0276926, acc 1
2016-09-07T01:12:43.468607: step 5457, loss 0.000217636, acc 1
2016-09-07T01:12:44.169952: step 5458, loss 0.00113331, acc 1
2016-09-07T01:12:44.871152: step 5459, loss 0.117701, acc 0.96
2016-09-07T01:12:45.571348: step 5460, loss 0.0101861, acc 1
2016-09-07T01:12:46.253060: step 5461, loss 0.0261309, acc 0.98
2016-09-07T01:12:46.959981: step 5462, loss 0.0467846, acc 0.98
2016-09-07T01:12:47.633211: step 5463, loss 0.0314931, acc 0.98
2016-09-07T01:12:48.314229: step 5464, loss 0.0261929, acc 1
2016-09-07T01:12:48.997097: step 5465, loss 0.0741795, acc 0.94
2016-09-07T01:12:49.683791: step 5466, loss 0.0244954, acc 0.98
2016-09-07T01:12:50.402612: step 5467, loss 0.124902, acc 0.98
2016-09-07T01:12:51.075065: step 5468, loss 0.0705574, acc 0.96
2016-09-07T01:12:51.784394: step 5469, loss 0.0245149, acc 0.98
2016-09-07T01:12:52.492184: step 5470, loss 0.0173986, acc 1
2016-09-07T01:12:53.194677: step 5471, loss 0.0469253, acc 0.98
2016-09-07T01:12:53.871384: step 5472, loss 0.0740303, acc 0.94
2016-09-07T01:12:54.536934: step 5473, loss 0.0123729, acc 1
2016-09-07T01:12:55.228279: step 5474, loss 0.0285083, acc 0.98
2016-09-07T01:12:55.879453: step 5475, loss 0.0546162, acc 0.98
2016-09-07T01:12:56.584966: step 5476, loss 0.0281986, acc 0.98
2016-09-07T01:12:57.259522: step 5477, loss 0.0134032, acc 1
2016-09-07T01:12:57.944182: step 5478, loss 0.0494006, acc 0.96
2016-09-07T01:12:58.628416: step 5479, loss 0.0344037, acc 0.98
2016-09-07T01:12:59.329035: step 5480, loss 0.0240134, acc 1
2016-09-07T01:13:00.023131: step 5481, loss 0.0148534, acc 1
2016-09-07T01:13:00.722860: step 5482, loss 0.0746904, acc 0.98
2016-09-07T01:13:01.430022: step 5483, loss 0.00332893, acc 1
2016-09-07T01:13:02.098371: step 5484, loss 0.0464151, acc 0.96
2016-09-07T01:13:02.786670: step 5485, loss 0.113786, acc 0.96
2016-09-07T01:13:03.466892: step 5486, loss 0.0190543, acc 1
2016-09-07T01:13:04.163491: step 5487, loss 0.0502119, acc 0.98
2016-09-07T01:13:04.867284: step 5488, loss 0.013393, acc 1
2016-09-07T01:13:05.525729: step 5489, loss 0.0148182, acc 1
2016-09-07T01:13:06.224274: step 5490, loss 0.0283925, acc 0.98
2016-09-07T01:13:06.896735: step 5491, loss 0.045714, acc 0.96
2016-09-07T01:13:07.596797: step 5492, loss 0.0858343, acc 0.98
2016-09-07T01:13:08.285480: step 5493, loss 0.0258437, acc 0.98
2016-09-07T01:13:08.961631: step 5494, loss 0.0426788, acc 0.98
2016-09-07T01:13:09.640340: step 5495, loss 0.0120511, acc 1
2016-09-07T01:13:10.317444: step 5496, loss 0.106166, acc 0.98
2016-09-07T01:13:11.020938: step 5497, loss 0.0531329, acc 0.96
2016-09-07T01:13:11.705128: step 5498, loss 0.0141407, acc 1
2016-09-07T01:13:12.386870: step 5499, loss 0.0354814, acc 0.98
2016-09-07T01:13:13.060221: step 5500, loss 0.0357696, acc 0.98

Evaluation:
2016-09-07T01:13:16.231550: step 5500, loss 1.61248, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-5500

2016-09-07T01:13:17.942113: step 5501, loss 0.00325394, acc 1
2016-09-07T01:13:18.612140: step 5502, loss 0.0181835, acc 1
2016-09-07T01:13:19.313641: step 5503, loss 0.0154152, acc 1
2016-09-07T01:13:19.974000: step 5504, loss 0.0220811, acc 0.98
2016-09-07T01:13:20.692897: step 5505, loss 0.0275522, acc 0.98
2016-09-07T01:13:21.368328: step 5506, loss 0.0234337, acc 1
2016-09-07T01:13:22.043212: step 5507, loss 0.21533, acc 0.9
2016-09-07T01:13:22.729914: step 5508, loss 0.0537491, acc 0.98
2016-09-07T01:13:23.409561: step 5509, loss 0.0178699, acc 1
2016-09-07T01:13:24.109635: step 5510, loss 0.0217849, acc 0.98
2016-09-07T01:13:24.795407: step 5511, loss 0.00173456, acc 1
2016-09-07T01:13:25.507276: step 5512, loss 0.0746271, acc 0.98
2016-09-07T01:13:26.186509: step 5513, loss 0.0229026, acc 0.98
2016-09-07T01:13:26.863207: step 5514, loss 0.00487122, acc 1
2016-09-07T01:13:27.550879: step 5515, loss 0.0261089, acc 0.98
2016-09-07T01:13:28.239525: step 5516, loss 0.00861488, acc 1
2016-09-07T01:13:28.938287: step 5517, loss 0.0170501, acc 1
2016-09-07T01:13:29.628007: step 5518, loss 0.00286467, acc 1
2016-09-07T01:13:30.319615: step 5519, loss 0.0039926, acc 1
2016-09-07T01:13:31.015873: step 5520, loss 0.0068768, acc 1
2016-09-07T01:13:31.698374: step 5521, loss 0.0472444, acc 1
2016-09-07T01:13:32.392141: step 5522, loss 0.00478027, acc 1
2016-09-07T01:13:33.086293: step 5523, loss 0.00940054, acc 1
2016-09-07T01:13:33.802322: step 5524, loss 0.0278826, acc 0.98
2016-09-07T01:13:34.476879: step 5525, loss 0.0433984, acc 0.98
2016-09-07T01:13:35.200633: step 5526, loss 0.0121181, acc 1
2016-09-07T01:13:35.910031: step 5527, loss 0.0386602, acc 0.98
2016-09-07T01:13:36.602378: step 5528, loss 0.0271709, acc 0.98
2016-09-07T01:13:37.315229: step 5529, loss 0.0227252, acc 1
2016-09-07T01:13:37.980497: step 5530, loss 0.0507776, acc 0.98
2016-09-07T01:13:38.686441: step 5531, loss 0.00808411, acc 1
2016-09-07T01:13:39.362773: step 5532, loss 0.000369734, acc 1
2016-09-07T01:13:40.054606: step 5533, loss 0.0118625, acc 1
2016-09-07T01:13:40.741323: step 5534, loss 0.0442968, acc 0.96
2016-09-07T01:13:41.432448: step 5535, loss 0.000170772, acc 1
2016-09-07T01:13:42.114952: step 5536, loss 0.00210549, acc 1
2016-09-07T01:13:42.781842: step 5537, loss 4.222e-05, acc 1
2016-09-07T01:13:43.490629: step 5538, loss 0.0174787, acc 0.98
2016-09-07T01:13:44.165602: step 5539, loss 0.00605574, acc 1
2016-09-07T01:13:44.865193: step 5540, loss 0.00289099, acc 1
2016-09-07T01:13:45.547481: step 5541, loss 0.00109041, acc 1
2016-09-07T01:13:46.250130: step 5542, loss 0.0281401, acc 1
2016-09-07T01:13:46.954867: step 5543, loss 0.00353704, acc 1
2016-09-07T01:13:47.618848: step 5544, loss 0.0264424, acc 1
2016-09-07T01:13:48.321791: step 5545, loss 0.0335002, acc 1
2016-09-07T01:13:49.027393: step 5546, loss 0.0127329, acc 1
2016-09-07T01:13:49.711635: step 5547, loss 0.00675112, acc 1
2016-09-07T01:13:50.401730: step 5548, loss 0.0442433, acc 0.96
2016-09-07T01:13:51.086962: step 5549, loss 0.0188321, acc 0.98
2016-09-07T01:13:51.768519: step 5550, loss 0.0186541, acc 0.98
2016-09-07T01:13:52.433461: step 5551, loss 0.000283397, acc 1
2016-09-07T01:13:53.149551: step 5552, loss 0.133193, acc 0.94
2016-09-07T01:13:53.842785: step 5553, loss 0.00911508, acc 1
2016-09-07T01:13:54.540514: step 5554, loss 0.0867545, acc 0.94
2016-09-07T01:13:55.222833: step 5555, loss 0.000568208, acc 1
2016-09-07T01:13:55.902430: step 5556, loss 0.00442538, acc 1
2016-09-07T01:13:56.602049: step 5557, loss 0.0378186, acc 0.98
2016-09-07T01:13:57.273026: step 5558, loss 0.00734571, acc 1
2016-09-07T01:13:57.972258: step 5559, loss 0.000114888, acc 1
2016-09-07T01:13:58.669819: step 5560, loss 0.11501, acc 0.94
2016-09-07T01:13:59.358637: step 5561, loss 0.0400473, acc 0.98
2016-09-07T01:14:00.061572: step 5562, loss 0.00209105, acc 1
2016-09-07T01:14:00.786701: step 5563, loss 0.0285656, acc 0.98
2016-09-07T01:14:01.492655: step 5564, loss 0.000549618, acc 1
2016-09-07T01:14:02.172687: step 5565, loss 0.0125017, acc 1
2016-09-07T01:14:02.856008: step 5566, loss 0.0368158, acc 0.98
2016-09-07T01:14:03.522601: step 5567, loss 0.0215563, acc 0.98
2016-09-07T01:14:04.171789: step 5568, loss 0.00270375, acc 1
2016-09-07T01:14:04.851144: step 5569, loss 0.0172258, acc 0.98
2016-09-07T01:14:05.533575: step 5570, loss 0.00126689, acc 1
2016-09-07T01:14:06.243331: step 5571, loss 0.011018, acc 1
2016-09-07T01:14:06.916867: step 5572, loss 0.00236059, acc 1
2016-09-07T01:14:07.617608: step 5573, loss 0.00937431, acc 1
2016-09-07T01:14:08.307950: step 5574, loss 0.0230808, acc 0.98
2016-09-07T01:14:08.987043: step 5575, loss 0.0455027, acc 0.98
2016-09-07T01:14:09.679530: step 5576, loss 0.0363533, acc 0.98
2016-09-07T01:14:10.352826: step 5577, loss 0.0309351, acc 0.98
2016-09-07T01:14:11.061855: step 5578, loss 0.0378671, acc 0.98
2016-09-07T01:14:11.746486: step 5579, loss 0.0066901, acc 1
2016-09-07T01:14:12.421651: step 5580, loss 0.0227965, acc 0.98
2016-09-07T01:14:13.112022: step 5581, loss 0.0351959, acc 0.98
2016-09-07T01:14:13.782017: step 5582, loss 0.0223335, acc 0.98
2016-09-07T01:14:14.489237: step 5583, loss 0.0545887, acc 0.98
2016-09-07T01:14:15.190927: step 5584, loss 0.000518499, acc 1
2016-09-07T01:14:15.889260: step 5585, loss 0.0104829, acc 1
2016-09-07T01:14:16.569867: step 5586, loss 0.0265715, acc 0.98
2016-09-07T01:14:17.249639: step 5587, loss 0.0379336, acc 0.98
2016-09-07T01:14:17.950334: step 5588, loss 0.000229657, acc 1
2016-09-07T01:14:18.639192: step 5589, loss 0.014622, acc 1
2016-09-07T01:14:19.319438: step 5590, loss 0.0188965, acc 0.98
2016-09-07T01:14:20.037969: step 5591, loss 0.00516999, acc 1
2016-09-07T01:14:20.736076: step 5592, loss 0.0217791, acc 1
2016-09-07T01:14:21.390173: step 5593, loss 0.0467873, acc 0.96
2016-09-07T01:14:22.069249: step 5594, loss 0.000510766, acc 1
2016-09-07T01:14:22.777486: step 5595, loss 0.0180426, acc 1
2016-09-07T01:14:23.461947: step 5596, loss 0.000879197, acc 1
2016-09-07T01:14:24.145724: step 5597, loss 0.00257175, acc 1
2016-09-07T01:14:24.828785: step 5598, loss 0.0148827, acc 1
2016-09-07T01:14:25.551801: step 5599, loss 0.014247, acc 1
2016-09-07T01:14:26.236581: step 5600, loss 0.0473818, acc 0.98

Evaluation:
2016-09-07T01:14:29.417634: step 5600, loss 2.17999, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-5600

2016-09-07T01:14:31.155148: step 5601, loss 0.00167417, acc 1
2016-09-07T01:14:31.855686: step 5602, loss 0.00534689, acc 1
2016-09-07T01:14:32.552963: step 5603, loss 0.00612225, acc 1
2016-09-07T01:14:33.249146: step 5604, loss 0.00759305, acc 1
2016-09-07T01:14:33.959920: step 5605, loss 0.000459273, acc 1
2016-09-07T01:14:34.688581: step 5606, loss 0.0319889, acc 1
2016-09-07T01:14:35.387902: step 5607, loss 0.0101957, acc 1
2016-09-07T01:14:36.070020: step 5608, loss 0.0103206, acc 1
2016-09-07T01:14:36.742532: step 5609, loss 0.0109363, acc 1
2016-09-07T01:14:37.438011: step 5610, loss 0.0491529, acc 0.96
2016-09-07T01:14:38.112310: step 5611, loss 0.0191748, acc 0.98
2016-09-07T01:14:38.828859: step 5612, loss 0.0261989, acc 0.98
2016-09-07T01:14:39.492454: step 5613, loss 0.0199784, acc 1
2016-09-07T01:14:40.163277: step 5614, loss 0.0757416, acc 0.94
2016-09-07T01:14:40.850685: step 5615, loss 0.0353377, acc 0.96
2016-09-07T01:14:41.539096: step 5616, loss 0.00913085, acc 1
2016-09-07T01:14:42.246434: step 5617, loss 0.0332303, acc 0.98
2016-09-07T01:14:42.919137: step 5618, loss 0.00211999, acc 1
2016-09-07T01:14:43.619933: step 5619, loss 0.0159028, acc 0.98
2016-09-07T01:14:44.307707: step 5620, loss 0.0063905, acc 1
2016-09-07T01:14:45.016025: step 5621, loss 0.0109834, acc 1
2016-09-07T01:14:45.696116: step 5622, loss 0.0142113, acc 1
2016-09-07T01:14:46.386583: step 5623, loss 0.00809646, acc 1
2016-09-07T01:14:47.083187: step 5624, loss 0.00803037, acc 1
2016-09-07T01:14:47.749891: step 5625, loss 0.0137621, acc 1
2016-09-07T01:14:48.447888: step 5626, loss 9.77673e-05, acc 1
2016-09-07T01:14:49.130397: step 5627, loss 0.158565, acc 0.98
2016-09-07T01:14:49.800846: step 5628, loss 0.0100903, acc 1
2016-09-07T01:14:50.482384: step 5629, loss 0.0179814, acc 1
2016-09-07T01:14:51.172694: step 5630, loss 0.00256121, acc 1
2016-09-07T01:14:51.864466: step 5631, loss 0.0273421, acc 1
2016-09-07T01:14:52.541331: step 5632, loss 0.0174296, acc 1
2016-09-07T01:14:53.240175: step 5633, loss 0.000107508, acc 1
2016-09-07T01:14:53.906671: step 5634, loss 0.0344839, acc 0.98
2016-09-07T01:14:54.594693: step 5635, loss 0.000634038, acc 1
2016-09-07T01:14:55.281218: step 5636, loss 0.00443645, acc 1
2016-09-07T01:14:55.964908: step 5637, loss 7.16728e-05, acc 1
2016-09-07T01:14:56.675549: step 5638, loss 0.011989, acc 1
2016-09-07T01:14:57.334975: step 5639, loss 0.00152683, acc 1
2016-09-07T01:14:58.026421: step 5640, loss 0.0325738, acc 0.98
2016-09-07T01:14:58.718613: step 5641, loss 0.00157901, acc 1
2016-09-07T01:14:59.388962: step 5642, loss 0.000201193, acc 1
2016-09-07T01:15:00.106294: step 5643, loss 0.0103641, acc 1
2016-09-07T01:15:00.833836: step 5644, loss 0.0156241, acc 1
2016-09-07T01:15:01.538655: step 5645, loss 0.00326044, acc 1
2016-09-07T01:15:02.190983: step 5646, loss 0.0124115, acc 1
2016-09-07T01:15:02.913662: step 5647, loss 0.0364877, acc 0.98
2016-09-07T01:15:03.606191: step 5648, loss 0.0175685, acc 0.98
2016-09-07T01:15:04.322473: step 5649, loss 0.00360167, acc 1
2016-09-07T01:15:05.012704: step 5650, loss 0.0151779, acc 0.98
2016-09-07T01:15:05.693100: step 5651, loss 0.117932, acc 0.98
2016-09-07T01:15:06.399319: step 5652, loss 0.000893283, acc 1
2016-09-07T01:15:07.061121: step 5653, loss 0.02019, acc 0.98
2016-09-07T01:15:07.756112: step 5654, loss 0.023993, acc 0.98
2016-09-07T01:15:08.460127: step 5655, loss 0.0755566, acc 0.96
2016-09-07T01:15:09.135710: step 5656, loss 0.00988173, acc 1
2016-09-07T01:15:09.864210: step 5657, loss 0.000973149, acc 1
2016-09-07T01:15:10.559263: step 5658, loss 0.0273974, acc 0.98
2016-09-07T01:15:11.258756: step 5659, loss 0.0173239, acc 1
2016-09-07T01:15:11.927071: step 5660, loss 0.0333164, acc 0.98
2016-09-07T01:15:12.612030: step 5661, loss 0.0120172, acc 1
2016-09-07T01:15:13.289603: step 5662, loss 0.0379062, acc 0.98
2016-09-07T01:15:13.965856: step 5663, loss 0.0294418, acc 0.98
2016-09-07T01:15:14.657628: step 5664, loss 0.00472159, acc 1
2016-09-07T01:15:15.353298: step 5665, loss 0.0308805, acc 0.98
2016-09-07T01:15:16.044772: step 5666, loss 0.103099, acc 0.94
2016-09-07T01:15:16.732161: step 5667, loss 0.0333025, acc 0.98
2016-09-07T01:15:17.404393: step 5668, loss 0.021767, acc 0.98
2016-09-07T01:15:18.093581: step 5669, loss 0.00134066, acc 1
2016-09-07T01:15:18.770155: step 5670, loss 0.00762514, acc 1
2016-09-07T01:15:19.446026: step 5671, loss 0.0283057, acc 1
2016-09-07T01:15:20.131665: step 5672, loss 0.0147297, acc 1
2016-09-07T01:15:20.844508: step 5673, loss 0.00814566, acc 1
2016-09-07T01:15:21.521210: step 5674, loss 0.0307214, acc 0.98
2016-09-07T01:15:22.205595: step 5675, loss 0.0235729, acc 0.98
2016-09-07T01:15:22.891423: step 5676, loss 0.0132326, acc 1
2016-09-07T01:15:23.582756: step 5677, loss 0.0601173, acc 0.96
2016-09-07T01:15:24.268939: step 5678, loss 0.0249003, acc 1
2016-09-07T01:15:24.976099: step 5679, loss 0.014744, acc 1
2016-09-07T01:15:25.674491: step 5680, loss 0.0171064, acc 0.98
2016-09-07T01:15:26.361815: step 5681, loss 0.000253497, acc 1
2016-09-07T01:15:27.038721: step 5682, loss 0.00192938, acc 1
2016-09-07T01:15:27.728391: step 5683, loss 0.0609257, acc 0.98
2016-09-07T01:15:28.434907: step 5684, loss 0.00432179, acc 1
2016-09-07T01:15:29.100777: step 5685, loss 3.55007e-05, acc 1
2016-09-07T01:15:29.769187: step 5686, loss 0.0210276, acc 0.98
2016-09-07T01:15:30.480139: step 5687, loss 0.00484503, acc 1
2016-09-07T01:15:31.131207: step 5688, loss 0.00747497, acc 1
2016-09-07T01:15:31.823959: step 5689, loss 0.0388172, acc 0.98
2016-09-07T01:15:32.524453: step 5690, loss 0.0223137, acc 0.98
2016-09-07T01:15:33.226319: step 5691, loss 0.0283203, acc 0.98
2016-09-07T01:15:33.911964: step 5692, loss 0.0178303, acc 1
2016-09-07T01:15:34.595234: step 5693, loss 0.00446664, acc 1
2016-09-07T01:15:35.311476: step 5694, loss 0.0483419, acc 0.98
2016-09-07T01:15:35.981760: step 5695, loss 0.0139386, acc 1
2016-09-07T01:15:36.674318: step 5696, loss 0.0194905, acc 0.98
2016-09-07T01:15:37.384372: step 5697, loss 0.00155216, acc 1
2016-09-07T01:15:38.079979: step 5698, loss 0.00497887, acc 1
2016-09-07T01:15:38.792007: step 5699, loss 0.0765766, acc 0.96
2016-09-07T01:15:39.452401: step 5700, loss 0.0179404, acc 1

Evaluation:
2016-09-07T01:15:42.659875: step 5700, loss 2.24073, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473177961/checkpoints/model-5700

2016-09-07T01:15:44.315995: step 5701, loss 0.0154345, acc 1
2016-09-07T01:15:45.022477: step 5702, loss 0.00318212, acc 1
2016-09-07T01:15:45.694714: step 5703, loss 3.58901e-05, acc 1
2016-09-07T01:15:46.388613: step 5704, loss 0.00112667, acc 1
2016-09-07T01:15:47.082732: step 5705, loss 0.0496323, acc 0.98
2016-09-07T01:15:47.757387: step 5706, loss 0.167688, acc 0.98
2016-09-07T01:15:48.460753: step 5707, loss 0.0854072, acc 0.96
2016-09-07T01:15:49.137845: step 5708, loss 0.0685205, acc 0.98
2016-09-07T01:15:49.830150: step 5709, loss 0.0527919, acc 0.96
2016-09-07T01:15:50.542477: step 5710, loss 0.039924, acc 0.98
2016-09-07T01:15:51.244396: step 5711, loss 0.00588472, acc 1
2016-09-07T01:15:51.930952: step 5712, loss 0.0265635, acc 1
2016-09-07T01:15:52.624605: step 5713, loss 0.014636, acc 0.98
2016-09-07T01:15:53.323339: step 5714, loss 0.0364187, acc 0.96
2016-09-07T01:15:53.995595: step 5715, loss 0.037753, acc 1
2016-09-07T01:15:54.684734: step 5716, loss 0.0863689, acc 0.94
2016-09-07T01:15:55.382293: step 5717, loss 0.105777, acc 0.96
2016-09-07T01:15:56.077864: step 5718, loss 0.000163337, acc 1
2016-09-07T01:15:56.757159: step 5719, loss 0.131112, acc 0.96
2016-09-07T01:15:57.450347: step 5720, loss 0.0347991, acc 0.98
2016-09-07T01:15:58.140005: step 5721, loss 0.038935, acc 0.98
2016-09-07T01:15:58.794166: step 5722, loss 0.016063, acc 1
2016-09-07T01:15:59.496400: step 5723, loss 0.0582015, acc 0.98
2016-09-07T01:16:00.180385: step 5724, loss 0.000295264, acc 1
2016-09-07T01:16:00.900119: step 5725, loss 0.00906413, acc 1
2016-09-07T01:16:01.591660: step 5726, loss 0.0338663, acc 0.98
2016-09-07T01:16:02.283060: step 5727, loss 0.00160582, acc 1
2016-09-07T01:16:02.997367: step 5728, loss 0.0451081, acc 0.98
2016-09-07T01:16:03.673882: step 5729, loss 0.00036673, acc 1
2016-09-07T01:16:04.373019: step 5730, loss 0.0107641, acc 1
2016-09-07T01:16:05.054264: step 5731, loss 0.0364114, acc 0.98
2016-09-07T01:16:05.749782: step 5732, loss 0.00126395, acc 1
2016-09-07T01:16:06.434723: step 5733, loss 0.0163088, acc 1
2016-09-07T01:16:07.131573: step 5734, loss 0.0360528, acc 0.96
2016-09-07T01:16:07.843973: step 5735, loss 0.0270996, acc 0.98
2016-09-07T01:16:08.514589: step 5736, loss 0.0248056, acc 1
2016-09-07T01:16:09.225339: step 5737, loss 0.0340406, acc 0.98
2016-09-07T01:16:09.906664: step 5738, loss 0.0259717, acc 1
2016-09-07T01:16:10.601843: step 5739, loss 0.0354932, acc 0.98
2016-09-07T01:16:11.287906: step 5740, loss 0.0267404, acc 0.98
2016-09-07T01:16:11.965054: step 5741, loss 0.0116796, acc 1
2016-09-07T01:16:12.662755: step 5742, loss 0.0321435, acc 0.98
2016-09-07T01:16:13.333891: step 5743, loss 0.00649042, acc 1
2016-09-07T01:16:14.033445: step 5744, loss 0.050552, acc 0.98
2016-09-07T01:16:14.713868: step 5745, loss 0.00824032, acc 1
2016-09-07T01:16:15.381051: step 5746, loss 0.0452434, acc 1
2016-09-07T01:16:16.069750: step 5747, loss 0.0227163, acc 0.98
2016-09-07T01:16:16.771444: step 5748, loss 0.0341992, acc 0.98
2016-09-07T01:16:17.491507: step 5749, loss 0.00563028, acc 1
2016-09-07T01:16:18.156107: step 5750, loss 0.00188687, acc 1
2016-09-07T01:16:18.849374: step 5751, loss 0.0379005, acc 0.98
2016-09-07T01:16:19.556388: step 5752, loss 0.00647687, acc 1
2016-09-07T01:16:20.242078: step 5753, loss 0.01515, acc 1
2016-09-07T01:16:20.944168: step 5754, loss 0.144733, acc 0.94
2016-09-07T01:16:21.602324: step 5755, loss 0.045403, acc 0.96
2016-09-07T01:16:22.330873: step 5756, loss 0.0015345, acc 1
2016-09-07T01:16:23.009855: step 5757, loss 0.0172058, acc 1
2016-09-07T01:16:23.697059: step 5758, loss 0.00770732, acc 1
2016-09-07T01:16:24.385346: step 5759, loss 0.00461407, acc 1
2016-09-07T01:16:25.019373: step 5760, loss 0.0069214, acc 1

WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fb539b45e90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fb539b45e50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
CV_INDEX=7
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=30
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9594/1066
Writing to /home/cil/lstm-context-embeddings/runs/1473186470

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-07T02:28:10.440899: step 1, loss 0.693147, acc 0.46
2016-09-07T02:28:11.128928: step 2, loss 0.70239, acc 0.48
2016-09-07T02:28:11.793882: step 3, loss 0.702063, acc 0.4
2016-09-07T02:28:12.474305: step 4, loss 0.697958, acc 0.48
2016-09-07T02:28:13.147524: step 5, loss 0.698006, acc 0.46
2016-09-07T02:28:13.825889: step 6, loss 0.694534, acc 0.5
2016-09-07T02:28:14.533862: step 7, loss 0.686692, acc 0.58
2016-09-07T02:28:15.241391: step 8, loss 0.716874, acc 0.48
2016-09-07T02:28:15.936463: step 9, loss 0.707696, acc 0.46
2016-09-07T02:28:16.598782: step 10, loss 0.687404, acc 0.5
2016-09-07T02:28:17.303594: step 11, loss 0.691758, acc 0.48
2016-09-07T02:28:18.000406: step 12, loss 0.682173, acc 0.6
2016-09-07T02:28:18.703412: step 13, loss 0.692874, acc 0.54
2016-09-07T02:28:19.386463: step 14, loss 0.684861, acc 0.52
2016-09-07T02:28:20.046384: step 15, loss 0.687477, acc 0.48
2016-09-07T02:28:20.724073: step 16, loss 0.658904, acc 0.6
2016-09-07T02:28:21.400819: step 17, loss 0.666121, acc 0.56
2016-09-07T02:28:22.116594: step 18, loss 0.689861, acc 0.5
2016-09-07T02:28:22.775141: step 19, loss 0.671988, acc 0.58
2016-09-07T02:28:23.452819: step 20, loss 0.696587, acc 0.48
2016-09-07T02:28:24.141154: step 21, loss 0.688859, acc 0.5
2016-09-07T02:28:24.819037: step 22, loss 0.648239, acc 0.62
2016-09-07T02:28:25.503654: step 23, loss 0.612725, acc 0.66
2016-09-07T02:28:26.184863: step 24, loss 0.794994, acc 0.42
2016-09-07T02:28:26.886276: step 25, loss 0.71352, acc 0.52
2016-09-07T02:28:27.545528: step 26, loss 0.647754, acc 0.66
2016-09-07T02:28:28.217852: step 27, loss 0.634435, acc 0.72
2016-09-07T02:28:28.898240: step 28, loss 0.642104, acc 0.64
2016-09-07T02:28:29.573646: step 29, loss 0.643942, acc 0.6
2016-09-07T02:28:30.257361: step 30, loss 0.640218, acc 0.6
2016-09-07T02:28:30.928657: step 31, loss 0.663431, acc 0.64
2016-09-07T02:28:31.632882: step 32, loss 0.587986, acc 0.78
2016-09-07T02:28:32.312442: step 33, loss 0.703227, acc 0.58
2016-09-07T02:28:33.010110: step 34, loss 0.594798, acc 0.68
2016-09-07T02:28:33.697342: step 35, loss 0.559069, acc 0.7
2016-09-07T02:28:34.382476: step 36, loss 0.695805, acc 0.56
2016-09-07T02:28:35.070688: step 37, loss 0.512984, acc 0.78
2016-09-07T02:28:35.773327: step 38, loss 0.696605, acc 0.6
2016-09-07T02:28:36.453369: step 39, loss 0.555445, acc 0.72
2016-09-07T02:28:37.116481: step 40, loss 0.548377, acc 0.78
2016-09-07T02:28:37.813719: step 41, loss 0.47874, acc 0.8
2016-09-07T02:28:38.483960: step 42, loss 0.52784, acc 0.76
2016-09-07T02:28:39.187022: step 43, loss 0.66068, acc 0.68
2016-09-07T02:28:39.882548: step 44, loss 0.676381, acc 0.66
2016-09-07T02:28:40.546233: step 45, loss 0.592638, acc 0.64
2016-09-07T02:28:41.230918: step 46, loss 0.601316, acc 0.66
2016-09-07T02:28:41.891702: step 47, loss 0.680287, acc 0.56
2016-09-07T02:28:42.576522: step 48, loss 0.58018, acc 0.64
2016-09-07T02:28:43.245376: step 49, loss 0.497485, acc 0.8
2016-09-07T02:28:43.936461: step 50, loss 0.5579, acc 0.64
2016-09-07T02:28:44.615118: step 51, loss 0.483518, acc 0.76
2016-09-07T02:28:45.307619: step 52, loss 0.627736, acc 0.64
2016-09-07T02:28:45.998042: step 53, loss 0.685873, acc 0.62
2016-09-07T02:28:46.666777: step 54, loss 0.643378, acc 0.68
2016-09-07T02:28:47.372408: step 55, loss 0.618329, acc 0.64
2016-09-07T02:28:48.051762: step 56, loss 0.750046, acc 0.56
2016-09-07T02:28:48.740518: step 57, loss 0.636972, acc 0.66
2016-09-07T02:28:49.422900: step 58, loss 0.627198, acc 0.58
2016-09-07T02:28:50.115109: step 59, loss 0.552369, acc 0.74
2016-09-07T02:28:50.832150: step 60, loss 0.657273, acc 0.72
2016-09-07T02:28:51.530088: step 61, loss 0.601472, acc 0.64
2016-09-07T02:28:52.231946: step 62, loss 0.61813, acc 0.72
2016-09-07T02:28:52.916539: step 63, loss 0.558827, acc 0.78
2016-09-07T02:28:53.628994: step 64, loss 0.577969, acc 0.74
2016-09-07T02:28:54.335504: step 65, loss 0.515707, acc 0.78
2016-09-07T02:28:55.029492: step 66, loss 0.584711, acc 0.68
2016-09-07T02:28:55.734887: step 67, loss 0.689435, acc 0.5
2016-09-07T02:28:56.410879: step 68, loss 0.547987, acc 0.72
2016-09-07T02:28:57.075420: step 69, loss 0.476032, acc 0.84
2016-09-07T02:28:57.763306: step 70, loss 0.589878, acc 0.66
2016-09-07T02:28:58.434265: step 71, loss 0.408847, acc 0.8
2016-09-07T02:28:59.113570: step 72, loss 0.472284, acc 0.84
2016-09-07T02:28:59.786197: step 73, loss 0.750705, acc 0.68
2016-09-07T02:29:00.522975: step 74, loss 0.461747, acc 0.8
2016-09-07T02:29:01.174638: step 75, loss 0.471026, acc 0.8
2016-09-07T02:29:01.885457: step 76, loss 0.615652, acc 0.66
2016-09-07T02:29:02.570928: step 77, loss 0.490624, acc 0.68
2016-09-07T02:29:03.249449: step 78, loss 0.576556, acc 0.76
2016-09-07T02:29:03.931321: step 79, loss 0.602755, acc 0.62
2016-09-07T02:29:04.629063: step 80, loss 0.534495, acc 0.68
2016-09-07T02:29:05.323082: step 81, loss 0.537582, acc 0.66
2016-09-07T02:29:05.987286: step 82, loss 0.497292, acc 0.76
2016-09-07T02:29:06.702176: step 83, loss 0.526106, acc 0.74
2016-09-07T02:29:07.401007: step 84, loss 0.567205, acc 0.68
2016-09-07T02:29:08.074665: step 85, loss 0.64268, acc 0.64
2016-09-07T02:29:08.749120: step 86, loss 0.553, acc 0.74
2016-09-07T02:29:09.402820: step 87, loss 0.554945, acc 0.7
2016-09-07T02:29:10.082331: step 88, loss 0.552317, acc 0.72
2016-09-07T02:29:10.759341: step 89, loss 0.588067, acc 0.7
2016-09-07T02:29:11.457584: step 90, loss 0.569186, acc 0.74
2016-09-07T02:29:12.135053: step 91, loss 0.583169, acc 0.66
2016-09-07T02:29:12.805364: step 92, loss 0.468501, acc 0.84
2016-09-07T02:29:13.488083: step 93, loss 0.575066, acc 0.74
2016-09-07T02:29:14.168188: step 94, loss 0.623092, acc 0.68
2016-09-07T02:29:14.845897: step 95, loss 0.658774, acc 0.7
2016-09-07T02:29:15.498558: step 96, loss 0.609981, acc 0.7
2016-09-07T02:29:16.194929: step 97, loss 0.413978, acc 0.84
2016-09-07T02:29:16.870485: step 98, loss 0.422636, acc 0.8
2016-09-07T02:29:17.578478: step 99, loss 0.561932, acc 0.68
2016-09-07T02:29:18.260678: step 100, loss 0.623268, acc 0.68

Evaluation:
2016-09-07T02:29:21.444819: step 100, loss 0.497067, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-100

2016-09-07T02:29:23.205795: step 101, loss 0.590332, acc 0.72
2016-09-07T02:29:23.898348: step 102, loss 0.664057, acc 0.56
2016-09-07T02:29:24.614629: step 103, loss 0.516981, acc 0.7
2016-09-07T02:29:25.301531: step 104, loss 0.563929, acc 0.72
2016-09-07T02:29:25.993548: step 105, loss 0.531173, acc 0.76
2016-09-07T02:29:26.676696: step 106, loss 0.55292, acc 0.72
2016-09-07T02:29:27.359002: step 107, loss 0.474753, acc 0.76
2016-09-07T02:29:28.046477: step 108, loss 0.479625, acc 0.76
2016-09-07T02:29:28.725075: step 109, loss 0.448754, acc 0.84
2016-09-07T02:29:29.427959: step 110, loss 0.442244, acc 0.82
2016-09-07T02:29:30.097681: step 111, loss 0.591206, acc 0.7
2016-09-07T02:29:30.795175: step 112, loss 0.562449, acc 0.66
2016-09-07T02:29:31.497085: step 113, loss 0.502077, acc 0.72
2016-09-07T02:29:32.195327: step 114, loss 0.419643, acc 0.82
2016-09-07T02:29:32.894395: step 115, loss 0.397559, acc 0.86
2016-09-07T02:29:33.558969: step 116, loss 0.646725, acc 0.62
2016-09-07T02:29:34.253565: step 117, loss 0.546371, acc 0.76
2016-09-07T02:29:34.932813: step 118, loss 0.479136, acc 0.72
2016-09-07T02:29:35.628265: step 119, loss 0.481051, acc 0.78
2016-09-07T02:29:36.315873: step 120, loss 0.565801, acc 0.76
2016-09-07T02:29:37.005342: step 121, loss 0.449232, acc 0.76
2016-09-07T02:29:37.687688: step 122, loss 0.46855, acc 0.8
2016-09-07T02:29:38.339199: step 123, loss 0.513524, acc 0.68
2016-09-07T02:29:39.055945: step 124, loss 0.485808, acc 0.74
2016-09-07T02:29:39.772189: step 125, loss 0.430746, acc 0.84
2016-09-07T02:29:40.448341: step 126, loss 0.486558, acc 0.76
2016-09-07T02:29:41.136879: step 127, loss 0.602145, acc 0.64
2016-09-07T02:29:41.806327: step 128, loss 0.555402, acc 0.72
2016-09-07T02:29:42.484471: step 129, loss 0.47331, acc 0.74
2016-09-07T02:29:43.154315: step 130, loss 0.413127, acc 0.8
2016-09-07T02:29:43.862298: step 131, loss 0.510062, acc 0.74
2016-09-07T02:29:44.540257: step 132, loss 0.532224, acc 0.74
2016-09-07T02:29:45.214051: step 133, loss 0.429211, acc 0.8
2016-09-07T02:29:45.883707: step 134, loss 0.508681, acc 0.8
2016-09-07T02:29:46.567062: step 135, loss 0.498245, acc 0.76
2016-09-07T02:29:47.270409: step 136, loss 0.472191, acc 0.82
2016-09-07T02:29:47.948503: step 137, loss 0.598966, acc 0.7
2016-09-07T02:29:48.650358: step 138, loss 0.604305, acc 0.72
2016-09-07T02:29:49.343633: step 139, loss 0.64681, acc 0.62
2016-09-07T02:29:50.002541: step 140, loss 0.479172, acc 0.8
2016-09-07T02:29:50.694913: step 141, loss 0.542393, acc 0.7
2016-09-07T02:29:51.380331: step 142, loss 0.380407, acc 0.9
2016-09-07T02:29:52.080250: step 143, loss 0.445405, acc 0.84
2016-09-07T02:29:52.782177: step 144, loss 0.621414, acc 0.64
2016-09-07T02:29:53.501454: step 145, loss 0.673267, acc 0.68
2016-09-07T02:29:54.194543: step 146, loss 0.650979, acc 0.68
2016-09-07T02:29:54.881903: step 147, loss 0.451373, acc 0.82
2016-09-07T02:29:55.568827: step 148, loss 0.567596, acc 0.76
2016-09-07T02:29:56.252609: step 149, loss 0.535094, acc 0.68
2016-09-07T02:29:56.931100: step 150, loss 0.512832, acc 0.72
2016-09-07T02:29:57.593306: step 151, loss 0.469646, acc 0.8
2016-09-07T02:29:58.295241: step 152, loss 0.573402, acc 0.78
2016-09-07T02:29:58.997015: step 153, loss 0.50351, acc 0.7
2016-09-07T02:29:59.691855: step 154, loss 0.525576, acc 0.66
2016-09-07T02:30:00.413657: step 155, loss 0.557248, acc 0.72
2016-09-07T02:30:01.087095: step 156, loss 0.630379, acc 0.72
2016-09-07T02:30:01.762350: step 157, loss 0.470781, acc 0.76
2016-09-07T02:30:02.434642: step 158, loss 0.487444, acc 0.72
2016-09-07T02:30:03.133873: step 159, loss 0.550215, acc 0.66
2016-09-07T02:30:03.817794: step 160, loss 0.510532, acc 0.74
2016-09-07T02:30:04.489352: step 161, loss 0.536039, acc 0.74
2016-09-07T02:30:05.156815: step 162, loss 0.443927, acc 0.78
2016-09-07T02:30:05.839241: step 163, loss 0.495016, acc 0.76
2016-09-07T02:30:06.527724: step 164, loss 0.556637, acc 0.7
2016-09-07T02:30:07.207721: step 165, loss 0.377608, acc 0.86
2016-09-07T02:30:07.909512: step 166, loss 0.516675, acc 0.76
2016-09-07T02:30:08.570738: step 167, loss 0.411296, acc 0.76
2016-09-07T02:30:09.260928: step 168, loss 0.523316, acc 0.78
2016-09-07T02:30:09.965344: step 169, loss 0.446696, acc 0.8
2016-09-07T02:30:10.656076: step 170, loss 0.446458, acc 0.78
2016-09-07T02:30:11.365149: step 171, loss 0.621064, acc 0.68
2016-09-07T02:30:12.029935: step 172, loss 0.475421, acc 0.78
2016-09-07T02:30:12.719396: step 173, loss 0.439018, acc 0.76
2016-09-07T02:30:13.416806: step 174, loss 0.435955, acc 0.76
2016-09-07T02:30:14.108549: step 175, loss 0.454056, acc 0.82
2016-09-07T02:30:14.779130: step 176, loss 0.479125, acc 0.78
2016-09-07T02:30:15.452560: step 177, loss 0.472733, acc 0.78
2016-09-07T02:30:16.156679: step 178, loss 0.484558, acc 0.76
2016-09-07T02:30:16.830167: step 179, loss 0.488105, acc 0.68
2016-09-07T02:30:17.555424: step 180, loss 0.59445, acc 0.64
2016-09-07T02:30:18.239880: step 181, loss 0.520337, acc 0.72
2016-09-07T02:30:18.931271: step 182, loss 0.471549, acc 0.82
2016-09-07T02:30:19.609527: step 183, loss 0.437959, acc 0.8
2016-09-07T02:30:20.306934: step 184, loss 0.541826, acc 0.76
2016-09-07T02:30:20.998810: step 185, loss 0.40296, acc 0.84
2016-09-07T02:30:21.654750: step 186, loss 0.494413, acc 0.78
2016-09-07T02:30:22.358230: step 187, loss 0.568799, acc 0.66
2016-09-07T02:30:23.029945: step 188, loss 0.557107, acc 0.74
2016-09-07T02:30:23.723614: step 189, loss 0.564091, acc 0.66
2016-09-07T02:30:24.400933: step 190, loss 0.455363, acc 0.76
2016-09-07T02:30:25.089738: step 191, loss 0.39672, acc 0.9
2016-09-07T02:30:25.745267: step 192, loss 0.543519, acc 0.75
2016-09-07T02:30:26.428190: step 193, loss 0.339671, acc 0.84
2016-09-07T02:30:27.144579: step 194, loss 0.367265, acc 0.84
2016-09-07T02:30:27.811034: step 195, loss 0.294111, acc 0.94
2016-09-07T02:30:28.478750: step 196, loss 0.317734, acc 0.86
2016-09-07T02:30:29.167021: step 197, loss 0.343599, acc 0.86
2016-09-07T02:30:29.832030: step 198, loss 0.287078, acc 0.86
2016-09-07T02:30:30.528367: step 199, loss 0.437661, acc 0.86
2016-09-07T02:30:31.211359: step 200, loss 0.325945, acc 0.82

Evaluation:
2016-09-07T02:30:34.443775: step 200, loss 0.502981, acc 0.780488

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-200

2016-09-07T02:30:36.163838: step 201, loss 0.354876, acc 0.82
2016-09-07T02:30:36.842359: step 202, loss 0.234828, acc 0.94
2016-09-07T02:30:37.514575: step 203, loss 0.339619, acc 0.86
2016-09-07T02:30:38.196015: step 204, loss 0.309289, acc 0.86
2016-09-07T02:30:38.872900: step 205, loss 0.416051, acc 0.8
2016-09-07T02:30:39.572080: step 206, loss 0.408558, acc 0.86
2016-09-07T02:30:40.270813: step 207, loss 0.361377, acc 0.84
2016-09-07T02:30:40.942874: step 208, loss 0.418409, acc 0.78
2016-09-07T02:30:41.674101: step 209, loss 0.401019, acc 0.86
2016-09-07T02:30:42.363774: step 210, loss 0.415464, acc 0.88
2016-09-07T02:30:43.060227: step 211, loss 0.306928, acc 0.84
2016-09-07T02:30:43.730139: step 212, loss 0.363792, acc 0.9
2016-09-07T02:30:44.407740: step 213, loss 0.285904, acc 0.88
2016-09-07T02:30:45.120265: step 214, loss 0.227055, acc 0.92
2016-09-07T02:30:45.779929: step 215, loss 0.379075, acc 0.82
2016-09-07T02:30:46.475382: step 216, loss 0.369253, acc 0.84
2016-09-07T02:30:47.163639: step 217, loss 0.384562, acc 0.8
2016-09-07T02:30:47.852996: step 218, loss 0.322634, acc 0.9
2016-09-07T02:30:48.542398: step 219, loss 0.388191, acc 0.82
2016-09-07T02:30:49.214522: step 220, loss 0.425063, acc 0.9
2016-09-07T02:30:49.910780: step 221, loss 0.295367, acc 0.84
2016-09-07T02:30:50.588036: step 222, loss 0.377315, acc 0.82
2016-09-07T02:30:51.286847: step 223, loss 0.337103, acc 0.82
2016-09-07T02:30:51.973886: step 224, loss 0.322675, acc 0.9
2016-09-07T02:30:52.639597: step 225, loss 0.333934, acc 0.86
2016-09-07T02:30:53.331775: step 226, loss 0.306393, acc 0.82
2016-09-07T02:30:53.999029: step 227, loss 0.23609, acc 0.9
2016-09-07T02:30:54.710667: step 228, loss 0.248869, acc 0.9
2016-09-07T02:30:55.394954: step 229, loss 0.538531, acc 0.78
2016-09-07T02:30:56.088651: step 230, loss 0.397289, acc 0.78
2016-09-07T02:30:56.774738: step 231, loss 0.470102, acc 0.8
2016-09-07T02:30:57.466643: step 232, loss 0.388858, acc 0.76
2016-09-07T02:30:58.140324: step 233, loss 0.591556, acc 0.78
2016-09-07T02:30:58.827365: step 234, loss 0.39976, acc 0.82
2016-09-07T02:30:59.517822: step 235, loss 0.142087, acc 0.98
2016-09-07T02:31:00.175815: step 236, loss 0.417186, acc 0.78
2016-09-07T02:31:00.885845: step 237, loss 0.318248, acc 0.88
2016-09-07T02:31:01.552583: step 238, loss 0.338942, acc 0.88
2016-09-07T02:31:02.236311: step 239, loss 0.397394, acc 0.86
2016-09-07T02:31:02.926533: step 240, loss 0.481008, acc 0.86
2016-09-07T02:31:03.620186: step 241, loss 0.36646, acc 0.8
2016-09-07T02:31:04.323716: step 242, loss 0.357308, acc 0.84
2016-09-07T02:31:04.983226: step 243, loss 0.409922, acc 0.78
2016-09-07T02:31:05.687258: step 244, loss 0.362878, acc 0.82
2016-09-07T02:31:06.388165: step 245, loss 0.384965, acc 0.86
2016-09-07T02:31:07.074003: step 246, loss 0.437735, acc 0.82
2016-09-07T02:31:07.771725: step 247, loss 0.532071, acc 0.74
2016-09-07T02:31:08.439342: step 248, loss 0.434946, acc 0.78
2016-09-07T02:31:09.143387: step 249, loss 0.161508, acc 1
2016-09-07T02:31:09.814755: step 250, loss 0.52309, acc 0.76
2016-09-07T02:31:10.517069: step 251, loss 0.435917, acc 0.84
2016-09-07T02:31:11.200436: step 252, loss 0.42865, acc 0.8
2016-09-07T02:31:11.898869: step 253, loss 0.383761, acc 0.82
2016-09-07T02:31:12.598600: step 254, loss 0.4109, acc 0.78
2016-09-07T02:31:13.335793: step 255, loss 0.291685, acc 0.84
2016-09-07T02:31:14.030721: step 256, loss 0.371363, acc 0.88
2016-09-07T02:31:14.691912: step 257, loss 0.331227, acc 0.84
2016-09-07T02:31:15.392867: step 258, loss 0.394302, acc 0.86
2016-09-07T02:31:16.068893: step 259, loss 0.409574, acc 0.84
2016-09-07T02:31:16.751854: step 260, loss 0.359168, acc 0.82
2016-09-07T02:31:17.432285: step 261, loss 0.344132, acc 0.8
2016-09-07T02:31:18.128079: step 262, loss 0.370864, acc 0.88
2016-09-07T02:31:18.827689: step 263, loss 0.579054, acc 0.78
2016-09-07T02:31:19.489805: step 264, loss 0.291302, acc 0.84
2016-09-07T02:31:20.215110: step 265, loss 0.246272, acc 0.9
2016-09-07T02:31:20.918923: step 266, loss 0.28235, acc 0.86
2016-09-07T02:31:21.615402: step 267, loss 0.26526, acc 0.86
2016-09-07T02:31:22.309190: step 268, loss 0.36306, acc 0.76
2016-09-07T02:31:22.998990: step 269, loss 0.474938, acc 0.74
2016-09-07T02:31:23.684158: step 270, loss 0.426748, acc 0.84
2016-09-07T02:31:24.338771: step 271, loss 0.371757, acc 0.84
2016-09-07T02:31:25.046718: step 272, loss 0.471842, acc 0.82
2016-09-07T02:31:25.751511: step 273, loss 0.287343, acc 0.86
2016-09-07T02:31:26.428631: step 274, loss 0.30122, acc 0.84
2016-09-07T02:31:27.103180: step 275, loss 0.278869, acc 0.9
2016-09-07T02:31:27.783731: step 276, loss 0.414938, acc 0.82
2016-09-07T02:31:28.487536: step 277, loss 0.49294, acc 0.72
2016-09-07T02:31:29.141046: step 278, loss 0.220824, acc 0.96
2016-09-07T02:31:29.830272: step 279, loss 0.351403, acc 0.86
2016-09-07T02:31:30.521701: step 280, loss 0.400304, acc 0.8
2016-09-07T02:31:31.222826: step 281, loss 0.383986, acc 0.78
2016-09-07T02:31:31.905005: step 282, loss 0.419111, acc 0.78
2016-09-07T02:31:32.604909: step 283, loss 0.511409, acc 0.8
2016-09-07T02:31:33.323195: step 284, loss 0.226561, acc 0.92
2016-09-07T02:31:33.993295: step 285, loss 0.394872, acc 0.8
2016-09-07T02:31:34.682903: step 286, loss 0.432983, acc 0.78
2016-09-07T02:31:35.362131: step 287, loss 0.438902, acc 0.8
2016-09-07T02:31:36.041526: step 288, loss 0.395751, acc 0.82
2016-09-07T02:31:36.727508: step 289, loss 0.219337, acc 0.9
2016-09-07T02:31:37.399284: step 290, loss 0.466599, acc 0.84
2016-09-07T02:31:38.099414: step 291, loss 0.371131, acc 0.82
2016-09-07T02:31:38.790665: step 292, loss 0.431992, acc 0.8
2016-09-07T02:31:39.478521: step 293, loss 0.257554, acc 0.9
2016-09-07T02:31:40.170876: step 294, loss 0.352125, acc 0.84
2016-09-07T02:31:40.873214: step 295, loss 0.45876, acc 0.82
2016-09-07T02:31:41.542637: step 296, loss 0.34925, acc 0.84
2016-09-07T02:31:42.212188: step 297, loss 0.363092, acc 0.82
2016-09-07T02:31:42.917952: step 298, loss 0.346007, acc 0.9
2016-09-07T02:31:43.596471: step 299, loss 0.34029, acc 0.86
2016-09-07T02:31:44.281055: step 300, loss 0.334036, acc 0.82

Evaluation:
2016-09-07T02:31:47.500103: step 300, loss 0.462912, acc 0.772983

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-300

2016-09-07T02:31:49.164202: step 301, loss 0.266492, acc 0.9
2016-09-07T02:31:49.857114: step 302, loss 0.453144, acc 0.8
2016-09-07T02:31:50.545668: step 303, loss 0.349408, acc 0.86
2016-09-07T02:31:51.234064: step 304, loss 0.262907, acc 0.9
2016-09-07T02:31:51.913626: step 305, loss 0.547333, acc 0.76
2016-09-07T02:31:52.621816: step 306, loss 0.42377, acc 0.8
2016-09-07T02:31:53.302092: step 307, loss 0.414629, acc 0.78
2016-09-07T02:31:53.987629: step 308, loss 0.273589, acc 0.94
2016-09-07T02:31:54.670093: step 309, loss 0.357587, acc 0.86
2016-09-07T02:31:55.359117: step 310, loss 0.277469, acc 0.9
2016-09-07T02:31:56.061185: step 311, loss 0.503287, acc 0.7
2016-09-07T02:31:56.729302: step 312, loss 0.383613, acc 0.82
2016-09-07T02:31:57.414405: step 313, loss 0.422339, acc 0.76
2016-09-07T02:31:58.086742: step 314, loss 0.382573, acc 0.82
2016-09-07T02:31:58.763861: step 315, loss 0.253305, acc 0.84
2016-09-07T02:31:59.459965: step 316, loss 0.33482, acc 0.86
2016-09-07T02:32:00.137287: step 317, loss 0.38505, acc 0.82
2016-09-07T02:32:00.870606: step 318, loss 0.307185, acc 0.88
2016-09-07T02:32:01.548034: step 319, loss 0.388394, acc 0.84
2016-09-07T02:32:02.225859: step 320, loss 0.49419, acc 0.82
2016-09-07T02:32:02.881966: step 321, loss 0.360793, acc 0.88
2016-09-07T02:32:03.566311: step 322, loss 0.432539, acc 0.86
2016-09-07T02:32:04.245790: step 323, loss 0.415314, acc 0.84
2016-09-07T02:32:04.919965: step 324, loss 0.438477, acc 0.74
2016-09-07T02:32:05.603046: step 325, loss 0.240299, acc 0.9
2016-09-07T02:32:06.269082: step 326, loss 0.317057, acc 0.86
2016-09-07T02:32:06.958288: step 327, loss 0.383946, acc 0.84
2016-09-07T02:32:07.651126: step 328, loss 0.345639, acc 0.84
2016-09-07T02:32:08.317244: step 329, loss 0.239306, acc 0.9
2016-09-07T02:32:08.999349: step 330, loss 0.420236, acc 0.84
2016-09-07T02:32:09.681929: step 331, loss 0.358771, acc 0.82
2016-09-07T02:32:10.357241: step 332, loss 0.454169, acc 0.76
2016-09-07T02:32:11.055942: step 333, loss 0.484456, acc 0.76
2016-09-07T02:32:11.764047: step 334, loss 0.577487, acc 0.76
2016-09-07T02:32:12.421692: step 335, loss 0.28312, acc 0.88
2016-09-07T02:32:13.127065: step 336, loss 0.309877, acc 0.86
2016-09-07T02:32:13.829118: step 337, loss 0.33104, acc 0.88
2016-09-07T02:32:14.521024: step 338, loss 0.364648, acc 0.86
2016-09-07T02:32:15.213818: step 339, loss 0.376243, acc 0.8
2016-09-07T02:32:15.896074: step 340, loss 0.309515, acc 0.84
2016-09-07T02:32:16.613230: step 341, loss 0.467857, acc 0.72
2016-09-07T02:32:17.274042: step 342, loss 0.369482, acc 0.84
2016-09-07T02:32:17.935576: step 343, loss 0.338817, acc 0.86
2016-09-07T02:32:18.609354: step 344, loss 0.398475, acc 0.82
2016-09-07T02:32:19.300714: step 345, loss 0.496736, acc 0.76
2016-09-07T02:32:19.981713: step 346, loss 0.342241, acc 0.84
2016-09-07T02:32:20.660702: step 347, loss 0.470137, acc 0.72
2016-09-07T02:32:21.342428: step 348, loss 0.303857, acc 0.88
2016-09-07T02:32:21.998366: step 349, loss 0.297537, acc 0.84
2016-09-07T02:32:22.720184: step 350, loss 0.372599, acc 0.82
2016-09-07T02:32:23.409645: step 351, loss 0.30658, acc 0.9
2016-09-07T02:32:24.092936: step 352, loss 0.323122, acc 0.84
2016-09-07T02:32:24.772721: step 353, loss 0.355187, acc 0.8
2016-09-07T02:32:25.449812: step 354, loss 0.34709, acc 0.82
2016-09-07T02:32:26.153935: step 355, loss 0.418779, acc 0.8
2016-09-07T02:32:26.814684: step 356, loss 0.348406, acc 0.84
2016-09-07T02:32:27.522082: step 357, loss 0.339503, acc 0.8
2016-09-07T02:32:28.210236: step 358, loss 0.252712, acc 0.9
2016-09-07T02:32:28.891458: step 359, loss 0.484205, acc 0.84
2016-09-07T02:32:29.580960: step 360, loss 0.314549, acc 0.82
2016-09-07T02:32:30.266861: step 361, loss 0.347813, acc 0.9
2016-09-07T02:32:30.972863: step 362, loss 0.623297, acc 0.68
2016-09-07T02:32:31.643964: step 363, loss 0.342903, acc 0.86
2016-09-07T02:32:32.347988: step 364, loss 0.230847, acc 0.88
2016-09-07T02:32:33.045555: step 365, loss 0.538591, acc 0.72
2016-09-07T02:32:33.727843: step 366, loss 0.42575, acc 0.84
2016-09-07T02:32:34.409188: step 367, loss 0.372997, acc 0.82
2016-09-07T02:32:35.098331: step 368, loss 0.485745, acc 0.8
2016-09-07T02:32:35.799841: step 369, loss 0.488615, acc 0.72
2016-09-07T02:32:36.456633: step 370, loss 0.699726, acc 0.78
2016-09-07T02:32:37.157932: step 371, loss 0.497628, acc 0.74
2016-09-07T02:32:37.835211: step 372, loss 0.358369, acc 0.86
2016-09-07T02:32:38.526728: step 373, loss 0.386123, acc 0.86
2016-09-07T02:32:39.201510: step 374, loss 0.336473, acc 0.86
2016-09-07T02:32:39.879168: step 375, loss 0.415507, acc 0.82
2016-09-07T02:32:40.586340: step 376, loss 0.370211, acc 0.86
2016-09-07T02:32:41.240067: step 377, loss 0.416887, acc 0.78
2016-09-07T02:32:41.967787: step 378, loss 0.418762, acc 0.82
2016-09-07T02:32:42.663197: step 379, loss 0.330691, acc 0.86
2016-09-07T02:32:43.372698: step 380, loss 0.382437, acc 0.84
2016-09-07T02:32:44.075308: step 381, loss 0.376338, acc 0.84
2016-09-07T02:32:44.749540: step 382, loss 0.312682, acc 0.88
2016-09-07T02:32:45.459875: step 383, loss 0.52745, acc 0.74
2016-09-07T02:32:46.081276: step 384, loss 0.27266, acc 0.886364
2016-09-07T02:32:46.781232: step 385, loss 0.205868, acc 0.92
2016-09-07T02:32:47.470516: step 386, loss 0.214645, acc 0.9
2016-09-07T02:32:48.162415: step 387, loss 0.14782, acc 0.96
2016-09-07T02:32:48.855103: step 388, loss 0.0982717, acc 0.98
2016-09-07T02:32:49.556914: step 389, loss 0.0927181, acc 1
2016-09-07T02:32:50.285687: step 390, loss 0.19566, acc 0.92
2016-09-07T02:32:50.972989: step 391, loss 0.230561, acc 0.9
2016-09-07T02:32:51.650066: step 392, loss 0.201665, acc 0.94
2016-09-07T02:32:52.338710: step 393, loss 0.242948, acc 0.92
2016-09-07T02:32:52.998677: step 394, loss 0.206963, acc 0.9
2016-09-07T02:32:53.673850: step 395, loss 0.162228, acc 0.96
2016-09-07T02:32:54.351948: step 396, loss 0.170481, acc 0.92
2016-09-07T02:32:55.042387: step 397, loss 0.133867, acc 0.92
2016-09-07T02:32:55.712842: step 398, loss 0.280669, acc 0.8
2016-09-07T02:32:56.393376: step 399, loss 0.251279, acc 0.86
2016-09-07T02:32:57.077424: step 400, loss 0.244829, acc 0.88

Evaluation:
2016-09-07T02:33:00.325375: step 400, loss 0.504387, acc 0.798311

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-400

2016-09-07T02:33:01.967772: step 401, loss 0.209752, acc 0.9
2016-09-07T02:33:02.660600: step 402, loss 0.266782, acc 0.88
2016-09-07T02:33:03.355265: step 403, loss 0.133795, acc 0.94
2016-09-07T02:33:04.012869: step 404, loss 0.132265, acc 0.96
2016-09-07T02:33:04.706044: step 405, loss 0.258385, acc 0.88
2016-09-07T02:33:05.365750: step 406, loss 0.219452, acc 0.92
2016-09-07T02:33:06.042162: step 407, loss 0.283758, acc 0.88
2016-09-07T02:33:06.731096: step 408, loss 0.170748, acc 0.92
2016-09-07T02:33:07.411108: step 409, loss 0.351526, acc 0.88
2016-09-07T02:33:08.109992: step 410, loss 0.117972, acc 0.94
2016-09-07T02:33:08.811633: step 411, loss 0.223939, acc 0.9
2016-09-07T02:33:09.519369: step 412, loss 0.118555, acc 0.98
2016-09-07T02:33:10.194634: step 413, loss 0.142281, acc 0.98
2016-09-07T02:33:10.868462: step 414, loss 0.33867, acc 0.84
2016-09-07T02:33:11.563213: step 415, loss 0.186347, acc 0.9
2016-09-07T02:33:12.248988: step 416, loss 0.381052, acc 0.86
2016-09-07T02:33:12.929387: step 417, loss 0.274695, acc 0.88
2016-09-07T02:33:13.619444: step 418, loss 0.347605, acc 0.84
2016-09-07T02:33:14.323463: step 419, loss 0.231964, acc 0.88
2016-09-07T02:33:14.989347: step 420, loss 0.213136, acc 0.92
2016-09-07T02:33:15.675462: step 421, loss 0.308467, acc 0.88
2016-09-07T02:33:16.407414: step 422, loss 0.235617, acc 0.86
2016-09-07T02:33:17.092178: step 423, loss 0.217282, acc 0.88
2016-09-07T02:33:17.778907: step 424, loss 0.209878, acc 0.92
2016-09-07T02:33:18.463585: step 425, loss 0.303188, acc 0.84
2016-09-07T02:33:19.143390: step 426, loss 0.294999, acc 0.88
2016-09-07T02:33:19.816818: step 427, loss 0.271708, acc 0.94
2016-09-07T02:33:20.520188: step 428, loss 0.168492, acc 0.94
2016-09-07T02:33:21.210983: step 429, loss 0.255425, acc 0.9
2016-09-07T02:33:21.902519: step 430, loss 0.44616, acc 0.82
2016-09-07T02:33:22.602747: step 431, loss 0.251673, acc 0.92
2016-09-07T02:33:23.271538: step 432, loss 0.19256, acc 0.92
2016-09-07T02:33:23.964141: step 433, loss 0.12638, acc 0.96
2016-09-07T02:33:24.637468: step 434, loss 0.130124, acc 0.94
2016-09-07T02:33:25.326129: step 435, loss 0.190728, acc 0.92
2016-09-07T02:33:26.006715: step 436, loss 0.223102, acc 0.92
2016-09-07T02:33:26.704520: step 437, loss 0.15292, acc 0.96
2016-09-07T02:33:27.395264: step 438, loss 0.153618, acc 0.94
2016-09-07T02:33:28.063149: step 439, loss 0.219612, acc 0.88
2016-09-07T02:33:28.764833: step 440, loss 0.186836, acc 0.88
2016-09-07T02:33:29.425107: step 441, loss 0.124237, acc 0.94
2016-09-07T02:33:30.113193: step 442, loss 0.428342, acc 0.88
2016-09-07T02:33:30.804544: step 443, loss 0.282518, acc 0.84
2016-09-07T02:33:31.490780: step 444, loss 0.241357, acc 0.88
2016-09-07T02:33:32.188484: step 445, loss 0.291052, acc 0.9
2016-09-07T02:33:32.855889: step 446, loss 0.223838, acc 0.86
2016-09-07T02:33:33.560247: step 447, loss 0.149849, acc 0.96
2016-09-07T02:33:34.239329: step 448, loss 0.279617, acc 0.9
2016-09-07T02:33:34.942732: step 449, loss 0.265003, acc 0.86
2016-09-07T02:33:35.656705: step 450, loss 0.168513, acc 0.94
2016-09-07T02:33:36.359300: step 451, loss 0.349503, acc 0.82
2016-09-07T02:33:37.048143: step 452, loss 0.153977, acc 0.94
2016-09-07T02:33:37.732841: step 453, loss 0.234262, acc 0.92
2016-09-07T02:33:38.443385: step 454, loss 0.151235, acc 0.96
2016-09-07T02:33:39.148789: step 455, loss 0.293744, acc 0.92
2016-09-07T02:33:39.829894: step 456, loss 0.142212, acc 0.94
2016-09-07T02:33:40.512452: step 457, loss 0.236361, acc 0.86
2016-09-07T02:33:41.196041: step 458, loss 0.234002, acc 0.94
2016-09-07T02:33:41.925340: step 459, loss 0.136614, acc 0.96
2016-09-07T02:33:42.607679: step 460, loss 0.328406, acc 0.88
2016-09-07T02:33:43.289979: step 461, loss 0.256924, acc 0.92
2016-09-07T02:33:43.973219: step 462, loss 0.219343, acc 0.9
2016-09-07T02:33:44.669408: step 463, loss 0.387974, acc 0.86
2016-09-07T02:33:45.355648: step 464, loss 0.216709, acc 0.92
2016-09-07T02:33:46.050760: step 465, loss 0.395868, acc 0.82
2016-09-07T02:33:46.766056: step 466, loss 0.271443, acc 0.86
2016-09-07T02:33:47.458026: step 467, loss 0.286749, acc 0.88
2016-09-07T02:33:48.133499: step 468, loss 0.324864, acc 0.86
2016-09-07T02:33:48.825982: step 469, loss 0.315337, acc 0.9
2016-09-07T02:33:49.517709: step 470, loss 0.214271, acc 0.92
2016-09-07T02:33:50.209683: step 471, loss 0.185734, acc 0.98
2016-09-07T02:33:50.878992: step 472, loss 0.195678, acc 0.94
2016-09-07T02:33:51.588982: step 473, loss 0.203316, acc 0.94
2016-09-07T02:33:52.269122: step 474, loss 0.246328, acc 0.9
2016-09-07T02:33:52.952855: step 475, loss 0.307775, acc 0.84
2016-09-07T02:33:53.637238: step 476, loss 0.308831, acc 0.86
2016-09-07T02:33:54.303727: step 477, loss 0.318324, acc 0.86
2016-09-07T02:33:54.975165: step 478, loss 0.251302, acc 0.9
2016-09-07T02:33:55.658262: step 479, loss 0.233184, acc 0.94
2016-09-07T02:33:56.369817: step 480, loss 0.261591, acc 0.88
2016-09-07T02:33:57.073231: step 481, loss 0.319529, acc 0.88
2016-09-07T02:33:57.746446: step 482, loss 0.246991, acc 0.86
2016-09-07T02:33:58.426731: step 483, loss 0.315645, acc 0.86
2016-09-07T02:33:59.097529: step 484, loss 0.306923, acc 0.86
2016-09-07T02:33:59.784043: step 485, loss 0.367893, acc 0.82
2016-09-07T02:34:00.513546: step 486, loss 0.193435, acc 0.92
2016-09-07T02:34:01.204294: step 487, loss 0.306552, acc 0.86
2016-09-07T02:34:01.869396: step 488, loss 0.174474, acc 0.92
2016-09-07T02:34:02.559426: step 489, loss 0.191599, acc 0.9
2016-09-07T02:34:03.226320: step 490, loss 0.24047, acc 0.92
2016-09-07T02:34:03.938565: step 491, loss 0.328006, acc 0.82
2016-09-07T02:34:04.608371: step 492, loss 0.13189, acc 1
2016-09-07T02:34:05.285394: step 493, loss 0.244018, acc 0.88
2016-09-07T02:34:05.980756: step 494, loss 0.222805, acc 0.9
2016-09-07T02:34:06.675015: step 495, loss 0.161652, acc 0.94
2016-09-07T02:34:07.377701: step 496, loss 0.178062, acc 0.9
2016-09-07T02:34:08.081943: step 497, loss 0.158797, acc 0.9
2016-09-07T02:34:08.766329: step 498, loss 0.19348, acc 0.9
2016-09-07T02:34:09.458579: step 499, loss 0.225158, acc 0.9
2016-09-07T02:34:10.154914: step 500, loss 0.159271, acc 0.96

Evaluation:
2016-09-07T02:34:13.444652: step 500, loss 0.541262, acc 0.796435

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-500

2016-09-07T02:34:15.169058: step 501, loss 0.174603, acc 0.94
2016-09-07T02:34:15.847577: step 502, loss 0.0965002, acc 0.94
2016-09-07T02:34:16.553966: step 503, loss 0.315501, acc 0.82
2016-09-07T02:34:17.234110: step 504, loss 0.209402, acc 0.94
2016-09-07T02:34:17.915429: step 505, loss 0.198861, acc 0.92
2016-09-07T02:34:18.590199: step 506, loss 0.253375, acc 0.88
2016-09-07T02:34:19.288794: step 507, loss 0.216728, acc 0.86
2016-09-07T02:34:19.964541: step 508, loss 0.129821, acc 0.94
2016-09-07T02:34:20.659707: step 509, loss 0.313526, acc 0.86
2016-09-07T02:34:21.338213: step 510, loss 0.579231, acc 0.78
2016-09-07T02:34:22.022560: step 511, loss 0.212743, acc 0.9
2016-09-07T02:34:22.737224: step 512, loss 0.325991, acc 0.8
2016-09-07T02:34:23.405394: step 513, loss 0.152497, acc 0.92
2016-09-07T02:34:24.100654: step 514, loss 0.151763, acc 0.96
2016-09-07T02:34:24.755009: step 515, loss 0.280328, acc 0.9
2016-09-07T02:34:25.437402: step 516, loss 0.317841, acc 0.92
2016-09-07T02:34:26.123037: step 517, loss 0.263381, acc 0.94
2016-09-07T02:34:26.801152: step 518, loss 0.36731, acc 0.86
2016-09-07T02:34:27.499501: step 519, loss 0.209136, acc 0.92
2016-09-07T02:34:28.222562: step 520, loss 0.193553, acc 0.96
2016-09-07T02:34:28.931356: step 521, loss 0.194025, acc 0.94
2016-09-07T02:34:29.612854: step 522, loss 0.399295, acc 0.76
2016-09-07T02:34:30.292163: step 523, loss 0.239457, acc 0.88
2016-09-07T02:34:30.975687: step 524, loss 0.237089, acc 0.86
2016-09-07T02:34:31.652877: step 525, loss 0.210099, acc 0.9
2016-09-07T02:34:32.356613: step 526, loss 0.307697, acc 0.82
2016-09-07T02:34:33.016126: step 527, loss 0.193736, acc 0.92
2016-09-07T02:34:33.715301: step 528, loss 0.259753, acc 0.84
2016-09-07T02:34:34.398558: step 529, loss 0.338448, acc 0.86
2016-09-07T02:34:35.075058: step 530, loss 0.256865, acc 0.88
2016-09-07T02:34:35.764511: step 531, loss 0.323261, acc 0.84
2016-09-07T02:34:36.456899: step 532, loss 0.404309, acc 0.8
2016-09-07T02:34:37.155110: step 533, loss 0.341519, acc 0.88
2016-09-07T02:34:37.814484: step 534, loss 0.354566, acc 0.86
2016-09-07T02:34:38.545050: step 535, loss 0.148811, acc 0.98
2016-09-07T02:34:39.249457: step 536, loss 0.243851, acc 0.9
2016-09-07T02:34:39.914158: step 537, loss 0.180402, acc 0.96
2016-09-07T02:34:40.583344: step 538, loss 0.354798, acc 0.86
2016-09-07T02:34:41.265159: step 539, loss 0.471938, acc 0.78
2016-09-07T02:34:41.968783: step 540, loss 0.135563, acc 0.94
2016-09-07T02:34:42.634315: step 541, loss 0.287145, acc 0.88
2016-09-07T02:34:43.356079: step 542, loss 0.253172, acc 0.88
2016-09-07T02:34:44.050623: step 543, loss 0.275685, acc 0.88
2016-09-07T02:34:44.735152: step 544, loss 0.448733, acc 0.84
2016-09-07T02:34:45.428073: step 545, loss 0.226311, acc 0.9
2016-09-07T02:34:46.112528: step 546, loss 0.287549, acc 0.86
2016-09-07T02:34:46.807930: step 547, loss 0.347935, acc 0.82
2016-09-07T02:34:47.473342: step 548, loss 0.243913, acc 0.88
2016-09-07T02:34:48.167311: step 549, loss 0.262774, acc 0.84
2016-09-07T02:34:48.817006: step 550, loss 0.246431, acc 0.9
2016-09-07T02:34:49.494059: step 551, loss 0.303119, acc 0.86
2016-09-07T02:34:50.198791: step 552, loss 0.276599, acc 0.88
2016-09-07T02:34:50.876066: step 553, loss 0.298374, acc 0.86
2016-09-07T02:34:51.575344: step 554, loss 0.464964, acc 0.78
2016-09-07T02:34:52.257411: step 555, loss 0.260567, acc 0.9
2016-09-07T02:34:52.980393: step 556, loss 0.277492, acc 0.84
2016-09-07T02:34:53.654763: step 557, loss 0.334621, acc 0.9
2016-09-07T02:34:54.331224: step 558, loss 0.346096, acc 0.82
2016-09-07T02:34:55.008945: step 559, loss 0.392621, acc 0.8
2016-09-07T02:34:55.702449: step 560, loss 0.382421, acc 0.84
2016-09-07T02:34:56.416133: step 561, loss 0.406258, acc 0.82
2016-09-07T02:34:57.086526: step 562, loss 0.331477, acc 0.86
2016-09-07T02:34:57.795335: step 563, loss 0.185604, acc 0.92
2016-09-07T02:34:58.479695: step 564, loss 0.14231, acc 0.96
2016-09-07T02:34:59.163815: step 565, loss 0.251256, acc 0.86
2016-09-07T02:34:59.863571: step 566, loss 0.150321, acc 0.94
2016-09-07T02:35:00.589682: step 567, loss 0.249479, acc 0.9
2016-09-07T02:35:01.288468: step 568, loss 0.224567, acc 0.9
2016-09-07T02:35:01.957427: step 569, loss 0.157803, acc 0.96
2016-09-07T02:35:02.653201: step 570, loss 0.231543, acc 0.92
2016-09-07T02:35:03.362320: step 571, loss 0.285188, acc 0.9
2016-09-07T02:35:04.036581: step 572, loss 0.259866, acc 0.92
2016-09-07T02:35:04.707531: step 573, loss 0.460716, acc 0.76
2016-09-07T02:35:05.381420: step 574, loss 0.283868, acc 0.92
2016-09-07T02:35:06.100422: step 575, loss 0.257563, acc 0.9
2016-09-07T02:35:06.724021: step 576, loss 0.297988, acc 0.863636
2016-09-07T02:35:07.434529: step 577, loss 0.122543, acc 0.94
2016-09-07T02:35:08.163816: step 578, loss 0.136126, acc 0.94
2016-09-07T02:35:08.840348: step 579, loss 0.133543, acc 0.98
2016-09-07T02:35:09.516932: step 580, loss 0.139945, acc 0.96
2016-09-07T02:35:10.209247: step 581, loss 0.0682252, acc 0.98
2016-09-07T02:35:10.904089: step 582, loss 0.206634, acc 0.9
2016-09-07T02:35:11.597077: step 583, loss 0.122425, acc 0.94
2016-09-07T02:35:12.301745: step 584, loss 0.116594, acc 0.98
2016-09-07T02:35:12.995157: step 585, loss 0.0857701, acc 0.96
2016-09-07T02:35:13.679316: step 586, loss 0.151053, acc 0.92
2016-09-07T02:35:14.392650: step 587, loss 0.238915, acc 0.88
2016-09-07T02:35:15.085871: step 588, loss 0.247112, acc 0.9
2016-09-07T02:35:15.786568: step 589, loss 0.145974, acc 0.92
2016-09-07T02:35:16.443289: step 590, loss 0.164929, acc 0.94
2016-09-07T02:35:17.105320: step 591, loss 0.104665, acc 0.98
2016-09-07T02:35:17.781994: step 592, loss 0.19685, acc 0.94
2016-09-07T02:35:18.472320: step 593, loss 0.259479, acc 0.88
2016-09-07T02:35:19.167897: step 594, loss 0.0849523, acc 0.98
2016-09-07T02:35:19.847086: step 595, loss 0.237021, acc 0.88
2016-09-07T02:35:20.552475: step 596, loss 0.0961582, acc 0.96
2016-09-07T02:35:21.221122: step 597, loss 0.214505, acc 0.9
2016-09-07T02:35:21.911630: step 598, loss 0.158141, acc 0.88
2016-09-07T02:35:22.616939: step 599, loss 0.12498, acc 0.98
2016-09-07T02:35:23.303225: step 600, loss 0.143323, acc 0.98

Evaluation:
2016-09-07T02:35:26.601340: step 600, loss 0.596901, acc 0.777674

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-600

2016-09-07T02:35:28.282791: step 601, loss 0.13871, acc 0.94
2016-09-07T02:35:28.988385: step 602, loss 0.124146, acc 0.96
2016-09-07T02:35:29.669881: step 603, loss 0.138228, acc 0.94
2016-09-07T02:35:30.337414: step 604, loss 0.138742, acc 0.96
2016-09-07T02:35:31.015755: step 605, loss 0.0985106, acc 0.96
2016-09-07T02:35:31.693915: step 606, loss 0.254242, acc 0.92
2016-09-07T02:35:32.382426: step 607, loss 0.174669, acc 0.9
2016-09-07T02:35:33.107357: step 608, loss 0.217952, acc 0.92
2016-09-07T02:35:33.827215: step 609, loss 0.136348, acc 0.94
2016-09-07T02:35:34.510884: step 610, loss 0.105571, acc 0.94
2016-09-07T02:35:35.186359: step 611, loss 0.10388, acc 0.96
2016-09-07T02:35:35.860392: step 612, loss 0.222436, acc 0.92
2016-09-07T02:35:36.539486: step 613, loss 0.182729, acc 0.9
2016-09-07T02:35:37.221925: step 614, loss 0.0535888, acc 0.98
2016-09-07T02:35:37.898652: step 615, loss 0.179878, acc 0.92
2016-09-07T02:35:38.633623: step 616, loss 0.222719, acc 0.92
2016-09-07T02:35:39.314416: step 617, loss 0.152779, acc 0.92
2016-09-07T02:35:39.988551: step 618, loss 0.118752, acc 0.94
2016-09-07T02:35:40.665865: step 619, loss 0.121244, acc 0.96
2016-09-07T02:35:41.340607: step 620, loss 0.148146, acc 0.92
2016-09-07T02:35:42.060745: step 621, loss 0.261373, acc 0.9
2016-09-07T02:35:42.753287: step 622, loss 0.217858, acc 0.9
2016-09-07T02:35:43.467554: step 623, loss 0.21169, acc 0.86
2016-09-07T02:35:44.148260: step 624, loss 0.106538, acc 0.96
2016-09-07T02:35:44.849479: step 625, loss 0.06563, acc 1
2016-09-07T02:35:45.555083: step 626, loss 0.336362, acc 0.86
2016-09-07T02:35:46.224947: step 627, loss 0.0773263, acc 0.96
2016-09-07T02:35:46.911320: step 628, loss 0.236509, acc 0.92
2016-09-07T02:35:47.590602: step 629, loss 0.160812, acc 0.94
2016-09-07T02:35:48.286490: step 630, loss 0.152977, acc 0.94
2016-09-07T02:35:48.954128: step 631, loss 0.173395, acc 0.92
2016-09-07T02:35:49.634069: step 632, loss 0.160154, acc 0.92
2016-09-07T02:35:50.314347: step 633, loss 0.183952, acc 0.88
2016-09-07T02:35:50.995341: step 634, loss 0.143215, acc 0.94
2016-09-07T02:35:51.696606: step 635, loss 0.233389, acc 0.88
2016-09-07T02:35:52.370238: step 636, loss 0.140591, acc 0.96
2016-09-07T02:35:53.098596: step 637, loss 0.197702, acc 0.9
2016-09-07T02:35:53.780715: step 638, loss 0.173402, acc 0.9
2016-09-07T02:35:54.460396: step 639, loss 0.120774, acc 0.96
2016-09-07T02:35:55.136579: step 640, loss 0.213312, acc 0.88
2016-09-07T02:35:55.813661: step 641, loss 0.124055, acc 0.9
2016-09-07T02:35:56.542541: step 642, loss 0.160919, acc 0.9
2016-09-07T02:35:57.217155: step 643, loss 0.147518, acc 0.94
2016-09-07T02:35:57.930844: step 644, loss 0.166374, acc 0.96
2016-09-07T02:35:58.618078: step 645, loss 0.309917, acc 0.88
2016-09-07T02:35:59.297104: step 646, loss 0.121912, acc 0.94
2016-09-07T02:35:59.989895: step 647, loss 0.146241, acc 0.94
2016-09-07T02:36:00.700970: step 648, loss 0.193984, acc 0.94
2016-09-07T02:36:01.414947: step 649, loss 0.22188, acc 0.9
2016-09-07T02:36:02.091158: step 650, loss 0.192032, acc 0.92
2016-09-07T02:36:02.785565: step 651, loss 0.114304, acc 0.96
2016-09-07T02:36:03.467016: step 652, loss 0.215874, acc 0.92
2016-09-07T02:36:04.140783: step 653, loss 0.106456, acc 0.98
2016-09-07T02:36:04.838086: step 654, loss 0.163493, acc 0.92
2016-09-07T02:36:05.530159: step 655, loss 0.222183, acc 0.92
2016-09-07T02:36:06.239763: step 656, loss 0.230061, acc 0.92
2016-09-07T02:36:06.923223: step 657, loss 0.461394, acc 0.84
2016-09-07T02:36:07.613101: step 658, loss 0.129315, acc 0.96
2016-09-07T02:36:08.305331: step 659, loss 0.184434, acc 0.94
2016-09-07T02:36:08.998406: step 660, loss 0.100182, acc 0.96
2016-09-07T02:36:09.691359: step 661, loss 0.0796997, acc 0.98
2016-09-07T02:36:10.376006: step 662, loss 0.181885, acc 0.96
2016-09-07T02:36:11.080706: step 663, loss 0.148752, acc 0.92
2016-09-07T02:36:11.766178: step 664, loss 0.121231, acc 0.96
2016-09-07T02:36:12.447057: step 665, loss 0.0755396, acc 0.98
2016-09-07T02:36:13.127796: step 666, loss 0.0854793, acc 0.96
2016-09-07T02:36:13.814729: step 667, loss 0.155528, acc 0.9
2016-09-07T02:36:14.508417: step 668, loss 0.264402, acc 0.9
2016-09-07T02:36:15.182309: step 669, loss 0.192861, acc 0.88
2016-09-07T02:36:15.869505: step 670, loss 0.385135, acc 0.88
2016-09-07T02:36:16.553515: step 671, loss 0.293436, acc 0.92
2016-09-07T02:36:17.239620: step 672, loss 0.147917, acc 0.96
2016-09-07T02:36:17.911667: step 673, loss 0.164741, acc 0.92
2016-09-07T02:36:18.591491: step 674, loss 0.206836, acc 0.92
2016-09-07T02:36:19.286321: step 675, loss 0.0901706, acc 0.96
2016-09-07T02:36:19.971766: step 676, loss 0.178738, acc 0.92
2016-09-07T02:36:20.642940: step 677, loss 0.18787, acc 0.92
2016-09-07T02:36:21.316892: step 678, loss 0.222221, acc 0.84
2016-09-07T02:36:22.054375: step 679, loss 0.344916, acc 0.86
2016-09-07T02:36:22.743991: step 680, loss 0.142411, acc 0.94
2016-09-07T02:36:23.434396: step 681, loss 0.200607, acc 0.92
2016-09-07T02:36:24.107850: step 682, loss 0.215459, acc 0.88
2016-09-07T02:36:24.799136: step 683, loss 0.184706, acc 0.9
2016-09-07T02:36:25.500188: step 684, loss 0.227458, acc 0.92
2016-09-07T02:36:26.146025: step 685, loss 0.237747, acc 0.88
2016-09-07T02:36:26.831103: step 686, loss 0.169544, acc 0.92
2016-09-07T02:36:27.491103: step 687, loss 0.203404, acc 0.88
2016-09-07T02:36:28.173109: step 688, loss 0.24766, acc 0.92
2016-09-07T02:36:28.860182: step 689, loss 0.24625, acc 0.9
2016-09-07T02:36:29.559436: step 690, loss 0.299107, acc 0.94
2016-09-07T02:36:30.258661: step 691, loss 0.263817, acc 0.9
2016-09-07T02:36:30.921163: step 692, loss 0.167707, acc 0.88
2016-09-07T02:36:31.614394: step 693, loss 0.193583, acc 0.92
2016-09-07T02:36:32.297765: step 694, loss 0.23713, acc 0.92
2016-09-07T02:36:32.985379: step 695, loss 0.117895, acc 0.96
2016-09-07T02:36:33.664756: step 696, loss 0.232832, acc 0.92
2016-09-07T02:36:34.371230: step 697, loss 0.190686, acc 0.98
2016-09-07T02:36:35.078802: step 698, loss 0.163225, acc 0.94
2016-09-07T02:36:35.728928: step 699, loss 0.212401, acc 0.94
2016-09-07T02:36:36.434996: step 700, loss 0.12657, acc 0.98

Evaluation:
2016-09-07T02:36:39.716434: step 700, loss 0.546911, acc 0.793621

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-700

2016-09-07T02:36:41.347460: step 701, loss 0.203758, acc 0.9
2016-09-07T02:36:42.034185: step 702, loss 0.17543, acc 0.94
2016-09-07T02:36:42.699304: step 703, loss 0.238259, acc 0.9
2016-09-07T02:36:43.380909: step 704, loss 0.0709333, acc 0.98
2016-09-07T02:36:44.089435: step 705, loss 0.0816782, acc 0.96
2016-09-07T02:36:44.803435: step 706, loss 0.242873, acc 0.84
2016-09-07T02:36:45.487272: step 707, loss 0.308033, acc 0.84
2016-09-07T02:36:46.170619: step 708, loss 0.27101, acc 0.9
2016-09-07T02:36:46.854276: step 709, loss 0.240263, acc 0.88
2016-09-07T02:36:47.514172: step 710, loss 0.175523, acc 0.9
2016-09-07T02:36:48.189226: step 711, loss 0.224173, acc 0.92
2016-09-07T02:36:48.853253: step 712, loss 0.21945, acc 0.88
2016-09-07T02:36:49.555413: step 713, loss 0.337175, acc 0.88
2016-09-07T02:36:50.233439: step 714, loss 0.189972, acc 0.9
2016-09-07T02:36:50.927472: step 715, loss 0.314165, acc 0.86
2016-09-07T02:36:51.612743: step 716, loss 0.231816, acc 0.9
2016-09-07T02:36:52.295788: step 717, loss 0.209354, acc 0.9
2016-09-07T02:36:52.980934: step 718, loss 0.216497, acc 0.88
2016-09-07T02:36:53.653333: step 719, loss 0.348619, acc 0.86
2016-09-07T02:36:54.366603: step 720, loss 0.267092, acc 0.88
2016-09-07T02:36:55.046697: step 721, loss 0.109916, acc 0.98
2016-09-07T02:36:55.724527: step 722, loss 0.112457, acc 0.98
2016-09-07T02:36:56.412428: step 723, loss 0.137839, acc 0.94
2016-09-07T02:36:57.126348: step 724, loss 0.180034, acc 0.94
2016-09-07T02:36:57.808339: step 725, loss 0.174276, acc 0.92
2016-09-07T02:36:58.475195: step 726, loss 0.269368, acc 0.84
2016-09-07T02:36:59.163811: step 727, loss 0.201016, acc 0.94
2016-09-07T02:36:59.833990: step 728, loss 0.147581, acc 0.94
2016-09-07T02:37:00.538789: step 729, loss 0.152434, acc 0.94
2016-09-07T02:37:01.230093: step 730, loss 0.274521, acc 0.92
2016-09-07T02:37:01.897490: step 731, loss 0.0369417, acc 0.98
2016-09-07T02:37:02.591355: step 732, loss 0.182253, acc 0.94
2016-09-07T02:37:03.274482: step 733, loss 0.21066, acc 0.92
2016-09-07T02:37:03.974638: step 734, loss 0.241604, acc 0.9
2016-09-07T02:37:04.649059: step 735, loss 0.14276, acc 0.92
2016-09-07T02:37:05.353683: step 736, loss 0.219185, acc 0.86
2016-09-07T02:37:06.023027: step 737, loss 0.155172, acc 0.92
2016-09-07T02:37:06.724474: step 738, loss 0.148491, acc 0.98
2016-09-07T02:37:07.418392: step 739, loss 0.206102, acc 0.9
2016-09-07T02:37:08.111113: step 740, loss 0.102299, acc 0.98
2016-09-07T02:37:08.837851: step 741, loss 0.195355, acc 0.9
2016-09-07T02:37:09.534447: step 742, loss 0.234019, acc 0.9
2016-09-07T02:37:10.235742: step 743, loss 0.102695, acc 0.94
2016-09-07T02:37:10.919780: step 744, loss 0.142318, acc 0.94
2016-09-07T02:37:11.606980: step 745, loss 0.16527, acc 0.94
2016-09-07T02:37:12.293089: step 746, loss 0.197972, acc 0.88
2016-09-07T02:37:12.993471: step 747, loss 0.145817, acc 0.92
2016-09-07T02:37:13.699691: step 748, loss 0.0848547, acc 1
2016-09-07T02:37:14.400391: step 749, loss 0.190284, acc 0.92
2016-09-07T02:37:15.067484: step 750, loss 0.239065, acc 0.9
2016-09-07T02:37:15.748267: step 751, loss 0.320813, acc 0.86
2016-09-07T02:37:16.435566: step 752, loss 0.227709, acc 0.94
2016-09-07T02:37:17.113524: step 753, loss 0.231595, acc 0.88
2016-09-07T02:37:17.797937: step 754, loss 0.164576, acc 0.92
2016-09-07T02:37:18.499225: step 755, loss 0.172119, acc 0.94
2016-09-07T02:37:19.183640: step 756, loss 0.111557, acc 0.94
2016-09-07T02:37:19.881129: step 757, loss 0.201286, acc 0.9
2016-09-07T02:37:20.558765: step 758, loss 0.115088, acc 0.94
2016-09-07T02:37:21.230487: step 759, loss 0.0894739, acc 0.96
2016-09-07T02:37:21.918199: step 760, loss 0.178972, acc 0.94
2016-09-07T02:37:22.595089: step 761, loss 0.114588, acc 0.96
2016-09-07T02:37:23.317129: step 762, loss 0.151504, acc 0.96
2016-09-07T02:37:23.987730: step 763, loss 0.131254, acc 0.94
2016-09-07T02:37:24.677837: step 764, loss 0.146754, acc 0.94
2016-09-07T02:37:25.354891: step 765, loss 0.222329, acc 0.92
2016-09-07T02:37:26.038389: step 766, loss 0.184219, acc 0.92
2016-09-07T02:37:26.725665: step 767, loss 0.236009, acc 0.86
2016-09-07T02:37:27.357373: step 768, loss 0.26256, acc 0.931818
2016-09-07T02:37:28.068439: step 769, loss 0.120793, acc 0.94
2016-09-07T02:37:28.738697: step 770, loss 0.123514, acc 0.94
2016-09-07T02:37:29.452504: step 771, loss 0.193797, acc 0.88
2016-09-07T02:37:30.160148: step 772, loss 0.196014, acc 0.92
2016-09-07T02:37:30.848063: step 773, loss 0.0888178, acc 0.96
2016-09-07T02:37:31.537757: step 774, loss 0.0839994, acc 0.96
2016-09-07T02:37:32.211762: step 775, loss 0.113045, acc 0.94
2016-09-07T02:37:32.911007: step 776, loss 0.108244, acc 0.92
2016-09-07T02:37:33.612818: step 777, loss 0.0664524, acc 0.98
2016-09-07T02:37:34.312063: step 778, loss 0.0803984, acc 0.98
2016-09-07T02:37:34.970035: step 779, loss 0.097925, acc 0.96
2016-09-07T02:37:35.643360: step 780, loss 0.138242, acc 0.96
2016-09-07T02:37:36.345324: step 781, loss 0.142519, acc 0.92
2016-09-07T02:37:37.052351: step 782, loss 0.15512, acc 0.96
2016-09-07T02:37:37.774548: step 783, loss 0.247778, acc 0.9
2016-09-07T02:37:38.449905: step 784, loss 0.0882552, acc 0.96
2016-09-07T02:37:39.139135: step 785, loss 0.130145, acc 0.96
2016-09-07T02:37:39.831868: step 786, loss 0.095919, acc 0.96
2016-09-07T02:37:40.504119: step 787, loss 0.0835728, acc 0.98
2016-09-07T02:37:41.193945: step 788, loss 0.127007, acc 0.92
2016-09-07T02:37:41.848830: step 789, loss 0.079134, acc 0.96
2016-09-07T02:37:42.550810: step 790, loss 0.0909173, acc 0.98
2016-09-07T02:37:43.219744: step 791, loss 0.0438363, acc 0.98
2016-09-07T02:37:43.904528: step 792, loss 0.111235, acc 0.96
2016-09-07T02:37:44.591024: step 793, loss 0.0387195, acc 1
2016-09-07T02:37:45.278170: step 794, loss 0.0508603, acc 0.96
2016-09-07T02:37:45.991964: step 795, loss 0.093346, acc 0.96
2016-09-07T02:37:46.664148: step 796, loss 0.125591, acc 0.94
2016-09-07T02:37:47.375099: step 797, loss 0.145161, acc 0.92
2016-09-07T02:37:48.076745: step 798, loss 0.109135, acc 0.96
2016-09-07T02:37:48.807221: step 799, loss 0.0448759, acc 0.98
2016-09-07T02:37:49.507319: step 800, loss 0.188258, acc 0.88

Evaluation:
2016-09-07T02:37:52.852756: step 800, loss 0.775231, acc 0.798311

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-800

2016-09-07T02:37:54.560266: step 801, loss 0.107214, acc 0.96
2016-09-07T02:37:55.242410: step 802, loss 0.111692, acc 0.96
2016-09-07T02:37:55.923999: step 803, loss 0.0939667, acc 0.96
2016-09-07T02:37:56.619901: step 804, loss 0.0539117, acc 0.98
2016-09-07T02:37:57.308027: step 805, loss 0.159287, acc 0.94
2016-09-07T02:37:57.991013: step 806, loss 0.139496, acc 0.92
2016-09-07T02:37:58.681103: step 807, loss 0.119198, acc 0.96
2016-09-07T02:37:59.382806: step 808, loss 0.0951184, acc 0.94
2016-09-07T02:38:00.064109: step 809, loss 0.221235, acc 0.92
2016-09-07T02:38:00.779150: step 810, loss 0.0562464, acc 0.96
2016-09-07T02:38:01.472337: step 811, loss 0.0404872, acc 1
2016-09-07T02:38:02.148519: step 812, loss 0.0753044, acc 0.94
2016-09-07T02:38:02.840304: step 813, loss 0.07168, acc 0.98
2016-09-07T02:38:03.529458: step 814, loss 0.228982, acc 0.92
2016-09-07T02:38:04.236821: step 815, loss 0.11588, acc 0.96
2016-09-07T02:38:04.904672: step 816, loss 0.124693, acc 0.96
2016-09-07T02:38:05.579121: step 817, loss 0.113941, acc 0.98
2016-09-07T02:38:06.273523: step 818, loss 0.245047, acc 0.92
2016-09-07T02:38:06.949118: step 819, loss 0.122892, acc 0.94
2016-09-07T02:38:07.639294: step 820, loss 0.0770537, acc 0.98
2016-09-07T02:38:08.315623: step 821, loss 0.162129, acc 0.92
2016-09-07T02:38:09.004145: step 822, loss 0.0626769, acc 0.96
2016-09-07T02:38:09.659395: step 823, loss 0.0396747, acc 1
2016-09-07T02:38:10.329549: step 824, loss 0.225871, acc 0.9
2016-09-07T02:38:10.998835: step 825, loss 0.151381, acc 0.92
2016-09-07T02:38:11.678596: step 826, loss 0.1989, acc 0.9
2016-09-07T02:38:12.368389: step 827, loss 0.131952, acc 0.96
2016-09-07T02:38:13.043705: step 828, loss 0.175641, acc 0.9
2016-09-07T02:38:13.738594: step 829, loss 0.16535, acc 0.9
2016-09-07T02:38:14.401750: step 830, loss 0.0651896, acc 0.98
2016-09-07T02:38:15.094619: step 831, loss 0.151902, acc 0.92
2016-09-07T02:38:15.783180: step 832, loss 0.130873, acc 0.94
2016-09-07T02:38:16.493545: step 833, loss 0.183758, acc 0.94
2016-09-07T02:38:17.193084: step 834, loss 0.181721, acc 0.94
2016-09-07T02:38:17.897410: step 835, loss 0.210116, acc 0.88
2016-09-07T02:38:18.603236: step 836, loss 0.0999026, acc 0.94
2016-09-07T02:38:19.280735: step 837, loss 0.170931, acc 0.88
2016-09-07T02:38:19.956978: step 838, loss 0.147307, acc 0.92
2016-09-07T02:38:20.651725: step 839, loss 0.095861, acc 0.98
2016-09-07T02:38:21.339725: step 840, loss 0.116439, acc 0.94
2016-09-07T02:38:22.025835: step 841, loss 0.225505, acc 0.92
2016-09-07T02:38:22.704229: step 842, loss 0.0687035, acc 0.98
2016-09-07T02:38:23.401153: step 843, loss 0.103779, acc 0.96
2016-09-07T02:38:24.074239: step 844, loss 0.165311, acc 0.94
2016-09-07T02:38:24.755947: step 845, loss 0.190724, acc 0.96
2016-09-07T02:38:25.460641: step 846, loss 0.0983243, acc 0.96
2016-09-07T02:38:26.132540: step 847, loss 0.0949137, acc 0.94
2016-09-07T02:38:26.836416: step 848, loss 0.0655829, acc 0.98
2016-09-07T02:38:27.518216: step 849, loss 0.0742678, acc 0.98
2016-09-07T02:38:28.215359: step 850, loss 0.119303, acc 0.96
2016-09-07T02:38:28.899211: step 851, loss 0.09763, acc 0.98
2016-09-07T02:38:29.581432: step 852, loss 0.207358, acc 0.92
2016-09-07T02:38:30.263705: step 853, loss 0.039806, acc 1
2016-09-07T02:38:30.948803: step 854, loss 0.110865, acc 0.96
2016-09-07T02:38:31.638900: step 855, loss 0.0588546, acc 0.96
2016-09-07T02:38:32.312783: step 856, loss 0.0655871, acc 0.98
2016-09-07T02:38:33.025441: step 857, loss 0.322086, acc 0.88
2016-09-07T02:38:33.697619: step 858, loss 0.127494, acc 0.94
2016-09-07T02:38:34.387079: step 859, loss 0.237711, acc 0.88
2016-09-07T02:38:35.074764: step 860, loss 0.0394658, acc 1
2016-09-07T02:38:35.755286: step 861, loss 0.211622, acc 0.92
2016-09-07T02:38:36.448702: step 862, loss 0.120635, acc 0.94
2016-09-07T02:38:37.160285: step 863, loss 0.0710627, acc 0.98
2016-09-07T02:38:37.869488: step 864, loss 0.242221, acc 0.96
2016-09-07T02:38:38.548262: step 865, loss 0.127537, acc 0.96
2016-09-07T02:38:39.232027: step 866, loss 0.12548, acc 0.94
2016-09-07T02:38:39.930349: step 867, loss 0.218573, acc 0.9
2016-09-07T02:38:40.618000: step 868, loss 0.0712147, acc 0.98
2016-09-07T02:38:41.311788: step 869, loss 0.124946, acc 0.92
2016-09-07T02:38:41.970040: step 870, loss 0.111747, acc 0.94
2016-09-07T02:38:42.662488: step 871, loss 0.159673, acc 0.92
2016-09-07T02:38:43.334032: step 872, loss 0.104596, acc 0.94
2016-09-07T02:38:44.021930: step 873, loss 0.242342, acc 0.86
2016-09-07T02:38:44.717021: step 874, loss 0.0836067, acc 0.96
2016-09-07T02:38:45.404731: step 875, loss 0.0680053, acc 0.96
2016-09-07T02:38:46.094912: step 876, loss 0.0745679, acc 0.98
2016-09-07T02:38:46.783659: step 877, loss 0.210238, acc 0.9
2016-09-07T02:38:47.490209: step 878, loss 0.108655, acc 0.94
2016-09-07T02:38:48.158061: step 879, loss 0.111928, acc 0.92
2016-09-07T02:38:48.836812: step 880, loss 0.0496588, acc 0.98
2016-09-07T02:38:49.518885: step 881, loss 0.13345, acc 0.9
2016-09-07T02:38:50.217022: step 882, loss 0.0740574, acc 1
2016-09-07T02:38:50.884654: step 883, loss 0.0815784, acc 0.98
2016-09-07T02:38:51.562422: step 884, loss 0.149653, acc 0.96
2016-09-07T02:38:52.284874: step 885, loss 0.480817, acc 0.8
2016-09-07T02:38:52.962442: step 886, loss 0.0464413, acc 1
2016-09-07T02:38:53.661769: step 887, loss 0.183371, acc 0.88
2016-09-07T02:38:54.343543: step 888, loss 0.0610564, acc 1
2016-09-07T02:38:55.009232: step 889, loss 0.163169, acc 0.92
2016-09-07T02:38:55.699326: step 890, loss 0.218967, acc 0.98
2016-09-07T02:38:56.402266: step 891, loss 0.0708645, acc 0.98
2016-09-07T02:38:57.087148: step 892, loss 0.0483168, acc 0.98
2016-09-07T02:38:57.771590: step 893, loss 0.12189, acc 0.96
2016-09-07T02:38:58.470476: step 894, loss 0.0960566, acc 0.94
2016-09-07T02:38:59.145424: step 895, loss 0.166871, acc 0.94
2016-09-07T02:38:59.820661: step 896, loss 0.145135, acc 0.94
2016-09-07T02:39:00.522802: step 897, loss 0.182286, acc 0.94
2016-09-07T02:39:01.214932: step 898, loss 0.116446, acc 0.94
2016-09-07T02:39:01.904378: step 899, loss 0.187252, acc 0.96
2016-09-07T02:39:02.575340: step 900, loss 0.122908, acc 0.94

Evaluation:
2016-09-07T02:39:05.944230: step 900, loss 0.724352, acc 0.775797

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-900

2016-09-07T02:39:07.563870: step 901, loss 0.164666, acc 0.92
2016-09-07T02:39:08.223747: step 902, loss 0.0643362, acc 1
2016-09-07T02:39:08.901611: step 903, loss 0.0991067, acc 0.96
2016-09-07T02:39:09.593105: step 904, loss 0.170537, acc 0.9
2016-09-07T02:39:10.290159: step 905, loss 0.123703, acc 0.98
2016-09-07T02:39:10.954254: step 906, loss 0.132901, acc 0.96
2016-09-07T02:39:11.634780: step 907, loss 0.081231, acc 0.96
2016-09-07T02:39:12.322112: step 908, loss 0.154759, acc 0.94
2016-09-07T02:39:13.015165: step 909, loss 0.176241, acc 0.9
2016-09-07T02:39:13.692288: step 910, loss 0.164615, acc 0.92
2016-09-07T02:39:14.385713: step 911, loss 0.107918, acc 0.96
2016-09-07T02:39:15.065884: step 912, loss 0.212411, acc 0.92
2016-09-07T02:39:15.730674: step 913, loss 0.0752117, acc 0.96
2016-09-07T02:39:16.428202: step 914, loss 0.125473, acc 0.96
2016-09-07T02:39:17.117563: step 915, loss 0.14699, acc 0.96
2016-09-07T02:39:17.802970: step 916, loss 0.0805091, acc 0.98
2016-09-07T02:39:18.496619: step 917, loss 0.0663328, acc 0.98
2016-09-07T02:39:19.179471: step 918, loss 0.115838, acc 0.92
2016-09-07T02:39:19.869051: step 919, loss 0.12, acc 0.94
2016-09-07T02:39:20.532275: step 920, loss 0.0991557, acc 0.94
2016-09-07T02:39:21.252120: step 921, loss 0.0875676, acc 0.96
2016-09-07T02:39:21.958394: step 922, loss 0.213817, acc 0.94
2016-09-07T02:39:22.654824: step 923, loss 0.262888, acc 0.9
2016-09-07T02:39:23.344867: step 924, loss 0.0850494, acc 0.96
2016-09-07T02:39:24.041827: step 925, loss 0.0849816, acc 0.98
2016-09-07T02:39:24.745072: step 926, loss 0.139115, acc 0.9
2016-09-07T02:39:25.426866: step 927, loss 0.05499, acc 0.96
2016-09-07T02:39:26.114778: step 928, loss 0.120126, acc 0.96
2016-09-07T02:39:26.810632: step 929, loss 0.0645142, acc 0.98
2016-09-07T02:39:27.493194: step 930, loss 0.178966, acc 0.92
2016-09-07T02:39:28.187290: step 931, loss 0.149696, acc 0.96
2016-09-07T02:39:28.862729: step 932, loss 0.0909937, acc 0.94
2016-09-07T02:39:29.576362: step 933, loss 0.133816, acc 0.92
2016-09-07T02:39:30.237690: step 934, loss 0.190235, acc 0.94
2016-09-07T02:39:30.902294: step 935, loss 0.118347, acc 0.94
2016-09-07T02:39:31.590012: step 936, loss 0.378398, acc 0.92
2016-09-07T02:39:32.274870: step 937, loss 0.199597, acc 0.88
2016-09-07T02:39:32.949911: step 938, loss 0.0818304, acc 0.98
2016-09-07T02:39:33.610869: step 939, loss 0.0619543, acc 1
2016-09-07T02:39:34.319847: step 940, loss 0.0692362, acc 0.96
2016-09-07T02:39:35.006953: step 941, loss 0.0860867, acc 0.96
2016-09-07T02:39:35.698870: step 942, loss 0.0755818, acc 0.96
2016-09-07T02:39:36.403429: step 943, loss 0.100298, acc 0.98
2016-09-07T02:39:37.093356: step 944, loss 0.0881279, acc 0.94
2016-09-07T02:39:37.764726: step 945, loss 0.126127, acc 0.92
2016-09-07T02:39:38.459475: step 946, loss 0.117192, acc 0.94
2016-09-07T02:39:39.172112: step 947, loss 0.0894005, acc 0.94
2016-09-07T02:39:39.852072: step 948, loss 0.0428014, acc 1
2016-09-07T02:39:40.535624: step 949, loss 0.0962155, acc 0.96
2016-09-07T02:39:41.226201: step 950, loss 0.0633191, acc 0.98
2016-09-07T02:39:41.928820: step 951, loss 0.153298, acc 0.9
2016-09-07T02:39:42.617346: step 952, loss 0.161079, acc 0.94
2016-09-07T02:39:43.286310: step 953, loss 0.128981, acc 0.96
2016-09-07T02:39:43.979362: step 954, loss 0.0534269, acc 0.98
2016-09-07T02:39:44.642977: step 955, loss 0.0818449, acc 0.98
2016-09-07T02:39:45.318403: step 956, loss 0.262383, acc 0.9
2016-09-07T02:39:46.007209: step 957, loss 0.260712, acc 0.9
2016-09-07T02:39:46.689118: step 958, loss 0.218534, acc 0.9
2016-09-07T02:39:47.368251: step 959, loss 0.12782, acc 0.96
2016-09-07T02:39:47.988794: step 960, loss 0.1023, acc 0.954545
2016-09-07T02:39:48.679107: step 961, loss 0.161419, acc 0.92
2016-09-07T02:39:49.351117: step 962, loss 0.155213, acc 0.92
2016-09-07T02:39:50.057110: step 963, loss 0.0541003, acc 0.98
2016-09-07T02:39:50.796364: step 964, loss 0.0772258, acc 0.96
2016-09-07T02:39:51.493674: step 965, loss 0.0759223, acc 0.98
2016-09-07T02:39:52.181233: step 966, loss 0.096683, acc 0.94
2016-09-07T02:39:52.892225: step 967, loss 0.0306747, acc 0.98
2016-09-07T02:39:53.583869: step 968, loss 0.0934859, acc 0.92
2016-09-07T02:39:54.247058: step 969, loss 0.0816444, acc 0.98
2016-09-07T02:39:54.941855: step 970, loss 0.0331636, acc 1
2016-09-07T02:39:55.616801: step 971, loss 0.070833, acc 0.98
2016-09-07T02:39:56.279851: step 972, loss 0.208314, acc 0.92
2016-09-07T02:39:56.968923: step 973, loss 0.221657, acc 0.92
2016-09-07T02:39:57.657511: step 974, loss 0.102814, acc 0.96
2016-09-07T02:39:58.346540: step 975, loss 0.182655, acc 0.92
2016-09-07T02:39:59.010912: step 976, loss 0.0894194, acc 0.98
2016-09-07T02:39:59.731092: step 977, loss 0.12442, acc 0.92
2016-09-07T02:40:00.432051: step 978, loss 0.129079, acc 0.96
2016-09-07T02:40:01.111839: step 979, loss 0.0926713, acc 0.94
2016-09-07T02:40:01.804429: step 980, loss 0.0406975, acc 1
2016-09-07T02:40:02.486756: step 981, loss 0.0366353, acc 1
2016-09-07T02:40:03.202617: step 982, loss 0.122548, acc 0.98
2016-09-07T02:40:03.892209: step 983, loss 0.0375881, acc 1
2016-09-07T02:40:04.578244: step 984, loss 0.0874385, acc 0.98
2016-09-07T02:40:05.295975: step 985, loss 0.121098, acc 0.96
2016-09-07T02:40:05.984586: step 986, loss 0.0883642, acc 0.98
2016-09-07T02:40:06.681674: step 987, loss 0.0134365, acc 1
2016-09-07T02:40:07.322983: step 988, loss 0.0629243, acc 0.98
2016-09-07T02:40:08.045462: step 989, loss 0.0364371, acc 1
2016-09-07T02:40:08.708761: step 990, loss 0.0855393, acc 0.96
2016-09-07T02:40:09.385236: step 991, loss 0.116789, acc 0.98
2016-09-07T02:40:10.066763: step 992, loss 0.0757378, acc 1
2016-09-07T02:40:10.776219: step 993, loss 0.129661, acc 0.92
2016-09-07T02:40:11.476556: step 994, loss 0.0517003, acc 0.98
2016-09-07T02:40:12.166415: step 995, loss 0.0663006, acc 0.96
2016-09-07T02:40:12.877148: step 996, loss 0.0798895, acc 0.96
2016-09-07T02:40:13.562142: step 997, loss 0.120631, acc 0.98
2016-09-07T02:40:14.262911: step 998, loss 0.0858379, acc 0.94
2016-09-07T02:40:14.927304: step 999, loss 0.10017, acc 0.96
2016-09-07T02:40:15.598942: step 1000, loss 0.0625959, acc 0.98

Evaluation:
2016-09-07T02:40:18.961068: step 1000, loss 0.846695, acc 0.771107

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-1000

2016-09-07T02:40:20.634697: step 1001, loss 0.103155, acc 0.94
2016-09-07T02:40:21.333425: step 1002, loss 0.0704113, acc 0.98
2016-09-07T02:40:22.017977: step 1003, loss 0.0346129, acc 1
2016-09-07T02:40:22.727070: step 1004, loss 0.068073, acc 0.98
2016-09-07T02:40:23.403227: step 1005, loss 0.0395536, acc 0.98
2016-09-07T02:40:24.077147: step 1006, loss 0.0835513, acc 0.96
2016-09-07T02:40:24.766284: step 1007, loss 0.0524202, acc 0.98
2016-09-07T02:40:25.440473: step 1008, loss 0.0475667, acc 0.98
2016-09-07T02:40:26.140023: step 1009, loss 0.0980609, acc 0.96
2016-09-07T02:40:26.841984: step 1010, loss 0.0889535, acc 0.96
2016-09-07T02:40:27.522447: step 1011, loss 0.202838, acc 0.92
2016-09-07T02:40:28.209069: step 1012, loss 0.0840956, acc 0.96
2016-09-07T02:40:28.894276: step 1013, loss 0.129423, acc 0.92
2016-09-07T02:40:29.563892: step 1014, loss 0.0800552, acc 0.96
2016-09-07T02:40:30.206319: step 1015, loss 0.0826222, acc 0.98
2016-09-07T02:40:30.923398: step 1016, loss 0.120554, acc 0.96
2016-09-07T02:40:31.607423: step 1017, loss 0.15953, acc 0.96
2016-09-07T02:40:32.283841: step 1018, loss 0.157299, acc 0.9
2016-09-07T02:40:32.960333: step 1019, loss 0.153277, acc 0.92
2016-09-07T02:40:33.651459: step 1020, loss 0.0714447, acc 0.98
2016-09-07T02:40:34.349965: step 1021, loss 0.112381, acc 0.92
2016-09-07T02:40:35.010053: step 1022, loss 0.0278268, acc 1
2016-09-07T02:40:35.718793: step 1023, loss 0.149152, acc 0.96
2016-09-07T02:40:36.394976: step 1024, loss 0.122183, acc 0.96
2016-09-07T02:40:37.076188: step 1025, loss 0.0609456, acc 0.98
2016-09-07T02:40:37.793953: step 1026, loss 0.234474, acc 0.92
2016-09-07T02:40:38.491098: step 1027, loss 0.144386, acc 0.96
2016-09-07T02:40:39.187491: step 1028, loss 0.100152, acc 0.94
2016-09-07T02:40:39.859858: step 1029, loss 0.0546478, acc 1
2016-09-07T02:40:40.560920: step 1030, loss 0.112096, acc 0.98
2016-09-07T02:40:41.252594: step 1031, loss 0.169553, acc 0.94
2016-09-07T02:40:41.937348: step 1032, loss 0.129015, acc 0.92
2016-09-07T02:40:42.623445: step 1033, loss 0.0896015, acc 0.94
2016-09-07T02:40:43.315136: step 1034, loss 0.0888751, acc 0.96
2016-09-07T02:40:43.999111: step 1035, loss 0.0713872, acc 0.98
2016-09-07T02:40:44.697656: step 1036, loss 0.0871647, acc 0.94
2016-09-07T02:40:45.413811: step 1037, loss 0.167472, acc 0.96
2016-09-07T02:40:46.104027: step 1038, loss 0.0620053, acc 0.98
2016-09-07T02:40:46.801562: step 1039, loss 0.161265, acc 0.94
2016-09-07T02:40:47.486497: step 1040, loss 0.104333, acc 0.92
2016-09-07T02:40:48.172564: step 1041, loss 0.0721205, acc 0.94
2016-09-07T02:40:48.879675: step 1042, loss 0.242576, acc 0.92
2016-09-07T02:40:49.544481: step 1043, loss 0.0235039, acc 1
2016-09-07T02:40:50.251174: step 1044, loss 0.0928615, acc 0.96
2016-09-07T02:40:50.925594: step 1045, loss 0.0985663, acc 0.98
2016-09-07T02:40:51.627895: step 1046, loss 0.0609174, acc 0.96
2016-09-07T02:40:52.309191: step 1047, loss 0.0522993, acc 1
2016-09-07T02:40:52.974048: step 1048, loss 0.0370897, acc 0.98
2016-09-07T02:40:53.692659: step 1049, loss 0.0448703, acc 1
2016-09-07T02:40:54.360846: step 1050, loss 0.0580639, acc 1
2016-09-07T02:40:55.070183: step 1051, loss 0.153331, acc 0.92
2016-09-07T02:40:55.749477: step 1052, loss 0.0911255, acc 0.98
2016-09-07T02:40:56.438202: step 1053, loss 0.10892, acc 0.94
2016-09-07T02:40:57.121486: step 1054, loss 0.0285082, acc 1
2016-09-07T02:40:57.806336: step 1055, loss 0.0874121, acc 0.94
2016-09-07T02:40:58.508035: step 1056, loss 0.0987622, acc 0.94
2016-09-07T02:40:59.167755: step 1057, loss 0.0335018, acc 0.98
2016-09-07T02:40:59.859895: step 1058, loss 0.138635, acc 0.98
2016-09-07T02:41:00.593145: step 1059, loss 0.0449114, acc 0.96
2016-09-07T02:41:01.282483: step 1060, loss 0.0214734, acc 1
2016-09-07T02:41:01.976948: step 1061, loss 0.0252173, acc 0.98
2016-09-07T02:41:02.670012: step 1062, loss 0.433884, acc 0.96
2016-09-07T02:41:03.374790: step 1063, loss 0.0664897, acc 0.96
2016-09-07T02:41:04.043662: step 1064, loss 0.177838, acc 0.94
2016-09-07T02:41:04.727931: step 1065, loss 0.0756041, acc 0.96
2016-09-07T02:41:05.407700: step 1066, loss 0.0895354, acc 0.96
2016-09-07T02:41:06.083549: step 1067, loss 0.0762669, acc 0.96
2016-09-07T02:41:06.760916: step 1068, loss 0.113056, acc 0.94
2016-09-07T02:41:07.442919: step 1069, loss 0.128295, acc 0.94
2016-09-07T02:41:08.146384: step 1070, loss 0.11664, acc 0.98
2016-09-07T02:41:08.833216: step 1071, loss 0.216523, acc 0.88
2016-09-07T02:41:09.516572: step 1072, loss 0.0587569, acc 0.98
2016-09-07T02:41:10.198650: step 1073, loss 0.0431449, acc 1
2016-09-07T02:41:10.901967: step 1074, loss 0.0536277, acc 1
2016-09-07T02:41:11.592719: step 1075, loss 0.130087, acc 0.94
2016-09-07T02:41:12.277095: step 1076, loss 0.0917599, acc 0.98
2016-09-07T02:41:12.998246: step 1077, loss 0.143173, acc 0.94
2016-09-07T02:41:13.679529: step 1078, loss 0.112532, acc 0.96
2016-09-07T02:41:14.377604: step 1079, loss 0.0743521, acc 0.94
2016-09-07T02:41:15.074890: step 1080, loss 0.125525, acc 0.92
2016-09-07T02:41:15.764829: step 1081, loss 0.101916, acc 0.94
2016-09-07T02:41:16.457023: step 1082, loss 0.128745, acc 0.96
2016-09-07T02:41:17.144091: step 1083, loss 0.0813579, acc 0.96
2016-09-07T02:41:17.852984: step 1084, loss 0.0315984, acc 1
2016-09-07T02:41:18.540943: step 1085, loss 0.156952, acc 0.94
2016-09-07T02:41:19.241710: step 1086, loss 0.122015, acc 0.96
2016-09-07T02:41:19.939095: step 1087, loss 0.0499278, acc 0.98
2016-09-07T02:41:20.638888: step 1088, loss 0.053622, acc 0.96
2016-09-07T02:41:21.329430: step 1089, loss 0.0893148, acc 0.98
2016-09-07T02:41:22.006161: step 1090, loss 0.0430088, acc 1
2016-09-07T02:41:22.734016: step 1091, loss 0.0619077, acc 0.98
2016-09-07T02:41:23.411131: step 1092, loss 0.132847, acc 0.98
2016-09-07T02:41:24.081824: step 1093, loss 0.0851027, acc 0.96
2016-09-07T02:41:24.773783: step 1094, loss 0.0821321, acc 0.98
2016-09-07T02:41:25.460823: step 1095, loss 0.115175, acc 0.94
2016-09-07T02:41:26.156827: step 1096, loss 0.137033, acc 0.92
2016-09-07T02:41:26.824500: step 1097, loss 0.198994, acc 0.9
2016-09-07T02:41:27.539073: step 1098, loss 0.114417, acc 0.92
2016-09-07T02:41:28.239893: step 1099, loss 0.0315219, acc 0.98
2016-09-07T02:41:28.937155: step 1100, loss 0.113819, acc 0.96

Evaluation:
2016-09-07T02:41:32.307905: step 1100, loss 1.05079, acc 0.774859

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-1100

2016-09-07T02:41:34.061179: step 1101, loss 0.0633836, acc 0.96
2016-09-07T02:41:34.772178: step 1102, loss 0.0419089, acc 1
2016-09-07T02:41:35.463620: step 1103, loss 0.215031, acc 0.9
2016-09-07T02:41:36.160794: step 1104, loss 0.125245, acc 0.98
2016-09-07T02:41:36.847496: step 1105, loss 0.141289, acc 0.96
2016-09-07T02:41:37.554490: step 1106, loss 0.0715244, acc 0.98
2016-09-07T02:41:38.253103: step 1107, loss 0.116019, acc 0.92
2016-09-07T02:41:38.898779: step 1108, loss 0.157849, acc 0.94
2016-09-07T02:41:39.594237: step 1109, loss 0.0247002, acc 1
2016-09-07T02:41:40.288863: step 1110, loss 0.112969, acc 0.96
2016-09-07T02:41:40.976139: step 1111, loss 0.0418789, acc 1
2016-09-07T02:41:41.650763: step 1112, loss 0.115004, acc 0.96
2016-09-07T02:41:42.344722: step 1113, loss 0.0547353, acc 0.98
2016-09-07T02:41:43.059714: step 1114, loss 0.118638, acc 0.96
2016-09-07T02:41:43.736294: step 1115, loss 0.256159, acc 0.88
2016-09-07T02:41:44.430754: step 1116, loss 0.12419, acc 0.94
2016-09-07T02:41:45.141888: step 1117, loss 0.102278, acc 0.94
2016-09-07T02:41:45.819567: step 1118, loss 0.206174, acc 0.94
2016-09-07T02:41:46.525839: step 1119, loss 0.173072, acc 0.94
2016-09-07T02:41:47.198256: step 1120, loss 0.120188, acc 0.94
2016-09-07T02:41:47.894165: step 1121, loss 0.0619809, acc 1
2016-09-07T02:41:48.578816: step 1122, loss 0.0494204, acc 0.98
2016-09-07T02:41:49.268004: step 1123, loss 0.174765, acc 0.92
2016-09-07T02:41:49.959793: step 1124, loss 0.160619, acc 0.96
2016-09-07T02:41:50.650269: step 1125, loss 0.185292, acc 0.88
2016-09-07T02:41:51.356636: step 1126, loss 0.120574, acc 0.92
2016-09-07T02:41:52.027295: step 1127, loss 0.0688948, acc 0.98
2016-09-07T02:41:52.741226: step 1128, loss 0.0998118, acc 0.92
2016-09-07T02:41:53.426916: step 1129, loss 0.0502399, acc 1
2016-09-07T02:41:54.114665: step 1130, loss 0.19549, acc 0.96
2016-09-07T02:41:54.799899: step 1131, loss 0.152849, acc 0.92
2016-09-07T02:41:55.486342: step 1132, loss 0.0945108, acc 0.94
2016-09-07T02:41:56.216652: step 1133, loss 0.219493, acc 0.96
2016-09-07T02:41:56.888683: step 1134, loss 0.0761054, acc 0.98
2016-09-07T02:41:57.590632: step 1135, loss 0.127847, acc 0.96
2016-09-07T02:41:58.283381: step 1136, loss 0.127272, acc 0.96
2016-09-07T02:41:58.974081: step 1137, loss 0.107394, acc 0.96
2016-09-07T02:41:59.663637: step 1138, loss 0.0942972, acc 0.94
2016-09-07T02:42:00.389353: step 1139, loss 0.266925, acc 0.9
2016-09-07T02:42:01.070679: step 1140, loss 0.161794, acc 0.9
2016-09-07T02:42:01.745546: step 1141, loss 0.0845573, acc 0.96
2016-09-07T02:42:02.437416: step 1142, loss 0.118217, acc 0.94
2016-09-07T02:42:03.119134: step 1143, loss 0.0539197, acc 1
2016-09-07T02:42:03.801651: step 1144, loss 0.18171, acc 0.96
2016-09-07T02:42:04.481607: step 1145, loss 0.141386, acc 0.94
2016-09-07T02:42:05.153262: step 1146, loss 0.0810678, acc 0.98
2016-09-07T02:42:05.838912: step 1147, loss 0.310808, acc 0.88
2016-09-07T02:42:06.509453: step 1148, loss 0.0942898, acc 0.94
2016-09-07T02:42:07.212654: step 1149, loss 0.105227, acc 0.94
2016-09-07T02:42:07.909310: step 1150, loss 0.140131, acc 0.94
2016-09-07T02:42:08.603370: step 1151, loss 0.119805, acc 0.92
2016-09-07T02:42:09.236685: step 1152, loss 0.0932906, acc 0.977273
2016-09-07T02:42:09.964989: step 1153, loss 0.0432641, acc 0.98
2016-09-07T02:42:10.671117: step 1154, loss 0.0776513, acc 0.96
2016-09-07T02:42:11.337504: step 1155, loss 0.200824, acc 0.9
2016-09-07T02:42:12.039659: step 1156, loss 0.0709601, acc 1
2016-09-07T02:42:12.722985: step 1157, loss 0.0307029, acc 1
2016-09-07T02:42:13.403389: step 1158, loss 0.0566495, acc 0.98
2016-09-07T02:42:14.083965: step 1159, loss 0.0409335, acc 1
2016-09-07T02:42:14.780586: step 1160, loss 0.121062, acc 0.98
2016-09-07T02:42:15.486789: step 1161, loss 0.0314957, acc 0.98
2016-09-07T02:42:16.146946: step 1162, loss 0.0997088, acc 0.96
2016-09-07T02:42:16.857179: step 1163, loss 0.0234276, acc 1
2016-09-07T02:42:17.568205: step 1164, loss 0.0507692, acc 0.96
2016-09-07T02:42:18.257514: step 1165, loss 0.0882727, acc 0.94
2016-09-07T02:42:18.946686: step 1166, loss 0.102739, acc 0.94
2016-09-07T02:42:19.647667: step 1167, loss 0.113246, acc 0.96
2016-09-07T02:42:20.356177: step 1168, loss 0.0304089, acc 0.98
2016-09-07T02:42:21.029316: step 1169, loss 0.0878729, acc 0.96
2016-09-07T02:42:21.704157: step 1170, loss 0.145246, acc 0.94
2016-09-07T02:42:22.406859: step 1171, loss 0.0606021, acc 0.96
2016-09-07T02:42:23.093855: step 1172, loss 0.115629, acc 0.94
2016-09-07T02:42:23.765962: step 1173, loss 0.0704956, acc 0.96
2016-09-07T02:42:24.463911: step 1174, loss 0.10225, acc 0.96
2016-09-07T02:42:25.171925: step 1175, loss 0.271316, acc 0.9
2016-09-07T02:42:25.859227: step 1176, loss 0.0784456, acc 0.96
2016-09-07T02:42:26.527899: step 1177, loss 0.0573811, acc 0.98
2016-09-07T02:42:27.202332: step 1178, loss 0.0238016, acc 1
2016-09-07T02:42:27.893841: step 1179, loss 0.0333507, acc 1
2016-09-07T02:42:28.564744: step 1180, loss 0.0434672, acc 0.98
2016-09-07T02:42:29.253982: step 1181, loss 0.0494147, acc 1
2016-09-07T02:42:29.961687: step 1182, loss 0.0248774, acc 0.98
2016-09-07T02:42:30.648909: step 1183, loss 0.0791753, acc 0.96
2016-09-07T02:42:31.331103: step 1184, loss 0.103691, acc 0.94
2016-09-07T02:42:32.001551: step 1185, loss 0.0634204, acc 0.98
2016-09-07T02:42:32.697806: step 1186, loss 0.0409271, acc 1
2016-09-07T02:42:33.371885: step 1187, loss 0.0635948, acc 0.96
2016-09-07T02:42:34.048586: step 1188, loss 0.0559966, acc 0.98
2016-09-07T02:42:34.764112: step 1189, loss 0.0729034, acc 0.98
2016-09-07T02:42:35.447523: step 1190, loss 0.106876, acc 0.94
2016-09-07T02:42:36.132412: step 1191, loss 0.0356481, acc 0.98
2016-09-07T02:42:36.834655: step 1192, loss 0.0120508, acc 1
2016-09-07T02:42:37.525432: step 1193, loss 0.0843238, acc 0.94
2016-09-07T02:42:38.197451: step 1194, loss 0.0377784, acc 0.98
2016-09-07T02:42:38.884325: step 1195, loss 0.0403528, acc 1
2016-09-07T02:42:39.584918: step 1196, loss 0.0377226, acc 0.96
2016-09-07T02:42:40.236424: step 1197, loss 0.0938068, acc 0.94
2016-09-07T02:42:40.913159: step 1198, loss 0.171993, acc 0.92
2016-09-07T02:42:41.597604: step 1199, loss 0.0608269, acc 0.98
2016-09-07T02:42:42.288868: step 1200, loss 0.075358, acc 0.96

Evaluation:
2016-09-07T02:42:45.652738: step 1200, loss 0.963147, acc 0.786116

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-1200

2016-09-07T02:42:47.358222: step 1201, loss 0.0455867, acc 0.98
2016-09-07T02:42:48.079531: step 1202, loss 0.0566845, acc 0.98
2016-09-07T02:42:48.766685: step 1203, loss 0.0386683, acc 1
2016-09-07T02:42:49.447869: step 1204, loss 0.063029, acc 0.98
2016-09-07T02:42:50.128385: step 1205, loss 0.0894556, acc 0.96
2016-09-07T02:42:50.829321: step 1206, loss 0.0812662, acc 0.96
2016-09-07T02:42:51.550387: step 1207, loss 0.112175, acc 0.92
2016-09-07T02:42:52.219718: step 1208, loss 0.102779, acc 0.96
2016-09-07T02:42:52.938517: step 1209, loss 0.0931712, acc 0.94
2016-09-07T02:42:53.633010: step 1210, loss 0.0143563, acc 1
2016-09-07T02:42:54.314945: step 1211, loss 0.0308718, acc 0.98
2016-09-07T02:42:54.999241: step 1212, loss 0.0115817, acc 1
2016-09-07T02:42:55.680440: step 1213, loss 0.149463, acc 0.9
2016-09-07T02:42:56.395928: step 1214, loss 0.194832, acc 0.92
2016-09-07T02:42:57.049695: step 1215, loss 0.033031, acc 0.98
2016-09-07T02:42:57.758369: step 1216, loss 0.0356189, acc 1
2016-09-07T02:42:58.443589: step 1217, loss 0.120918, acc 0.94
2016-09-07T02:42:59.120200: step 1218, loss 0.175191, acc 0.98
2016-09-07T02:42:59.807101: step 1219, loss 0.0797111, acc 0.98
2016-09-07T02:43:00.531814: step 1220, loss 0.104385, acc 0.98
2016-09-07T02:43:01.232900: step 1221, loss 0.0927651, acc 0.96
2016-09-07T02:43:01.902312: step 1222, loss 0.0968104, acc 0.94
2016-09-07T02:43:02.579966: step 1223, loss 0.0503485, acc 0.96
2016-09-07T02:43:03.265875: step 1224, loss 0.0408836, acc 1
2016-09-07T02:43:03.961945: step 1225, loss 0.0348812, acc 0.98
2016-09-07T02:43:04.652688: step 1226, loss 0.0330905, acc 0.98
2016-09-07T02:43:05.347013: step 1227, loss 0.140067, acc 0.96
2016-09-07T02:43:06.050506: step 1228, loss 0.130661, acc 0.92
2016-09-07T02:43:06.715572: step 1229, loss 0.127934, acc 0.9
2016-09-07T02:43:07.402496: step 1230, loss 0.208266, acc 0.94
2016-09-07T02:43:08.086125: step 1231, loss 0.082447, acc 0.96
2016-09-07T02:43:08.768622: step 1232, loss 0.0651495, acc 0.98
2016-09-07T02:43:09.446360: step 1233, loss 0.075608, acc 0.96
2016-09-07T02:43:10.132195: step 1234, loss 0.0564304, acc 0.98
2016-09-07T02:43:10.840764: step 1235, loss 0.0523702, acc 0.96
2016-09-07T02:43:11.502091: step 1236, loss 0.103281, acc 0.94
2016-09-07T02:43:12.195283: step 1237, loss 0.041801, acc 1
2016-09-07T02:43:12.899189: step 1238, loss 0.176214, acc 0.96
2016-09-07T02:43:13.580350: step 1239, loss 0.0695707, acc 0.98
2016-09-07T02:43:14.275818: step 1240, loss 0.0822837, acc 0.98
2016-09-07T02:43:14.960085: step 1241, loss 0.0752594, acc 0.96
2016-09-07T02:43:15.683517: step 1242, loss 0.118839, acc 0.94
2016-09-07T02:43:16.363255: step 1243, loss 0.0775845, acc 0.96
2016-09-07T02:43:17.034979: step 1244, loss 0.0735437, acc 0.94
2016-09-07T02:43:17.722309: step 1245, loss 0.0322233, acc 0.98
2016-09-07T02:43:18.414988: step 1246, loss 0.100653, acc 0.94
2016-09-07T02:43:19.103973: step 1247, loss 0.0648342, acc 0.98
2016-09-07T02:43:19.784751: step 1248, loss 0.0346558, acc 0.98
2016-09-07T02:43:20.496588: step 1249, loss 0.0350025, acc 0.98
2016-09-07T02:43:21.190226: step 1250, loss 0.0975623, acc 0.96
2016-09-07T02:43:21.889536: step 1251, loss 0.0945744, acc 0.98
2016-09-07T02:43:22.604542: step 1252, loss 0.0896713, acc 0.96
2016-09-07T02:43:23.289816: step 1253, loss 0.125822, acc 0.96
2016-09-07T02:43:23.997010: step 1254, loss 0.170393, acc 0.96
2016-09-07T02:43:24.657386: step 1255, loss 0.11189, acc 0.96
2016-09-07T02:43:25.349929: step 1256, loss 0.0861008, acc 0.94
2016-09-07T02:43:26.027874: step 1257, loss 0.0533454, acc 0.98
2016-09-07T02:43:26.709603: step 1258, loss 0.070198, acc 0.96
2016-09-07T02:43:27.400837: step 1259, loss 0.0519506, acc 0.98
2016-09-07T02:43:28.070701: step 1260, loss 0.0588827, acc 0.98
2016-09-07T02:43:28.766948: step 1261, loss 0.138969, acc 0.94
2016-09-07T02:43:29.428572: step 1262, loss 0.0221928, acc 1
2016-09-07T02:43:30.131180: step 1263, loss 0.0486045, acc 0.96
2016-09-07T02:43:30.832368: step 1264, loss 0.0511774, acc 0.96
2016-09-07T02:43:31.553830: step 1265, loss 0.2061, acc 0.9
2016-09-07T02:43:32.263241: step 1266, loss 0.0721507, acc 0.96
2016-09-07T02:43:32.944376: step 1267, loss 0.0231018, acc 0.98
2016-09-07T02:43:33.673762: step 1268, loss 0.0714241, acc 0.96
2016-09-07T02:43:34.336353: step 1269, loss 0.0466407, acc 0.98
2016-09-07T02:43:35.026711: step 1270, loss 0.157585, acc 0.96
2016-09-07T02:43:35.711010: step 1271, loss 0.166889, acc 0.88
2016-09-07T02:43:36.413062: step 1272, loss 0.169622, acc 0.94
2016-09-07T02:43:37.135177: step 1273, loss 0.0392897, acc 1
2016-09-07T02:43:37.825126: step 1274, loss 0.173744, acc 0.9
2016-09-07T02:43:38.534080: step 1275, loss 0.062155, acc 0.98
2016-09-07T02:43:39.220865: step 1276, loss 0.0502847, acc 0.98
2016-09-07T02:43:39.932239: step 1277, loss 0.118417, acc 0.98
2016-09-07T02:43:40.636325: step 1278, loss 0.196763, acc 0.9
2016-09-07T02:43:41.329538: step 1279, loss 0.082131, acc 0.98
2016-09-07T02:43:42.051662: step 1280, loss 0.0703151, acc 0.96
2016-09-07T02:43:42.703696: step 1281, loss 0.0361028, acc 1
2016-09-07T02:43:43.399515: step 1282, loss 0.0743913, acc 0.94
2016-09-07T02:43:44.087102: step 1283, loss 0.11539, acc 0.96
2016-09-07T02:43:44.787744: step 1284, loss 0.107654, acc 0.94
2016-09-07T02:43:45.489928: step 1285, loss 0.102069, acc 0.98
2016-09-07T02:43:46.193436: step 1286, loss 0.13182, acc 0.92
2016-09-07T02:43:46.907037: step 1287, loss 0.119516, acc 0.98
2016-09-07T02:43:47.566255: step 1288, loss 0.0570373, acc 0.96
2016-09-07T02:43:48.259398: step 1289, loss 0.0714984, acc 0.98
2016-09-07T02:43:48.943469: step 1290, loss 0.0449784, acc 1
2016-09-07T02:43:49.624100: step 1291, loss 0.0995644, acc 0.94
2016-09-07T02:43:50.335800: step 1292, loss 0.0697147, acc 0.98
2016-09-07T02:43:51.018260: step 1293, loss 0.120572, acc 0.96
2016-09-07T02:43:51.730388: step 1294, loss 0.141284, acc 0.92
2016-09-07T02:43:52.398226: step 1295, loss 0.0791834, acc 0.96
2016-09-07T02:43:53.074721: step 1296, loss 0.0363293, acc 1
2016-09-07T02:43:53.793705: step 1297, loss 0.0324438, acc 1
2016-09-07T02:43:54.493275: step 1298, loss 0.0611292, acc 0.96
2016-09-07T02:43:55.190686: step 1299, loss 0.109935, acc 0.96
2016-09-07T02:43:55.898911: step 1300, loss 0.0508996, acc 0.98

Evaluation:
2016-09-07T02:43:59.265909: step 1300, loss 0.96173, acc 0.773921

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-1300

2016-09-07T02:44:00.973306: step 1301, loss 0.0740441, acc 0.98
2016-09-07T02:44:01.679867: step 1302, loss 0.0372476, acc 1
2016-09-07T02:44:02.373894: step 1303, loss 0.100305, acc 0.96
2016-09-07T02:44:03.064225: step 1304, loss 0.0579101, acc 0.96
2016-09-07T02:44:03.764812: step 1305, loss 0.091624, acc 0.96
2016-09-07T02:44:04.445776: step 1306, loss 0.0895354, acc 0.94
2016-09-07T02:44:05.152154: step 1307, loss 0.168681, acc 0.9
2016-09-07T02:44:05.828596: step 1308, loss 0.0247291, acc 0.98
2016-09-07T02:44:06.502169: step 1309, loss 0.0670722, acc 0.96
2016-09-07T02:44:07.210290: step 1310, loss 0.0798587, acc 0.96
2016-09-07T02:44:07.903287: step 1311, loss 0.213587, acc 0.94
2016-09-07T02:44:08.613650: step 1312, loss 0.17011, acc 0.96
2016-09-07T02:44:09.301554: step 1313, loss 0.0299594, acc 0.98
2016-09-07T02:44:09.984809: step 1314, loss 0.0867471, acc 0.98
2016-09-07T02:44:10.645790: step 1315, loss 0.0229368, acc 1
2016-09-07T02:44:11.335141: step 1316, loss 0.109793, acc 0.98
2016-09-07T02:44:12.026446: step 1317, loss 0.0724455, acc 0.96
2016-09-07T02:44:12.714539: step 1318, loss 0.0579767, acc 1
2016-09-07T02:44:13.439142: step 1319, loss 0.0314706, acc 0.98
2016-09-07T02:44:14.122133: step 1320, loss 0.216625, acc 0.92
2016-09-07T02:44:14.793024: step 1321, loss 0.0583653, acc 0.96
2016-09-07T02:44:15.483451: step 1322, loss 0.0339772, acc 1
2016-09-07T02:44:16.160181: step 1323, loss 0.0177601, acc 1
2016-09-07T02:44:16.840884: step 1324, loss 0.141902, acc 0.92
2016-09-07T02:44:17.526388: step 1325, loss 0.0916443, acc 0.96
2016-09-07T02:44:18.226784: step 1326, loss 0.0892627, acc 0.96
2016-09-07T02:44:18.900432: step 1327, loss 0.123944, acc 0.98
2016-09-07T02:44:19.569538: step 1328, loss 0.021763, acc 1
2016-09-07T02:44:20.261721: step 1329, loss 0.0464772, acc 1
2016-09-07T02:44:20.943528: step 1330, loss 0.117625, acc 0.9
2016-09-07T02:44:21.627609: step 1331, loss 0.153955, acc 0.94
2016-09-07T02:44:22.302012: step 1332, loss 0.0612753, acc 0.94
2016-09-07T02:44:23.020869: step 1333, loss 0.0302632, acc 1
2016-09-07T02:44:23.698878: step 1334, loss 0.128586, acc 0.96
2016-09-07T02:44:24.382585: step 1335, loss 0.21746, acc 0.92
2016-09-07T02:44:25.091721: step 1336, loss 0.155748, acc 0.94
2016-09-07T02:44:25.802340: step 1337, loss 0.0698476, acc 0.96
2016-09-07T02:44:26.507563: step 1338, loss 0.165897, acc 0.92
2016-09-07T02:44:27.194732: step 1339, loss 0.198188, acc 0.94
2016-09-07T02:44:27.889815: step 1340, loss 0.00733437, acc 1
2016-09-07T02:44:28.562791: step 1341, loss 0.0712769, acc 0.98
2016-09-07T02:44:29.260714: step 1342, loss 0.0736148, acc 0.96
2016-09-07T02:44:29.985431: step 1343, loss 0.0365187, acc 1
2016-09-07T02:44:30.636239: step 1344, loss 0.10971, acc 0.954545
2016-09-07T02:44:31.325534: step 1345, loss 0.144072, acc 0.94
2016-09-07T02:44:31.988272: step 1346, loss 0.112705, acc 0.94
2016-09-07T02:44:32.712221: step 1347, loss 0.154301, acc 0.92
2016-09-07T02:44:33.406647: step 1348, loss 0.130041, acc 0.94
2016-09-07T02:44:34.093875: step 1349, loss 0.155264, acc 0.96
2016-09-07T02:44:34.774995: step 1350, loss 0.0842264, acc 0.98
2016-09-07T02:44:35.474626: step 1351, loss 0.0458239, acc 0.98
2016-09-07T02:44:36.176424: step 1352, loss 0.140424, acc 0.94
2016-09-07T02:44:36.845538: step 1353, loss 0.164399, acc 0.94
2016-09-07T02:44:37.541654: step 1354, loss 0.150238, acc 0.94
2016-09-07T02:44:38.230438: step 1355, loss 0.127537, acc 0.92
2016-09-07T02:44:38.931337: step 1356, loss 0.0678174, acc 0.96
2016-09-07T02:44:39.619481: step 1357, loss 0.119068, acc 0.96
2016-09-07T02:44:40.305690: step 1358, loss 0.0877875, acc 0.96
2016-09-07T02:44:41.007297: step 1359, loss 0.0694226, acc 0.98
2016-09-07T02:44:41.670795: step 1360, loss 0.0194239, acc 1
2016-09-07T02:44:42.362053: step 1361, loss 0.154891, acc 0.96
2016-09-07T02:44:43.049107: step 1362, loss 0.0902104, acc 0.96
2016-09-07T02:44:43.737240: step 1363, loss 0.12963, acc 0.98
2016-09-07T02:44:44.410037: step 1364, loss 0.0389844, acc 0.98
2016-09-07T02:44:45.101769: step 1365, loss 0.127514, acc 0.92
2016-09-07T02:44:45.802416: step 1366, loss 0.0884892, acc 0.96
2016-09-07T02:44:46.466313: step 1367, loss 0.127006, acc 0.96
2016-09-07T02:44:47.173841: step 1368, loss 0.0715253, acc 0.98
2016-09-07T02:44:47.853042: step 1369, loss 0.0483397, acc 0.98
2016-09-07T02:44:48.555797: step 1370, loss 0.0742196, acc 0.94
2016-09-07T02:44:49.244146: step 1371, loss 0.0399065, acc 0.98
2016-09-07T02:44:49.938881: step 1372, loss 0.0313059, acc 1
2016-09-07T02:44:50.652365: step 1373, loss 0.0652032, acc 0.96
2016-09-07T02:44:51.333863: step 1374, loss 0.0280773, acc 1
2016-09-07T02:44:52.038130: step 1375, loss 0.0759995, acc 0.94
2016-09-07T02:44:52.724885: step 1376, loss 0.198166, acc 0.92
2016-09-07T02:44:53.407948: step 1377, loss 0.117408, acc 0.94
2016-09-07T02:44:54.088887: step 1378, loss 0.12929, acc 0.94
2016-09-07T02:44:54.789452: step 1379, loss 0.0740438, acc 0.94
2016-09-07T02:44:55.492963: step 1380, loss 0.229277, acc 0.92
2016-09-07T02:44:56.158457: step 1381, loss 0.0276826, acc 1
2016-09-07T02:44:56.857242: step 1382, loss 0.136755, acc 0.94
2016-09-07T02:44:57.555012: step 1383, loss 0.0310328, acc 0.98
2016-09-07T02:44:58.260652: step 1384, loss 0.0211462, acc 1
2016-09-07T02:44:58.959969: step 1385, loss 0.0819171, acc 0.94
2016-09-07T02:44:59.628189: step 1386, loss 0.109416, acc 0.92
2016-09-07T02:45:00.358603: step 1387, loss 0.0656416, acc 0.98
2016-09-07T02:45:01.047088: step 1388, loss 0.0824986, acc 0.96
2016-09-07T02:45:01.736632: step 1389, loss 0.0707487, acc 0.98
2016-09-07T02:45:02.420293: step 1390, loss 0.0266034, acc 1
2016-09-07T02:45:03.098685: step 1391, loss 0.17576, acc 0.92
2016-09-07T02:45:03.788045: step 1392, loss 0.0410674, acc 0.98
2016-09-07T02:45:04.449351: step 1393, loss 0.0352088, acc 0.98
2016-09-07T02:45:05.150664: step 1394, loss 0.128038, acc 0.96
2016-09-07T02:45:05.813962: step 1395, loss 0.0323911, acc 0.98
2016-09-07T02:45:06.485748: step 1396, loss 0.0914679, acc 0.96
2016-09-07T02:45:07.170071: step 1397, loss 0.0159526, acc 1
2016-09-07T02:45:07.877510: step 1398, loss 0.0352593, acc 0.98
2016-09-07T02:45:08.588925: step 1399, loss 0.0575123, acc 0.98
2016-09-07T02:45:09.251377: step 1400, loss 0.0618283, acc 0.96

Evaluation:
2016-09-07T02:45:12.674170: step 1400, loss 1.03901, acc 0.775797

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-1400

2016-09-07T02:45:14.411633: step 1401, loss 0.0373063, acc 1
2016-09-07T02:45:15.137253: step 1402, loss 0.120406, acc 0.92
2016-09-07T02:45:15.840105: step 1403, loss 0.0872657, acc 0.96
2016-09-07T02:45:16.535371: step 1404, loss 0.009764, acc 1
2016-09-07T02:45:17.243171: step 1405, loss 0.106056, acc 0.94
2016-09-07T02:45:17.931754: step 1406, loss 0.0294107, acc 1
2016-09-07T02:45:18.636102: step 1407, loss 0.0330949, acc 0.98
2016-09-07T02:45:19.336667: step 1408, loss 0.0724717, acc 0.98
2016-09-07T02:45:20.035329: step 1409, loss 0.0295246, acc 1
2016-09-07T02:45:20.749677: step 1410, loss 0.153936, acc 0.9
2016-09-07T02:45:21.439670: step 1411, loss 0.0544819, acc 0.96
2016-09-07T02:45:22.120888: step 1412, loss 0.0447221, acc 0.98
2016-09-07T02:45:22.798531: step 1413, loss 0.0893503, acc 0.98
2016-09-07T02:45:23.499702: step 1414, loss 0.106232, acc 0.98
2016-09-07T02:45:24.186595: step 1415, loss 0.10189, acc 0.98
2016-09-07T02:45:24.856009: step 1416, loss 0.039632, acc 0.98
2016-09-07T02:45:25.561983: step 1417, loss 0.0200061, acc 1
2016-09-07T02:45:26.243252: step 1418, loss 0.0318731, acc 0.98
2016-09-07T02:45:26.928307: step 1419, loss 0.0879946, acc 0.98
2016-09-07T02:45:27.623600: step 1420, loss 0.027462, acc 1
2016-09-07T02:45:28.307209: step 1421, loss 0.069581, acc 0.96
2016-09-07T02:45:29.008041: step 1422, loss 0.0272749, acc 0.98
2016-09-07T02:45:29.699050: step 1423, loss 0.0645764, acc 0.96
2016-09-07T02:45:30.429893: step 1424, loss 0.055826, acc 0.96
2016-09-07T02:45:31.112266: step 1425, loss 0.0483632, acc 0.98
2016-09-07T02:45:31.807985: step 1426, loss 0.0825352, acc 0.94
2016-09-07T02:45:32.514631: step 1427, loss 0.0712503, acc 0.94
2016-09-07T02:45:33.199153: step 1428, loss 0.0449178, acc 0.96
2016-09-07T02:45:33.903224: step 1429, loss 0.0839471, acc 0.98
2016-09-07T02:45:34.610820: step 1430, loss 0.0445117, acc 1
2016-09-07T02:45:35.282026: step 1431, loss 0.0723304, acc 0.96
2016-09-07T02:45:35.971495: step 1432, loss 0.0825389, acc 0.96
2016-09-07T02:45:36.650178: step 1433, loss 0.205663, acc 0.94
2016-09-07T02:45:37.348618: step 1434, loss 0.0878638, acc 0.96
2016-09-07T02:45:38.031056: step 1435, loss 0.113719, acc 0.94
2016-09-07T02:45:38.744213: step 1436, loss 0.150671, acc 0.94
2016-09-07T02:45:39.420436: step 1437, loss 0.109162, acc 0.94
2016-09-07T02:45:40.121145: step 1438, loss 0.0566645, acc 0.98
2016-09-07T02:45:40.824243: step 1439, loss 0.0966675, acc 0.96
2016-09-07T02:45:41.522715: step 1440, loss 0.138656, acc 0.92
2016-09-07T02:45:42.226726: step 1441, loss 0.0618684, acc 0.96
2016-09-07T02:45:42.895081: step 1442, loss 0.119774, acc 0.92
2016-09-07T02:45:43.592879: step 1443, loss 0.0391673, acc 1
2016-09-07T02:45:44.285729: step 1444, loss 0.0856811, acc 0.96
2016-09-07T02:45:44.975207: step 1445, loss 0.0645226, acc 0.96
2016-09-07T02:45:45.663584: step 1446, loss 0.120281, acc 0.96
2016-09-07T02:45:46.342717: step 1447, loss 0.0977331, acc 0.96
2016-09-07T02:45:47.042534: step 1448, loss 0.0447495, acc 0.98
2016-09-07T02:45:47.747053: step 1449, loss 0.0341517, acc 0.98
2016-09-07T02:45:48.443919: step 1450, loss 0.0401202, acc 1
2016-09-07T02:45:49.149639: step 1451, loss 0.0379819, acc 0.98
2016-09-07T02:45:49.839308: step 1452, loss 0.0541549, acc 0.96
2016-09-07T02:45:50.552505: step 1453, loss 0.0269609, acc 1
2016-09-07T02:45:51.247744: step 1454, loss 0.0436614, acc 1
2016-09-07T02:45:51.953357: step 1455, loss 0.0804607, acc 0.94
2016-09-07T02:45:52.617391: step 1456, loss 0.0564297, acc 0.98
2016-09-07T02:45:53.307128: step 1457, loss 0.022597, acc 1
2016-09-07T02:45:54.004870: step 1458, loss 0.0902862, acc 0.98
2016-09-07T02:45:54.693841: step 1459, loss 0.0587987, acc 0.98
2016-09-07T02:45:55.388712: step 1460, loss 0.0884316, acc 0.94
2016-09-07T02:45:56.061905: step 1461, loss 0.0849037, acc 0.98
2016-09-07T02:45:56.768622: step 1462, loss 0.0616553, acc 0.96
2016-09-07T02:45:57.445088: step 1463, loss 0.0528418, acc 0.98
2016-09-07T02:45:58.145011: step 1464, loss 0.103745, acc 0.96
2016-09-07T02:45:58.819394: step 1465, loss 0.0563423, acc 0.94
2016-09-07T02:45:59.514820: step 1466, loss 0.1013, acc 0.94
2016-09-07T02:46:00.248044: step 1467, loss 0.0296446, acc 1
2016-09-07T02:46:00.905094: step 1468, loss 0.0385508, acc 0.98
2016-09-07T02:46:01.596119: step 1469, loss 0.0278308, acc 1
2016-09-07T02:46:02.302247: step 1470, loss 0.0128725, acc 1
2016-09-07T02:46:02.984909: step 1471, loss 0.0570812, acc 0.98
2016-09-07T02:46:03.675994: step 1472, loss 0.110032, acc 0.92
2016-09-07T02:46:04.372621: step 1473, loss 0.163601, acc 0.9
2016-09-07T02:46:05.081587: step 1474, loss 0.043669, acc 0.98
2016-09-07T02:46:05.735929: step 1475, loss 0.0335193, acc 1
2016-09-07T02:46:06.430612: step 1476, loss 0.0994364, acc 0.98
2016-09-07T02:46:07.136841: step 1477, loss 0.0372098, acc 0.98
2016-09-07T02:46:07.833721: step 1478, loss 0.0241311, acc 0.98
2016-09-07T02:46:08.524115: step 1479, loss 0.113545, acc 0.92
2016-09-07T02:46:09.214247: step 1480, loss 0.122596, acc 0.94
2016-09-07T02:46:09.902041: step 1481, loss 0.0468229, acc 0.98
2016-09-07T02:46:10.559133: step 1482, loss 0.0501626, acc 0.98
2016-09-07T02:46:11.244216: step 1483, loss 0.0377408, acc 0.98
2016-09-07T02:46:11.941044: step 1484, loss 0.0516326, acc 0.98
2016-09-07T02:46:12.623419: step 1485, loss 0.298659, acc 0.94
2016-09-07T02:46:13.306260: step 1486, loss 0.051698, acc 0.98
2016-09-07T02:46:14.000626: step 1487, loss 0.0574835, acc 0.98
2016-09-07T02:46:14.686735: step 1488, loss 0.142857, acc 0.94
2016-09-07T02:46:15.351370: step 1489, loss 0.061182, acc 0.98
2016-09-07T02:46:16.058644: step 1490, loss 0.0838398, acc 0.98
2016-09-07T02:46:16.758803: step 1491, loss 0.051165, acc 0.96
2016-09-07T02:46:17.447710: step 1492, loss 0.0780639, acc 0.94
2016-09-07T02:46:18.141089: step 1493, loss 0.0585319, acc 0.98
2016-09-07T02:46:18.838594: step 1494, loss 0.0602564, acc 0.96
2016-09-07T02:46:19.542275: step 1495, loss 0.0475774, acc 0.98
2016-09-07T02:46:20.207908: step 1496, loss 0.0933204, acc 0.94
2016-09-07T02:46:20.911674: step 1497, loss 0.0362422, acc 0.98
2016-09-07T02:46:21.608453: step 1498, loss 0.118682, acc 0.94
2016-09-07T02:46:22.304273: step 1499, loss 0.0486137, acc 0.98
2016-09-07T02:46:23.003823: step 1500, loss 0.0312187, acc 0.98

Evaluation:
2016-09-07T02:46:26.418397: step 1500, loss 1.12877, acc 0.766416

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-1500

2016-09-07T02:46:28.210395: step 1501, loss 0.153813, acc 0.96
2016-09-07T02:46:28.905527: step 1502, loss 0.0596311, acc 1
2016-09-07T02:46:29.585187: step 1503, loss 0.0201276, acc 1
2016-09-07T02:46:30.258962: step 1504, loss 0.157746, acc 0.9
2016-09-07T02:46:30.949826: step 1505, loss 0.0731744, acc 0.96
2016-09-07T02:46:31.655303: step 1506, loss 0.0526275, acc 0.98
2016-09-07T02:46:32.326548: step 1507, loss 0.0351766, acc 1
2016-09-07T02:46:33.016437: step 1508, loss 0.0279739, acc 0.98
2016-09-07T02:46:33.695438: step 1509, loss 0.236731, acc 0.88
2016-09-07T02:46:34.399723: step 1510, loss 0.0925938, acc 0.96
2016-09-07T02:46:35.092689: step 1511, loss 0.0233163, acc 1
2016-09-07T02:46:35.783814: step 1512, loss 0.283564, acc 0.94
2016-09-07T02:46:36.483606: step 1513, loss 0.108113, acc 0.96
2016-09-07T02:46:37.147162: step 1514, loss 0.102235, acc 0.96
2016-09-07T02:46:37.813226: step 1515, loss 0.0329797, acc 1
2016-09-07T02:46:38.497662: step 1516, loss 0.0933509, acc 0.92
2016-09-07T02:46:39.196959: step 1517, loss 0.0341212, acc 0.98
2016-09-07T02:46:39.901252: step 1518, loss 0.0779165, acc 0.96
2016-09-07T02:46:40.577039: step 1519, loss 0.195392, acc 0.94
2016-09-07T02:46:41.270907: step 1520, loss 0.0805998, acc 0.98
2016-09-07T02:46:41.936293: step 1521, loss 0.0505534, acc 1
2016-09-07T02:46:42.609141: step 1522, loss 0.0634962, acc 0.98
2016-09-07T02:46:43.284778: step 1523, loss 0.0936617, acc 0.96
2016-09-07T02:46:43.972098: step 1524, loss 0.0757083, acc 0.94
2016-09-07T02:46:44.658828: step 1525, loss 0.196024, acc 0.96
2016-09-07T02:46:45.357410: step 1526, loss 0.0868043, acc 0.98
2016-09-07T02:46:46.097981: step 1527, loss 0.0706184, acc 0.98
2016-09-07T02:46:46.797513: step 1528, loss 0.112558, acc 0.96
2016-09-07T02:46:47.502788: step 1529, loss 0.144419, acc 0.94
2016-09-07T02:46:48.211934: step 1530, loss 0.27571, acc 0.9
2016-09-07T02:46:48.897341: step 1531, loss 0.0919719, acc 0.98
2016-09-07T02:46:49.614464: step 1532, loss 0.084657, acc 0.94
2016-09-07T02:46:50.259634: step 1533, loss 0.0811637, acc 0.96
2016-09-07T02:46:50.966599: step 1534, loss 0.0631601, acc 0.96
2016-09-07T02:46:51.651754: step 1535, loss 0.039056, acc 1
2016-09-07T02:46:52.302049: step 1536, loss 0.0457377, acc 1
2016-09-07T02:46:53.002739: step 1537, loss 0.0940377, acc 0.94
2016-09-07T02:46:53.687663: step 1538, loss 0.0801227, acc 0.98
2016-09-07T02:46:54.397311: step 1539, loss 0.0772552, acc 0.96
2016-09-07T02:46:55.048192: step 1540, loss 0.0833875, acc 0.96
2016-09-07T02:46:55.746841: step 1541, loss 0.065186, acc 0.98
2016-09-07T02:46:56.433734: step 1542, loss 0.0607144, acc 0.98
2016-09-07T02:46:57.117347: step 1543, loss 0.0501136, acc 0.98
2016-09-07T02:46:57.807254: step 1544, loss 0.0885986, acc 0.96
2016-09-07T02:46:58.496571: step 1545, loss 0.0992117, acc 0.96
2016-09-07T02:46:59.193461: step 1546, loss 0.0366885, acc 1
2016-09-07T02:46:59.896165: step 1547, loss 0.0504632, acc 0.98
2016-09-07T02:47:00.654117: step 1548, loss 0.0487095, acc 0.96
2016-09-07T02:47:01.365542: step 1549, loss 0.0630722, acc 0.96
2016-09-07T02:47:02.057840: step 1550, loss 0.0476747, acc 0.98
2016-09-07T02:47:02.740164: step 1551, loss 0.0445602, acc 0.98
2016-09-07T02:47:03.418667: step 1552, loss 0.00883383, acc 1
2016-09-07T02:47:04.124574: step 1553, loss 0.0256541, acc 1
2016-09-07T02:47:04.801472: step 1554, loss 0.0291261, acc 0.98
2016-09-07T02:47:05.484073: step 1555, loss 0.127429, acc 0.94
2016-09-07T02:47:06.179370: step 1556, loss 0.0388027, acc 0.98
2016-09-07T02:47:06.888632: step 1557, loss 0.0688001, acc 0.98
2016-09-07T02:47:07.611457: step 1558, loss 0.0608679, acc 0.96
2016-09-07T02:47:08.285526: step 1559, loss 0.0322899, acc 0.98
2016-09-07T02:47:08.979298: step 1560, loss 0.131003, acc 0.94
2016-09-07T02:47:09.682181: step 1561, loss 0.0250047, acc 1
2016-09-07T02:47:10.376369: step 1562, loss 0.0176857, acc 1
2016-09-07T02:47:11.067119: step 1563, loss 0.0183771, acc 0.98
2016-09-07T02:47:11.737052: step 1564, loss 0.160325, acc 0.92
2016-09-07T02:47:12.444113: step 1565, loss 0.0551759, acc 0.98
2016-09-07T02:47:13.099854: step 1566, loss 0.0270419, acc 1
2016-09-07T02:47:13.795255: step 1567, loss 0.0247491, acc 1
2016-09-07T02:47:14.470469: step 1568, loss 0.0741163, acc 0.98
2016-09-07T02:47:15.161412: step 1569, loss 0.084867, acc 0.94
2016-09-07T02:47:15.852701: step 1570, loss 0.0730176, acc 0.98
2016-09-07T02:47:16.546389: step 1571, loss 0.051835, acc 0.98
2016-09-07T02:47:17.254360: step 1572, loss 0.145052, acc 0.92
2016-09-07T02:47:17.930141: step 1573, loss 0.0116402, acc 1
2016-09-07T02:47:18.640278: step 1574, loss 0.0313565, acc 0.98
2016-09-07T02:47:19.324993: step 1575, loss 0.128251, acc 0.92
2016-09-07T02:47:20.023782: step 1576, loss 0.0532298, acc 0.98
2016-09-07T02:47:20.707286: step 1577, loss 0.124566, acc 0.96
2016-09-07T02:47:21.398575: step 1578, loss 0.0431846, acc 0.98
2016-09-07T02:47:22.105395: step 1579, loss 0.0440737, acc 0.98
2016-09-07T02:47:22.789733: step 1580, loss 0.0296051, acc 1
2016-09-07T02:47:23.485524: step 1581, loss 0.057246, acc 0.96
2016-09-07T02:47:24.199057: step 1582, loss 0.0511444, acc 0.98
2016-09-07T02:47:24.894688: step 1583, loss 0.028121, acc 0.98
2016-09-07T02:47:25.584445: step 1584, loss 0.070507, acc 0.98
2016-09-07T02:47:26.278840: step 1585, loss 0.0418448, acc 0.98
2016-09-07T02:47:26.992836: step 1586, loss 0.0632385, acc 0.98
2016-09-07T02:47:27.672547: step 1587, loss 0.0552963, acc 0.96
2016-09-07T02:47:28.350835: step 1588, loss 0.188802, acc 0.94
2016-09-07T02:47:29.050552: step 1589, loss 0.0404932, acc 0.98
2016-09-07T02:47:29.738660: step 1590, loss 0.0664396, acc 0.96
2016-09-07T02:47:30.425677: step 1591, loss 0.0888272, acc 0.92
2016-09-07T02:47:31.099393: step 1592, loss 0.039871, acc 1
2016-09-07T02:47:31.799871: step 1593, loss 0.0351537, acc 0.98
2016-09-07T02:47:32.473573: step 1594, loss 0.057186, acc 0.94
2016-09-07T02:47:33.158832: step 1595, loss 0.0130532, acc 1
2016-09-07T02:47:33.874612: step 1596, loss 0.0691813, acc 0.98
2016-09-07T02:47:34.558896: step 1597, loss 0.0482385, acc 0.98
2016-09-07T02:47:35.248855: step 1598, loss 0.0372045, acc 0.96
2016-09-07T02:47:35.918663: step 1599, loss 0.0508283, acc 0.96
2016-09-07T02:47:36.603810: step 1600, loss 0.356821, acc 0.92

Evaluation:
2016-09-07T02:47:40.029817: step 1600, loss 1.0889, acc 0.777674

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-1600

2016-09-07T02:47:41.756547: step 1601, loss 0.0648616, acc 0.96
2016-09-07T02:47:42.448163: step 1602, loss 0.0737615, acc 0.96
2016-09-07T02:47:43.157480: step 1603, loss 0.0554037, acc 0.98
2016-09-07T02:47:43.860921: step 1604, loss 0.080148, acc 0.96
2016-09-07T02:47:44.554766: step 1605, loss 0.0950214, acc 0.96
2016-09-07T02:47:45.243029: step 1606, loss 0.0939654, acc 0.94
2016-09-07T02:47:45.925291: step 1607, loss 0.0603904, acc 0.96
2016-09-07T02:47:46.616092: step 1608, loss 0.0333697, acc 1
2016-09-07T02:47:47.293688: step 1609, loss 0.0235365, acc 1
2016-09-07T02:47:47.959389: step 1610, loss 0.0797959, acc 0.98
2016-09-07T02:47:48.649235: step 1611, loss 0.0690794, acc 0.98
2016-09-07T02:47:49.342337: step 1612, loss 0.0995778, acc 0.94
2016-09-07T02:47:50.043381: step 1613, loss 0.0237608, acc 1
2016-09-07T02:47:50.723270: step 1614, loss 0.108021, acc 0.94
2016-09-07T02:47:51.411362: step 1615, loss 0.0636925, acc 0.98
2016-09-07T02:47:52.095976: step 1616, loss 0.042303, acc 1
2016-09-07T02:47:52.765872: step 1617, loss 0.0669189, acc 0.96
2016-09-07T02:47:53.461916: step 1618, loss 0.18757, acc 0.92
2016-09-07T02:47:54.140383: step 1619, loss 0.0348575, acc 0.98
2016-09-07T02:47:54.835486: step 1620, loss 0.0171221, acc 1
2016-09-07T02:47:55.526951: step 1621, loss 0.0350526, acc 1
2016-09-07T02:47:56.206605: step 1622, loss 0.0362689, acc 0.98
2016-09-07T02:47:56.892891: step 1623, loss 0.006839, acc 1
2016-09-07T02:47:57.565324: step 1624, loss 0.092561, acc 0.96
2016-09-07T02:47:58.267377: step 1625, loss 0.0344038, acc 0.98
2016-09-07T02:47:58.936849: step 1626, loss 0.0600191, acc 0.98
2016-09-07T02:47:59.624231: step 1627, loss 0.0901611, acc 0.94
2016-09-07T02:48:00.360426: step 1628, loss 0.0571599, acc 0.96
2016-09-07T02:48:01.052567: step 1629, loss 0.0106552, acc 1
2016-09-07T02:48:01.754649: step 1630, loss 0.0862544, acc 0.96
2016-09-07T02:48:02.433106: step 1631, loss 0.104669, acc 0.98
2016-09-07T02:48:03.137796: step 1632, loss 0.051797, acc 0.98
2016-09-07T02:48:03.824784: step 1633, loss 0.0695751, acc 0.98
2016-09-07T02:48:04.503624: step 1634, loss 0.115836, acc 0.96
2016-09-07T02:48:05.199800: step 1635, loss 0.033839, acc 0.98
2016-09-07T02:48:05.905317: step 1636, loss 0.0397555, acc 1
2016-09-07T02:48:06.618555: step 1637, loss 0.0177448, acc 0.98
2016-09-07T02:48:07.289683: step 1638, loss 0.0519027, acc 0.96
2016-09-07T02:48:07.990966: step 1639, loss 0.0195914, acc 1
2016-09-07T02:48:08.704371: step 1640, loss 0.10533, acc 0.98
2016-09-07T02:48:09.396458: step 1641, loss 0.0277872, acc 0.98
2016-09-07T02:48:10.075972: step 1642, loss 0.013087, acc 1
2016-09-07T02:48:10.756070: step 1643, loss 0.246406, acc 0.94
2016-09-07T02:48:11.444198: step 1644, loss 0.0280428, acc 0.98
2016-09-07T02:48:12.127236: step 1645, loss 0.0948254, acc 0.96
2016-09-07T02:48:12.803587: step 1646, loss 0.106844, acc 0.98
2016-09-07T02:48:13.477657: step 1647, loss 0.0256604, acc 0.98
2016-09-07T02:48:14.157759: step 1648, loss 0.0120802, acc 1
2016-09-07T02:48:14.858889: step 1649, loss 0.0306491, acc 0.98
2016-09-07T02:48:15.561026: step 1650, loss 0.0639399, acc 0.98
2016-09-07T02:48:16.277599: step 1651, loss 0.0665022, acc 0.96
2016-09-07T02:48:16.963560: step 1652, loss 0.161216, acc 0.96
2016-09-07T02:48:17.638782: step 1653, loss 0.0148199, acc 1
2016-09-07T02:48:18.322440: step 1654, loss 0.0530306, acc 0.96
2016-09-07T02:48:18.999783: step 1655, loss 0.0806636, acc 0.96
2016-09-07T02:48:19.682522: step 1656, loss 0.0374452, acc 1
2016-09-07T02:48:20.364952: step 1657, loss 0.0925857, acc 0.96
2016-09-07T02:48:21.050073: step 1658, loss 0.0175676, acc 1
2016-09-07T02:48:21.768845: step 1659, loss 0.031044, acc 0.98
2016-09-07T02:48:22.455529: step 1660, loss 0.0412227, acc 0.98
2016-09-07T02:48:23.140262: step 1661, loss 0.0923484, acc 0.96
2016-09-07T02:48:23.820023: step 1662, loss 0.0380987, acc 0.98
2016-09-07T02:48:24.525157: step 1663, loss 0.196833, acc 0.94
2016-09-07T02:48:25.213032: step 1664, loss 0.0263251, acc 0.98
2016-09-07T02:48:25.919642: step 1665, loss 0.0196257, acc 1
2016-09-07T02:48:26.613499: step 1666, loss 0.0790138, acc 0.96
2016-09-07T02:48:27.282231: step 1667, loss 0.0707846, acc 0.96
2016-09-07T02:48:27.962676: step 1668, loss 0.0697062, acc 0.94
2016-09-07T02:48:28.661868: step 1669, loss 0.0718711, acc 0.98
2016-09-07T02:48:29.360979: step 1670, loss 0.0881678, acc 0.92
2016-09-07T02:48:30.024518: step 1671, loss 0.0892692, acc 0.96
2016-09-07T02:48:30.725089: step 1672, loss 0.111316, acc 0.96
2016-09-07T02:48:31.404705: step 1673, loss 0.187433, acc 0.94
2016-09-07T02:48:32.090509: step 1674, loss 0.0403451, acc 1
2016-09-07T02:48:32.777885: step 1675, loss 0.034243, acc 1
2016-09-07T02:48:33.452840: step 1676, loss 0.143821, acc 0.92
2016-09-07T02:48:34.142075: step 1677, loss 0.139254, acc 0.94
2016-09-07T02:48:34.828674: step 1678, loss 0.0863576, acc 0.96
2016-09-07T02:48:35.543283: step 1679, loss 0.138743, acc 0.94
2016-09-07T02:48:36.226232: step 1680, loss 0.0303247, acc 0.98
2016-09-07T02:48:36.916936: step 1681, loss 0.0725479, acc 0.98
2016-09-07T02:48:37.617046: step 1682, loss 0.0931183, acc 0.94
2016-09-07T02:48:38.291200: step 1683, loss 0.0973544, acc 0.94
2016-09-07T02:48:38.980977: step 1684, loss 0.0474282, acc 1
2016-09-07T02:48:39.655987: step 1685, loss 0.0627683, acc 0.96
2016-09-07T02:48:40.347616: step 1686, loss 0.0480411, acc 0.98
2016-09-07T02:48:41.024382: step 1687, loss 0.143004, acc 0.96
2016-09-07T02:48:41.717823: step 1688, loss 0.155147, acc 0.96
2016-09-07T02:48:42.396665: step 1689, loss 0.119852, acc 0.94
2016-09-07T02:48:43.100342: step 1690, loss 0.05176, acc 0.98
2016-09-07T02:48:43.771179: step 1691, loss 0.0468726, acc 0.98
2016-09-07T02:48:44.419898: step 1692, loss 0.0373904, acc 0.98
2016-09-07T02:48:45.118727: step 1693, loss 0.0401497, acc 1
2016-09-07T02:48:45.803125: step 1694, loss 0.0446246, acc 0.98
2016-09-07T02:48:46.492879: step 1695, loss 0.0564134, acc 0.98
2016-09-07T02:48:47.186207: step 1696, loss 0.109355, acc 0.92
2016-09-07T02:48:47.858673: step 1697, loss 0.0107779, acc 1
2016-09-07T02:48:48.534890: step 1698, loss 0.00752063, acc 1
2016-09-07T02:48:49.214545: step 1699, loss 0.0557307, acc 0.98
2016-09-07T02:48:49.917986: step 1700, loss 0.0754033, acc 0.96

Evaluation:
2016-09-07T02:48:53.357845: step 1700, loss 1.03308, acc 0.773921

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-1700

2016-09-07T02:48:55.025163: step 1701, loss 0.022457, acc 1
2016-09-07T02:48:55.716237: step 1702, loss 0.0482415, acc 1
2016-09-07T02:48:56.416677: step 1703, loss 0.0718479, acc 0.98
2016-09-07T02:48:57.125606: step 1704, loss 0.0532757, acc 0.98
2016-09-07T02:48:57.816296: step 1705, loss 0.0197438, acc 1
2016-09-07T02:48:58.514049: step 1706, loss 0.112169, acc 0.96
2016-09-07T02:48:59.193773: step 1707, loss 0.0652585, acc 0.98
2016-09-07T02:48:59.866375: step 1708, loss 0.0522055, acc 0.98
2016-09-07T02:49:00.583139: step 1709, loss 0.0542824, acc 0.96
2016-09-07T02:49:01.261576: step 1710, loss 0.05573, acc 0.96
2016-09-07T02:49:01.978094: step 1711, loss 0.0294971, acc 0.98
2016-09-07T02:49:02.656917: step 1712, loss 0.0119055, acc 1
2016-09-07T02:49:03.337608: step 1713, loss 0.0657373, acc 0.98
2016-09-07T02:49:04.015863: step 1714, loss 0.0837813, acc 0.96
2016-09-07T02:49:04.714852: step 1715, loss 0.0765922, acc 0.96
2016-09-07T02:49:05.393649: step 1716, loss 0.0413398, acc 0.98
2016-09-07T02:49:06.062960: step 1717, loss 0.081082, acc 0.98
2016-09-07T02:49:06.762344: step 1718, loss 0.0354467, acc 1
2016-09-07T02:49:07.428453: step 1719, loss 0.0171343, acc 1
2016-09-07T02:49:08.106939: step 1720, loss 0.127007, acc 0.94
2016-09-07T02:49:08.823077: step 1721, loss 0.100597, acc 0.96
2016-09-07T02:49:09.736199: step 1722, loss 0.101952, acc 0.94
2016-09-07T02:49:10.436418: step 1723, loss 0.201012, acc 0.92
2016-09-07T02:49:11.138570: step 1724, loss 0.0242503, acc 0.98
2016-09-07T02:49:11.832329: step 1725, loss 0.130939, acc 0.98
2016-09-07T02:49:12.514931: step 1726, loss 0.064602, acc 0.98
2016-09-07T02:49:13.190195: step 1727, loss 0.0254063, acc 1
2016-09-07T02:49:13.852842: step 1728, loss 0.066317, acc 0.977273
2016-09-07T02:49:14.501710: step 1729, loss 0.0319341, acc 1
2016-09-07T02:49:15.201006: step 1730, loss 0.0566929, acc 0.98
2016-09-07T02:49:15.856093: step 1731, loss 0.0868103, acc 0.98
2016-09-07T02:49:16.536079: step 1732, loss 0.0289291, acc 1
2016-09-07T02:49:17.216524: step 1733, loss 0.0880014, acc 0.94
2016-09-07T02:49:17.901633: step 1734, loss 0.0435045, acc 0.98
2016-09-07T02:49:18.601192: step 1735, loss 0.0487511, acc 0.98
2016-09-07T02:49:19.325967: step 1736, loss 0.0452526, acc 0.98
2016-09-07T02:49:20.046436: step 1737, loss 0.0228968, acc 1
2016-09-07T02:49:20.735163: step 1738, loss 0.0462897, acc 0.98
2016-09-07T02:49:21.427154: step 1739, loss 0.083333, acc 0.96
2016-09-07T02:49:22.129024: step 1740, loss 0.119322, acc 0.94
2016-09-07T02:49:22.833515: step 1741, loss 0.0482321, acc 0.98
2016-09-07T02:49:23.546594: step 1742, loss 0.121397, acc 0.98
2016-09-07T02:49:24.213850: step 1743, loss 0.0081259, acc 1
2016-09-07T02:49:24.906318: step 1744, loss 0.13615, acc 0.96
2016-09-07T02:49:25.593331: step 1745, loss 0.10385, acc 0.96
2016-09-07T02:49:26.276988: step 1746, loss 0.0605844, acc 0.96
2016-09-07T02:49:26.971398: step 1747, loss 0.0738498, acc 0.94
2016-09-07T02:49:27.670088: step 1748, loss 0.115462, acc 0.94
2016-09-07T02:49:28.396666: step 1749, loss 0.025403, acc 0.98
2016-09-07T02:49:29.072693: step 1750, loss 0.0443073, acc 0.98
2016-09-07T02:49:29.763788: step 1751, loss 0.04233, acc 0.98
2016-09-07T02:49:30.439858: step 1752, loss 0.0647882, acc 0.98
2016-09-07T02:49:31.119645: step 1753, loss 0.0885369, acc 0.94
2016-09-07T02:49:31.793679: step 1754, loss 0.0314938, acc 0.98
2016-09-07T02:49:32.478644: step 1755, loss 0.0236164, acc 0.98
2016-09-07T02:49:33.198067: step 1756, loss 0.0361976, acc 1
2016-09-07T02:49:33.889933: step 1757, loss 0.0754675, acc 0.96
2016-09-07T02:49:34.597853: step 1758, loss 0.126292, acc 0.96
2016-09-07T02:49:35.279366: step 1759, loss 0.100834, acc 0.96
2016-09-07T02:49:35.979245: step 1760, loss 0.185588, acc 0.88
2016-09-07T02:49:36.658542: step 1761, loss 0.0261493, acc 0.98
2016-09-07T02:49:37.343860: step 1762, loss 0.0743698, acc 0.96
2016-09-07T02:49:38.059065: step 1763, loss 0.0791675, acc 0.96
2016-09-07T02:49:38.740247: step 1764, loss 0.0325302, acc 1
2016-09-07T02:49:39.424614: step 1765, loss 0.0183916, acc 1
2016-09-07T02:49:40.112172: step 1766, loss 0.032025, acc 1
2016-09-07T02:49:40.807613: step 1767, loss 0.0205952, acc 0.98
2016-09-07T02:49:41.501995: step 1768, loss 0.00509521, acc 1
2016-09-07T02:49:42.171196: step 1769, loss 0.0477728, acc 0.96
2016-09-07T02:49:42.887565: step 1770, loss 0.0364878, acc 1
2016-09-07T02:49:43.564559: step 1771, loss 0.0239284, acc 1
2016-09-07T02:49:44.240683: step 1772, loss 0.107365, acc 0.96
2016-09-07T02:49:44.918250: step 1773, loss 0.0184558, acc 0.98
2016-09-07T02:49:45.597640: step 1774, loss 0.0254531, acc 1
2016-09-07T02:49:46.286998: step 1775, loss 0.0234382, acc 0.98
2016-09-07T02:49:46.960831: step 1776, loss 0.0581214, acc 0.98
2016-09-07T02:49:47.657908: step 1777, loss 0.00215312, acc 1
2016-09-07T02:49:48.334762: step 1778, loss 0.0425916, acc 0.96
2016-09-07T02:49:48.999761: step 1779, loss 0.0450878, acc 1
2016-09-07T02:49:49.687459: step 1780, loss 0.0234174, acc 1
2016-09-07T02:49:50.378428: step 1781, loss 0.0852512, acc 0.96
2016-09-07T02:49:51.079065: step 1782, loss 0.165533, acc 0.96
2016-09-07T02:49:51.760082: step 1783, loss 0.00528089, acc 1
2016-09-07T02:49:52.466118: step 1784, loss 0.0054563, acc 1
2016-09-07T02:49:53.146788: step 1785, loss 0.0676642, acc 0.96
2016-09-07T02:49:53.850427: step 1786, loss 0.0215465, acc 1
2016-09-07T02:49:54.531420: step 1787, loss 0.065887, acc 0.96
2016-09-07T02:49:55.212225: step 1788, loss 0.0397061, acc 0.98
2016-09-07T02:49:55.896753: step 1789, loss 0.047034, acc 0.98
2016-09-07T02:49:56.583974: step 1790, loss 0.00192935, acc 1
2016-09-07T02:49:57.293300: step 1791, loss 0.0910857, acc 0.94
2016-09-07T02:49:57.961254: step 1792, loss 0.105625, acc 0.96
2016-09-07T02:49:58.635680: step 1793, loss 0.0250824, acc 1
2016-09-07T02:49:59.319043: step 1794, loss 0.10441, acc 0.96
2016-09-07T02:50:00.001933: step 1795, loss 0.0880995, acc 0.96
2016-09-07T02:50:00.710824: step 1796, loss 0.0393262, acc 0.98
2016-09-07T02:50:01.402145: step 1797, loss 0.0485953, acc 0.96
2016-09-07T02:50:02.120143: step 1798, loss 0.118388, acc 0.94
2016-09-07T02:50:02.800876: step 1799, loss 0.0590148, acc 0.98
2016-09-07T02:50:03.486394: step 1800, loss 0.095444, acc 0.96

Evaluation:
2016-09-07T02:50:06.858775: step 1800, loss 1.20114, acc 0.772045

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-1800

2016-09-07T02:50:08.534457: step 1801, loss 0.0275073, acc 0.98
2016-09-07T02:50:09.239105: step 1802, loss 0.0186113, acc 1
2016-09-07T02:50:09.895487: step 1803, loss 0.0543219, acc 0.98
2016-09-07T02:50:10.595782: step 1804, loss 0.158421, acc 0.94
2016-09-07T02:50:11.295524: step 1805, loss 0.0183164, acc 0.98
2016-09-07T02:50:11.993787: step 1806, loss 0.0609126, acc 0.98
2016-09-07T02:50:12.678396: step 1807, loss 0.0694679, acc 0.96
2016-09-07T02:50:13.356084: step 1808, loss 0.107816, acc 0.94
2016-09-07T02:50:14.055362: step 1809, loss 0.0675938, acc 0.98
2016-09-07T02:50:14.741112: step 1810, loss 0.0940731, acc 0.96
2016-09-07T02:50:15.441861: step 1811, loss 0.0491132, acc 1
2016-09-07T02:50:16.141843: step 1812, loss 0.0616852, acc 0.96
2016-09-07T02:50:16.833776: step 1813, loss 0.036791, acc 0.98
2016-09-07T02:50:17.518425: step 1814, loss 0.112398, acc 0.96
2016-09-07T02:50:18.217096: step 1815, loss 0.0213114, acc 1
2016-09-07T02:50:18.952828: step 1816, loss 0.0364452, acc 1
2016-09-07T02:50:19.650857: step 1817, loss 0.0434114, acc 0.98
2016-09-07T02:50:20.340510: step 1818, loss 0.0858584, acc 0.94
2016-09-07T02:50:21.015469: step 1819, loss 0.0992789, acc 0.92
2016-09-07T02:50:21.706540: step 1820, loss 0.053334, acc 0.98
2016-09-07T02:50:22.392531: step 1821, loss 0.0167281, acc 1
2016-09-07T02:50:23.084073: step 1822, loss 0.0390005, acc 0.98
2016-09-07T02:50:23.797447: step 1823, loss 0.156559, acc 0.92
2016-09-07T02:50:24.471120: step 1824, loss 0.0829407, acc 0.96
2016-09-07T02:50:25.169386: step 1825, loss 0.0814767, acc 0.98
2016-09-07T02:50:25.847063: step 1826, loss 0.22206, acc 0.92
2016-09-07T02:50:26.526960: step 1827, loss 0.191752, acc 0.96
2016-09-07T02:50:27.217735: step 1828, loss 0.0486059, acc 0.98
2016-09-07T02:50:27.891009: step 1829, loss 0.0255216, acc 1
2016-09-07T02:50:28.599230: step 1830, loss 0.0987885, acc 0.92
2016-09-07T02:50:29.260878: step 1831, loss 0.133363, acc 0.94
2016-09-07T02:50:29.934697: step 1832, loss 0.21454, acc 0.94
2016-09-07T02:50:30.616246: step 1833, loss 0.0785793, acc 0.94
2016-09-07T02:50:31.300432: step 1834, loss 0.0760121, acc 0.96
2016-09-07T02:50:32.012630: step 1835, loss 0.0871688, acc 0.98
2016-09-07T02:50:32.695109: step 1836, loss 0.073264, acc 0.96
2016-09-07T02:50:33.401953: step 1837, loss 0.026657, acc 1
2016-09-07T02:50:34.097907: step 1838, loss 0.0286238, acc 1
2016-09-07T02:50:34.756664: step 1839, loss 0.15246, acc 0.94
2016-09-07T02:50:35.429736: step 1840, loss 0.0383268, acc 0.96
2016-09-07T02:50:36.119525: step 1841, loss 0.0338894, acc 1
2016-09-07T02:50:36.822906: step 1842, loss 0.0278253, acc 1
2016-09-07T02:50:37.500799: step 1843, loss 0.0431518, acc 0.98
2016-09-07T02:50:38.203958: step 1844, loss 0.053315, acc 0.98
2016-09-07T02:50:38.864937: step 1845, loss 0.0349184, acc 0.98
2016-09-07T02:50:39.553846: step 1846, loss 0.0276075, acc 1
2016-09-07T02:50:40.243502: step 1847, loss 0.0259205, acc 1
2016-09-07T02:50:40.956154: step 1848, loss 0.0890852, acc 0.94
2016-09-07T02:50:41.672415: step 1849, loss 0.0990938, acc 0.94
2016-09-07T02:50:42.335967: step 1850, loss 0.0356039, acc 1
2016-09-07T02:50:43.027031: step 1851, loss 0.0451563, acc 0.98
2016-09-07T02:50:43.708141: step 1852, loss 0.021519, acc 1
2016-09-07T02:50:44.379050: step 1853, loss 0.114995, acc 0.96
2016-09-07T02:50:45.067932: step 1854, loss 0.122904, acc 0.92
2016-09-07T02:50:45.747014: step 1855, loss 0.048783, acc 0.94
2016-09-07T02:50:46.435024: step 1856, loss 0.0248169, acc 0.98
2016-09-07T02:50:47.080977: step 1857, loss 0.0172213, acc 1
2016-09-07T02:50:47.805509: step 1858, loss 0.0302406, acc 0.98
2016-09-07T02:50:48.475156: step 1859, loss 0.0289697, acc 1
2016-09-07T02:50:49.156396: step 1860, loss 0.0783802, acc 0.96
2016-09-07T02:50:49.836607: step 1861, loss 0.0891816, acc 0.94
2016-09-07T02:50:50.535836: step 1862, loss 0.0193804, acc 0.98
2016-09-07T02:50:51.229625: step 1863, loss 0.141667, acc 0.94
2016-09-07T02:50:51.913221: step 1864, loss 0.0733211, acc 0.98
2016-09-07T02:50:52.626998: step 1865, loss 0.03722, acc 0.96
2016-09-07T02:50:53.312451: step 1866, loss 0.00360253, acc 1
2016-09-07T02:50:53.991646: step 1867, loss 0.0405183, acc 0.98
2016-09-07T02:50:54.678602: step 1868, loss 0.0749276, acc 0.96
2016-09-07T02:50:55.377069: step 1869, loss 0.0278326, acc 1
2016-09-07T02:50:56.080322: step 1870, loss 0.112556, acc 0.94
2016-09-07T02:50:56.743761: step 1871, loss 0.0388789, acc 0.98
2016-09-07T02:50:57.457010: step 1872, loss 0.0225677, acc 0.98
2016-09-07T02:50:58.123753: step 1873, loss 0.0461099, acc 0.98
2016-09-07T02:50:58.798690: step 1874, loss 0.0623018, acc 0.96
2016-09-07T02:50:59.483837: step 1875, loss 0.0638643, acc 0.96
2016-09-07T02:51:00.168871: step 1876, loss 0.0251067, acc 0.98
2016-09-07T02:51:00.889572: step 1877, loss 0.056656, acc 0.94
2016-09-07T02:51:01.547956: step 1878, loss 0.040252, acc 1
2016-09-07T02:51:02.258605: step 1879, loss 0.0994617, acc 0.96
2016-09-07T02:51:02.935146: step 1880, loss 0.0230991, acc 1
2016-09-07T02:51:03.631485: step 1881, loss 0.0330967, acc 0.98
2016-09-07T02:51:04.328543: step 1882, loss 0.0887756, acc 0.94
2016-09-07T02:51:05.017970: step 1883, loss 0.0435869, acc 0.98
2016-09-07T02:51:05.729619: step 1884, loss 0.0234326, acc 1
2016-09-07T02:51:06.397401: step 1885, loss 0.014757, acc 1
2016-09-07T02:51:07.089379: step 1886, loss 0.0343843, acc 1
2016-09-07T02:51:07.760735: step 1887, loss 0.126391, acc 0.96
2016-09-07T02:51:08.454511: step 1888, loss 0.0522843, acc 1
2016-09-07T02:51:09.154553: step 1889, loss 0.0198922, acc 1
2016-09-07T02:51:09.839959: step 1890, loss 0.133372, acc 0.96
2016-09-07T02:51:10.532513: step 1891, loss 0.124417, acc 0.94
2016-09-07T02:51:11.209997: step 1892, loss 0.0250616, acc 0.98
2016-09-07T02:51:11.935878: step 1893, loss 0.118328, acc 0.94
2016-09-07T02:51:12.673350: step 1894, loss 0.047494, acc 0.96
2016-09-07T02:51:13.361029: step 1895, loss 0.0168386, acc 0.98
2016-09-07T02:51:14.026991: step 1896, loss 0.074, acc 0.96
2016-09-07T02:51:14.702969: step 1897, loss 0.139773, acc 0.98
2016-09-07T02:51:15.405857: step 1898, loss 0.0256647, acc 1
2016-09-07T02:51:16.085452: step 1899, loss 0.0922848, acc 0.94
2016-09-07T02:51:16.787688: step 1900, loss 0.020525, acc 0.98

Evaluation:
2016-09-07T02:51:20.349202: step 1900, loss 1.27627, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-1900

2016-09-07T02:51:22.090291: step 1901, loss 0.0502876, acc 0.96
2016-09-07T02:51:22.786046: step 1902, loss 0.021373, acc 1
2016-09-07T02:51:23.492508: step 1903, loss 0.143803, acc 0.92
2016-09-07T02:51:24.196287: step 1904, loss 0.0285811, acc 1
2016-09-07T02:51:24.894662: step 1905, loss 0.043849, acc 0.98
2016-09-07T02:51:25.576392: step 1906, loss 0.0286388, acc 0.98
2016-09-07T02:51:26.275918: step 1907, loss 0.0752453, acc 0.98
2016-09-07T02:51:26.960398: step 1908, loss 0.0915767, acc 0.96
2016-09-07T02:51:27.626946: step 1909, loss 0.130405, acc 0.92
2016-09-07T02:51:28.338180: step 1910, loss 0.0158923, acc 1
2016-09-07T02:51:29.018616: step 1911, loss 0.0841364, acc 0.96
2016-09-07T02:51:29.687412: step 1912, loss 0.099935, acc 0.94
2016-09-07T02:51:30.355818: step 1913, loss 0.0725552, acc 0.96
2016-09-07T02:51:31.048097: step 1914, loss 0.0752635, acc 0.94
2016-09-07T02:51:31.706556: step 1915, loss 0.0329249, acc 1
2016-09-07T02:51:32.385071: step 1916, loss 0.0477921, acc 0.98
2016-09-07T02:51:33.088405: step 1917, loss 0.0817746, acc 0.96
2016-09-07T02:51:33.758440: step 1918, loss 0.086584, acc 0.96
2016-09-07T02:51:34.432345: step 1919, loss 0.0273651, acc 0.98
2016-09-07T02:51:35.058421: step 1920, loss 0.0224884, acc 0.977273
2016-09-07T02:51:35.754863: step 1921, loss 0.0764946, acc 0.94
2016-09-07T02:51:36.411679: step 1922, loss 0.0485155, acc 0.96
2016-09-07T02:51:37.098793: step 1923, loss 0.0226386, acc 1
2016-09-07T02:51:37.755746: step 1924, loss 0.0935139, acc 0.94
2016-09-07T02:51:38.428200: step 1925, loss 0.0537338, acc 0.98
2016-09-07T02:51:39.127651: step 1926, loss 0.0584446, acc 0.98
2016-09-07T02:51:39.843234: step 1927, loss 0.0249043, acc 1
2016-09-07T02:51:40.547646: step 1928, loss 0.148956, acc 0.98
2016-09-07T02:51:41.202569: step 1929, loss 0.0508384, acc 0.98
2016-09-07T02:51:41.899261: step 1930, loss 0.0979499, acc 0.96
2016-09-07T02:51:42.558915: step 1931, loss 0.0818423, acc 0.96
2016-09-07T02:51:43.231442: step 1932, loss 0.02463, acc 1
2016-09-07T02:51:43.931073: step 1933, loss 0.0471659, acc 0.98
2016-09-07T02:51:44.603285: step 1934, loss 0.0325169, acc 0.98
2016-09-07T02:51:45.273609: step 1935, loss 0.0471928, acc 0.96
2016-09-07T02:51:45.970670: step 1936, loss 0.0101131, acc 1
2016-09-07T02:51:46.672309: step 1937, loss 0.0319978, acc 0.98
2016-09-07T02:51:47.351564: step 1938, loss 0.0336222, acc 0.98
2016-09-07T02:51:48.048867: step 1939, loss 0.0959067, acc 0.94
2016-09-07T02:51:48.735413: step 1940, loss 0.0378265, acc 0.98
2016-09-07T02:51:49.414697: step 1941, loss 0.172926, acc 0.96
2016-09-07T02:51:50.098242: step 1942, loss 0.0452225, acc 0.96
2016-09-07T02:51:50.796906: step 1943, loss 0.0545023, acc 0.98
2016-09-07T02:51:51.505401: step 1944, loss 0.0216991, acc 1
2016-09-07T02:51:52.177081: step 1945, loss 0.131845, acc 0.92
2016-09-07T02:51:52.852729: step 1946, loss 0.0346108, acc 0.98
2016-09-07T02:51:53.527357: step 1947, loss 0.0453422, acc 0.98
2016-09-07T02:51:54.219849: step 1948, loss 0.0205939, acc 1
2016-09-07T02:51:54.908600: step 1949, loss 0.0805747, acc 0.96
2016-09-07T02:51:55.607268: step 1950, loss 0.0458546, acc 0.98
2016-09-07T02:51:56.307715: step 1951, loss 0.00173944, acc 1
2016-09-07T02:51:56.992716: step 1952, loss 0.124465, acc 0.98
2016-09-07T02:51:57.695707: step 1953, loss 0.0208127, acc 1
2016-09-07T02:51:58.401703: step 1954, loss 0.119996, acc 0.96
2016-09-07T02:51:59.105277: step 1955, loss 0.0422551, acc 0.96
2016-09-07T02:51:59.804717: step 1956, loss 0.0627173, acc 0.96
2016-09-07T02:52:00.491144: step 1957, loss 0.0788277, acc 0.98
2016-09-07T02:52:01.186526: step 1958, loss 0.0283524, acc 0.98
2016-09-07T02:52:01.860529: step 1959, loss 0.0226112, acc 1
2016-09-07T02:52:02.550001: step 1960, loss 0.0445512, acc 0.98
2016-09-07T02:52:03.225036: step 1961, loss 0.0317929, acc 0.98
2016-09-07T02:52:03.904902: step 1962, loss 0.0680855, acc 0.96
2016-09-07T02:52:04.580092: step 1963, loss 0.110156, acc 0.94
2016-09-07T02:52:05.250331: step 1964, loss 0.00830692, acc 1
2016-09-07T02:52:05.966907: step 1965, loss 0.058223, acc 0.96
2016-09-07T02:52:06.642527: step 1966, loss 0.0756138, acc 0.96
2016-09-07T02:52:07.329417: step 1967, loss 0.0604507, acc 0.96
2016-09-07T02:52:08.019754: step 1968, loss 0.02784, acc 1
2016-09-07T02:52:08.701878: step 1969, loss 0.0354598, acc 1
2016-09-07T02:52:09.381355: step 1970, loss 0.0178665, acc 1
2016-09-07T02:52:10.053040: step 1971, loss 0.0309917, acc 0.98
2016-09-07T02:52:10.781067: step 1972, loss 0.0109742, acc 1
2016-09-07T02:52:11.459496: step 1973, loss 0.0149831, acc 1
2016-09-07T02:52:12.159253: step 1974, loss 0.0192304, acc 1
2016-09-07T02:52:12.871113: step 1975, loss 0.0539636, acc 0.98
2016-09-07T02:52:13.570996: step 1976, loss 0.156133, acc 0.94
2016-09-07T02:52:14.257636: step 1977, loss 0.0143438, acc 1
2016-09-07T02:52:14.943144: step 1978, loss 0.0816228, acc 0.98
2016-09-07T02:52:15.656762: step 1979, loss 0.0194874, acc 1
2016-09-07T02:52:16.328007: step 1980, loss 0.0338468, acc 0.98
2016-09-07T02:52:17.013477: step 1981, loss 0.235803, acc 0.96
2016-09-07T02:52:17.693334: step 1982, loss 0.00274012, acc 1
2016-09-07T02:52:18.376779: step 1983, loss 0.0357654, acc 0.96
2016-09-07T02:52:19.058117: step 1984, loss 0.128485, acc 0.96
2016-09-07T02:52:19.722684: step 1985, loss 0.081529, acc 0.96
2016-09-07T02:52:20.415407: step 1986, loss 0.0329593, acc 0.98
2016-09-07T02:52:21.098976: step 1987, loss 0.0131715, acc 1
2016-09-07T02:52:21.773559: step 1988, loss 0.0425802, acc 0.96
2016-09-07T02:52:22.456955: step 1989, loss 0.0558881, acc 0.98
2016-09-07T02:52:23.150269: step 1990, loss 0.0324549, acc 0.98
2016-09-07T02:52:23.822875: step 1991, loss 0.00325377, acc 1
2016-09-07T02:52:24.506612: step 1992, loss 0.0354762, acc 0.98
2016-09-07T02:52:25.210367: step 1993, loss 0.0590607, acc 0.98
2016-09-07T02:52:25.863545: step 1994, loss 0.0764225, acc 0.94
2016-09-07T02:52:26.550579: step 1995, loss 0.00826922, acc 1
2016-09-07T02:52:27.225436: step 1996, loss 0.0710741, acc 0.94
2016-09-07T02:52:27.910523: step 1997, loss 0.0350971, acc 1
2016-09-07T02:52:28.583310: step 1998, loss 0.0122238, acc 1
2016-09-07T02:52:29.286433: step 1999, loss 0.0170175, acc 1
2016-09-07T02:52:30.016370: step 2000, loss 0.180684, acc 0.96

Evaluation:
2016-09-07T02:52:33.498156: step 2000, loss 1.3554, acc 0.763602

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-2000

2016-09-07T02:52:35.256794: step 2001, loss 0.0987114, acc 0.96
2016-09-07T02:52:35.931001: step 2002, loss 0.0679269, acc 0.98
2016-09-07T02:52:36.611224: step 2003, loss 0.0361959, acc 1
2016-09-07T02:52:37.308126: step 2004, loss 0.0441917, acc 0.98
2016-09-07T02:52:38.009481: step 2005, loss 0.027046, acc 0.98
2016-09-07T02:52:38.690374: step 2006, loss 0.036645, acc 1
2016-09-07T02:52:39.387623: step 2007, loss 0.0927907, acc 0.94
2016-09-07T02:52:40.089552: step 2008, loss 0.052597, acc 0.98
2016-09-07T02:52:40.809244: step 2009, loss 0.067707, acc 0.96
2016-09-07T02:52:41.485283: step 2010, loss 0.174963, acc 0.94
2016-09-07T02:52:42.206563: step 2011, loss 0.0248695, acc 1
2016-09-07T02:52:42.885459: step 2012, loss 0.0225637, acc 1
2016-09-07T02:52:43.576880: step 2013, loss 0.0449885, acc 1
2016-09-07T02:52:44.268897: step 2014, loss 0.0189477, acc 1
2016-09-07T02:52:44.965495: step 2015, loss 0.062472, acc 0.98
2016-09-07T02:52:45.670105: step 2016, loss 0.0292062, acc 1
2016-09-07T02:52:46.347986: step 2017, loss 0.0166068, acc 1
2016-09-07T02:52:47.037375: step 2018, loss 0.07738, acc 0.94
2016-09-07T02:52:47.738860: step 2019, loss 0.0469316, acc 0.98
2016-09-07T02:52:48.420547: step 2020, loss 0.0395738, acc 1
2016-09-07T02:52:49.118266: step 2021, loss 0.00331512, acc 1
2016-09-07T02:52:49.791782: step 2022, loss 0.0233863, acc 1
2016-09-07T02:52:50.493359: step 2023, loss 0.0540085, acc 0.96
2016-09-07T02:52:51.162455: step 2024, loss 0.0583494, acc 0.96
2016-09-07T02:52:51.854354: step 2025, loss 0.0327816, acc 1
2016-09-07T02:52:52.529249: step 2026, loss 0.0321087, acc 1
2016-09-07T02:52:53.206081: step 2027, loss 0.0518215, acc 0.98
2016-09-07T02:52:53.886072: step 2028, loss 0.0282858, acc 1
2016-09-07T02:52:54.595283: step 2029, loss 0.049941, acc 0.96
2016-09-07T02:52:55.295425: step 2030, loss 0.0451432, acc 0.96
2016-09-07T02:52:55.990061: step 2031, loss 0.132265, acc 0.94
2016-09-07T02:52:56.697531: step 2032, loss 0.0324507, acc 0.98
2016-09-07T02:52:57.376540: step 2033, loss 0.080547, acc 0.96
2016-09-07T02:52:58.050591: step 2034, loss 0.105077, acc 0.96
2016-09-07T02:52:58.750243: step 2035, loss 0.0580558, acc 0.98
2016-09-07T02:52:59.414819: step 2036, loss 0.128389, acc 0.96
2016-09-07T02:53:00.143469: step 2037, loss 0.0487317, acc 0.98
2016-09-07T02:53:00.907958: step 2038, loss 0.0268104, acc 1
2016-09-07T02:53:01.595987: step 2039, loss 0.0512759, acc 0.98
2016-09-07T02:53:02.307624: step 2040, loss 0.0232476, acc 1
2016-09-07T02:53:03.007754: step 2041, loss 0.173725, acc 0.98
2016-09-07T02:53:03.733493: step 2042, loss 0.0337231, acc 0.98
2016-09-07T02:53:04.406321: step 2043, loss 0.0700127, acc 0.96
2016-09-07T02:53:05.093578: step 2044, loss 0.0569991, acc 0.98
2016-09-07T02:53:05.775318: step 2045, loss 0.0255017, acc 1
2016-09-07T02:53:06.463020: step 2046, loss 0.0620372, acc 0.98
2016-09-07T02:53:07.153583: step 2047, loss 0.0443726, acc 1
2016-09-07T02:53:07.820407: step 2048, loss 0.106987, acc 0.96
2016-09-07T02:53:08.508590: step 2049, loss 0.0231353, acc 0.98
2016-09-07T02:53:09.180540: step 2050, loss 0.210291, acc 0.94
2016-09-07T02:53:09.894923: step 2051, loss 0.0576971, acc 0.96
2016-09-07T02:53:10.587984: step 2052, loss 0.0656293, acc 0.96
2016-09-07T02:53:11.299149: step 2053, loss 0.0676723, acc 0.96
2016-09-07T02:53:11.994189: step 2054, loss 0.0182702, acc 1
2016-09-07T02:53:12.676878: step 2055, loss 0.0673809, acc 0.96
2016-09-07T02:53:13.388688: step 2056, loss 0.0466578, acc 0.98
2016-09-07T02:53:14.081759: step 2057, loss 0.0708647, acc 0.98
2016-09-07T02:53:14.768845: step 2058, loss 0.0383696, acc 0.98
2016-09-07T02:53:15.456351: step 2059, loss 0.0116611, acc 1
2016-09-07T02:53:16.158072: step 2060, loss 0.0580245, acc 0.98
2016-09-07T02:53:16.845855: step 2061, loss 0.00551048, acc 1
2016-09-07T02:53:17.529393: step 2062, loss 0.0482643, acc 0.98
2016-09-07T02:53:18.242620: step 2063, loss 0.0357524, acc 0.98
2016-09-07T02:53:18.936197: step 2064, loss 0.0168709, acc 1
2016-09-07T02:53:19.616874: step 2065, loss 0.0644547, acc 0.94
2016-09-07T02:53:20.290303: step 2066, loss 0.121434, acc 0.96
2016-09-07T02:53:20.999212: step 2067, loss 0.0492678, acc 0.96
2016-09-07T02:53:21.706798: step 2068, loss 0.0407169, acc 0.98
2016-09-07T02:53:22.383508: step 2069, loss 0.0521533, acc 0.98
2016-09-07T02:53:23.078091: step 2070, loss 0.0900425, acc 0.98
2016-09-07T02:53:23.764802: step 2071, loss 0.00693278, acc 1
2016-09-07T02:53:24.433548: step 2072, loss 0.00918752, acc 1
2016-09-07T02:53:25.116243: step 2073, loss 0.0199697, acc 1
2016-09-07T02:53:25.782742: step 2074, loss 0.124743, acc 0.96
2016-09-07T02:53:26.489355: step 2075, loss 0.00809531, acc 1
2016-09-07T02:53:27.153307: step 2076, loss 0.0365634, acc 0.98
2016-09-07T02:53:27.845337: step 2077, loss 0.0217702, acc 1
2016-09-07T02:53:28.546045: step 2078, loss 0.0312017, acc 1
2016-09-07T02:53:29.234932: step 2079, loss 0.0755965, acc 0.96
2016-09-07T02:53:29.929905: step 2080, loss 0.085635, acc 0.94
2016-09-07T02:53:30.610242: step 2081, loss 0.0129738, acc 1
2016-09-07T02:53:31.320311: step 2082, loss 0.163427, acc 0.96
2016-09-07T02:53:32.006940: step 2083, loss 0.102375, acc 0.98
2016-09-07T02:53:32.716254: step 2084, loss 0.0120079, acc 1
2016-09-07T02:53:33.400687: step 2085, loss 0.0449071, acc 1
2016-09-07T02:53:34.109488: step 2086, loss 0.0505243, acc 0.98
2016-09-07T02:53:34.800506: step 2087, loss 0.124538, acc 0.98
2016-09-07T02:53:35.448912: step 2088, loss 0.161177, acc 0.92
2016-09-07T02:53:36.158721: step 2089, loss 0.00902001, acc 1
2016-09-07T02:53:36.827745: step 2090, loss 0.0145242, acc 1
2016-09-07T02:53:37.509127: step 2091, loss 0.102533, acc 0.92
2016-09-07T02:53:38.205018: step 2092, loss 0.0058454, acc 1
2016-09-07T02:53:38.904049: step 2093, loss 0.0621197, acc 0.98
2016-09-07T02:53:39.602706: step 2094, loss 0.119216, acc 0.94
2016-09-07T02:53:40.278619: step 2095, loss 0.0401662, acc 0.96
2016-09-07T02:53:40.997234: step 2096, loss 0.035329, acc 0.98
2016-09-07T02:53:41.711593: step 2097, loss 0.0749779, acc 0.96
2016-09-07T02:53:42.396570: step 2098, loss 0.0711902, acc 0.96
2016-09-07T02:53:43.095851: step 2099, loss 0.0300023, acc 0.98
2016-09-07T02:53:43.779269: step 2100, loss 0.0146512, acc 1

Evaluation:
2016-09-07T02:53:47.279896: step 2100, loss 1.18925, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-2100

2016-09-07T02:53:49.065900: step 2101, loss 0.0175372, acc 1
2016-09-07T02:53:49.780024: step 2102, loss 0.0618854, acc 0.98
2016-09-07T02:53:50.465121: step 2103, loss 0.0969554, acc 0.94
2016-09-07T02:53:51.132293: step 2104, loss 0.119848, acc 0.92
2016-09-07T02:53:51.846083: step 2105, loss 0.0287043, acc 1
2016-09-07T02:53:52.554307: step 2106, loss 0.0743579, acc 0.98
2016-09-07T02:53:53.261936: step 2107, loss 0.0642855, acc 0.98
2016-09-07T02:53:53.946382: step 2108, loss 0.0372595, acc 0.98
2016-09-07T02:53:54.631545: step 2109, loss 0.126044, acc 0.96
2016-09-07T02:53:55.329235: step 2110, loss 0.0703553, acc 0.96
2016-09-07T02:53:55.987308: step 2111, loss 0.112813, acc 0.96
2016-09-07T02:53:56.665827: step 2112, loss 0.0244178, acc 0.977273
2016-09-07T02:53:57.376085: step 2113, loss 0.0514296, acc 0.96
2016-09-07T02:53:58.052923: step 2114, loss 0.129541, acc 0.92
2016-09-07T02:53:58.731354: step 2115, loss 0.0903946, acc 0.92
2016-09-07T02:53:59.424305: step 2116, loss 0.0309926, acc 1
2016-09-07T02:54:00.099245: step 2117, loss 0.15844, acc 0.96
2016-09-07T02:54:00.809927: step 2118, loss 0.0556197, acc 0.98
2016-09-07T02:54:01.527867: step 2119, loss 0.0169112, acc 1
2016-09-07T02:54:02.212075: step 2120, loss 0.0170234, acc 1
2016-09-07T02:54:02.904467: step 2121, loss 0.0217558, acc 1
2016-09-07T02:54:03.606966: step 2122, loss 0.0284022, acc 0.98
2016-09-07T02:54:04.300417: step 2123, loss 0.0234172, acc 1
2016-09-07T02:54:05.009348: step 2124, loss 0.129181, acc 0.96
2016-09-07T02:54:05.690345: step 2125, loss 0.0686012, acc 0.96
2016-09-07T02:54:06.388365: step 2126, loss 0.0703077, acc 0.98
2016-09-07T02:54:07.061268: step 2127, loss 0.00466312, acc 1
2016-09-07T02:54:07.773067: step 2128, loss 0.0414111, acc 0.98
2016-09-07T02:54:08.467146: step 2129, loss 0.00679765, acc 1
2016-09-07T02:54:09.130315: step 2130, loss 0.0446291, acc 0.98
2016-09-07T02:54:09.836253: step 2131, loss 0.0671555, acc 0.98
2016-09-07T02:54:10.508076: step 2132, loss 0.0304697, acc 0.98
2016-09-07T02:54:11.187455: step 2133, loss 0.0468076, acc 0.98
2016-09-07T02:54:11.888343: step 2134, loss 0.00467608, acc 1
2016-09-07T02:54:12.617688: step 2135, loss 0.156696, acc 0.9
2016-09-07T02:54:13.324811: step 2136, loss 0.0245317, acc 0.98
2016-09-07T02:54:13.984676: step 2137, loss 0.0447955, acc 0.96
2016-09-07T02:54:14.701791: step 2138, loss 0.0195823, acc 1
2016-09-07T02:54:15.418210: step 2139, loss 0.0767831, acc 0.96
2016-09-07T02:54:16.110771: step 2140, loss 0.0298233, acc 0.98
2016-09-07T02:54:16.797891: step 2141, loss 0.0769973, acc 0.94
2016-09-07T02:54:17.483385: step 2142, loss 0.0305534, acc 1
2016-09-07T02:54:18.175606: step 2143, loss 0.0579794, acc 0.96
2016-09-07T02:54:18.845244: step 2144, loss 0.0572596, acc 1
2016-09-07T02:54:19.566863: step 2145, loss 0.0255162, acc 1
2016-09-07T02:54:20.261638: step 2146, loss 0.0285849, acc 1
2016-09-07T02:54:20.943740: step 2147, loss 0.104647, acc 0.98
2016-09-07T02:54:21.632593: step 2148, loss 0.0320497, acc 0.98
2016-09-07T02:54:22.326516: step 2149, loss 0.0281637, acc 0.98
2016-09-07T02:54:23.031022: step 2150, loss 0.0438063, acc 0.96
2016-09-07T02:54:23.698804: step 2151, loss 0.0111688, acc 1
2016-09-07T02:54:24.391395: step 2152, loss 0.0457409, acc 0.98
2016-09-07T02:54:25.093302: step 2153, loss 0.0279667, acc 1
2016-09-07T02:54:25.781544: step 2154, loss 0.068028, acc 0.96
2016-09-07T02:54:26.461761: step 2155, loss 0.0162186, acc 1
2016-09-07T02:54:27.144162: step 2156, loss 0.00662467, acc 1
2016-09-07T02:54:27.856243: step 2157, loss 0.0194597, acc 0.98
2016-09-07T02:54:28.530610: step 2158, loss 0.039522, acc 0.98
2016-09-07T02:54:29.219065: step 2159, loss 0.0587205, acc 0.98
2016-09-07T02:54:29.900198: step 2160, loss 0.0189388, acc 1
2016-09-07T02:54:30.602281: step 2161, loss 0.0330481, acc 0.98
2016-09-07T02:54:31.282802: step 2162, loss 0.327576, acc 0.9
2016-09-07T02:54:31.983639: step 2163, loss 0.014385, acc 1
2016-09-07T02:54:32.693387: step 2164, loss 0.0421446, acc 0.96
2016-09-07T02:54:33.421673: step 2165, loss 0.0178875, acc 1
2016-09-07T02:54:34.110690: step 2166, loss 0.0148973, acc 1
2016-09-07T02:54:34.815745: step 2167, loss 0.0147868, acc 0.98
2016-09-07T02:54:35.516516: step 2168, loss 0.0275467, acc 1
2016-09-07T02:54:36.186698: step 2169, loss 0.108936, acc 0.98
2016-09-07T02:54:36.850612: step 2170, loss 0.0110099, acc 1
2016-09-07T02:54:37.547162: step 2171, loss 0.0563093, acc 0.98
2016-09-07T02:54:38.215193: step 2172, loss 0.0638195, acc 0.96
2016-09-07T02:54:38.885392: step 2173, loss 0.195252, acc 0.94
2016-09-07T02:54:39.561781: step 2174, loss 0.00522779, acc 1
2016-09-07T02:54:40.252979: step 2175, loss 0.0467419, acc 0.98
2016-09-07T02:54:40.949798: step 2176, loss 0.154245, acc 0.96
2016-09-07T02:54:41.623240: step 2177, loss 0.0585627, acc 0.98
2016-09-07T02:54:42.327610: step 2178, loss 0.0627706, acc 1
2016-09-07T02:54:43.022964: step 2179, loss 0.00608497, acc 1
2016-09-07T02:54:43.687348: step 2180, loss 0.00340893, acc 1
2016-09-07T02:54:44.385749: step 2181, loss 0.0923778, acc 0.98
2016-09-07T02:54:45.060677: step 2182, loss 0.0689486, acc 0.96
2016-09-07T02:54:45.748137: step 2183, loss 0.0167755, acc 1
2016-09-07T02:54:46.404601: step 2184, loss 0.0623147, acc 0.96
2016-09-07T02:54:47.095987: step 2185, loss 0.0238892, acc 1
2016-09-07T02:54:47.773512: step 2186, loss 0.0343321, acc 0.98
2016-09-07T02:54:48.462674: step 2187, loss 0.0648439, acc 0.96
2016-09-07T02:54:49.162343: step 2188, loss 0.220298, acc 0.96
2016-09-07T02:54:49.846154: step 2189, loss 0.11221, acc 0.94
2016-09-07T02:54:50.559634: step 2190, loss 0.00598347, acc 1
2016-09-07T02:54:51.232005: step 2191, loss 0.0585387, acc 0.98
2016-09-07T02:54:51.926679: step 2192, loss 0.0694392, acc 0.96
2016-09-07T02:54:52.602198: step 2193, loss 0.067296, acc 0.94
2016-09-07T02:54:53.301978: step 2194, loss 0.0444947, acc 0.98
2016-09-07T02:54:53.995198: step 2195, loss 0.068385, acc 0.98
2016-09-07T02:54:54.670064: step 2196, loss 0.0137245, acc 1
2016-09-07T02:54:55.356689: step 2197, loss 0.0408343, acc 0.96
2016-09-07T02:54:56.029967: step 2198, loss 0.0410813, acc 0.98
2016-09-07T02:54:56.738908: step 2199, loss 0.0840014, acc 0.98
2016-09-07T02:54:57.404004: step 2200, loss 0.0934956, acc 0.96

Evaluation:
2016-09-07T02:55:00.916561: step 2200, loss 1.28381, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-2200

2016-09-07T02:55:02.721601: step 2201, loss 0.17043, acc 0.92
2016-09-07T02:55:03.418372: step 2202, loss 0.0293054, acc 1
2016-09-07T02:55:04.096991: step 2203, loss 0.0832557, acc 0.98
2016-09-07T02:55:04.776371: step 2204, loss 0.0621683, acc 0.98
2016-09-07T02:55:05.449369: step 2205, loss 0.0327407, acc 0.98
2016-09-07T02:55:06.134020: step 2206, loss 0.0856649, acc 0.96
2016-09-07T02:55:06.834796: step 2207, loss 0.0667997, acc 0.98
2016-09-07T02:55:07.536820: step 2208, loss 0.0256855, acc 1
2016-09-07T02:55:08.214288: step 2209, loss 0.0541995, acc 0.98
2016-09-07T02:55:08.966863: step 2210, loss 0.118929, acc 0.88
2016-09-07T02:55:09.654833: step 2211, loss 0.115321, acc 0.94
2016-09-07T02:55:10.348075: step 2212, loss 0.0955797, acc 0.94
2016-09-07T02:55:11.053603: step 2213, loss 0.0245032, acc 1
2016-09-07T02:55:11.733895: step 2214, loss 0.0837296, acc 0.94
2016-09-07T02:55:12.461338: step 2215, loss 0.0761084, acc 0.96
2016-09-07T02:55:13.142505: step 2216, loss 0.134233, acc 0.96
2016-09-07T02:55:13.837397: step 2217, loss 0.065602, acc 0.98
2016-09-07T02:55:14.540461: step 2218, loss 0.0327103, acc 1
2016-09-07T02:55:15.246484: step 2219, loss 0.0759814, acc 0.96
2016-09-07T02:55:15.957325: step 2220, loss 0.0609876, acc 0.96
2016-09-07T02:55:16.651671: step 2221, loss 0.0647577, acc 0.96
2016-09-07T02:55:17.324371: step 2222, loss 0.0257557, acc 1
2016-09-07T02:55:18.036320: step 2223, loss 0.131534, acc 0.9
2016-09-07T02:55:18.731885: step 2224, loss 0.00837677, acc 1
2016-09-07T02:55:19.436745: step 2225, loss 0.0595029, acc 0.96
2016-09-07T02:55:20.099170: step 2226, loss 0.0215673, acc 1
2016-09-07T02:55:20.831884: step 2227, loss 0.0395254, acc 0.98
2016-09-07T02:55:21.539631: step 2228, loss 0.057587, acc 0.98
2016-09-07T02:55:22.231141: step 2229, loss 0.0990296, acc 0.98
2016-09-07T02:55:22.923135: step 2230, loss 0.0351181, acc 1
2016-09-07T02:55:23.612736: step 2231, loss 0.129353, acc 0.94
2016-09-07T02:55:24.329058: step 2232, loss 0.0521625, acc 0.96
2016-09-07T02:55:25.019418: step 2233, loss 0.0107146, acc 1
2016-09-07T02:55:25.721196: step 2234, loss 0.081193, acc 0.94
2016-09-07T02:55:26.397194: step 2235, loss 0.0259914, acc 1
2016-09-07T02:55:27.098021: step 2236, loss 0.030197, acc 0.98
2016-09-07T02:55:27.765808: step 2237, loss 0.0295694, acc 0.98
2016-09-07T02:55:28.455291: step 2238, loss 0.066266, acc 0.94
2016-09-07T02:55:29.159008: step 2239, loss 0.073603, acc 0.94
2016-09-07T02:55:29.817218: step 2240, loss 0.121513, acc 0.9
2016-09-07T02:55:30.528790: step 2241, loss 0.047231, acc 0.96
2016-09-07T02:55:31.229309: step 2242, loss 0.0171772, acc 1
2016-09-07T02:55:31.914052: step 2243, loss 0.0479818, acc 0.96
2016-09-07T02:55:32.608697: step 2244, loss 0.0467181, acc 1
2016-09-07T02:55:33.306318: step 2245, loss 0.0915457, acc 0.98
2016-09-07T02:55:34.014705: step 2246, loss 0.0191105, acc 1
2016-09-07T02:55:34.678046: step 2247, loss 0.100183, acc 0.92
2016-09-07T02:55:35.397914: step 2248, loss 0.0343968, acc 0.98
2016-09-07T02:55:36.084515: step 2249, loss 0.024703, acc 1
2016-09-07T02:55:36.784337: step 2250, loss 0.0199453, acc 1
2016-09-07T02:55:37.486689: step 2251, loss 0.122366, acc 0.94
2016-09-07T02:55:38.179866: step 2252, loss 0.0620283, acc 0.94
2016-09-07T02:55:38.889497: step 2253, loss 0.0816318, acc 0.98
2016-09-07T02:55:39.595529: step 2254, loss 0.0191366, acc 0.98
2016-09-07T02:55:40.278713: step 2255, loss 0.111169, acc 0.96
2016-09-07T02:55:40.960206: step 2256, loss 0.0316472, acc 0.98
2016-09-07T02:55:41.650068: step 2257, loss 0.0338676, acc 1
2016-09-07T02:55:42.355266: step 2258, loss 0.0610951, acc 1
2016-09-07T02:55:43.002508: step 2259, loss 0.0325621, acc 1
2016-09-07T02:55:43.700960: step 2260, loss 0.0179967, acc 1
2016-09-07T02:55:44.400438: step 2261, loss 0.0408, acc 0.98
2016-09-07T02:55:45.094296: step 2262, loss 0.0118558, acc 1
2016-09-07T02:55:45.778267: step 2263, loss 0.0375759, acc 1
2016-09-07T02:55:46.479879: step 2264, loss 0.051056, acc 0.98
2016-09-07T02:55:47.199260: step 2265, loss 0.0303552, acc 0.98
2016-09-07T02:55:47.888034: step 2266, loss 0.0846528, acc 0.96
2016-09-07T02:55:48.594727: step 2267, loss 0.0443328, acc 0.96
2016-09-07T02:55:49.289819: step 2268, loss 0.00231412, acc 1
2016-09-07T02:55:49.978265: step 2269, loss 0.0401418, acc 0.98
2016-09-07T02:55:50.664716: step 2270, loss 0.0279043, acc 1
2016-09-07T02:55:51.337717: step 2271, loss 0.00944911, acc 1
2016-09-07T02:55:52.032254: step 2272, loss 0.0282093, acc 1
2016-09-07T02:55:52.716888: step 2273, loss 0.00670295, acc 1
2016-09-07T02:55:53.402179: step 2274, loss 0.0605248, acc 0.98
2016-09-07T02:55:54.102281: step 2275, loss 0.0238703, acc 0.98
2016-09-07T02:55:54.802145: step 2276, loss 0.0433655, acc 0.96
2016-09-07T02:55:55.485874: step 2277, loss 0.0672238, acc 0.98
2016-09-07T02:55:56.164997: step 2278, loss 0.0659966, acc 0.98
2016-09-07T02:55:56.891132: step 2279, loss 0.0306299, acc 0.98
2016-09-07T02:55:57.569535: step 2280, loss 0.00830821, acc 1
2016-09-07T02:55:58.259286: step 2281, loss 0.0824208, acc 0.96
2016-09-07T02:55:58.934280: step 2282, loss 0.122482, acc 0.96
2016-09-07T02:55:59.634864: step 2283, loss 0.0680653, acc 0.96
2016-09-07T02:56:00.326726: step 2284, loss 0.0485681, acc 0.98
2016-09-07T02:56:00.982240: step 2285, loss 0.0251972, acc 0.98
2016-09-07T02:56:01.687758: step 2286, loss 0.0710586, acc 0.96
2016-09-07T02:56:02.389310: step 2287, loss 0.0067651, acc 1
2016-09-07T02:56:03.066047: step 2288, loss 0.0563765, acc 0.98
2016-09-07T02:56:03.772799: step 2289, loss 0.0137521, acc 1
2016-09-07T02:56:04.494639: step 2290, loss 0.0823545, acc 0.94
2016-09-07T02:56:05.188562: step 2291, loss 0.120837, acc 0.96
2016-09-07T02:56:05.858274: step 2292, loss 0.0564398, acc 0.98
2016-09-07T02:56:06.593088: step 2293, loss 0.0163994, acc 1
2016-09-07T02:56:07.296456: step 2294, loss 0.063757, acc 0.98
2016-09-07T02:56:07.982431: step 2295, loss 0.00921833, acc 1
2016-09-07T02:56:08.673898: step 2296, loss 0.0420761, acc 0.98
2016-09-07T02:56:09.360842: step 2297, loss 0.0814736, acc 0.96
2016-09-07T02:56:10.049913: step 2298, loss 0.0356536, acc 0.98
2016-09-07T02:56:10.723904: step 2299, loss 0.0502496, acc 1
2016-09-07T02:56:11.409763: step 2300, loss 0.0632223, acc 0.96

Evaluation:
2016-09-07T02:56:14.939083: step 2300, loss 1.45598, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-2300

2016-09-07T02:56:16.630182: step 2301, loss 0.0336863, acc 1
2016-09-07T02:56:17.309903: step 2302, loss 0.0527575, acc 0.96
2016-09-07T02:56:17.978937: step 2303, loss 0.0792729, acc 0.98
2016-09-07T02:56:18.633033: step 2304, loss 0.0174682, acc 1
2016-09-07T02:56:19.328764: step 2305, loss 0.0352507, acc 0.98
2016-09-07T02:56:20.016394: step 2306, loss 0.0681358, acc 0.94
2016-09-07T02:56:20.717518: step 2307, loss 0.164352, acc 0.9
2016-09-07T02:56:21.422158: step 2308, loss 0.0514453, acc 0.98
2016-09-07T02:56:22.135539: step 2309, loss 0.0541799, acc 1
2016-09-07T02:56:22.803692: step 2310, loss 0.0501098, acc 0.98
2016-09-07T02:56:23.481061: step 2311, loss 0.0186883, acc 1
2016-09-07T02:56:24.185308: step 2312, loss 0.0762445, acc 0.98
2016-09-07T02:56:24.867709: step 2313, loss 0.0146426, acc 1
2016-09-07T02:56:25.547277: step 2314, loss 0.0595552, acc 0.98
2016-09-07T02:56:26.233827: step 2315, loss 0.0137584, acc 1
2016-09-07T02:56:26.943132: step 2316, loss 0.104836, acc 0.96
2016-09-07T02:56:27.606360: step 2317, loss 0.0877722, acc 0.94
2016-09-07T02:56:28.272499: step 2318, loss 0.182098, acc 0.96
2016-09-07T02:56:28.954781: step 2319, loss 0.00911554, acc 1
2016-09-07T02:56:29.657830: step 2320, loss 0.00381271, acc 1
2016-09-07T02:56:30.342996: step 2321, loss 0.0184585, acc 1
2016-09-07T02:56:31.050536: step 2322, loss 0.00512723, acc 1
2016-09-07T02:56:31.776817: step 2323, loss 0.059837, acc 0.96
2016-09-07T02:56:32.446705: step 2324, loss 0.0818442, acc 0.98
2016-09-07T02:56:33.146760: step 2325, loss 0.018137, acc 1
2016-09-07T02:56:33.817132: step 2326, loss 0.031014, acc 1
2016-09-07T02:56:34.477234: step 2327, loss 0.0554181, acc 0.96
2016-09-07T02:56:35.253328: step 2328, loss 0.00517984, acc 1
2016-09-07T02:56:35.944272: step 2329, loss 0.0325284, acc 0.98
2016-09-07T02:56:36.651959: step 2330, loss 0.0129002, acc 1
2016-09-07T02:56:37.337580: step 2331, loss 0.00567434, acc 1
2016-09-07T02:56:38.033501: step 2332, loss 0.00973901, acc 1
2016-09-07T02:56:38.724535: step 2333, loss 0.0223325, acc 1
2016-09-07T02:56:39.418350: step 2334, loss 0.00682514, acc 1
2016-09-07T02:56:40.111761: step 2335, loss 0.0447973, acc 0.98
2016-09-07T02:56:40.796240: step 2336, loss 0.12924, acc 0.92
2016-09-07T02:56:41.495707: step 2337, loss 0.0236099, acc 0.98
2016-09-07T02:56:42.205508: step 2338, loss 0.0218263, acc 1
2016-09-07T02:56:42.891813: step 2339, loss 0.0537368, acc 0.98
2016-09-07T02:56:43.598582: step 2340, loss 0.0925901, acc 0.92
2016-09-07T02:56:44.300436: step 2341, loss 0.0265252, acc 1
2016-09-07T02:56:45.002901: step 2342, loss 0.00697059, acc 1
2016-09-07T02:56:45.683739: step 2343, loss 0.0311445, acc 1
2016-09-07T02:56:46.379657: step 2344, loss 0.0030142, acc 1
2016-09-07T02:56:47.057833: step 2345, loss 0.0413471, acc 1
2016-09-07T02:56:47.735709: step 2346, loss 0.154005, acc 0.94
2016-09-07T02:56:48.410941: step 2347, loss 0.0561597, acc 0.98
2016-09-07T02:56:49.085933: step 2348, loss 0.21586, acc 0.94
2016-09-07T02:56:49.798066: step 2349, loss 0.0726322, acc 0.94
2016-09-07T02:56:50.468144: step 2350, loss 0.0479794, acc 0.98
2016-09-07T02:56:51.143701: step 2351, loss 0.0489385, acc 0.96
2016-09-07T02:56:51.835143: step 2352, loss 0.0781384, acc 0.92
2016-09-07T02:56:52.531472: step 2353, loss 0.0210637, acc 1
2016-09-07T02:56:53.248267: step 2354, loss 0.0277225, acc 1
2016-09-07T02:56:53.954833: step 2355, loss 0.0433016, acc 0.98
2016-09-07T02:56:54.662356: step 2356, loss 0.0105265, acc 1
2016-09-07T02:56:55.351887: step 2357, loss 0.00230881, acc 1
2016-09-07T02:56:56.029291: step 2358, loss 0.0101857, acc 1
2016-09-07T02:56:56.704798: step 2359, loss 0.112179, acc 0.98
2016-09-07T02:56:57.380210: step 2360, loss 0.0236397, acc 1
2016-09-07T02:56:58.080198: step 2361, loss 0.0566317, acc 0.98
2016-09-07T02:56:58.761442: step 2362, loss 0.0111501, acc 1
2016-09-07T02:56:59.462341: step 2363, loss 0.0256194, acc 0.98
2016-09-07T02:57:00.144492: step 2364, loss 0.0083386, acc 1
2016-09-07T02:57:00.852722: step 2365, loss 0.0877729, acc 0.98
2016-09-07T02:57:01.540709: step 2366, loss 0.00898967, acc 1
2016-09-07T02:57:02.243848: step 2367, loss 0.0822205, acc 0.96
2016-09-07T02:57:02.960194: step 2368, loss 0.113183, acc 0.94
2016-09-07T02:57:03.611105: step 2369, loss 0.0507262, acc 0.98
2016-09-07T02:57:04.301841: step 2370, loss 0.0526458, acc 0.96
2016-09-07T02:57:04.984625: step 2371, loss 0.1588, acc 0.98
2016-09-07T02:57:05.664398: step 2372, loss 0.00349199, acc 1
2016-09-07T02:57:06.349026: step 2373, loss 0.0324232, acc 1
2016-09-07T02:57:07.036148: step 2374, loss 0.0330482, acc 1
2016-09-07T02:57:07.753570: step 2375, loss 0.0802269, acc 0.96
2016-09-07T02:57:08.459235: step 2376, loss 0.0375523, acc 0.98
2016-09-07T02:57:09.136976: step 2377, loss 0.00998873, acc 1
2016-09-07T02:57:09.814132: step 2378, loss 0.00719788, acc 1
2016-09-07T02:57:10.514620: step 2379, loss 0.038222, acc 1
2016-09-07T02:57:11.198723: step 2380, loss 0.0322423, acc 0.96
2016-09-07T02:57:11.881482: step 2381, loss 0.0402854, acc 0.98
2016-09-07T02:57:12.578972: step 2382, loss 0.0772316, acc 0.98
2016-09-07T02:57:13.253352: step 2383, loss 0.0202006, acc 1
2016-09-07T02:57:13.947935: step 2384, loss 0.0465017, acc 0.96
2016-09-07T02:57:14.661370: step 2385, loss 0.0404974, acc 0.98
2016-09-07T02:57:15.341565: step 2386, loss 0.0284028, acc 1
2016-09-07T02:57:16.039219: step 2387, loss 0.0080255, acc 1
2016-09-07T02:57:16.742077: step 2388, loss 0.0167051, acc 1
2016-09-07T02:57:17.473258: step 2389, loss 0.0136641, acc 1
2016-09-07T02:57:18.165056: step 2390, loss 0.0144956, acc 1
2016-09-07T02:57:18.856810: step 2391, loss 0.0437086, acc 0.98
2016-09-07T02:57:19.550896: step 2392, loss 0.116005, acc 0.96
2016-09-07T02:57:20.251668: step 2393, loss 0.0172254, acc 1
2016-09-07T02:57:20.957753: step 2394, loss 0.059821, acc 0.96
2016-09-07T02:57:21.643656: step 2395, loss 0.030357, acc 1
2016-09-07T02:57:22.363838: step 2396, loss 0.0940964, acc 0.94
2016-09-07T02:57:23.052284: step 2397, loss 0.00714529, acc 1
2016-09-07T02:57:23.741334: step 2398, loss 0.0762823, acc 0.94
2016-09-07T02:57:24.441166: step 2399, loss 0.020869, acc 0.98
2016-09-07T02:57:25.136758: step 2400, loss 0.0137449, acc 1

Evaluation:
2016-09-07T02:57:28.655461: step 2400, loss 1.57566, acc 0.768293

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-2400

2016-09-07T02:57:30.431744: step 2401, loss 0.082446, acc 0.98
2016-09-07T02:57:31.139486: step 2402, loss 0.0645468, acc 0.94
2016-09-07T02:57:31.847331: step 2403, loss 0.014347, acc 1
2016-09-07T02:57:32.531854: step 2404, loss 0.0153698, acc 0.98
2016-09-07T02:57:33.213700: step 2405, loss 0.00867053, acc 1
2016-09-07T02:57:33.905920: step 2406, loss 0.0226225, acc 1
2016-09-07T02:57:34.595713: step 2407, loss 0.104428, acc 0.98
2016-09-07T02:57:35.276979: step 2408, loss 0.113671, acc 0.98
2016-09-07T02:57:35.950442: step 2409, loss 0.0552596, acc 0.98
2016-09-07T02:57:36.670727: step 2410, loss 0.131529, acc 0.96
2016-09-07T02:57:37.358585: step 2411, loss 0.029824, acc 1
2016-09-07T02:57:38.057830: step 2412, loss 0.0495916, acc 0.98
2016-09-07T02:57:38.745109: step 2413, loss 0.0557347, acc 0.96
2016-09-07T02:57:39.438398: step 2414, loss 0.0234966, acc 0.98
2016-09-07T02:57:40.116157: step 2415, loss 0.0685186, acc 0.96
2016-09-07T02:57:40.781955: step 2416, loss 0.0549216, acc 0.98
2016-09-07T02:57:41.494685: step 2417, loss 0.112172, acc 0.96
2016-09-07T02:57:42.188069: step 2418, loss 0.032548, acc 0.98
2016-09-07T02:57:42.858656: step 2419, loss 0.0957225, acc 0.98
2016-09-07T02:57:43.535300: step 2420, loss 0.171868, acc 0.94
2016-09-07T02:57:44.225426: step 2421, loss 0.0831325, acc 0.98
2016-09-07T02:57:44.909544: step 2422, loss 0.0175539, acc 1
2016-09-07T02:57:45.595916: step 2423, loss 0.0101205, acc 1
2016-09-07T02:57:46.305986: step 2424, loss 0.0203134, acc 1
2016-09-07T02:57:46.978780: step 2425, loss 0.0452375, acc 0.98
2016-09-07T02:57:47.675231: step 2426, loss 0.0113765, acc 1
2016-09-07T02:57:48.396422: step 2427, loss 0.00600132, acc 1
2016-09-07T02:57:49.098808: step 2428, loss 0.00220772, acc 1
2016-09-07T02:57:49.803988: step 2429, loss 0.0281159, acc 0.98
2016-09-07T02:57:50.467514: step 2430, loss 0.0338743, acc 0.98
2016-09-07T02:57:51.168152: step 2431, loss 0.0430644, acc 0.98
2016-09-07T02:57:51.848389: step 2432, loss 0.0431871, acc 0.98
2016-09-07T02:57:52.530561: step 2433, loss 0.0528993, acc 0.98
2016-09-07T02:57:53.214682: step 2434, loss 0.0338623, acc 0.98
2016-09-07T02:57:53.891818: step 2435, loss 0.0202237, acc 1
2016-09-07T02:57:54.563117: step 2436, loss 0.0447048, acc 0.96
2016-09-07T02:57:55.229704: step 2437, loss 0.0641607, acc 0.98
2016-09-07T02:57:55.948225: step 2438, loss 0.0622541, acc 0.96
2016-09-07T02:57:56.627991: step 2439, loss 0.0933266, acc 0.98
2016-09-07T02:57:57.303746: step 2440, loss 0.0672699, acc 0.96
2016-09-07T02:57:57.984105: step 2441, loss 0.0193478, acc 0.98
2016-09-07T02:57:58.669270: step 2442, loss 0.063089, acc 0.98
2016-09-07T02:57:59.362156: step 2443, loss 0.00545453, acc 1
2016-09-07T02:58:00.022939: step 2444, loss 0.0408587, acc 1
2016-09-07T02:58:00.758334: step 2445, loss 0.0708211, acc 0.96
2016-09-07T02:58:01.438392: step 2446, loss 0.0520312, acc 0.98
2016-09-07T02:58:02.125504: step 2447, loss 0.0720959, acc 0.98
2016-09-07T02:58:02.821103: step 2448, loss 0.124725, acc 0.98
2016-09-07T02:58:03.527048: step 2449, loss 0.0509185, acc 0.98
2016-09-07T02:58:04.223082: step 2450, loss 0.0641018, acc 0.98
2016-09-07T02:58:04.892406: step 2451, loss 0.0847706, acc 0.96
2016-09-07T02:58:05.593374: step 2452, loss 0.0928374, acc 0.96
2016-09-07T02:58:06.271058: step 2453, loss 0.049685, acc 0.96
2016-09-07T02:58:06.951583: step 2454, loss 0.0724236, acc 0.94
2016-09-07T02:58:07.639640: step 2455, loss 0.0345907, acc 0.98
2016-09-07T02:58:08.351603: step 2456, loss 0.0188843, acc 1
2016-09-07T02:58:09.076690: step 2457, loss 0.021837, acc 1
2016-09-07T02:58:09.764607: step 2458, loss 0.0432821, acc 0.98
2016-09-07T02:58:10.480211: step 2459, loss 0.0389479, acc 0.98
2016-09-07T02:58:11.155779: step 2460, loss 0.0304183, acc 1
2016-09-07T02:58:11.835367: step 2461, loss 0.0346148, acc 0.98
2016-09-07T02:58:12.502717: step 2462, loss 0.0324806, acc 1
2016-09-07T02:58:13.190644: step 2463, loss 0.0235237, acc 0.98
2016-09-07T02:58:13.880308: step 2464, loss 0.0574798, acc 0.98
2016-09-07T02:58:14.558181: step 2465, loss 0.00119958, acc 1
2016-09-07T02:58:15.255937: step 2466, loss 0.0252083, acc 0.98
2016-09-07T02:58:15.958117: step 2467, loss 0.0287963, acc 0.98
2016-09-07T02:58:16.654708: step 2468, loss 0.0292151, acc 0.98
2016-09-07T02:58:17.339265: step 2469, loss 0.0161926, acc 1
2016-09-07T02:58:18.033899: step 2470, loss 0.0334763, acc 0.98
2016-09-07T02:58:18.750752: step 2471, loss 0.00998262, acc 1
2016-09-07T02:58:19.423015: step 2472, loss 0.081003, acc 0.96
2016-09-07T02:58:20.097888: step 2473, loss 0.0836967, acc 0.94
2016-09-07T02:58:20.783728: step 2474, loss 0.0534746, acc 0.94
2016-09-07T02:58:21.469531: step 2475, loss 0.02481, acc 1
2016-09-07T02:58:22.180806: step 2476, loss 0.0828814, acc 0.94
2016-09-07T02:58:22.853333: step 2477, loss 0.0407214, acc 0.98
2016-09-07T02:58:23.556741: step 2478, loss 0.056361, acc 0.98
2016-09-07T02:58:24.213020: step 2479, loss 0.0638513, acc 0.96
2016-09-07T02:58:24.896976: step 2480, loss 0.039858, acc 0.96
2016-09-07T02:58:25.571986: step 2481, loss 0.0188575, acc 1
2016-09-07T02:58:26.257833: step 2482, loss 0.0281912, acc 1
2016-09-07T02:58:26.989394: step 2483, loss 0.026193, acc 0.98
2016-09-07T02:58:27.681832: step 2484, loss 0.0142826, acc 1
2016-09-07T02:58:28.385914: step 2485, loss 0.0112552, acc 1
2016-09-07T02:58:29.062517: step 2486, loss 0.124528, acc 0.94
2016-09-07T02:58:29.755792: step 2487, loss 0.21791, acc 0.98
2016-09-07T02:58:30.446043: step 2488, loss 0.0495941, acc 0.98
2016-09-07T02:58:31.121521: step 2489, loss 0.0309515, acc 0.98
2016-09-07T02:58:31.811696: step 2490, loss 0.0619103, acc 0.96
2016-09-07T02:58:32.480770: step 2491, loss 0.0871295, acc 0.96
2016-09-07T02:58:33.185956: step 2492, loss 0.00281272, acc 1
2016-09-07T02:58:33.849206: step 2493, loss 0.0730948, acc 0.96
2016-09-07T02:58:34.536373: step 2494, loss 0.0547477, acc 0.96
2016-09-07T02:58:35.205626: step 2495, loss 0.079923, acc 0.96
2016-09-07T02:58:35.844331: step 2496, loss 0.0813113, acc 0.931818
2016-09-07T02:58:36.527946: step 2497, loss 0.0330008, acc 0.98
2016-09-07T02:58:37.197721: step 2498, loss 0.120952, acc 0.9
2016-09-07T02:58:37.910483: step 2499, loss 0.0240923, acc 1
2016-09-07T02:58:38.583229: step 2500, loss 0.0486428, acc 0.98

Evaluation:
2016-09-07T02:58:42.143516: step 2500, loss 1.27233, acc 0.767355

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-2500

2016-09-07T02:58:43.858388: step 2501, loss 0.0302622, acc 1
2016-09-07T02:58:44.558138: step 2502, loss 0.101997, acc 0.96
2016-09-07T02:58:45.255265: step 2503, loss 0.084346, acc 0.96
2016-09-07T02:58:45.935501: step 2504, loss 0.0362556, acc 0.98
2016-09-07T02:58:46.614261: step 2505, loss 0.0248329, acc 1
2016-09-07T02:58:47.300092: step 2506, loss 0.0572996, acc 0.96
2016-09-07T02:58:47.988664: step 2507, loss 0.0830574, acc 0.94
2016-09-07T02:58:48.692394: step 2508, loss 0.0347771, acc 1
2016-09-07T02:58:49.356411: step 2509, loss 0.132325, acc 0.96
2016-09-07T02:58:50.065031: step 2510, loss 0.037763, acc 0.98
2016-09-07T02:58:50.756847: step 2511, loss 0.0250477, acc 1
2016-09-07T02:58:51.425663: step 2512, loss 0.0073548, acc 1
2016-09-07T02:58:52.115911: step 2513, loss 0.0530495, acc 0.96
2016-09-07T02:58:52.812279: step 2514, loss 0.053916, acc 0.98
2016-09-07T02:58:53.492655: step 2515, loss 0.111064, acc 0.96
2016-09-07T02:58:54.158500: step 2516, loss 0.00619476, acc 1
2016-09-07T02:58:54.875727: step 2517, loss 0.051444, acc 0.98
2016-09-07T02:58:55.572447: step 2518, loss 0.0727122, acc 0.98
2016-09-07T02:58:56.263694: step 2519, loss 0.0454819, acc 0.96
2016-09-07T02:58:56.968720: step 2520, loss 0.0309264, acc 0.98
2016-09-07T02:58:57.672733: step 2521, loss 0.0378614, acc 0.98
2016-09-07T02:58:58.397382: step 2522, loss 0.00209158, acc 1
2016-09-07T02:58:59.088366: step 2523, loss 0.0283062, acc 1
2016-09-07T02:58:59.770420: step 2524, loss 0.0158759, acc 1
2016-09-07T02:59:00.466725: step 2525, loss 0.0231119, acc 1
2016-09-07T02:59:01.152052: step 2526, loss 0.00848339, acc 1
2016-09-07T02:59:01.845414: step 2527, loss 0.0217162, acc 0.98
2016-09-07T02:59:02.555715: step 2528, loss 0.0793004, acc 0.94
2016-09-07T02:59:03.301592: step 2529, loss 0.0292389, acc 0.98
2016-09-07T02:59:04.007987: step 2530, loss 0.0303155, acc 0.98
2016-09-07T02:59:04.698972: step 2531, loss 0.0207437, acc 1
2016-09-07T02:59:05.402311: step 2532, loss 0.0354281, acc 0.98
2016-09-07T02:59:06.081178: step 2533, loss 0.07352, acc 0.98
2016-09-07T02:59:06.793794: step 2534, loss 0.0165935, acc 1
2016-09-07T02:59:07.456907: step 2535, loss 0.00578055, acc 1
2016-09-07T02:59:08.153352: step 2536, loss 0.0162118, acc 1
2016-09-07T02:59:08.843740: step 2537, loss 0.0409166, acc 0.96
2016-09-07T02:59:09.529100: step 2538, loss 0.0423657, acc 0.98
2016-09-07T02:59:10.205252: step 2539, loss 0.136139, acc 0.98
2016-09-07T02:59:10.872556: step 2540, loss 0.0249786, acc 1
2016-09-07T02:59:11.576784: step 2541, loss 0.0623894, acc 0.96
2016-09-07T02:59:12.243394: step 2542, loss 0.00574712, acc 1
2016-09-07T02:59:12.958760: step 2543, loss 0.0354861, acc 0.98
2016-09-07T02:59:13.639211: step 2544, loss 0.0148734, acc 1
2016-09-07T02:59:14.363587: step 2545, loss 0.051525, acc 0.98
2016-09-07T02:59:15.060067: step 2546, loss 0.0289073, acc 0.98
2016-09-07T02:59:15.732421: step 2547, loss 0.0216518, acc 0.98
2016-09-07T02:59:16.446934: step 2548, loss 0.0284805, acc 0.98
2016-09-07T02:59:17.121291: step 2549, loss 0.0172341, acc 0.98
2016-09-07T02:59:17.802711: step 2550, loss 0.0244673, acc 1
2016-09-07T02:59:18.492612: step 2551, loss 0.0294057, acc 1
2016-09-07T02:59:19.176196: step 2552, loss 0.0558746, acc 0.94
2016-09-07T02:59:19.856962: step 2553, loss 0.0529205, acc 0.94
2016-09-07T02:59:20.553227: step 2554, loss 0.000216797, acc 1
2016-09-07T02:59:21.267616: step 2555, loss 0.0491765, acc 0.96
2016-09-07T02:59:21.961912: step 2556, loss 0.0283884, acc 0.98
2016-09-07T02:59:22.660439: step 2557, loss 0.0166671, acc 1
2016-09-07T02:59:23.352042: step 2558, loss 0.028608, acc 0.98
2016-09-07T02:59:24.050757: step 2559, loss 0.0695637, acc 0.96
2016-09-07T02:59:24.749150: step 2560, loss 0.0900453, acc 0.98
2016-09-07T02:59:25.398260: step 2561, loss 0.0183299, acc 1
2016-09-07T02:59:26.119623: step 2562, loss 0.105684, acc 0.96
2016-09-07T02:59:26.808903: step 2563, loss 0.0360393, acc 0.98
2016-09-07T02:59:27.495248: step 2564, loss 0.064766, acc 0.98
2016-09-07T02:59:28.185809: step 2565, loss 0.174984, acc 0.96
2016-09-07T02:59:28.892619: step 2566, loss 0.0125682, acc 1
2016-09-07T02:59:29.580383: step 2567, loss 0.0276361, acc 1
2016-09-07T02:59:30.236215: step 2568, loss 0.0489516, acc 1
2016-09-07T02:59:30.938279: step 2569, loss 0.0164893, acc 1
2016-09-07T02:59:31.641494: step 2570, loss 0.0380533, acc 0.98
2016-09-07T02:59:32.349479: step 2571, loss 0.00154347, acc 1
2016-09-07T02:59:33.048846: step 2572, loss 0.0689584, acc 0.94
2016-09-07T02:59:33.739707: step 2573, loss 0.00391363, acc 1
2016-09-07T02:59:34.415314: step 2574, loss 0.00511621, acc 1
2016-09-07T02:59:35.097302: step 2575, loss 0.0412567, acc 0.98
2016-09-07T02:59:35.795169: step 2576, loss 0.0691532, acc 0.96
2016-09-07T02:59:36.504658: step 2577, loss 0.0211125, acc 1
2016-09-07T02:59:37.200056: step 2578, loss 0.0143559, acc 1
2016-09-07T02:59:37.907579: step 2579, loss 0.00482042, acc 1
2016-09-07T02:59:38.613045: step 2580, loss 0.0950366, acc 0.96
2016-09-07T02:59:39.319178: step 2581, loss 0.0103771, acc 1
2016-09-07T02:59:39.995475: step 2582, loss 0.0370779, acc 0.98
2016-09-07T02:59:40.671175: step 2583, loss 0.0341825, acc 0.98
2016-09-07T02:59:41.374207: step 2584, loss 0.0697236, acc 0.96
2016-09-07T02:59:42.058330: step 2585, loss 0.0422917, acc 0.98
2016-09-07T02:59:42.762746: step 2586, loss 0.0410912, acc 0.98
2016-09-07T02:59:43.440274: step 2587, loss 0.0606692, acc 0.96
2016-09-07T02:59:44.160586: step 2588, loss 0.0631416, acc 0.96
2016-09-07T02:59:44.843308: step 2589, loss 0.00454232, acc 1
2016-09-07T02:59:45.549493: step 2590, loss 0.0370376, acc 0.96
2016-09-07T02:59:46.230363: step 2591, loss 0.0370374, acc 0.98
2016-09-07T02:59:46.922177: step 2592, loss 0.0508699, acc 0.98
2016-09-07T02:59:47.615009: step 2593, loss 0.111457, acc 0.96
2016-09-07T02:59:48.276572: step 2594, loss 0.0834778, acc 0.96
2016-09-07T02:59:48.996447: step 2595, loss 0.102777, acc 0.92
2016-09-07T02:59:49.683178: step 2596, loss 0.0333999, acc 1
2016-09-07T02:59:50.397433: step 2597, loss 0.0162694, acc 1
2016-09-07T02:59:51.086570: step 2598, loss 0.0307277, acc 0.98
2016-09-07T02:59:51.773590: step 2599, loss 0.0668126, acc 0.96
2016-09-07T02:59:52.471101: step 2600, loss 0.0359702, acc 0.98

Evaluation:
2016-09-07T02:59:55.920611: step 2600, loss 1.52947, acc 0.760788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-2600

2016-09-07T02:59:57.766977: step 2601, loss 0.0308368, acc 1
2016-09-07T02:59:58.458810: step 2602, loss 0.123383, acc 0.96
2016-09-07T02:59:59.120800: step 2603, loss 0.0511849, acc 0.96
2016-09-07T02:59:59.817459: step 2604, loss 0.0346924, acc 0.98
2016-09-07T03:00:00.540628: step 2605, loss 0.0445125, acc 0.98
2016-09-07T03:00:01.229839: step 2606, loss 0.0549097, acc 0.96
2016-09-07T03:00:01.967753: step 2607, loss 0.0436997, acc 0.98
2016-09-07T03:00:02.666797: step 2608, loss 0.0132016, acc 1
2016-09-07T03:00:03.379499: step 2609, loss 0.0738833, acc 0.96
2016-09-07T03:00:04.056207: step 2610, loss 0.0291245, acc 0.98
2016-09-07T03:00:04.747541: step 2611, loss 0.0132283, acc 1
2016-09-07T03:00:05.472951: step 2612, loss 0.0312399, acc 0.98
2016-09-07T03:00:06.156010: step 2613, loss 0.0203183, acc 1
2016-09-07T03:00:06.844174: step 2614, loss 0.0294891, acc 0.98
2016-09-07T03:00:07.506182: step 2615, loss 0.0278989, acc 0.98
2016-09-07T03:00:08.231908: step 2616, loss 0.0338915, acc 1
2016-09-07T03:00:08.965840: step 2617, loss 0.0244788, acc 1
2016-09-07T03:00:09.646590: step 2618, loss 0.0480058, acc 0.98
2016-09-07T03:00:10.350146: step 2619, loss 0.0169683, acc 1
2016-09-07T03:00:11.045770: step 2620, loss 0.022783, acc 0.98
2016-09-07T03:00:11.743399: step 2621, loss 0.0267642, acc 1
2016-09-07T03:00:12.400576: step 2622, loss 0.0412672, acc 0.96
2016-09-07T03:00:13.102298: step 2623, loss 0.00734338, acc 1
2016-09-07T03:00:13.817905: step 2624, loss 0.0459985, acc 0.98
2016-09-07T03:00:14.522202: step 2625, loss 0.00326885, acc 1
2016-09-07T03:00:15.204591: step 2626, loss 0.0277177, acc 0.98
2016-09-07T03:00:15.888849: step 2627, loss 0.129211, acc 0.94
2016-09-07T03:00:16.592908: step 2628, loss 0.141505, acc 0.98
2016-09-07T03:00:17.285460: step 2629, loss 0.0340228, acc 0.98
2016-09-07T03:00:17.974587: step 2630, loss 0.0304592, acc 0.98
2016-09-07T03:00:18.665438: step 2631, loss 0.0617977, acc 0.98
2016-09-07T03:00:19.340838: step 2632, loss 0.0924172, acc 0.96
2016-09-07T03:00:20.014911: step 2633, loss 0.0731567, acc 0.96
2016-09-07T03:00:20.710497: step 2634, loss 0.03177, acc 0.98
2016-09-07T03:00:21.467213: step 2635, loss 0.00865784, acc 1
2016-09-07T03:00:22.165923: step 2636, loss 0.0333041, acc 0.98
2016-09-07T03:00:22.856523: step 2637, loss 0.0197574, acc 1
2016-09-07T03:00:23.555738: step 2638, loss 0.0290453, acc 0.98
2016-09-07T03:00:24.243089: step 2639, loss 0.06981, acc 0.98
2016-09-07T03:00:24.974242: step 2640, loss 0.0464865, acc 0.98
2016-09-07T03:00:25.651328: step 2641, loss 0.00896081, acc 1
2016-09-07T03:00:26.351618: step 2642, loss 0.0405434, acc 0.98
2016-09-07T03:00:27.046663: step 2643, loss 0.00356177, acc 1
2016-09-07T03:00:27.748380: step 2644, loss 0.148268, acc 0.92
2016-09-07T03:00:28.442978: step 2645, loss 0.0824759, acc 0.96
2016-09-07T03:00:29.135978: step 2646, loss 0.0566789, acc 0.96
2016-09-07T03:00:29.846324: step 2647, loss 0.0856744, acc 0.98
2016-09-07T03:00:30.517781: step 2648, loss 0.0252947, acc 1
2016-09-07T03:00:31.192276: step 2649, loss 0.017338, acc 1
2016-09-07T03:00:31.870633: step 2650, loss 0.0333884, acc 0.98
2016-09-07T03:00:32.554794: step 2651, loss 0.0350831, acc 0.98
2016-09-07T03:00:33.227288: step 2652, loss 0.0036051, acc 1
2016-09-07T03:00:33.913964: step 2653, loss 0.0083785, acc 1
2016-09-07T03:00:34.624055: step 2654, loss 0.026608, acc 1
2016-09-07T03:00:35.315045: step 2655, loss 0.0661413, acc 0.98
2016-09-07T03:00:36.007256: step 2656, loss 0.0822173, acc 0.96
2016-09-07T03:00:36.697671: step 2657, loss 0.00216362, acc 1
2016-09-07T03:00:37.379318: step 2658, loss 0.110659, acc 0.94
2016-09-07T03:00:38.069548: step 2659, loss 0.00642466, acc 1
2016-09-07T03:00:38.747645: step 2660, loss 0.000690552, acc 1
2016-09-07T03:00:39.456299: step 2661, loss 0.106358, acc 0.94
2016-09-07T03:00:40.131887: step 2662, loss 0.0721784, acc 0.98
2016-09-07T03:00:40.811831: step 2663, loss 0.0220472, acc 0.98
2016-09-07T03:00:41.523309: step 2664, loss 0.1457, acc 0.94
2016-09-07T03:00:42.197435: step 2665, loss 0.068507, acc 0.96
2016-09-07T03:00:42.866882: step 2666, loss 0.188443, acc 0.94
2016-09-07T03:00:43.553674: step 2667, loss 0.0302142, acc 1
2016-09-07T03:00:44.274873: step 2668, loss 0.00805659, acc 1
2016-09-07T03:00:44.942044: step 2669, loss 0.0151198, acc 1
2016-09-07T03:00:45.621747: step 2670, loss 0.094329, acc 0.96
2016-09-07T03:00:46.318821: step 2671, loss 0.00103104, acc 1
2016-09-07T03:00:47.027558: step 2672, loss 0.103048, acc 0.98
2016-09-07T03:00:47.751544: step 2673, loss 0.143559, acc 0.92
2016-09-07T03:00:48.423838: step 2674, loss 0.0185742, acc 1
2016-09-07T03:00:49.127483: step 2675, loss 0.0270075, acc 0.98
2016-09-07T03:00:49.831942: step 2676, loss 0.0074747, acc 1
2016-09-07T03:00:50.521709: step 2677, loss 0.0308827, acc 0.98
2016-09-07T03:00:51.222673: step 2678, loss 0.00725712, acc 1
2016-09-07T03:00:51.903908: step 2679, loss 0.0454053, acc 0.96
2016-09-07T03:00:52.595065: step 2680, loss 0.003873, acc 1
2016-09-07T03:00:53.275731: step 2681, loss 0.0499041, acc 0.98
2016-09-07T03:00:53.957899: step 2682, loss 0.0166551, acc 1
2016-09-07T03:00:54.648893: step 2683, loss 0.016986, acc 1
2016-09-07T03:00:55.329410: step 2684, loss 0.0273052, acc 0.98
2016-09-07T03:00:56.027413: step 2685, loss 0.040396, acc 1
2016-09-07T03:00:56.721875: step 2686, loss 0.0496926, acc 0.98
2016-09-07T03:00:57.396207: step 2687, loss 0.0196218, acc 1
2016-09-07T03:00:58.031482: step 2688, loss 0.103864, acc 0.977273
2016-09-07T03:00:58.742018: step 2689, loss 0.0320068, acc 0.98
2016-09-07T03:00:59.422218: step 2690, loss 0.0499263, acc 0.96
2016-09-07T03:01:00.126058: step 2691, loss 0.0298867, acc 0.98
2016-09-07T03:01:00.845838: step 2692, loss 0.0684118, acc 0.96
2016-09-07T03:01:01.537611: step 2693, loss 0.0208221, acc 1
2016-09-07T03:01:02.252537: step 2694, loss 0.0417058, acc 0.98
2016-09-07T03:01:02.928081: step 2695, loss 0.0194303, acc 0.98
2016-09-07T03:01:03.624486: step 2696, loss 0.0789381, acc 0.98
2016-09-07T03:01:04.324968: step 2697, loss 0.0357417, acc 0.96
2016-09-07T03:01:05.018269: step 2698, loss 0.00978456, acc 1
2016-09-07T03:01:05.693773: step 2699, loss 0.1426, acc 0.94
2016-09-07T03:01:06.384631: step 2700, loss 0.0413164, acc 0.98

Evaluation:
2016-09-07T03:01:09.933618: step 2700, loss 1.39931, acc 0.773921

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-2700

2016-09-07T03:01:11.678698: step 2701, loss 0.044897, acc 0.96
2016-09-07T03:01:12.355580: step 2702, loss 0.08248, acc 0.96
2016-09-07T03:01:13.052363: step 2703, loss 0.0203988, acc 0.98
2016-09-07T03:01:13.729668: step 2704, loss 0.00601109, acc 1
2016-09-07T03:01:14.434821: step 2705, loss 0.0390754, acc 0.98
2016-09-07T03:01:15.137010: step 2706, loss 0.0984812, acc 0.96
2016-09-07T03:01:15.842104: step 2707, loss 0.0777158, acc 0.98
2016-09-07T03:01:16.542922: step 2708, loss 0.0707227, acc 0.98
2016-09-07T03:01:17.224128: step 2709, loss 0.0494813, acc 0.96
2016-09-07T03:01:17.930810: step 2710, loss 0.0818267, acc 0.96
2016-09-07T03:01:18.613427: step 2711, loss 0.042708, acc 0.98
2016-09-07T03:01:19.291727: step 2712, loss 0.011943, acc 1
2016-09-07T03:01:19.967221: step 2713, loss 0.0499801, acc 0.96
2016-09-07T03:01:20.682909: step 2714, loss 0.0056832, acc 1
2016-09-07T03:01:21.394198: step 2715, loss 0.00539345, acc 1
2016-09-07T03:01:22.057219: step 2716, loss 0.0211744, acc 1
2016-09-07T03:01:22.756139: step 2717, loss 0.107989, acc 0.96
2016-09-07T03:01:23.451443: step 2718, loss 0.049913, acc 0.96
2016-09-07T03:01:24.138889: step 2719, loss 0.00875901, acc 1
2016-09-07T03:01:24.828824: step 2720, loss 0.0782819, acc 0.96
2016-09-07T03:01:25.517357: step 2721, loss 0.0206492, acc 1
2016-09-07T03:01:26.222535: step 2722, loss 0.0442312, acc 0.98
2016-09-07T03:01:26.891011: step 2723, loss 0.0344064, acc 0.98
2016-09-07T03:01:27.605621: step 2724, loss 0.0140787, acc 1
2016-09-07T03:01:28.293242: step 2725, loss 0.0121529, acc 1
2016-09-07T03:01:28.997432: step 2726, loss 0.0277273, acc 0.98
2016-09-07T03:01:29.687203: step 2727, loss 0.0148285, acc 0.98
2016-09-07T03:01:30.380775: step 2728, loss 0.00828567, acc 1
2016-09-07T03:01:31.109467: step 2729, loss 0.038192, acc 0.98
2016-09-07T03:01:31.789927: step 2730, loss 0.0363746, acc 0.98
2016-09-07T03:01:32.480465: step 2731, loss 0.00451371, acc 1
2016-09-07T03:01:33.184840: step 2732, loss 0.0466342, acc 0.98
2016-09-07T03:01:33.880568: step 2733, loss 0.0245811, acc 1
2016-09-07T03:01:34.599230: step 2734, loss 0.0204108, acc 1
2016-09-07T03:01:35.312734: step 2735, loss 0.0114612, acc 1
2016-09-07T03:01:36.018835: step 2736, loss 0.098144, acc 0.96
2016-09-07T03:01:36.720856: step 2737, loss 0.0168625, acc 1
2016-09-07T03:01:37.421776: step 2738, loss 0.076834, acc 0.96
2016-09-07T03:01:38.101410: step 2739, loss 0.11296, acc 0.96
2016-09-07T03:01:38.790895: step 2740, loss 0.00357547, acc 1
2016-09-07T03:01:39.479765: step 2741, loss 0.000335707, acc 1
2016-09-07T03:01:40.148988: step 2742, loss 0.0214338, acc 1
2016-09-07T03:01:40.875803: step 2743, loss 0.0274295, acc 0.98
2016-09-07T03:01:41.578502: step 2744, loss 0.0190036, acc 1
2016-09-07T03:01:42.254813: step 2745, loss 0.0709508, acc 0.98
2016-09-07T03:01:42.939953: step 2746, loss 0.0651997, acc 0.98
2016-09-07T03:01:43.628551: step 2747, loss 0.012323, acc 1
2016-09-07T03:01:44.308895: step 2748, loss 0.0908463, acc 0.94
2016-09-07T03:01:44.972964: step 2749, loss 0.151787, acc 0.92
2016-09-07T03:01:45.679738: step 2750, loss 0.0507263, acc 0.96
2016-09-07T03:01:46.383219: step 2751, loss 0.019368, acc 1
2016-09-07T03:01:47.054494: step 2752, loss 0.00986893, acc 1
2016-09-07T03:01:47.721394: step 2753, loss 0.0476623, acc 0.96
2016-09-07T03:01:48.394968: step 2754, loss 0.0409332, acc 0.98
2016-09-07T03:01:49.098500: step 2755, loss 0.0186688, acc 1
2016-09-07T03:01:49.795488: step 2756, loss 0.0115323, acc 1
2016-09-07T03:01:50.502910: step 2757, loss 0.0189403, acc 1
2016-09-07T03:01:51.190890: step 2758, loss 0.0468059, acc 0.98
2016-09-07T03:01:51.877414: step 2759, loss 0.00232643, acc 1
2016-09-07T03:01:52.546503: step 2760, loss 0.109652, acc 0.96
2016-09-07T03:01:53.245147: step 2761, loss 0.00738123, acc 1
2016-09-07T03:01:53.942561: step 2762, loss 0.0107817, acc 1
2016-09-07T03:01:54.618243: step 2763, loss 0.0109445, acc 1
2016-09-07T03:01:55.320915: step 2764, loss 0.0913564, acc 0.94
2016-09-07T03:01:56.036019: step 2765, loss 0.0108653, acc 1
2016-09-07T03:01:56.734379: step 2766, loss 0.0109357, acc 1
2016-09-07T03:01:57.415709: step 2767, loss 0.0521076, acc 0.98
2016-09-07T03:01:58.099768: step 2768, loss 0.0203376, acc 1
2016-09-07T03:01:58.835959: step 2769, loss 0.0312412, acc 0.98
2016-09-07T03:01:59.552514: step 2770, loss 0.0277157, acc 0.98
2016-09-07T03:02:00.257201: step 2771, loss 0.0565886, acc 0.96
2016-09-07T03:02:00.931147: step 2772, loss 0.0520851, acc 0.98
2016-09-07T03:02:01.626065: step 2773, loss 0.0567565, acc 0.96
2016-09-07T03:02:02.337951: step 2774, loss 0.0208549, acc 1
2016-09-07T03:02:03.006848: step 2775, loss 0.00205138, acc 1
2016-09-07T03:02:03.727197: step 2776, loss 0.0298363, acc 0.98
2016-09-07T03:02:04.430098: step 2777, loss 0.086786, acc 0.98
2016-09-07T03:02:05.139381: step 2778, loss 0.0566781, acc 0.98
2016-09-07T03:02:05.835800: step 2779, loss 0.00843293, acc 1
2016-09-07T03:02:06.518783: step 2780, loss 0.0168519, acc 0.98
2016-09-07T03:02:07.230843: step 2781, loss 0.0183428, acc 1
2016-09-07T03:02:07.915724: step 2782, loss 0.0186093, acc 1
2016-09-07T03:02:08.582423: step 2783, loss 0.0339886, acc 1
2016-09-07T03:02:09.270987: step 2784, loss 0.00306487, acc 1
2016-09-07T03:02:09.962504: step 2785, loss 0.0615303, acc 0.98
2016-09-07T03:02:10.641907: step 2786, loss 0.2474, acc 0.96
2016-09-07T03:02:11.312005: step 2787, loss 0.0161661, acc 1
2016-09-07T03:02:12.023591: step 2788, loss 0.0274019, acc 0.98
2016-09-07T03:02:12.701106: step 2789, loss 0.0178223, acc 0.98
2016-09-07T03:02:13.398349: step 2790, loss 0.022683, acc 1
2016-09-07T03:02:14.099359: step 2791, loss 0.0638354, acc 0.98
2016-09-07T03:02:14.791672: step 2792, loss 0.0860358, acc 0.96
2016-09-07T03:02:15.466653: step 2793, loss 0.047088, acc 0.96
2016-09-07T03:02:16.162819: step 2794, loss 0.0589512, acc 0.96
2016-09-07T03:02:16.886355: step 2795, loss 0.0117188, acc 1
2016-09-07T03:02:17.583627: step 2796, loss 0.229024, acc 0.96
2016-09-07T03:02:18.279394: step 2797, loss 0.0340643, acc 1
2016-09-07T03:02:18.968722: step 2798, loss 0.0243195, acc 1
2016-09-07T03:02:19.666185: step 2799, loss 0.116124, acc 0.9
2016-09-07T03:02:20.368689: step 2800, loss 0.00169658, acc 1

Evaluation:
2016-09-07T03:02:24.036386: step 2800, loss 1.4105, acc 0.766416

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-2800

2016-09-07T03:02:25.788540: step 2801, loss 0.0662668, acc 0.96
2016-09-07T03:02:26.507179: step 2802, loss 0.109828, acc 0.96
2016-09-07T03:02:27.189150: step 2803, loss 0.00441897, acc 1
2016-09-07T03:02:27.871793: step 2804, loss 0.0708424, acc 0.96
2016-09-07T03:02:28.567708: step 2805, loss 0.0180882, acc 1
2016-09-07T03:02:29.260301: step 2806, loss 0.0313688, acc 0.98
2016-09-07T03:02:29.949269: step 2807, loss 0.0234189, acc 1
2016-09-07T03:02:30.605850: step 2808, loss 0.0151875, acc 1
2016-09-07T03:02:31.301196: step 2809, loss 0.0255522, acc 1
2016-09-07T03:02:31.982426: step 2810, loss 0.0611463, acc 0.96
2016-09-07T03:02:32.671101: step 2811, loss 0.122505, acc 0.94
2016-09-07T03:02:33.350134: step 2812, loss 0.0162715, acc 0.98
2016-09-07T03:02:34.053326: step 2813, loss 0.0809837, acc 0.92
2016-09-07T03:02:34.732516: step 2814, loss 0.0766561, acc 0.98
2016-09-07T03:02:35.392524: step 2815, loss 0.0858767, acc 0.96
2016-09-07T03:02:36.113287: step 2816, loss 0.0073176, acc 1
2016-09-07T03:02:36.802324: step 2817, loss 0.016828, acc 1
2016-09-07T03:02:37.481026: step 2818, loss 0.0303639, acc 0.98
2016-09-07T03:02:38.173330: step 2819, loss 0.11715, acc 0.94
2016-09-07T03:02:38.875118: step 2820, loss 0.00905555, acc 1
2016-09-07T03:02:39.576352: step 2821, loss 0.0482441, acc 0.96
2016-09-07T03:02:40.251193: step 2822, loss 0.0266417, acc 0.98
2016-09-07T03:02:40.951122: step 2823, loss 0.0298589, acc 0.98
2016-09-07T03:02:41.651015: step 2824, loss 0.0644282, acc 0.94
2016-09-07T03:02:42.342482: step 2825, loss 0.0267256, acc 1
2016-09-07T03:02:43.017240: step 2826, loss 0.00577566, acc 1
2016-09-07T03:02:43.719677: step 2827, loss 0.0510027, acc 0.98
2016-09-07T03:02:44.421023: step 2828, loss 0.026425, acc 0.98
2016-09-07T03:02:45.074593: step 2829, loss 0.0151579, acc 1
2016-09-07T03:02:45.782479: step 2830, loss 0.0521414, acc 0.98
2016-09-07T03:02:46.488336: step 2831, loss 0.0204249, acc 1
2016-09-07T03:02:47.178071: step 2832, loss 0.0759398, acc 0.96
2016-09-07T03:02:47.871092: step 2833, loss 0.00884425, acc 1
2016-09-07T03:02:48.567428: step 2834, loss 0.089487, acc 0.98
2016-09-07T03:02:49.276856: step 2835, loss 0.0394994, acc 0.98
2016-09-07T03:02:49.947342: step 2836, loss 0.0230588, acc 0.98
2016-09-07T03:02:50.643940: step 2837, loss 0.0443035, acc 0.98
2016-09-07T03:02:51.315565: step 2838, loss 0.0431442, acc 0.98
2016-09-07T03:02:52.004695: step 2839, loss 0.0392259, acc 0.98
2016-09-07T03:02:52.693254: step 2840, loss 0.0104661, acc 1
2016-09-07T03:02:53.391653: step 2841, loss 0.00177609, acc 1
2016-09-07T03:02:54.099936: step 2842, loss 0.0817621, acc 0.94
2016-09-07T03:02:54.770640: step 2843, loss 0.242315, acc 0.94
2016-09-07T03:02:55.475189: step 2844, loss 0.0141048, acc 1
2016-09-07T03:02:56.178538: step 2845, loss 0.0453306, acc 0.96
2016-09-07T03:02:56.877128: step 2846, loss 0.0588717, acc 0.96
2016-09-07T03:02:57.581664: step 2847, loss 0.0280716, acc 1
2016-09-07T03:02:58.251636: step 2848, loss 0.0570573, acc 0.98
2016-09-07T03:02:58.986345: step 2849, loss 0.0180713, acc 0.98
2016-09-07T03:02:59.682432: step 2850, loss 0.0395378, acc 0.98
2016-09-07T03:03:00.399151: step 2851, loss 0.0511999, acc 0.96
2016-09-07T03:03:01.092450: step 2852, loss 0.0484581, acc 0.96
2016-09-07T03:03:01.800486: step 2853, loss 0.0607272, acc 0.98
2016-09-07T03:03:02.525221: step 2854, loss 0.0477563, acc 0.98
2016-09-07T03:03:03.209624: step 2855, loss 0.00664194, acc 1
2016-09-07T03:03:03.899263: step 2856, loss 0.135561, acc 0.92
2016-09-07T03:03:04.600507: step 2857, loss 0.0500832, acc 0.98
2016-09-07T03:03:05.298165: step 2858, loss 0.00367411, acc 1
2016-09-07T03:03:06.022219: step 2859, loss 0.0286586, acc 1
2016-09-07T03:03:06.688454: step 2860, loss 0.0294528, acc 0.98
2016-09-07T03:03:07.396821: step 2861, loss 0.00731431, acc 1
2016-09-07T03:03:08.067588: step 2862, loss 0.0302925, acc 0.98
2016-09-07T03:03:08.774799: step 2863, loss 0.0147627, acc 1
2016-09-07T03:03:09.453932: step 2864, loss 0.0989277, acc 0.94
2016-09-07T03:03:10.130036: step 2865, loss 0.0544544, acc 1
2016-09-07T03:03:10.818029: step 2866, loss 0.154088, acc 0.92
2016-09-07T03:03:11.488152: step 2867, loss 0.103164, acc 0.94
2016-09-07T03:03:12.198305: step 2868, loss 0.137041, acc 0.96
2016-09-07T03:03:12.885617: step 2869, loss 0.0415116, acc 0.98
2016-09-07T03:03:13.578667: step 2870, loss 0.0248196, acc 1
2016-09-07T03:03:14.271180: step 2871, loss 0.080419, acc 0.96
2016-09-07T03:03:14.963588: step 2872, loss 0.0915489, acc 0.94
2016-09-07T03:03:15.654749: step 2873, loss 0.0394862, acc 0.98
2016-09-07T03:03:16.323617: step 2874, loss 0.0455095, acc 0.96
2016-09-07T03:03:17.027731: step 2875, loss 0.0228245, acc 1
2016-09-07T03:03:17.727420: step 2876, loss 0.0132299, acc 1
2016-09-07T03:03:18.412109: step 2877, loss 0.0482209, acc 0.96
2016-09-07T03:03:19.091979: step 2878, loss 0.123021, acc 0.96
2016-09-07T03:03:19.785556: step 2879, loss 0.066391, acc 0.98
2016-09-07T03:03:20.447435: step 2880, loss 0.00355719, acc 1
2016-09-07T03:03:21.119492: step 2881, loss 0.0372327, acc 0.98
2016-09-07T03:03:21.841082: step 2882, loss 0.0186261, acc 1
2016-09-07T03:03:22.538969: step 2883, loss 0.0515833, acc 0.94
2016-09-07T03:03:23.241759: step 2884, loss 0.0119102, acc 1
2016-09-07T03:03:23.916707: step 2885, loss 0.0177478, acc 1
2016-09-07T03:03:24.618855: step 2886, loss 0.108231, acc 0.98
2016-09-07T03:03:25.328057: step 2887, loss 0.0235856, acc 1
2016-09-07T03:03:26.017736: step 2888, loss 0.0160552, acc 1
2016-09-07T03:03:26.706375: step 2889, loss 0.0238043, acc 1
2016-09-07T03:03:27.382686: step 2890, loss 0.00778232, acc 1
2016-09-07T03:03:28.076561: step 2891, loss 0.00805079, acc 1
2016-09-07T03:03:28.761873: step 2892, loss 0.0419401, acc 0.96
2016-09-07T03:03:29.447716: step 2893, loss 0.0839997, acc 0.98
2016-09-07T03:03:30.137886: step 2894, loss 0.0217416, acc 1
2016-09-07T03:03:30.844643: step 2895, loss 0.000992393, acc 1
2016-09-07T03:03:31.512727: step 2896, loss 0.00481794, acc 1
2016-09-07T03:03:32.196550: step 2897, loss 0.154381, acc 0.96
2016-09-07T03:03:32.899863: step 2898, loss 0.0294485, acc 1
2016-09-07T03:03:33.597805: step 2899, loss 0.0372232, acc 0.98
2016-09-07T03:03:34.280279: step 2900, loss 0.0955707, acc 0.96

Evaluation:
2016-09-07T03:03:37.869695: step 2900, loss 1.5046, acc 0.763602

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-2900

2016-09-07T03:03:39.665945: step 2901, loss 0.131673, acc 0.98
2016-09-07T03:03:40.333926: step 2902, loss 0.0343981, acc 0.98
2016-09-07T03:03:41.038365: step 2903, loss 0.0673177, acc 0.96
2016-09-07T03:03:41.711151: step 2904, loss 0.00136911, acc 1
2016-09-07T03:03:42.390884: step 2905, loss 0.0779091, acc 0.96
2016-09-07T03:03:43.065872: step 2906, loss 0.0110941, acc 1
2016-09-07T03:03:43.753612: step 2907, loss 0.00411696, acc 1
2016-09-07T03:03:44.435317: step 2908, loss 0.0334091, acc 0.98
2016-09-07T03:03:45.102910: step 2909, loss 0.0190008, acc 1
2016-09-07T03:03:45.810028: step 2910, loss 0.0304384, acc 0.98
2016-09-07T03:03:46.486289: step 2911, loss 0.0180485, acc 1
2016-09-07T03:03:47.185499: step 2912, loss 0.0701651, acc 0.98
2016-09-07T03:03:47.869157: step 2913, loss 0.0255837, acc 1
2016-09-07T03:03:48.554908: step 2914, loss 0.0380549, acc 0.98
2016-09-07T03:03:49.242444: step 2915, loss 0.00134295, acc 1
2016-09-07T03:03:49.914579: step 2916, loss 0.0181863, acc 1
2016-09-07T03:03:50.636221: step 2917, loss 0.0676858, acc 0.96
2016-09-07T03:03:51.339877: step 2918, loss 0.0315164, acc 1
2016-09-07T03:03:52.036425: step 2919, loss 0.00319201, acc 1
2016-09-07T03:03:52.737152: step 2920, loss 0.00194094, acc 1
2016-09-07T03:03:53.422833: step 2921, loss 0.0673598, acc 0.98
2016-09-07T03:03:54.118877: step 2922, loss 0.02138, acc 1
2016-09-07T03:03:54.786181: step 2923, loss 0.0680389, acc 0.94
2016-09-07T03:03:55.505674: step 2924, loss 0.021099, acc 1
2016-09-07T03:03:56.201569: step 2925, loss 0.074137, acc 0.96
2016-09-07T03:03:56.928256: step 2926, loss 0.0670764, acc 0.94
2016-09-07T03:03:57.606455: step 2927, loss 0.0273766, acc 0.98
2016-09-07T03:03:58.308830: step 2928, loss 0.0201436, acc 1
2016-09-07T03:03:59.018187: step 2929, loss 0.00401358, acc 1
2016-09-07T03:03:59.699132: step 2930, loss 0.0279375, acc 0.98
2016-09-07T03:04:00.456025: step 2931, loss 0.017999, acc 1
2016-09-07T03:04:01.177937: step 2932, loss 0.0110457, acc 1
2016-09-07T03:04:01.873791: step 2933, loss 0.0657929, acc 0.96
2016-09-07T03:04:02.580569: step 2934, loss 0.0285595, acc 0.98
2016-09-07T03:04:03.274564: step 2935, loss 0.0166739, acc 1
2016-09-07T03:04:03.974804: step 2936, loss 0.0617691, acc 0.98
2016-09-07T03:04:04.654642: step 2937, loss 0.138037, acc 0.98
2016-09-07T03:04:05.335429: step 2938, loss 0.00227818, acc 1
2016-09-07T03:04:06.044302: step 2939, loss 0.00277908, acc 1
2016-09-07T03:04:06.733741: step 2940, loss 0.0738806, acc 0.98
2016-09-07T03:04:07.450412: step 2941, loss 0.0188047, acc 1
2016-09-07T03:04:08.124924: step 2942, loss 0.0598577, acc 0.98
2016-09-07T03:04:08.817212: step 2943, loss 0.0192336, acc 0.98
2016-09-07T03:04:09.527819: step 2944, loss 0.128176, acc 0.96
2016-09-07T03:04:10.231193: step 2945, loss 0.073223, acc 0.98
2016-09-07T03:04:10.934862: step 2946, loss 0.0643134, acc 0.98
2016-09-07T03:04:11.601064: step 2947, loss 0.022209, acc 1
2016-09-07T03:04:12.319789: step 2948, loss 0.0366688, acc 0.98
2016-09-07T03:04:13.029465: step 2949, loss 0.0252469, acc 1
2016-09-07T03:04:13.740660: step 2950, loss 0.0277663, acc 1
2016-09-07T03:04:14.416791: step 2951, loss 0.0529763, acc 0.96
2016-09-07T03:04:15.131093: step 2952, loss 0.0833351, acc 0.94
2016-09-07T03:04:15.873706: step 2953, loss 0.00329049, acc 1
2016-09-07T03:04:16.586281: step 2954, loss 0.0110066, acc 1
2016-09-07T03:04:17.304230: step 2955, loss 0.03897, acc 0.98
2016-09-07T03:04:18.004441: step 2956, loss 0.108078, acc 0.94
2016-09-07T03:04:18.690292: step 2957, loss 0.00637472, acc 1
2016-09-07T03:04:19.376396: step 2958, loss 0.0185637, acc 0.98
2016-09-07T03:04:20.030017: step 2959, loss 0.0673956, acc 0.96
2016-09-07T03:04:20.744739: step 2960, loss 0.0232382, acc 1
2016-09-07T03:04:21.426167: step 2961, loss 0.00874613, acc 1
2016-09-07T03:04:22.111949: step 2962, loss 0.0441346, acc 0.98
2016-09-07T03:04:22.808526: step 2963, loss 0.0460124, acc 0.96
2016-09-07T03:04:23.489907: step 2964, loss 0.0527597, acc 0.98
2016-09-07T03:04:24.218504: step 2965, loss 0.00587002, acc 1
2016-09-07T03:04:24.893870: step 2966, loss 0.0633754, acc 0.98
2016-09-07T03:04:25.604437: step 2967, loss 0.0320771, acc 0.98
2016-09-07T03:04:26.309946: step 2968, loss 0.0181947, acc 1
2016-09-07T03:04:26.999976: step 2969, loss 0.040623, acc 0.98
2016-09-07T03:04:27.703717: step 2970, loss 0.0264394, acc 1
2016-09-07T03:04:28.381057: step 2971, loss 0.00970495, acc 1
2016-09-07T03:04:29.075070: step 2972, loss 0.0234631, acc 1
2016-09-07T03:04:29.755134: step 2973, loss 0.0113119, acc 1
2016-09-07T03:04:30.423325: step 2974, loss 0.027121, acc 0.98
2016-09-07T03:04:31.112354: step 2975, loss 0.0387651, acc 0.98
2016-09-07T03:04:31.799154: step 2976, loss 0.0605797, acc 0.96
2016-09-07T03:04:32.507319: step 2977, loss 0.0303321, acc 0.98
2016-09-07T03:04:33.206723: step 2978, loss 0.044448, acc 0.98
2016-09-07T03:04:33.934689: step 2979, loss 0.00847051, acc 1
2016-09-07T03:04:34.635697: step 2980, loss 0.0268041, acc 0.98
2016-09-07T03:04:35.336037: step 2981, loss 0.0653737, acc 0.98
2016-09-07T03:04:36.020625: step 2982, loss 0.0109453, acc 1
2016-09-07T03:04:36.703373: step 2983, loss 0.071835, acc 0.94
2016-09-07T03:04:37.391361: step 2984, loss 0.067594, acc 0.98
2016-09-07T03:04:38.067837: step 2985, loss 0.0196562, acc 1
2016-09-07T03:04:38.784942: step 2986, loss 0.0161579, acc 1
2016-09-07T03:04:39.468482: step 2987, loss 0.0172447, acc 1
2016-09-07T03:04:40.158292: step 2988, loss 0.0324186, acc 0.98
2016-09-07T03:04:40.838260: step 2989, loss 0.019642, acc 1
2016-09-07T03:04:41.523407: step 2990, loss 0.0489145, acc 0.96
2016-09-07T03:04:42.278336: step 2991, loss 0.0284005, acc 0.98
2016-09-07T03:04:42.935367: step 2992, loss 0.000951779, acc 1
2016-09-07T03:04:43.635374: step 2993, loss 0.0608254, acc 0.98
2016-09-07T03:04:44.343333: step 2994, loss 0.0348253, acc 0.98
2016-09-07T03:04:45.048046: step 2995, loss 0.10612, acc 0.98
2016-09-07T03:04:45.728620: step 2996, loss 0.0279605, acc 1
2016-09-07T03:04:46.421021: step 2997, loss 0.137507, acc 0.98
2016-09-07T03:04:47.143972: step 2998, loss 0.0134475, acc 1
2016-09-07T03:04:47.848011: step 2999, loss 0.00469869, acc 1
2016-09-07T03:04:48.551543: step 3000, loss 0.0359566, acc 0.98

Evaluation:
2016-09-07T03:04:52.128106: step 3000, loss 1.74473, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-3000

2016-09-07T03:04:53.819449: step 3001, loss 0.0370328, acc 0.98
2016-09-07T03:04:54.518373: step 3002, loss 0.0552004, acc 0.96
2016-09-07T03:04:55.223631: step 3003, loss 0.00268648, acc 1
2016-09-07T03:04:55.896798: step 3004, loss 0.00841006, acc 1
2016-09-07T03:04:56.613086: step 3005, loss 0.0509912, acc 1
2016-09-07T03:04:57.282028: step 3006, loss 0.0103203, acc 1
2016-09-07T03:04:57.981379: step 3007, loss 0.0453073, acc 0.98
2016-09-07T03:04:58.668402: step 3008, loss 0.0203235, acc 0.98
2016-09-07T03:04:59.351969: step 3009, loss 0.156589, acc 0.94
2016-09-07T03:05:00.047068: step 3010, loss 0.0906645, acc 0.94
2016-09-07T03:05:00.767609: step 3011, loss 0.180118, acc 0.94
2016-09-07T03:05:01.455658: step 3012, loss 0.0766578, acc 0.98
2016-09-07T03:05:02.137435: step 3013, loss 0.0152889, acc 1
2016-09-07T03:05:02.846559: step 3014, loss 0.0365404, acc 0.98
2016-09-07T03:05:03.532212: step 3015, loss 0.0451021, acc 0.98
2016-09-07T03:05:04.222627: step 3016, loss 0.109651, acc 0.96
2016-09-07T03:05:04.916353: step 3017, loss 0.0230027, acc 0.98
2016-09-07T03:05:05.597143: step 3018, loss 0.0927042, acc 0.98
2016-09-07T03:05:06.278987: step 3019, loss 0.0149399, acc 1
2016-09-07T03:05:06.957395: step 3020, loss 0.0485499, acc 0.98
2016-09-07T03:05:07.677450: step 3021, loss 0.00446144, acc 1
2016-09-07T03:05:08.380236: step 3022, loss 0.0581615, acc 0.98
2016-09-07T03:05:09.090607: step 3023, loss 0.0773733, acc 0.98
2016-09-07T03:05:09.782969: step 3024, loss 0.0415424, acc 0.98
2016-09-07T03:05:10.465596: step 3025, loss 0.0164766, acc 1
2016-09-07T03:05:11.194408: step 3026, loss 0.0333232, acc 0.98
2016-09-07T03:05:11.881448: step 3027, loss 0.0884608, acc 0.94
2016-09-07T03:05:12.565815: step 3028, loss 0.0143459, acc 1
2016-09-07T03:05:13.260612: step 3029, loss 0.0893872, acc 0.96
2016-09-07T03:05:13.946762: step 3030, loss 0.0822868, acc 0.98
2016-09-07T03:05:14.642876: step 3031, loss 0.0361212, acc 0.98
2016-09-07T03:05:15.326626: step 3032, loss 0.0173713, acc 1
2016-09-07T03:05:16.034529: step 3033, loss 0.00430386, acc 1
2016-09-07T03:05:16.739180: step 3034, loss 0.116532, acc 0.94
2016-09-07T03:05:17.439292: step 3035, loss 0.02913, acc 0.98
2016-09-07T03:05:18.157729: step 3036, loss 0.00167742, acc 1
2016-09-07T03:05:18.839005: step 3037, loss 0.00589893, acc 1
2016-09-07T03:05:19.545732: step 3038, loss 0.0484363, acc 0.96
2016-09-07T03:05:20.236498: step 3039, loss 0.0367358, acc 0.96
2016-09-07T03:05:20.898897: step 3040, loss 0.0244072, acc 1
2016-09-07T03:05:21.585501: step 3041, loss 0.0208134, acc 1
2016-09-07T03:05:22.271344: step 3042, loss 0.0432271, acc 0.98
2016-09-07T03:05:22.966384: step 3043, loss 0.0308878, acc 1
2016-09-07T03:05:23.651679: step 3044, loss 0.0161124, acc 0.98
2016-09-07T03:05:24.371338: step 3045, loss 0.0190109, acc 0.98
2016-09-07T03:05:25.049935: step 3046, loss 0.0214129, acc 0.98
2016-09-07T03:05:25.726836: step 3047, loss 0.0667533, acc 0.96
2016-09-07T03:05:26.399896: step 3048, loss 0.0173255, acc 1
2016-09-07T03:05:27.126200: step 3049, loss 0.0281881, acc 1
2016-09-07T03:05:27.842086: step 3050, loss 0.0355095, acc 0.98
2016-09-07T03:05:28.516532: step 3051, loss 0.00436375, acc 1
2016-09-07T03:05:29.213764: step 3052, loss 0.00207747, acc 1
2016-09-07T03:05:29.911153: step 3053, loss 0.0466063, acc 0.98
2016-09-07T03:05:30.607701: step 3054, loss 0.0213846, acc 1
2016-09-07T03:05:31.302859: step 3055, loss 0.0347222, acc 0.98
2016-09-07T03:05:31.988424: step 3056, loss 0.0194641, acc 0.98
2016-09-07T03:05:32.699233: step 3057, loss 0.0475498, acc 0.96
2016-09-07T03:05:33.365359: step 3058, loss 0.0342877, acc 0.98
2016-09-07T03:05:34.071059: step 3059, loss 0.0351177, acc 0.98
2016-09-07T03:05:34.768885: step 3060, loss 0.031315, acc 1
2016-09-07T03:05:35.479549: step 3061, loss 0.0584494, acc 0.98
2016-09-07T03:05:36.167184: step 3062, loss 0.00751144, acc 1
2016-09-07T03:05:36.832657: step 3063, loss 0.0729752, acc 0.96
2016-09-07T03:05:37.517583: step 3064, loss 0.0588413, acc 0.98
2016-09-07T03:05:38.193822: step 3065, loss 0.086442, acc 0.96
2016-09-07T03:05:38.900331: step 3066, loss 0.0224236, acc 1
2016-09-07T03:05:39.598956: step 3067, loss 0.0138776, acc 1
2016-09-07T03:05:40.283608: step 3068, loss 0.0101272, acc 1
2016-09-07T03:05:40.979281: step 3069, loss 0.0112343, acc 1
2016-09-07T03:05:41.679912: step 3070, loss 0.0152821, acc 1
2016-09-07T03:05:42.372671: step 3071, loss 0.0580113, acc 0.98
2016-09-07T03:05:43.021404: step 3072, loss 0.0563217, acc 0.954545
2016-09-07T03:05:43.729112: step 3073, loss 0.0686752, acc 0.96
2016-09-07T03:05:44.444733: step 3074, loss 0.0531723, acc 0.98
2016-09-07T03:05:45.136843: step 3075, loss 0.0155515, acc 0.98
2016-09-07T03:05:45.847144: step 3076, loss 0.0422761, acc 0.98
2016-09-07T03:05:46.555882: step 3077, loss 0.142238, acc 0.98
2016-09-07T03:05:47.268153: step 3078, loss 0.0585588, acc 0.96
2016-09-07T03:05:47.945116: step 3079, loss 0.0288401, acc 1
2016-09-07T03:05:48.638163: step 3080, loss 0.0465086, acc 0.98
2016-09-07T03:05:49.334825: step 3081, loss 0.0265545, acc 1
2016-09-07T03:05:50.011681: step 3082, loss 0.0575193, acc 0.98
2016-09-07T03:05:50.714338: step 3083, loss 0.0710274, acc 0.96
2016-09-07T03:05:51.364842: step 3084, loss 0.034069, acc 1
2016-09-07T03:05:52.067835: step 3085, loss 0.116864, acc 0.94
2016-09-07T03:05:52.748116: step 3086, loss 0.0356444, acc 0.98
2016-09-07T03:05:53.427041: step 3087, loss 0.00738697, acc 1
2016-09-07T03:05:54.111260: step 3088, loss 0.0025731, acc 1
2016-09-07T03:05:54.827866: step 3089, loss 0.0556263, acc 0.98
2016-09-07T03:05:55.517132: step 3090, loss 0.0398736, acc 0.98
2016-09-07T03:05:56.192182: step 3091, loss 0.0391039, acc 1
2016-09-07T03:05:56.900878: step 3092, loss 0.000781686, acc 1
2016-09-07T03:05:57.611981: step 3093, loss 0.0437314, acc 0.98
2016-09-07T03:05:58.302433: step 3094, loss 0.0542134, acc 0.98
2016-09-07T03:05:59.006840: step 3095, loss 0.0393657, acc 0.98
2016-09-07T03:05:59.702100: step 3096, loss 0.0794213, acc 0.96
2016-09-07T03:06:00.455962: step 3097, loss 0.0775754, acc 0.96
2016-09-07T03:06:01.125835: step 3098, loss 0.0038289, acc 1
2016-09-07T03:06:01.842891: step 3099, loss 0.00287585, acc 1
2016-09-07T03:06:02.543915: step 3100, loss 0.0127754, acc 1

Evaluation:
2016-09-07T03:06:06.190068: step 3100, loss 1.57447, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-3100

2016-09-07T03:06:07.926228: step 3101, loss 0.0210959, acc 0.98
2016-09-07T03:06:08.625072: step 3102, loss 0.0206522, acc 1
2016-09-07T03:06:09.325868: step 3103, loss 0.0887331, acc 0.94
2016-09-07T03:06:10.037639: step 3104, loss 0.0637235, acc 0.98
2016-09-07T03:06:10.730531: step 3105, loss 0.00876467, acc 1
2016-09-07T03:06:11.406400: step 3106, loss 0.0502292, acc 0.98
2016-09-07T03:06:12.098509: step 3107, loss 0.00707252, acc 1
2016-09-07T03:06:12.796991: step 3108, loss 0.0030969, acc 1
2016-09-07T03:06:13.475623: step 3109, loss 0.0432171, acc 0.96
2016-09-07T03:06:14.185115: step 3110, loss 0.16538, acc 0.92
2016-09-07T03:06:14.904566: step 3111, loss 0.0149907, acc 0.98
2016-09-07T03:06:15.601502: step 3112, loss 0.00077124, acc 1
2016-09-07T03:06:16.290458: step 3113, loss 0.0462992, acc 0.98
2016-09-07T03:06:16.974673: step 3114, loss 0.0243168, acc 1
2016-09-07T03:06:17.719976: step 3115, loss 0.125424, acc 0.96
2016-09-07T03:06:18.418423: step 3116, loss 0.052929, acc 0.96
2016-09-07T03:06:19.099593: step 3117, loss 0.0126299, acc 1
2016-09-07T03:06:19.820309: step 3118, loss 0.0426206, acc 0.96
2016-09-07T03:06:20.527595: step 3119, loss 0.0335636, acc 0.98
2016-09-07T03:06:21.214732: step 3120, loss 0.0639745, acc 0.96
2016-09-07T03:06:21.903176: step 3121, loss 0.0482745, acc 0.96
2016-09-07T03:06:22.588659: step 3122, loss 0.0295655, acc 0.98
2016-09-07T03:06:23.295342: step 3123, loss 0.0403229, acc 0.98
2016-09-07T03:06:23.963933: step 3124, loss 0.0232375, acc 0.98
2016-09-07T03:06:24.679694: step 3125, loss 0.0738569, acc 0.96
2016-09-07T03:06:25.373156: step 3126, loss 0.042297, acc 0.98
2016-09-07T03:06:26.074482: step 3127, loss 0.00290072, acc 1
2016-09-07T03:06:26.777696: step 3128, loss 0.00815941, acc 1
2016-09-07T03:06:27.437530: step 3129, loss 0.020693, acc 1
2016-09-07T03:06:28.173962: step 3130, loss 0.012582, acc 1
2016-09-07T03:06:28.871770: step 3131, loss 0.00453561, acc 1
2016-09-07T03:06:29.561799: step 3132, loss 0.0348126, acc 0.98
2016-09-07T03:06:30.265996: step 3133, loss 0.0353286, acc 0.98
2016-09-07T03:06:30.951617: step 3134, loss 0.033645, acc 0.98
2016-09-07T03:06:31.654878: step 3135, loss 0.066644, acc 0.94
2016-09-07T03:06:32.322525: step 3136, loss 0.038734, acc 0.98
2016-09-07T03:06:33.016086: step 3137, loss 0.0904699, acc 0.96
2016-09-07T03:06:33.715577: step 3138, loss 0.0247952, acc 1
2016-09-07T03:06:34.405358: step 3139, loss 0.0130252, acc 1
2016-09-07T03:06:35.085949: step 3140, loss 0.0118799, acc 1
2016-09-07T03:06:35.777182: step 3141, loss 0.017256, acc 0.98
2016-09-07T03:06:36.516260: step 3142, loss 0.0518514, acc 0.98
2016-09-07T03:06:37.204979: step 3143, loss 0.00575517, acc 1
2016-09-07T03:06:37.894872: step 3144, loss 0.0246032, acc 0.98
2016-09-07T03:06:38.592977: step 3145, loss 0.0114014, acc 1
2016-09-07T03:06:39.280847: step 3146, loss 0.0166934, acc 1
2016-09-07T03:06:39.960272: step 3147, loss 0.0194365, acc 0.98
2016-09-07T03:06:40.659684: step 3148, loss 0.0771833, acc 0.96
2016-09-07T03:06:41.382284: step 3149, loss 0.019971, acc 0.98
2016-09-07T03:06:42.069051: step 3150, loss 0.0168309, acc 1
2016-09-07T03:06:42.775174: step 3151, loss 0.0176372, acc 0.98
2016-09-07T03:06:43.469652: step 3152, loss 0.0140486, acc 1
2016-09-07T03:06:44.206059: step 3153, loss 0.0214758, acc 0.98
2016-09-07T03:06:44.916955: step 3154, loss 0.0440751, acc 0.98
2016-09-07T03:06:45.605219: step 3155, loss 0.0172317, acc 0.98
2016-09-07T03:06:46.291728: step 3156, loss 0.024605, acc 0.98
2016-09-07T03:06:46.964879: step 3157, loss 0.0374066, acc 0.98
2016-09-07T03:06:47.646394: step 3158, loss 0.0692824, acc 0.98
2016-09-07T03:06:48.342696: step 3159, loss 0.00961488, acc 1
2016-09-07T03:06:49.040949: step 3160, loss 0.037364, acc 0.98
2016-09-07T03:06:49.736147: step 3161, loss 0.0188666, acc 1
2016-09-07T03:06:50.387822: step 3162, loss 0.0430637, acc 0.96
2016-09-07T03:06:51.094951: step 3163, loss 0.0139596, acc 1
2016-09-07T03:06:51.791339: step 3164, loss 0.0615386, acc 0.98
2016-09-07T03:06:52.474092: step 3165, loss 0.130635, acc 0.98
2016-09-07T03:06:53.175303: step 3166, loss 0.0184677, acc 0.98
2016-09-07T03:06:53.874525: step 3167, loss 0.0408725, acc 0.96
2016-09-07T03:06:54.586681: step 3168, loss 0.0135512, acc 1
2016-09-07T03:06:55.272092: step 3169, loss 0.0214048, acc 1
2016-09-07T03:06:55.967350: step 3170, loss 0.012652, acc 1
2016-09-07T03:06:56.649877: step 3171, loss 0.00996306, acc 1
2016-09-07T03:06:57.351887: step 3172, loss 0.0418212, acc 1
2016-09-07T03:06:58.049535: step 3173, loss 0.0162056, acc 1
2016-09-07T03:06:58.722848: step 3174, loss 0.0397948, acc 0.98
2016-09-07T03:06:59.426538: step 3175, loss 0.0289476, acc 0.98
2016-09-07T03:07:00.115523: step 3176, loss 0.0469802, acc 0.96
2016-09-07T03:07:00.839212: step 3177, loss 0.0322447, acc 0.98
2016-09-07T03:07:01.524624: step 3178, loss 0.0277949, acc 0.98
2016-09-07T03:07:02.206179: step 3179, loss 0.00124016, acc 1
2016-09-07T03:07:02.922398: step 3180, loss 0.0440753, acc 0.98
2016-09-07T03:07:03.645949: step 3181, loss 0.027227, acc 0.98
2016-09-07T03:07:04.328714: step 3182, loss 0.030215, acc 1
2016-09-07T03:07:05.030160: step 3183, loss 0.0428364, acc 0.98
2016-09-07T03:07:05.730946: step 3184, loss 0.000923637, acc 1
2016-09-07T03:07:06.420406: step 3185, loss 0.0325752, acc 0.98
2016-09-07T03:07:07.099466: step 3186, loss 0.0175775, acc 0.98
2016-09-07T03:07:07.797868: step 3187, loss 0.185273, acc 0.96
2016-09-07T03:07:08.465389: step 3188, loss 0.102106, acc 0.94
2016-09-07T03:07:09.154483: step 3189, loss 0.00940236, acc 1
2016-09-07T03:07:09.842022: step 3190, loss 0.0798097, acc 0.98
2016-09-07T03:07:10.521600: step 3191, loss 0.0694597, acc 0.96
2016-09-07T03:07:11.204077: step 3192, loss 0.10962, acc 0.94
2016-09-07T03:07:11.885278: step 3193, loss 0.0258287, acc 0.98
2016-09-07T03:07:12.600149: step 3194, loss 0.212259, acc 0.98
2016-09-07T03:07:13.283282: step 3195, loss 0.00565467, acc 1
2016-09-07T03:07:13.978134: step 3196, loss 0.0291646, acc 0.98
2016-09-07T03:07:14.693867: step 3197, loss 0.0590984, acc 0.96
2016-09-07T03:07:15.381256: step 3198, loss 0.0358039, acc 0.98
2016-09-07T03:07:16.087036: step 3199, loss 0.0265462, acc 0.98
2016-09-07T03:07:16.751084: step 3200, loss 0.0151254, acc 1

Evaluation:
2016-09-07T03:07:20.332875: step 3200, loss 1.57793, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-3200

2016-09-07T03:07:22.034886: step 3201, loss 0.0568536, acc 0.98
2016-09-07T03:07:22.726821: step 3202, loss 0.0748067, acc 0.94
2016-09-07T03:07:23.410123: step 3203, loss 0.0330447, acc 1
2016-09-07T03:07:24.095112: step 3204, loss 0.0646969, acc 0.98
2016-09-07T03:07:24.770305: step 3205, loss 0.00795611, acc 1
2016-09-07T03:07:25.457571: step 3206, loss 0.0281054, acc 0.98
2016-09-07T03:07:26.121226: step 3207, loss 0.131055, acc 0.94
2016-09-07T03:07:26.814999: step 3208, loss 0.0782309, acc 0.94
2016-09-07T03:07:27.517412: step 3209, loss 0.00979378, acc 1
2016-09-07T03:07:28.233382: step 3210, loss 0.00024863, acc 1
2016-09-07T03:07:28.934035: step 3211, loss 0.0250238, acc 0.98
2016-09-07T03:07:29.649361: step 3212, loss 0.0649977, acc 0.96
2016-09-07T03:07:30.341915: step 3213, loss 0.0420285, acc 0.98
2016-09-07T03:07:31.041255: step 3214, loss 0.0236688, acc 1
2016-09-07T03:07:31.740974: step 3215, loss 0.063032, acc 0.96
2016-09-07T03:07:32.403686: step 3216, loss 0.00607378, acc 1
2016-09-07T03:07:33.129492: step 3217, loss 0.0158709, acc 1
2016-09-07T03:07:33.833894: step 3218, loss 0.0813777, acc 0.94
2016-09-07T03:07:34.518874: step 3219, loss 0.0356658, acc 0.98
2016-09-07T03:07:35.215842: step 3220, loss 0.0304467, acc 0.98
2016-09-07T03:07:35.916041: step 3221, loss 0.0509303, acc 0.98
2016-09-07T03:07:36.650125: step 3222, loss 0.0502327, acc 0.96
2016-09-07T03:07:37.351465: step 3223, loss 0.0396147, acc 0.96
2016-09-07T03:07:38.038719: step 3224, loss 0.0296878, acc 0.98
2016-09-07T03:07:38.730614: step 3225, loss 0.00339219, acc 1
2016-09-07T03:07:39.431984: step 3226, loss 0.0256531, acc 1
2016-09-07T03:07:40.115703: step 3227, loss 0.0848047, acc 0.98
2016-09-07T03:07:40.800430: step 3228, loss 0.00377717, acc 1
2016-09-07T03:07:41.520251: step 3229, loss 0.00370421, acc 1
2016-09-07T03:07:42.192653: step 3230, loss 0.145997, acc 0.92
2016-09-07T03:07:42.877508: step 3231, loss 0.0918877, acc 0.98
2016-09-07T03:07:43.572345: step 3232, loss 0.0484707, acc 1
2016-09-07T03:07:44.263589: step 3233, loss 0.0113763, acc 1
2016-09-07T03:07:44.967712: step 3234, loss 0.0350401, acc 1
2016-09-07T03:07:45.624008: step 3235, loss 0.0310148, acc 1
2016-09-07T03:07:46.336563: step 3236, loss 0.0380263, acc 0.96
2016-09-07T03:07:47.006331: step 3237, loss 0.00229599, acc 1
2016-09-07T03:07:47.700961: step 3238, loss 0.0262168, acc 0.98
2016-09-07T03:07:48.392269: step 3239, loss 0.0853488, acc 0.96
2016-09-07T03:07:49.079069: step 3240, loss 0.0476237, acc 0.98
2016-09-07T03:07:49.779657: step 3241, loss 0.020806, acc 1
2016-09-07T03:07:50.450182: step 3242, loss 0.0366338, acc 0.98
2016-09-07T03:07:51.180374: step 3243, loss 0.0140761, acc 1
2016-09-07T03:07:51.875049: step 3244, loss 0.011271, acc 1
2016-09-07T03:07:52.575969: step 3245, loss 0.133856, acc 0.96
2016-09-07T03:07:53.273004: step 3246, loss 0.0626443, acc 0.96
2016-09-07T03:07:53.976373: step 3247, loss 0.00800138, acc 1
2016-09-07T03:07:54.687930: step 3248, loss 0.0239635, acc 1
2016-09-07T03:07:55.382507: step 3249, loss 0.0462158, acc 0.96
2016-09-07T03:07:56.069859: step 3250, loss 0.0302494, acc 0.98
2016-09-07T03:07:56.765945: step 3251, loss 0.0196187, acc 1
2016-09-07T03:07:57.451707: step 3252, loss 0.0229549, acc 0.98
2016-09-07T03:07:58.149909: step 3253, loss 0.0161634, acc 1
2016-09-07T03:07:58.827977: step 3254, loss 0.00388448, acc 1
2016-09-07T03:07:59.532385: step 3255, loss 0.0271558, acc 0.98
2016-09-07T03:08:00.221903: step 3256, loss 0.0578403, acc 0.96
2016-09-07T03:08:00.911303: step 3257, loss 0.0155601, acc 1
2016-09-07T03:08:01.609553: step 3258, loss 0.0205833, acc 1
2016-09-07T03:08:02.305273: step 3259, loss 0.0310163, acc 0.98
2016-09-07T03:08:03.008415: step 3260, loss 0.0390333, acc 0.98
2016-09-07T03:08:03.675794: step 3261, loss 0.020975, acc 1
2016-09-07T03:08:04.405025: step 3262, loss 0.101947, acc 0.98
2016-09-07T03:08:05.088747: step 3263, loss 0.0511623, acc 0.96
2016-09-07T03:08:05.727521: step 3264, loss 0.0360042, acc 0.977273
2016-09-07T03:08:06.420542: step 3265, loss 0.0262849, acc 0.98
2016-09-07T03:08:07.097604: step 3266, loss 0.0883198, acc 0.96
2016-09-07T03:08:07.781294: step 3267, loss 0.130433, acc 0.98
2016-09-07T03:08:08.453182: step 3268, loss 0.0806029, acc 0.98
2016-09-07T03:08:09.146157: step 3269, loss 0.0316938, acc 0.98
2016-09-07T03:08:09.818749: step 3270, loss 0.0620393, acc 0.96
2016-09-07T03:08:10.501642: step 3271, loss 0.116889, acc 0.94
2016-09-07T03:08:11.188111: step 3272, loss 0.052304, acc 0.98
2016-09-07T03:08:11.881504: step 3273, loss 0.0603677, acc 0.96
2016-09-07T03:08:12.580831: step 3274, loss 0.12784, acc 0.96
2016-09-07T03:08:13.244637: step 3275, loss 0.00130937, acc 1
2016-09-07T03:08:13.947105: step 3276, loss 0.0525053, acc 0.98
2016-09-07T03:08:14.629301: step 3277, loss 0.0979415, acc 0.96
2016-09-07T03:08:15.320719: step 3278, loss 0.0461479, acc 0.98
2016-09-07T03:08:16.018543: step 3279, loss 0.133575, acc 0.98
2016-09-07T03:08:16.701284: step 3280, loss 0.00840186, acc 1
2016-09-07T03:08:17.414278: step 3281, loss 0.054146, acc 0.96
2016-09-07T03:08:18.074776: step 3282, loss 0.0103772, acc 1
2016-09-07T03:08:18.807541: step 3283, loss 0.0297854, acc 0.98
2016-09-07T03:08:19.488701: step 3284, loss 0.00309431, acc 1
2016-09-07T03:08:20.171282: step 3285, loss 0.0126741, acc 1
2016-09-07T03:08:20.873452: step 3286, loss 0.036891, acc 0.98
2016-09-07T03:08:21.564093: step 3287, loss 0.0318938, acc 0.98
2016-09-07T03:08:22.279544: step 3288, loss 0.0538052, acc 0.96
2016-09-07T03:08:22.957815: step 3289, loss 0.0220023, acc 0.98
2016-09-07T03:08:23.684979: step 3290, loss 0.0111682, acc 1
2016-09-07T03:08:24.366650: step 3291, loss 0.00483693, acc 1
2016-09-07T03:08:25.049717: step 3292, loss 0.020304, acc 0.98
2016-09-07T03:08:25.735566: step 3293, loss 0.0284278, acc 0.98
2016-09-07T03:08:26.420413: step 3294, loss 0.0896656, acc 0.96
2016-09-07T03:08:27.128502: step 3295, loss 0.0362173, acc 0.98
2016-09-07T03:08:27.809565: step 3296, loss 0.167924, acc 0.94
2016-09-07T03:08:28.478523: step 3297, loss 0.0219823, acc 1
2016-09-07T03:08:29.182796: step 3298, loss 0.0287577, acc 0.98
2016-09-07T03:08:29.866893: step 3299, loss 0.0758985, acc 0.96
2016-09-07T03:08:30.564017: step 3300, loss 0.0221264, acc 0.98

Evaluation:
2016-09-07T03:08:34.234738: step 3300, loss 1.45493, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-3300

2016-09-07T03:08:36.147418: step 3301, loss 0.000559824, acc 1
2016-09-07T03:08:36.851362: step 3302, loss 0.00543931, acc 1
2016-09-07T03:08:37.564008: step 3303, loss 0.0194915, acc 0.98
2016-09-07T03:08:38.273445: step 3304, loss 0.0484464, acc 0.98
2016-09-07T03:08:38.990154: step 3305, loss 0.0301283, acc 0.98
2016-09-07T03:08:39.678933: step 3306, loss 0.0209274, acc 0.98
2016-09-07T03:08:40.385707: step 3307, loss 0.0136236, acc 1
2016-09-07T03:08:41.049283: step 3308, loss 0.00014582, acc 1
2016-09-07T03:08:41.733897: step 3309, loss 0.0205045, acc 1
2016-09-07T03:08:42.424074: step 3310, loss 0.0100626, acc 1
2016-09-07T03:08:43.094139: step 3311, loss 0.05027, acc 0.98
2016-09-07T03:08:43.776593: step 3312, loss 0.017589, acc 1
2016-09-07T03:08:44.468365: step 3313, loss 0.0380569, acc 0.96
2016-09-07T03:08:45.178097: step 3314, loss 0.012185, acc 1
2016-09-07T03:08:45.843719: step 3315, loss 0.0582859, acc 0.96
2016-09-07T03:08:46.517103: step 3316, loss 0.000200311, acc 1
2016-09-07T03:08:47.200442: step 3317, loss 0.016146, acc 1
2016-09-07T03:08:47.900667: step 3318, loss 0.0344193, acc 0.96
2016-09-07T03:08:48.603102: step 3319, loss 0.0882276, acc 0.96
2016-09-07T03:08:49.290996: step 3320, loss 0.00893426, acc 1
2016-09-07T03:08:50.008832: step 3321, loss 0.0576438, acc 0.98
2016-09-07T03:08:50.684289: step 3322, loss 0.0536985, acc 0.98
2016-09-07T03:08:51.380063: step 3323, loss 0.0718402, acc 0.96
2016-09-07T03:08:52.071878: step 3324, loss 0.0317392, acc 0.98
2016-09-07T03:08:52.754178: step 3325, loss 0.0221988, acc 0.98
2016-09-07T03:08:53.451187: step 3326, loss 0.0433246, acc 0.98
2016-09-07T03:08:54.115862: step 3327, loss 0.0514261, acc 0.98
2016-09-07T03:08:54.807914: step 3328, loss 0.0409375, acc 0.98
2016-09-07T03:08:55.496742: step 3329, loss 0.00211807, acc 1
2016-09-07T03:08:56.188185: step 3330, loss 0.0341878, acc 1
2016-09-07T03:08:56.889132: step 3331, loss 0.00244113, acc 1
2016-09-07T03:08:57.583210: step 3332, loss 0.00457013, acc 1
2016-09-07T03:08:58.276099: step 3333, loss 0.013698, acc 1
2016-09-07T03:08:58.972198: step 3334, loss 0.0688229, acc 0.96
2016-09-07T03:08:59.655998: step 3335, loss 0.0368086, acc 1
2016-09-07T03:09:00.360440: step 3336, loss 0.0211823, acc 0.98
2016-09-07T03:09:01.046124: step 3337, loss 0.0287521, acc 0.98
2016-09-07T03:09:01.736460: step 3338, loss 0.0336792, acc 0.98
2016-09-07T03:09:02.417189: step 3339, loss 0.0349551, acc 0.98
2016-09-07T03:09:03.108073: step 3340, loss 0.0445022, acc 0.98
2016-09-07T03:09:03.817998: step 3341, loss 0.0278985, acc 1
2016-09-07T03:09:04.521868: step 3342, loss 0.0452728, acc 0.96
2016-09-07T03:09:05.194518: step 3343, loss 0.0188371, acc 1
2016-09-07T03:09:05.877145: step 3344, loss 0.0279337, acc 0.98
2016-09-07T03:09:06.566448: step 3345, loss 0.00471948, acc 1
2016-09-07T03:09:07.252039: step 3346, loss 0.0381239, acc 0.98
2016-09-07T03:09:07.956748: step 3347, loss 0.0189447, acc 0.98
2016-09-07T03:09:08.614968: step 3348, loss 0.00646021, acc 1
2016-09-07T03:09:09.358339: step 3349, loss 0.0558074, acc 0.98
2016-09-07T03:09:10.052732: step 3350, loss 0.0670217, acc 0.96
2016-09-07T03:09:10.748035: step 3351, loss 0.0517014, acc 0.98
2016-09-07T03:09:11.430567: step 3352, loss 0.0738814, acc 0.94
2016-09-07T03:09:12.109615: step 3353, loss 0.0233453, acc 1
2016-09-07T03:09:12.829889: step 3354, loss 0.0323996, acc 1
2016-09-07T03:09:13.511048: step 3355, loss 0.0101099, acc 1
2016-09-07T03:09:14.213587: step 3356, loss 0.0415287, acc 0.96
2016-09-07T03:09:14.919253: step 3357, loss 0.000893385, acc 1
2016-09-07T03:09:15.614367: step 3358, loss 0.0837056, acc 0.98
2016-09-07T03:09:16.305860: step 3359, loss 0.0211051, acc 0.98
2016-09-07T03:09:16.971085: step 3360, loss 0.00338683, acc 1
2016-09-07T03:09:17.676699: step 3361, loss 0.0273836, acc 0.98
2016-09-07T03:09:18.358417: step 3362, loss 0.0545288, acc 0.98
2016-09-07T03:09:19.071829: step 3363, loss 0.0889412, acc 0.98
2016-09-07T03:09:19.770079: step 3364, loss 0.0374348, acc 0.98
2016-09-07T03:09:20.450281: step 3365, loss 0.0064476, acc 1
2016-09-07T03:09:21.139198: step 3366, loss 0.123796, acc 0.94
2016-09-07T03:09:21.823148: step 3367, loss 0.0776171, acc 0.92
2016-09-07T03:09:22.533308: step 3368, loss 0.0119299, acc 1
2016-09-07T03:09:23.215274: step 3369, loss 0.00233436, acc 1
2016-09-07T03:09:23.938912: step 3370, loss 0.0636044, acc 0.96
2016-09-07T03:09:24.610747: step 3371, loss 0.00900168, acc 1
2016-09-07T03:09:25.317135: step 3372, loss 0.010397, acc 1
2016-09-07T03:09:26.029768: step 3373, loss 0.136769, acc 0.96
2016-09-07T03:09:26.702171: step 3374, loss 0.0150518, acc 1
2016-09-07T03:09:27.408291: step 3375, loss 0.0330882, acc 1
2016-09-07T03:09:28.087357: step 3376, loss 0.0555476, acc 0.98
2016-09-07T03:09:28.777048: step 3377, loss 0.056273, acc 0.98
2016-09-07T03:09:29.473479: step 3378, loss 0.0183887, acc 0.98
2016-09-07T03:09:30.165395: step 3379, loss 0.0332239, acc 0.98
2016-09-07T03:09:30.877467: step 3380, loss 0.152194, acc 0.92
2016-09-07T03:09:31.557416: step 3381, loss 0.0162698, acc 1
2016-09-07T03:09:32.265039: step 3382, loss 0.180813, acc 0.98
2016-09-07T03:09:32.956544: step 3383, loss 0.0298807, acc 0.98
2016-09-07T03:09:33.641994: step 3384, loss 0.0223927, acc 1
2016-09-07T03:09:34.327301: step 3385, loss 0.073151, acc 0.96
2016-09-07T03:09:35.004243: step 3386, loss 0.0156149, acc 1
2016-09-07T03:09:35.710240: step 3387, loss 0.0156434, acc 1
2016-09-07T03:09:36.406097: step 3388, loss 0.0463716, acc 0.98
2016-09-07T03:09:37.096045: step 3389, loss 0.12539, acc 0.92
2016-09-07T03:09:37.776647: step 3390, loss 0.0211318, acc 1
2016-09-07T03:09:38.457694: step 3391, loss 0.0307974, acc 0.98
2016-09-07T03:09:39.136279: step 3392, loss 0.0312781, acc 0.96
2016-09-07T03:09:39.822720: step 3393, loss 0.0195585, acc 0.98
2016-09-07T03:09:40.521821: step 3394, loss 0.0467371, acc 0.98
2016-09-07T03:09:41.203785: step 3395, loss 0.0347544, acc 1
2016-09-07T03:09:41.885011: step 3396, loss 0.0275458, acc 1
2016-09-07T03:09:42.581002: step 3397, loss 0.0111445, acc 1
2016-09-07T03:09:43.271162: step 3398, loss 0.0433844, acc 0.96
2016-09-07T03:09:43.950816: step 3399, loss 0.0722365, acc 0.96
2016-09-07T03:09:44.639290: step 3400, loss 0.0639826, acc 0.98

Evaluation:
2016-09-07T03:09:48.238065: step 3400, loss 1.4265, acc 0.765478

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-3400

2016-09-07T03:09:49.990508: step 3401, loss 0.0304503, acc 0.98
2016-09-07T03:09:50.666325: step 3402, loss 0.0233847, acc 1
2016-09-07T03:09:51.388395: step 3403, loss 0.0386954, acc 0.96
2016-09-07T03:09:52.094090: step 3404, loss 0.0405393, acc 0.98
2016-09-07T03:09:52.774891: step 3405, loss 0.0499243, acc 0.96
2016-09-07T03:09:53.470976: step 3406, loss 0.105669, acc 0.96
2016-09-07T03:09:54.156489: step 3407, loss 0.0165441, acc 1
2016-09-07T03:09:54.903936: step 3408, loss 0.00855321, acc 1
2016-09-07T03:09:55.617901: step 3409, loss 0.00745436, acc 1
2016-09-07T03:09:56.290891: step 3410, loss 0.0268212, acc 1
2016-09-07T03:09:56.984345: step 3411, loss 0.0447623, acc 0.98
2016-09-07T03:09:57.695687: step 3412, loss 0.026537, acc 1
2016-09-07T03:09:58.382920: step 3413, loss 0.0133311, acc 1
2016-09-07T03:09:59.088546: step 3414, loss 0.0709111, acc 0.96
2016-09-07T03:09:59.797462: step 3415, loss 0.0120447, acc 1
2016-09-07T03:10:00.522461: step 3416, loss 0.0634479, acc 0.98
2016-09-07T03:10:01.218272: step 3417, loss 0.0692983, acc 0.96
2016-09-07T03:10:01.910108: step 3418, loss 0.0321207, acc 1
2016-09-07T03:10:02.615040: step 3419, loss 0.0115022, acc 1
2016-09-07T03:10:03.350600: step 3420, loss 0.00955646, acc 1
2016-09-07T03:10:04.051183: step 3421, loss 0.000590876, acc 1
2016-09-07T03:10:04.751526: step 3422, loss 0.0471724, acc 0.98
2016-09-07T03:10:05.444581: step 3423, loss 0.0191571, acc 0.98
2016-09-07T03:10:06.115927: step 3424, loss 0.0221062, acc 0.98
2016-09-07T03:10:06.795729: step 3425, loss 0.022664, acc 0.98
2016-09-07T03:10:07.486433: step 3426, loss 0.0167147, acc 1
2016-09-07T03:10:08.199043: step 3427, loss 0.0266792, acc 1
2016-09-07T03:10:08.878243: step 3428, loss 0.00794213, acc 1
2016-09-07T03:10:09.554713: step 3429, loss 0.043318, acc 0.96
2016-09-07T03:10:10.249106: step 3430, loss 0.0350832, acc 0.96
2016-09-07T03:10:10.941713: step 3431, loss 0.0144786, acc 1
2016-09-07T03:10:11.672189: step 3432, loss 0.0157208, acc 1
2016-09-07T03:10:12.367651: step 3433, loss 0.0198735, acc 0.98
2016-09-07T03:10:13.061509: step 3434, loss 0.132386, acc 0.94
2016-09-07T03:10:13.750402: step 3435, loss 0.0150838, acc 1
2016-09-07T03:10:14.469735: step 3436, loss 0.00513347, acc 1
2016-09-07T03:10:15.181052: step 3437, loss 0.0980275, acc 0.98
2016-09-07T03:10:15.884640: step 3438, loss 0.0253881, acc 1
2016-09-07T03:10:16.587826: step 3439, loss 0.000697303, acc 1
2016-09-07T03:10:17.251990: step 3440, loss 0.0227695, acc 0.98
2016-09-07T03:10:17.934743: step 3441, loss 0.0585497, acc 0.98
2016-09-07T03:10:18.615901: step 3442, loss 0.102698, acc 0.96
2016-09-07T03:10:19.309988: step 3443, loss 0.0279317, acc 1
2016-09-07T03:10:20.007415: step 3444, loss 0.0363079, acc 0.98
2016-09-07T03:10:20.700771: step 3445, loss 0.0674088, acc 0.98
2016-09-07T03:10:21.403499: step 3446, loss 0.0054525, acc 1
2016-09-07T03:10:22.087285: step 3447, loss 0.00633774, acc 1
2016-09-07T03:10:22.766989: step 3448, loss 0.0676987, acc 0.98
2016-09-07T03:10:23.460235: step 3449, loss 0.0118876, acc 1
2016-09-07T03:10:24.142424: step 3450, loss 0.0313645, acc 1
2016-09-07T03:10:24.863196: step 3451, loss 0.0178143, acc 1
2016-09-07T03:10:25.530771: step 3452, loss 0.0181615, acc 0.98
2016-09-07T03:10:26.222135: step 3453, loss 0.00535148, acc 1
2016-09-07T03:10:26.895526: step 3454, loss 0.0262347, acc 0.98
2016-09-07T03:10:27.570335: step 3455, loss 0.0144418, acc 1
2016-09-07T03:10:28.212760: step 3456, loss 0.0282132, acc 0.977273
2016-09-07T03:10:28.900254: step 3457, loss 0.0164298, acc 1
2016-09-07T03:10:29.588741: step 3458, loss 0.0101974, acc 1
2016-09-07T03:10:30.292630: step 3459, loss 0.0259438, acc 1
2016-09-07T03:10:30.993887: step 3460, loss 0.0171739, acc 0.98
2016-09-07T03:10:31.670066: step 3461, loss 0.0583958, acc 0.96
2016-09-07T03:10:32.336610: step 3462, loss 0.0225437, acc 1
2016-09-07T03:10:33.043208: step 3463, loss 0.0140107, acc 1
2016-09-07T03:10:33.718305: step 3464, loss 0.00678191, acc 1
2016-09-07T03:10:34.393392: step 3465, loss 0.0301393, acc 0.98
2016-09-07T03:10:35.071444: step 3466, loss 0.00776687, acc 1
2016-09-07T03:10:35.796265: step 3467, loss 0.0621421, acc 0.96
2016-09-07T03:10:36.490227: step 3468, loss 0.0582647, acc 0.98
2016-09-07T03:10:37.175485: step 3469, loss 0.244868, acc 0.96
2016-09-07T03:10:37.886674: step 3470, loss 0.0489383, acc 0.98
2016-09-07T03:10:38.570209: step 3471, loss 0.00430676, acc 1
2016-09-07T03:10:39.260799: step 3472, loss 0.0103015, acc 1
2016-09-07T03:10:39.929785: step 3473, loss 0.0743016, acc 0.98
2016-09-07T03:10:40.638240: step 3474, loss 0.208778, acc 0.98
2016-09-07T03:10:41.326707: step 3475, loss 0.0180434, acc 1
2016-09-07T03:10:42.040209: step 3476, loss 0.0208379, acc 1
2016-09-07T03:10:42.741936: step 3477, loss 0.0242557, acc 1
2016-09-07T03:10:43.433629: step 3478, loss 0.0944989, acc 0.96
2016-09-07T03:10:44.117759: step 3479, loss 0.00479611, acc 1
2016-09-07T03:10:44.795363: step 3480, loss 0.0068352, acc 1
2016-09-07T03:10:45.495470: step 3481, loss 0.00634185, acc 1
2016-09-07T03:10:46.169690: step 3482, loss 0.0127661, acc 1
2016-09-07T03:10:46.858409: step 3483, loss 0.0144381, acc 1
2016-09-07T03:10:47.533034: step 3484, loss 0.00187197, acc 1
2016-09-07T03:10:48.232625: step 3485, loss 0.0120583, acc 1
2016-09-07T03:10:48.923609: step 3486, loss 0.00932696, acc 1
2016-09-07T03:10:49.621012: step 3487, loss 0.0443766, acc 0.96
2016-09-07T03:10:50.319857: step 3488, loss 0.0339628, acc 1
2016-09-07T03:10:50.981748: step 3489, loss 0.0335901, acc 0.98
2016-09-07T03:10:51.669339: step 3490, loss 0.0186511, acc 1
2016-09-07T03:10:52.370567: step 3491, loss 0.059166, acc 0.98
2016-09-07T03:10:53.056158: step 3492, loss 0.0254876, acc 0.98
2016-09-07T03:10:53.754928: step 3493, loss 0.0642382, acc 0.98
2016-09-07T03:10:54.420839: step 3494, loss 0.0249865, acc 0.98
2016-09-07T03:10:55.126679: step 3495, loss 0.00672121, acc 1
2016-09-07T03:10:55.799044: step 3496, loss 0.00478857, acc 1
2016-09-07T03:10:56.492794: step 3497, loss 0.0332419, acc 0.98
2016-09-07T03:10:57.208634: step 3498, loss 0.0402677, acc 0.98
2016-09-07T03:10:57.890822: step 3499, loss 0.00851769, acc 1
2016-09-07T03:10:58.607724: step 3500, loss 0.0523315, acc 0.98

Evaluation:
2016-09-07T03:11:02.183380: step 3500, loss 1.71795, acc 0.77955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-3500

2016-09-07T03:11:03.936743: step 3501, loss 0.0293303, acc 0.98
2016-09-07T03:11:04.645406: step 3502, loss 0.0357891, acc 1
2016-09-07T03:11:05.321970: step 3503, loss 0.0795624, acc 0.98
2016-09-07T03:11:06.021305: step 3504, loss 0.0286909, acc 1
2016-09-07T03:11:06.714635: step 3505, loss 0.0750561, acc 0.96
2016-09-07T03:11:07.399441: step 3506, loss 0.0309452, acc 0.98
2016-09-07T03:11:08.087908: step 3507, loss 0.00671195, acc 1
2016-09-07T03:11:08.773536: step 3508, loss 0.0512248, acc 0.98
2016-09-07T03:11:09.482583: step 3509, loss 0.0301224, acc 1
2016-09-07T03:11:10.159585: step 3510, loss 0.00026371, acc 1
2016-09-07T03:11:10.851733: step 3511, loss 0.037264, acc 0.96
2016-09-07T03:11:11.546131: step 3512, loss 0.0212169, acc 1
2016-09-07T03:11:12.230950: step 3513, loss 0.0472078, acc 0.96
2016-09-07T03:11:12.930029: step 3514, loss 0.00380859, acc 1
2016-09-07T03:11:13.619001: step 3515, loss 0.00174925, acc 1
2016-09-07T03:11:14.314678: step 3516, loss 0.00886882, acc 1
2016-09-07T03:11:15.014107: step 3517, loss 0.0198238, acc 0.98
2016-09-07T03:11:15.716266: step 3518, loss 0.00218034, acc 1
2016-09-07T03:11:16.416245: step 3519, loss 0.0178492, acc 1
2016-09-07T03:11:17.133537: step 3520, loss 0.038061, acc 0.98
2016-09-07T03:11:17.833907: step 3521, loss 0.01663, acc 1
2016-09-07T03:11:18.520827: step 3522, loss 0.0454434, acc 0.98
2016-09-07T03:11:19.243219: step 3523, loss 0.0639126, acc 0.98
2016-09-07T03:11:19.952428: step 3524, loss 0.0183431, acc 0.98
2016-09-07T03:11:20.684114: step 3525, loss 0.0170889, acc 0.98
2016-09-07T03:11:21.388776: step 3526, loss 0.0445255, acc 0.98
2016-09-07T03:11:22.078972: step 3527, loss 0.000899843, acc 1
2016-09-07T03:11:22.809733: step 3528, loss 0.0394454, acc 0.98
2016-09-07T03:11:23.516065: step 3529, loss 0.0184804, acc 1
2016-09-07T03:11:24.228409: step 3530, loss 0.000300033, acc 1
2016-09-07T03:11:24.928022: step 3531, loss 0.0197026, acc 1
2016-09-07T03:11:25.620414: step 3532, loss 0.0369387, acc 0.98
2016-09-07T03:11:26.339537: step 3533, loss 0.0309515, acc 0.98
2016-09-07T03:11:27.018068: step 3534, loss 0.000463616, acc 1
2016-09-07T03:11:27.690606: step 3535, loss 0.00187528, acc 1
2016-09-07T03:11:28.384412: step 3536, loss 0.0341624, acc 0.98
2016-09-07T03:11:29.066323: step 3537, loss 0.00265625, acc 1
2016-09-07T03:11:29.768040: step 3538, loss 0.0757344, acc 0.98
2016-09-07T03:11:30.453804: step 3539, loss 0.024954, acc 0.98
2016-09-07T03:11:31.154654: step 3540, loss 0.0258545, acc 0.98
2016-09-07T03:11:31.844449: step 3541, loss 0.0039201, acc 1
2016-09-07T03:11:32.547510: step 3542, loss 0.0746957, acc 0.98
2016-09-07T03:11:33.237550: step 3543, loss 8.82897e-05, acc 1
2016-09-07T03:11:33.912721: step 3544, loss 0.0485456, acc 0.96
2016-09-07T03:11:34.604487: step 3545, loss 0.0140758, acc 1
2016-09-07T03:11:35.262878: step 3546, loss 0.0534233, acc 0.98
2016-09-07T03:11:35.958215: step 3547, loss 0.0442794, acc 0.98
2016-09-07T03:11:36.641028: step 3548, loss 0.0822717, acc 0.96
2016-09-07T03:11:37.344947: step 3549, loss 0.000321719, acc 1
2016-09-07T03:11:38.050627: step 3550, loss 0.0675385, acc 0.96
2016-09-07T03:11:38.753896: step 3551, loss 0.0164258, acc 1
2016-09-07T03:11:39.435429: step 3552, loss 0.0370941, acc 0.98
2016-09-07T03:11:40.105216: step 3553, loss 0.0146988, acc 1
2016-09-07T03:11:40.809558: step 3554, loss 0.0725568, acc 0.98
2016-09-07T03:11:41.508895: step 3555, loss 0.00135501, acc 1
2016-09-07T03:11:42.192559: step 3556, loss 0.026786, acc 0.98
2016-09-07T03:11:42.896472: step 3557, loss 8.10101e-05, acc 1
2016-09-07T03:11:43.587693: step 3558, loss 0.00219429, acc 1
2016-09-07T03:11:44.294204: step 3559, loss 0.0639377, acc 0.98
2016-09-07T03:11:44.986477: step 3560, loss 0.0562317, acc 0.96
2016-09-07T03:11:45.705753: step 3561, loss 0.00824907, acc 1
2016-09-07T03:11:46.420835: step 3562, loss 0.0468302, acc 0.98
2016-09-07T03:11:47.120587: step 3563, loss 0.00117171, acc 1
2016-09-07T03:11:47.809426: step 3564, loss 0.0486139, acc 0.98
2016-09-07T03:11:48.502166: step 3565, loss 0.0246129, acc 0.98
2016-09-07T03:11:49.219534: step 3566, loss 0.00406666, acc 1
2016-09-07T03:11:49.910683: step 3567, loss 0.0996628, acc 0.92
2016-09-07T03:11:50.601124: step 3568, loss 0.00815179, acc 1
2016-09-07T03:11:51.294618: step 3569, loss 0.0405257, acc 0.98
2016-09-07T03:11:51.979755: step 3570, loss 0.0590544, acc 0.96
2016-09-07T03:11:52.714462: step 3571, loss 0.0280807, acc 0.98
2016-09-07T03:11:53.370832: step 3572, loss 0.0253631, acc 1
2016-09-07T03:11:54.078579: step 3573, loss 0.0654285, acc 0.98
2016-09-07T03:11:54.748601: step 3574, loss 0.062387, acc 0.96
2016-09-07T03:11:55.422906: step 3575, loss 0.026487, acc 0.98
2016-09-07T03:11:56.097560: step 3576, loss 0.0913204, acc 0.94
2016-09-07T03:11:56.801055: step 3577, loss 0.0570143, acc 0.96
2016-09-07T03:11:57.506303: step 3578, loss 0.0366277, acc 0.96
2016-09-07T03:11:58.205883: step 3579, loss 0.010738, acc 1
2016-09-07T03:11:58.895200: step 3580, loss 0.026198, acc 0.98
2016-09-07T03:11:59.587365: step 3581, loss 0.168522, acc 0.96
2016-09-07T03:12:00.305616: step 3582, loss 0.0327132, acc 0.98
2016-09-07T03:12:00.999462: step 3583, loss 0.0906849, acc 0.96
2016-09-07T03:12:01.704667: step 3584, loss 0.0721689, acc 0.96
2016-09-07T03:12:02.425276: step 3585, loss 0.0130731, acc 1
2016-09-07T03:12:03.094435: step 3586, loss 0.0769527, acc 0.96
2016-09-07T03:12:03.779439: step 3587, loss 0.0655046, acc 0.96
2016-09-07T03:12:04.459339: step 3588, loss 0.0683641, acc 0.98
2016-09-07T03:12:05.149209: step 3589, loss 0.00900485, acc 1
2016-09-07T03:12:05.840933: step 3590, loss 0.0359668, acc 0.96
2016-09-07T03:12:06.533381: step 3591, loss 0.0302955, acc 0.98
2016-09-07T03:12:07.220934: step 3592, loss 0.0404517, acc 0.98
2016-09-07T03:12:07.906994: step 3593, loss 0.0996232, acc 0.96
2016-09-07T03:12:08.605794: step 3594, loss 0.0166691, acc 1
2016-09-07T03:12:09.276833: step 3595, loss 0.0268451, acc 0.98
2016-09-07T03:12:09.972891: step 3596, loss 0.0423426, acc 1
2016-09-07T03:12:10.657654: step 3597, loss 0.0176963, acc 1
2016-09-07T03:12:11.357587: step 3598, loss 0.0291145, acc 1
2016-09-07T03:12:12.069454: step 3599, loss 0.0379648, acc 0.98
2016-09-07T03:12:12.760519: step 3600, loss 0.0424568, acc 0.98

Evaluation:
2016-09-07T03:12:16.341206: step 3600, loss 1.62911, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-3600

2016-09-07T03:12:18.125896: step 3601, loss 0.0618004, acc 0.98
2016-09-07T03:12:18.809542: step 3602, loss 0.0139268, acc 1
2016-09-07T03:12:19.507249: step 3603, loss 0.0192871, acc 1
2016-09-07T03:12:20.223096: step 3604, loss 0.0442093, acc 0.98
2016-09-07T03:12:20.915530: step 3605, loss 0.0257884, acc 0.98
2016-09-07T03:12:21.617377: step 3606, loss 0.0768838, acc 0.94
2016-09-07T03:12:22.310764: step 3607, loss 0.0199701, acc 0.98
2016-09-07T03:12:22.996604: step 3608, loss 0.00722874, acc 1
2016-09-07T03:12:23.681853: step 3609, loss 0.0102992, acc 1
2016-09-07T03:12:24.359598: step 3610, loss 0.0262409, acc 0.98
2016-09-07T03:12:25.044291: step 3611, loss 0.00500226, acc 1
2016-09-07T03:12:25.730532: step 3612, loss 0.00336994, acc 1
2016-09-07T03:12:26.440651: step 3613, loss 0.000555657, acc 1
2016-09-07T03:12:27.119429: step 3614, loss 0.0352139, acc 1
2016-09-07T03:12:27.824471: step 3615, loss 0.0233127, acc 1
2016-09-07T03:12:28.523100: step 3616, loss 0.055573, acc 0.94
2016-09-07T03:12:29.219215: step 3617, loss 0.050813, acc 0.98
2016-09-07T03:12:29.925332: step 3618, loss 0.0492649, acc 0.96
2016-09-07T03:12:30.604311: step 3619, loss 0.0605289, acc 0.98
2016-09-07T03:12:31.318134: step 3620, loss 0.161166, acc 0.98
2016-09-07T03:12:32.016843: step 3621, loss 0.0297094, acc 1
2016-09-07T03:12:32.705345: step 3622, loss 0.0808871, acc 0.94
2016-09-07T03:12:33.435341: step 3623, loss 0.0465707, acc 0.98
2016-09-07T03:12:34.131100: step 3624, loss 0.10689, acc 0.94
2016-09-07T03:12:34.836689: step 3625, loss 0.070682, acc 0.94
2016-09-07T03:12:35.508868: step 3626, loss 0.0988763, acc 0.98
2016-09-07T03:12:36.219913: step 3627, loss 0.0468892, acc 0.98
2016-09-07T03:12:36.918574: step 3628, loss 0.105589, acc 0.98
2016-09-07T03:12:37.618046: step 3629, loss 0.0541233, acc 0.98
2016-09-07T03:12:38.321831: step 3630, loss 0.0223244, acc 1
2016-09-07T03:12:39.013802: step 3631, loss 0.00166658, acc 1
2016-09-07T03:12:39.709591: step 3632, loss 0.00893163, acc 1
2016-09-07T03:12:40.412078: step 3633, loss 0.00507041, acc 1
2016-09-07T03:12:41.116789: step 3634, loss 0.0465352, acc 1
2016-09-07T03:12:41.820906: step 3635, loss 0.0303026, acc 0.98
2016-09-07T03:12:42.521969: step 3636, loss 0.0507458, acc 0.96
2016-09-07T03:12:43.230971: step 3637, loss 0.0304211, acc 0.98
2016-09-07T03:12:43.916840: step 3638, loss 0.0211487, acc 0.98
2016-09-07T03:12:44.639307: step 3639, loss 0.00514848, acc 1
2016-09-07T03:12:45.344321: step 3640, loss 0.0279406, acc 1
2016-09-07T03:12:46.039080: step 3641, loss 0.0147597, acc 1
2016-09-07T03:12:46.740601: step 3642, loss 0.0106601, acc 1
2016-09-07T03:12:47.432918: step 3643, loss 0.0232101, acc 1
2016-09-07T03:12:48.147706: step 3644, loss 0.0370297, acc 0.98
2016-09-07T03:12:48.844347: step 3645, loss 0.0354206, acc 0.98
2016-09-07T03:12:49.535063: step 3646, loss 0.0866956, acc 0.96
2016-09-07T03:12:50.234538: step 3647, loss 0.0651957, acc 0.94
2016-09-07T03:12:50.878747: step 3648, loss 0.0278512, acc 0.977273
2016-09-07T03:12:51.583651: step 3649, loss 0.0173046, acc 1
2016-09-07T03:12:52.243575: step 3650, loss 0.029297, acc 1
2016-09-07T03:12:52.952747: step 3651, loss 0.0994226, acc 0.96
2016-09-07T03:12:53.620313: step 3652, loss 0.0127334, acc 1
2016-09-07T03:12:54.318375: step 3653, loss 0.105482, acc 0.96
2016-09-07T03:12:55.009021: step 3654, loss 0.112851, acc 0.92
2016-09-07T03:12:55.707328: step 3655, loss 0.0267681, acc 0.98
2016-09-07T03:12:56.404931: step 3656, loss 0.0369665, acc 1
2016-09-07T03:12:57.089230: step 3657, loss 0.000761472, acc 1
2016-09-07T03:12:57.799646: step 3658, loss 0.0480874, acc 0.98
2016-09-07T03:12:58.493522: step 3659, loss 0.0662176, acc 0.96
2016-09-07T03:12:59.197048: step 3660, loss 0.0378625, acc 0.98
2016-09-07T03:12:59.888124: step 3661, loss 0.0502935, acc 0.98
2016-09-07T03:13:00.628088: step 3662, loss 0.113013, acc 0.96
2016-09-07T03:13:01.334352: step 3663, loss 0.0217208, acc 1
2016-09-07T03:13:02.034904: step 3664, loss 0.00336508, acc 1
2016-09-07T03:13:02.730667: step 3665, loss 0.0357371, acc 1
2016-09-07T03:13:03.421878: step 3666, loss 0.0351718, acc 0.98
2016-09-07T03:13:04.092419: step 3667, loss 0.0204177, acc 1
2016-09-07T03:13:04.797036: step 3668, loss 0.0103637, acc 1
2016-09-07T03:13:05.448437: step 3669, loss 0.0135659, acc 1
2016-09-07T03:13:06.167078: step 3670, loss 0.0169648, acc 1
2016-09-07T03:13:06.844047: step 3671, loss 0.00684917, acc 1
2016-09-07T03:13:07.534227: step 3672, loss 0.0263117, acc 0.98
2016-09-07T03:13:08.250247: step 3673, loss 0.0151014, acc 0.98
2016-09-07T03:13:08.940310: step 3674, loss 0.00118018, acc 1
2016-09-07T03:13:09.641543: step 3675, loss 0.00309273, acc 1
2016-09-07T03:13:10.312783: step 3676, loss 0.0426146, acc 0.98
2016-09-07T03:13:11.022152: step 3677, loss 0.0438587, acc 0.98
2016-09-07T03:13:11.716868: step 3678, loss 0.0136668, acc 1
2016-09-07T03:13:12.421009: step 3679, loss 0.0218205, acc 0.98
2016-09-07T03:13:13.107504: step 3680, loss 0.0959908, acc 0.96
2016-09-07T03:13:13.805054: step 3681, loss 0.0183259, acc 1
2016-09-07T03:13:14.493068: step 3682, loss 0.0251265, acc 0.98
2016-09-07T03:13:15.165018: step 3683, loss 0.00612547, acc 1
2016-09-07T03:13:15.872562: step 3684, loss 0.0835741, acc 0.94
2016-09-07T03:13:16.589641: step 3685, loss 0.0202005, acc 1
2016-09-07T03:13:17.280186: step 3686, loss 0.0131433, acc 1
2016-09-07T03:13:17.973821: step 3687, loss 0.00341498, acc 1
2016-09-07T03:13:18.681219: step 3688, loss 0.00495952, acc 1
2016-09-07T03:13:19.398262: step 3689, loss 0.0868205, acc 0.96
2016-09-07T03:13:20.078289: step 3690, loss 0.0289751, acc 0.98
2016-09-07T03:13:20.811123: step 3691, loss 0.0130261, acc 1
2016-09-07T03:13:21.509159: step 3692, loss 0.0723629, acc 0.96
2016-09-07T03:13:22.207287: step 3693, loss 0.00149842, acc 1
2016-09-07T03:13:22.898308: step 3694, loss 0.0301722, acc 0.98
2016-09-07T03:13:23.572318: step 3695, loss 0.0227463, acc 0.98
2016-09-07T03:13:24.286191: step 3696, loss 0.0586283, acc 0.96
2016-09-07T03:13:24.975842: step 3697, loss 0.0017066, acc 1
2016-09-07T03:13:25.668770: step 3698, loss 0.016471, acc 1
2016-09-07T03:13:26.366941: step 3699, loss 0.0184576, acc 1
2016-09-07T03:13:27.065569: step 3700, loss 0.00696301, acc 1

Evaluation:
2016-09-07T03:13:30.647347: step 3700, loss 1.92728, acc 0.766416

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-3700

2016-09-07T03:13:32.498969: step 3701, loss 0.107239, acc 0.96
2016-09-07T03:13:33.182430: step 3702, loss 0.0107739, acc 1
2016-09-07T03:13:33.889112: step 3703, loss 0.00340806, acc 1
2016-09-07T03:13:34.556920: step 3704, loss 0.0240497, acc 1
2016-09-07T03:13:35.254940: step 3705, loss 0.00322787, acc 1
2016-09-07T03:13:35.947326: step 3706, loss 0.00429361, acc 1
2016-09-07T03:13:36.635258: step 3707, loss 0.0155387, acc 1
2016-09-07T03:13:37.312196: step 3708, loss 0.0179826, acc 0.98
2016-09-07T03:13:37.984968: step 3709, loss 0.0942289, acc 0.96
2016-09-07T03:13:38.714089: step 3710, loss 0.0616923, acc 0.98
2016-09-07T03:13:39.422377: step 3711, loss 0.0513641, acc 0.96
2016-09-07T03:13:40.119669: step 3712, loss 0.0326313, acc 1
2016-09-07T03:13:40.821303: step 3713, loss 0.0222077, acc 1
2016-09-07T03:13:41.515920: step 3714, loss 0.0328623, acc 0.98
2016-09-07T03:13:42.229892: step 3715, loss 0.0367863, acc 0.98
2016-09-07T03:13:42.918169: step 3716, loss 0.002961, acc 1
2016-09-07T03:13:43.601960: step 3717, loss 0.0172657, acc 0.98
2016-09-07T03:13:44.294092: step 3718, loss 0.0219295, acc 0.98
2016-09-07T03:13:44.983974: step 3719, loss 0.0033305, acc 1
2016-09-07T03:13:45.693849: step 3720, loss 0.0127815, acc 1
2016-09-07T03:13:46.388431: step 3721, loss 0.00714393, acc 1
2016-09-07T03:13:47.102673: step 3722, loss 0.0015134, acc 1
2016-09-07T03:13:47.794834: step 3723, loss 0.0140568, acc 1
2016-09-07T03:13:48.483283: step 3724, loss 0.225362, acc 0.94
2016-09-07T03:13:49.178061: step 3725, loss 0.0701714, acc 0.94
2016-09-07T03:13:49.880341: step 3726, loss 0.00489009, acc 1
2016-09-07T03:13:50.568414: step 3727, loss 0.0342449, acc 0.98
2016-09-07T03:13:51.240580: step 3728, loss 0.0586228, acc 0.96
2016-09-07T03:13:51.965877: step 3729, loss 0.0172695, acc 0.98
2016-09-07T03:13:52.662235: step 3730, loss 0.00938587, acc 1
2016-09-07T03:13:53.353847: step 3731, loss 0.0107215, acc 1
2016-09-07T03:13:54.066716: step 3732, loss 0.019431, acc 0.98
2016-09-07T03:13:54.773625: step 3733, loss 0.0102454, acc 1
2016-09-07T03:13:55.491218: step 3734, loss 0.0168421, acc 0.98
2016-09-07T03:13:56.171791: step 3735, loss 0.0141, acc 1
2016-09-07T03:13:56.860889: step 3736, loss 0.0212535, acc 1
2016-09-07T03:13:57.561314: step 3737, loss 0.0310217, acc 0.98
2016-09-07T03:13:58.264530: step 3738, loss 0.0323608, acc 0.98
2016-09-07T03:13:58.954035: step 3739, loss 0.100398, acc 0.94
2016-09-07T03:13:59.624876: step 3740, loss 0.0119821, acc 1
2016-09-07T03:14:00.375794: step 3741, loss 0.028525, acc 0.98
2016-09-07T03:14:01.094315: step 3742, loss 0.0824978, acc 0.98
2016-09-07T03:14:01.790174: step 3743, loss 0.0586708, acc 0.98
2016-09-07T03:14:02.493542: step 3744, loss 0.0252005, acc 1
2016-09-07T03:14:03.179690: step 3745, loss 0.0142369, acc 1
2016-09-07T03:14:03.883973: step 3746, loss 0.232616, acc 0.94
2016-09-07T03:14:04.587602: step 3747, loss 0.00041063, acc 1
2016-09-07T03:14:05.262391: step 3748, loss 0.0176212, acc 1
2016-09-07T03:14:05.972394: step 3749, loss 0.0651212, acc 0.98
2016-09-07T03:14:06.684029: step 3750, loss 0.0352351, acc 0.98
2016-09-07T03:14:07.417490: step 3751, loss 0.130987, acc 0.96
2016-09-07T03:14:08.093613: step 3752, loss 0.0906624, acc 0.96
2016-09-07T03:14:08.808672: step 3753, loss 0.0409039, acc 0.98
2016-09-07T03:14:09.494798: step 3754, loss 0.0129966, acc 1
2016-09-07T03:14:10.172017: step 3755, loss 0.0361932, acc 0.98
2016-09-07T03:14:10.867922: step 3756, loss 0.0583959, acc 0.98
2016-09-07T03:14:11.585036: step 3757, loss 0.0419211, acc 0.98
2016-09-07T03:14:12.302785: step 3758, loss 0.041623, acc 0.98
2016-09-07T03:14:12.983757: step 3759, loss 0.0481945, acc 1
2016-09-07T03:14:13.665212: step 3760, loss 0.11994, acc 0.98
2016-09-07T03:14:14.348776: step 3761, loss 0.0325937, acc 0.98
2016-09-07T03:14:15.029689: step 3762, loss 0.042303, acc 1
2016-09-07T03:14:15.718174: step 3763, loss 0.00816358, acc 1
2016-09-07T03:14:16.406527: step 3764, loss 0.116434, acc 0.96
2016-09-07T03:14:17.107503: step 3765, loss 0.0694874, acc 0.98
2016-09-07T03:14:17.792672: step 3766, loss 0.0246262, acc 1
2016-09-07T03:14:18.483351: step 3767, loss 0.025102, acc 1
2016-09-07T03:14:19.174973: step 3768, loss 0.0277467, acc 1
2016-09-07T03:14:19.868448: step 3769, loss 0.0166285, acc 1
2016-09-07T03:14:20.570812: step 3770, loss 0.0536034, acc 0.96
2016-09-07T03:14:21.233220: step 3771, loss 0.00759807, acc 1
2016-09-07T03:14:21.951173: step 3772, loss 0.0337343, acc 0.98
2016-09-07T03:14:22.645046: step 3773, loss 0.0668461, acc 0.96
2016-09-07T03:14:23.342425: step 3774, loss 0.042017, acc 0.96
2016-09-07T03:14:24.041301: step 3775, loss 0.0702066, acc 0.94
2016-09-07T03:14:24.734195: step 3776, loss 0.00513473, acc 1
2016-09-07T03:14:25.420842: step 3777, loss 0.00287264, acc 1
2016-09-07T03:14:26.084300: step 3778, loss 0.0489397, acc 0.98
2016-09-07T03:14:26.775123: step 3779, loss 0.0437299, acc 0.98
2016-09-07T03:14:27.462887: step 3780, loss 0.0236481, acc 0.98
2016-09-07T03:14:28.153434: step 3781, loss 0.0129219, acc 1
2016-09-07T03:14:28.854549: step 3782, loss 0.0443221, acc 0.98
2016-09-07T03:14:29.552555: step 3783, loss 0.00513683, acc 1
2016-09-07T03:14:30.260772: step 3784, loss 0.0343067, acc 0.98
2016-09-07T03:14:30.951332: step 3785, loss 0.0104895, acc 1
2016-09-07T03:14:31.680387: step 3786, loss 0.00053933, acc 1
2016-09-07T03:14:32.390540: step 3787, loss 0.00857435, acc 1
2016-09-07T03:14:33.085648: step 3788, loss 0.0349659, acc 0.98
2016-09-07T03:14:33.780335: step 3789, loss 0.000821651, acc 1
2016-09-07T03:14:34.473398: step 3790, loss 0.0289664, acc 0.98
2016-09-07T03:14:35.179609: step 3791, loss 0.0540395, acc 0.96
2016-09-07T03:14:35.854498: step 3792, loss 0.00253659, acc 1
2016-09-07T03:14:36.538778: step 3793, loss 0.0273932, acc 1
2016-09-07T03:14:37.239438: step 3794, loss 0.0167534, acc 1
2016-09-07T03:14:37.939280: step 3795, loss 0.0307411, acc 1
2016-09-07T03:14:38.635070: step 3796, loss 0.00131567, acc 1
2016-09-07T03:14:39.311143: step 3797, loss 0.0270928, acc 0.98
2016-09-07T03:14:40.024261: step 3798, loss 0.0731117, acc 0.98
2016-09-07T03:14:40.725989: step 3799, loss 0.0327747, acc 0.98
2016-09-07T03:14:41.426114: step 3800, loss 0.000699883, acc 1

Evaluation:
2016-09-07T03:14:45.061430: step 3800, loss 1.82833, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-3800

2016-09-07T03:14:46.805135: step 3801, loss 0.0100072, acc 1
2016-09-07T03:14:47.500517: step 3802, loss 0.02813, acc 0.98
2016-09-07T03:14:48.187918: step 3803, loss 0.0277855, acc 0.98
2016-09-07T03:14:48.872488: step 3804, loss 0.00330222, acc 1
2016-09-07T03:14:49.590237: step 3805, loss 0.0095366, acc 1
2016-09-07T03:14:50.271773: step 3806, loss 0.0774198, acc 0.96
2016-09-07T03:14:50.946925: step 3807, loss 0.000785306, acc 1
2016-09-07T03:14:51.656291: step 3808, loss 0.102954, acc 0.96
2016-09-07T03:14:52.363589: step 3809, loss 7.00109e-05, acc 1
2016-09-07T03:14:53.051541: step 3810, loss 0.0241472, acc 0.98
2016-09-07T03:14:53.715778: step 3811, loss 0.00870672, acc 1
2016-09-07T03:14:54.425482: step 3812, loss 0.0106131, acc 1
2016-09-07T03:14:55.157856: step 3813, loss 0.0649898, acc 0.98
2016-09-07T03:14:55.828039: step 3814, loss 0.0305631, acc 0.96
2016-09-07T03:14:56.529397: step 3815, loss 0.000742377, acc 1
2016-09-07T03:14:57.215843: step 3816, loss 0.0620896, acc 0.96
2016-09-07T03:14:57.911296: step 3817, loss 0.0162343, acc 0.98
2016-09-07T03:14:58.578650: step 3818, loss 0.0553803, acc 0.96
2016-09-07T03:14:59.286838: step 3819, loss 0.0105236, acc 1
2016-09-07T03:14:59.979311: step 3820, loss 0.053736, acc 0.98
2016-09-07T03:15:00.683307: step 3821, loss 0.0889135, acc 0.96
2016-09-07T03:15:01.349865: step 3822, loss 0.0500785, acc 0.98
2016-09-07T03:15:02.015577: step 3823, loss 0.00601649, acc 1
2016-09-07T03:15:02.690768: step 3824, loss 0.0290915, acc 0.98
2016-09-07T03:15:03.380879: step 3825, loss 0.0116846, acc 1
2016-09-07T03:15:04.083437: step 3826, loss 0.0495891, acc 0.98
2016-09-07T03:15:04.754734: step 3827, loss 0.0288542, acc 0.98
2016-09-07T03:15:05.436356: step 3828, loss 0.00150718, acc 1
2016-09-07T03:15:06.114499: step 3829, loss 0.0144331, acc 1
2016-09-07T03:15:06.793156: step 3830, loss 0.000261153, acc 1
2016-09-07T03:15:07.477488: step 3831, loss 0.0028772, acc 1
2016-09-07T03:15:08.173043: step 3832, loss 0.0143737, acc 1
2016-09-07T03:15:08.871797: step 3833, loss 0.0165565, acc 1
2016-09-07T03:15:09.554313: step 3834, loss 0.00560477, acc 1
2016-09-07T03:15:10.252033: step 3835, loss 0.102892, acc 0.94
2016-09-07T03:15:10.938927: step 3836, loss 0.00632041, acc 1
2016-09-07T03:15:11.614208: step 3837, loss 0.0236618, acc 1
2016-09-07T03:15:12.300454: step 3838, loss 0.0213055, acc 1
2016-09-07T03:15:12.974833: step 3839, loss 0.026437, acc 0.98
2016-09-07T03:15:13.608909: step 3840, loss 0.0216932, acc 1
2016-09-07T03:15:14.290879: step 3841, loss 0.0357219, acc 0.98
2016-09-07T03:15:14.977864: step 3842, loss 0.0145338, acc 1
2016-09-07T03:15:15.651688: step 3843, loss 0.035624, acc 0.96
2016-09-07T03:15:16.326915: step 3844, loss 0.0435662, acc 0.96
2016-09-07T03:15:17.031541: step 3845, loss 0.0200277, acc 1
2016-09-07T03:15:17.735812: step 3846, loss 0.0334339, acc 1
2016-09-07T03:15:18.464845: step 3847, loss 0.07288, acc 0.96
2016-09-07T03:15:19.118258: step 3848, loss 0.00351768, acc 1
2016-09-07T03:15:19.807064: step 3849, loss 0.000227134, acc 1
2016-09-07T03:15:20.493279: step 3850, loss 0.0194266, acc 0.98
2016-09-07T03:15:21.187069: step 3851, loss 0.0222298, acc 1
2016-09-07T03:15:21.872380: step 3852, loss 0.0173116, acc 0.98
2016-09-07T03:15:22.538443: step 3853, loss 0.00775662, acc 1
2016-09-07T03:15:23.260388: step 3854, loss 0.0422872, acc 0.98
2016-09-07T03:15:23.928900: step 3855, loss 0.0292955, acc 0.98
2016-09-07T03:15:24.633310: step 3856, loss 0.00358467, acc 1
2016-09-07T03:15:25.335243: step 3857, loss 0.0035773, acc 1
2016-09-07T03:15:26.019631: step 3858, loss 0.0645505, acc 0.98
2016-09-07T03:15:26.716563: step 3859, loss 0.0957693, acc 0.92
2016-09-07T03:15:27.394754: step 3860, loss 0.0425794, acc 0.98
2016-09-07T03:15:28.100050: step 3861, loss 0.02588, acc 0.98
2016-09-07T03:15:28.784235: step 3862, loss 0.0808408, acc 0.98
2016-09-07T03:15:29.477147: step 3863, loss 0.00738341, acc 1
2016-09-07T03:15:30.171100: step 3864, loss 0.014301, acc 1
2016-09-07T03:15:30.859667: step 3865, loss 0.0959552, acc 0.98
2016-09-07T03:15:31.578924: step 3866, loss 0.11629, acc 0.96
2016-09-07T03:15:32.254899: step 3867, loss 0.00929159, acc 1
2016-09-07T03:15:32.961792: step 3868, loss 0.00375477, acc 1
2016-09-07T03:15:33.650453: step 3869, loss 0.0538466, acc 0.96
2016-09-07T03:15:34.347627: step 3870, loss 0.0227235, acc 0.98
2016-09-07T03:15:35.032766: step 3871, loss 0.0636231, acc 0.96
2016-09-07T03:15:35.723888: step 3872, loss 0.0728083, acc 0.94
2016-09-07T03:15:36.410022: step 3873, loss 0.0173218, acc 1
2016-09-07T03:15:37.090700: step 3874, loss 0.0434898, acc 0.98
2016-09-07T03:15:37.793116: step 3875, loss 0.049615, acc 0.96
2016-09-07T03:15:38.474604: step 3876, loss 0.00779302, acc 1
2016-09-07T03:15:39.160920: step 3877, loss 0.0100423, acc 1
2016-09-07T03:15:39.865783: step 3878, loss 0.0116681, acc 1
2016-09-07T03:15:40.588540: step 3879, loss 0.00365501, acc 1
2016-09-07T03:15:41.275344: step 3880, loss 0.172289, acc 0.94
2016-09-07T03:15:41.948960: step 3881, loss 0.111911, acc 0.92
2016-09-07T03:15:42.677071: step 3882, loss 0.00561997, acc 1
2016-09-07T03:15:43.389330: step 3883, loss 0.0706804, acc 0.96
2016-09-07T03:15:44.075222: step 3884, loss 0.0391579, acc 0.98
2016-09-07T03:15:44.762167: step 3885, loss 0.0934289, acc 0.94
2016-09-07T03:15:45.440817: step 3886, loss 0.0951576, acc 0.96
2016-09-07T03:15:46.156493: step 3887, loss 0.0105795, acc 1
2016-09-07T03:15:46.835297: step 3888, loss 0.0141413, acc 1
2016-09-07T03:15:47.512470: step 3889, loss 0.023193, acc 0.98
2016-09-07T03:15:48.196740: step 3890, loss 0.0518692, acc 0.98
2016-09-07T03:15:48.894297: step 3891, loss 0.0262098, acc 0.98
2016-09-07T03:15:49.567521: step 3892, loss 0.00173217, acc 1
2016-09-07T03:15:50.278648: step 3893, loss 0.00885349, acc 1
2016-09-07T03:15:50.976921: step 3894, loss 0.00977492, acc 1
2016-09-07T03:15:51.636311: step 3895, loss 0.0545155, acc 0.96
2016-09-07T03:15:52.323778: step 3896, loss 0.0468984, acc 0.98
2016-09-07T03:15:53.028779: step 3897, loss 0.0236818, acc 1
2016-09-07T03:15:53.720289: step 3898, loss 0.00128609, acc 1
2016-09-07T03:15:54.394951: step 3899, loss 0.0102582, acc 1
2016-09-07T03:15:55.071866: step 3900, loss 0.0224521, acc 0.98

Evaluation:
2016-09-07T03:15:58.601117: step 3900, loss 1.6934, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-3900

2016-09-07T03:16:00.312725: step 3901, loss 0.103195, acc 0.98
2016-09-07T03:16:01.014645: step 3902, loss 0.015505, acc 0.98
2016-09-07T03:16:01.731946: step 3903, loss 0.032953, acc 1
2016-09-07T03:16:02.393474: step 3904, loss 0.0266684, acc 0.98
2016-09-07T03:16:03.093239: step 3905, loss 0.00330923, acc 1
2016-09-07T03:16:03.773401: step 3906, loss 0.0478466, acc 0.98
2016-09-07T03:16:04.482243: step 3907, loss 0.0209924, acc 0.98
2016-09-07T03:16:05.166234: step 3908, loss 0.0146101, acc 1
2016-09-07T03:16:05.863092: step 3909, loss 0.0171635, acc 1
2016-09-07T03:16:06.582257: step 3910, loss 0.0389792, acc 0.96
2016-09-07T03:16:07.259054: step 3911, loss 0.0227716, acc 0.98
2016-09-07T03:16:07.939873: step 3912, loss 0.0137116, acc 1
2016-09-07T03:16:08.620434: step 3913, loss 0.00248402, acc 1
2016-09-07T03:16:09.295038: step 3914, loss 0.0749779, acc 0.98
2016-09-07T03:16:09.983005: step 3915, loss 0.0268706, acc 0.98
2016-09-07T03:16:10.666890: step 3916, loss 0.122574, acc 0.94
2016-09-07T03:16:11.378508: step 3917, loss 0.0853719, acc 0.96
2016-09-07T03:16:12.060173: step 3918, loss 0.00201734, acc 1
2016-09-07T03:16:12.742181: step 3919, loss 0.0163113, acc 0.98
2016-09-07T03:16:13.417037: step 3920, loss 0.020724, acc 0.98
2016-09-07T03:16:14.091397: step 3921, loss 0.0652993, acc 0.96
2016-09-07T03:16:14.774886: step 3922, loss 0.0139066, acc 1
2016-09-07T03:16:15.464950: step 3923, loss 0.0361272, acc 1
2016-09-07T03:16:16.166696: step 3924, loss 0.000534775, acc 1
2016-09-07T03:16:16.836271: step 3925, loss 0.0294282, acc 0.98
2016-09-07T03:16:17.541385: step 3926, loss 0.0593437, acc 0.96
2016-09-07T03:16:18.213286: step 3927, loss 0.0115262, acc 1
2016-09-07T03:16:18.906069: step 3928, loss 0.0680736, acc 0.98
2016-09-07T03:16:19.587598: step 3929, loss 0.0263133, acc 1
2016-09-07T03:16:20.288482: step 3930, loss 0.0266453, acc 1
2016-09-07T03:16:20.999999: step 3931, loss 0.146021, acc 0.98
2016-09-07T03:16:21.673733: step 3932, loss 0.069188, acc 0.98
2016-09-07T03:16:22.366138: step 3933, loss 0.0324969, acc 0.98
2016-09-07T03:16:23.038197: step 3934, loss 0.00970451, acc 1
2016-09-07T03:16:23.710099: step 3935, loss 0.072787, acc 0.98
2016-09-07T03:16:24.398628: step 3936, loss 0.0646781, acc 0.98
2016-09-07T03:16:25.075900: step 3937, loss 0.0438127, acc 0.96
2016-09-07T03:16:25.768740: step 3938, loss 0.0539449, acc 0.96
2016-09-07T03:16:26.438824: step 3939, loss 0.0221511, acc 1
2016-09-07T03:16:27.166036: step 3940, loss 0.0503965, acc 0.98
2016-09-07T03:16:27.904062: step 3941, loss 0.00132106, acc 1
2016-09-07T03:16:28.582969: step 3942, loss 0.00440852, acc 1
2016-09-07T03:16:29.258822: step 3943, loss 0.0422984, acc 0.98
2016-09-07T03:16:29.938181: step 3944, loss 0.0637146, acc 0.96
2016-09-07T03:16:30.657348: step 3945, loss 0.0204269, acc 0.98
2016-09-07T03:16:31.340344: step 3946, loss 0.00577373, acc 1
2016-09-07T03:16:32.024457: step 3947, loss 0.00603027, acc 1
2016-09-07T03:16:32.722327: step 3948, loss 0.040864, acc 0.98
2016-09-07T03:16:33.421317: step 3949, loss 0.0918046, acc 0.96
2016-09-07T03:16:34.113363: step 3950, loss 0.0218647, acc 1
2016-09-07T03:16:34.780070: step 3951, loss 0.0842469, acc 0.96
2016-09-07T03:16:35.518567: step 3952, loss 0.00798501, acc 1
2016-09-07T03:16:36.197942: step 3953, loss 0.0236942, acc 1
2016-09-07T03:16:36.878706: step 3954, loss 0.025139, acc 1
2016-09-07T03:16:37.563269: step 3955, loss 0.0336257, acc 0.98
2016-09-07T03:16:38.254378: step 3956, loss 0.042825, acc 0.98
2016-09-07T03:16:38.972617: step 3957, loss 0.0205454, acc 1
2016-09-07T03:16:39.657763: step 3958, loss 0.0318009, acc 0.98
2016-09-07T03:16:40.376621: step 3959, loss 0.00727696, acc 1
2016-09-07T03:16:41.068911: step 3960, loss 0.053226, acc 0.98
2016-09-07T03:16:41.754011: step 3961, loss 0.0174417, acc 0.98
2016-09-07T03:16:42.423987: step 3962, loss 0.0770262, acc 0.98
2016-09-07T03:16:43.124528: step 3963, loss 0.00375016, acc 1
2016-09-07T03:16:43.842398: step 3964, loss 0.00203722, acc 1
2016-09-07T03:16:44.505639: step 3965, loss 0.124831, acc 0.92
2016-09-07T03:16:45.200464: step 3966, loss 0.083771, acc 0.94
2016-09-07T03:16:45.886434: step 3967, loss 0.034921, acc 0.98
2016-09-07T03:16:46.582934: step 3968, loss 0.0396871, acc 0.98
2016-09-07T03:16:47.282343: step 3969, loss 0.0173262, acc 1
2016-09-07T03:16:47.966117: step 3970, loss 0.0297848, acc 1
2016-09-07T03:16:48.646762: step 3971, loss 0.0598075, acc 0.98
2016-09-07T03:16:49.295847: step 3972, loss 0.0192681, acc 1
2016-09-07T03:16:50.017968: step 3973, loss 0.0172379, acc 0.98
2016-09-07T03:16:50.719999: step 3974, loss 0.018362, acc 1
2016-09-07T03:16:51.407743: step 3975, loss 0.0906077, acc 0.98
2016-09-07T03:16:52.098351: step 3976, loss 0.0355049, acc 0.98
2016-09-07T03:16:52.784637: step 3977, loss 0.0699459, acc 0.96
2016-09-07T03:16:53.485719: step 3978, loss 0.0100112, acc 1
2016-09-07T03:16:54.149472: step 3979, loss 0.0920393, acc 0.94
2016-09-07T03:16:54.826102: step 3980, loss 0.0313435, acc 1
2016-09-07T03:16:55.493640: step 3981, loss 0.00815172, acc 1
2016-09-07T03:16:56.195279: step 3982, loss 0.0266619, acc 1
2016-09-07T03:16:56.897612: step 3983, loss 0.014129, acc 1
2016-09-07T03:16:57.600925: step 3984, loss 0.0260829, acc 0.98
2016-09-07T03:16:58.312027: step 3985, loss 0.0173684, acc 1
2016-09-07T03:16:58.982174: step 3986, loss 0.0453032, acc 0.96
2016-09-07T03:16:59.676323: step 3987, loss 0.0175168, acc 1
2016-09-07T03:17:00.399912: step 3988, loss 0.0057716, acc 1
2016-09-07T03:17:01.098842: step 3989, loss 0.0183841, acc 1
2016-09-07T03:17:01.778881: step 3990, loss 0.0287682, acc 0.98
2016-09-07T03:17:02.444465: step 3991, loss 0.00845858, acc 1
2016-09-07T03:17:03.146940: step 3992, loss 0.0260037, acc 1
2016-09-07T03:17:03.834534: step 3993, loss 0.033655, acc 0.98
2016-09-07T03:17:04.523550: step 3994, loss 0.0231713, acc 1
2016-09-07T03:17:05.192089: step 3995, loss 0.0569839, acc 0.96
2016-09-07T03:17:05.890029: step 3996, loss 0.000622268, acc 1
2016-09-07T03:17:06.564375: step 3997, loss 0.0204064, acc 1
2016-09-07T03:17:07.221708: step 3998, loss 0.0258299, acc 1
2016-09-07T03:17:07.915885: step 3999, loss 0.041159, acc 0.98
2016-09-07T03:17:08.600105: step 4000, loss 0.0127097, acc 1

Evaluation:
2016-09-07T03:17:12.110148: step 4000, loss 1.8401, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-4000

2016-09-07T03:17:13.988077: step 4001, loss 0.00322096, acc 1
2016-09-07T03:17:14.681350: step 4002, loss 0.0475327, acc 0.96
2016-09-07T03:17:15.359375: step 4003, loss 0.0216575, acc 0.98
2016-09-07T03:17:16.068177: step 4004, loss 0.00226977, acc 1
2016-09-07T03:17:16.748457: step 4005, loss 0.0049572, acc 1
2016-09-07T03:17:17.447651: step 4006, loss 0.0136385, acc 1
2016-09-07T03:17:18.104741: step 4007, loss 0.000100516, acc 1
2016-09-07T03:17:18.831451: step 4008, loss 0.121334, acc 0.94
2016-09-07T03:17:19.536965: step 4009, loss 0.0755951, acc 0.94
2016-09-07T03:17:20.239136: step 4010, loss 0.0604019, acc 0.98
2016-09-07T03:17:20.965021: step 4011, loss 0.0401548, acc 0.98
2016-09-07T03:17:21.662807: step 4012, loss 0.0365655, acc 0.96
2016-09-07T03:17:22.367621: step 4013, loss 0.0151248, acc 1
2016-09-07T03:17:23.039898: step 4014, loss 0.0617421, acc 0.96
2016-09-07T03:17:23.724727: step 4015, loss 0.0459995, acc 0.98
2016-09-07T03:17:24.428285: step 4016, loss 0.0146973, acc 1
2016-09-07T03:17:25.141405: step 4017, loss 0.00568464, acc 1
2016-09-07T03:17:25.833015: step 4018, loss 0.0874662, acc 0.96
2016-09-07T03:17:26.507781: step 4019, loss 0.00119762, acc 1
2016-09-07T03:17:27.214383: step 4020, loss 0.0968704, acc 0.92
2016-09-07T03:17:27.910029: step 4021, loss 0.0170918, acc 0.98
2016-09-07T03:17:28.599307: step 4022, loss 0.000762029, acc 1
2016-09-07T03:17:29.291030: step 4023, loss 0.160753, acc 0.98
2016-09-07T03:17:30.000012: step 4024, loss 0.13105, acc 0.94
2016-09-07T03:17:30.722967: step 4025, loss 0.01512, acc 1
2016-09-07T03:17:31.418801: step 4026, loss 0.155838, acc 0.94
2016-09-07T03:17:32.098391: step 4027, loss 0.0217861, acc 1
2016-09-07T03:17:32.793088: step 4028, loss 0.0740499, acc 0.96
2016-09-07T03:17:33.488888: step 4029, loss 0.206183, acc 0.98
2016-09-07T03:17:34.197765: step 4030, loss 0.0235486, acc 1
2016-09-07T03:17:34.891594: step 4031, loss 0.0260993, acc 0.98
2016-09-07T03:17:35.545570: step 4032, loss 0.000596078, acc 1
2016-09-07T03:17:36.235629: step 4033, loss 0.0301436, acc 1
2016-09-07T03:17:36.913642: step 4034, loss 0.0187819, acc 1
2016-09-07T03:17:37.594306: step 4035, loss 0.112057, acc 0.92
2016-09-07T03:17:38.275370: step 4036, loss 0.00951096, acc 1
2016-09-07T03:17:38.961806: step 4037, loss 0.0208717, acc 1
2016-09-07T03:17:39.639082: step 4038, loss 0.0710735, acc 0.94
2016-09-07T03:17:40.351795: step 4039, loss 0.0279284, acc 1
2016-09-07T03:17:41.046848: step 4040, loss 0.0376894, acc 1
2016-09-07T03:17:41.725571: step 4041, loss 0.0299912, acc 0.98
2016-09-07T03:17:42.410811: step 4042, loss 0.0282005, acc 0.98
2016-09-07T03:17:43.099494: step 4043, loss 0.0278304, acc 0.98
2016-09-07T03:17:43.776912: step 4044, loss 0.0794566, acc 0.96
2016-09-07T03:17:44.465652: step 4045, loss 0.0472578, acc 1
2016-09-07T03:17:45.173229: step 4046, loss 0.0503337, acc 0.98
2016-09-07T03:17:45.880142: step 4047, loss 0.0330105, acc 0.98
2016-09-07T03:17:46.571636: step 4048, loss 0.0328302, acc 0.98
2016-09-07T03:17:47.279103: step 4049, loss 0.0501619, acc 0.98
2016-09-07T03:17:47.975697: step 4050, loss 0.0125199, acc 1
2016-09-07T03:17:48.653019: step 4051, loss 0.033593, acc 0.98
2016-09-07T03:17:49.325060: step 4052, loss 0.00892525, acc 1
2016-09-07T03:17:50.033878: step 4053, loss 0.0266966, acc 1
2016-09-07T03:17:50.720542: step 4054, loss 0.0130574, acc 1
2016-09-07T03:17:51.392289: step 4055, loss 0.00731553, acc 1
2016-09-07T03:17:52.084180: step 4056, loss 0.00862447, acc 1
2016-09-07T03:17:52.778600: step 4057, loss 0.0490653, acc 0.96
2016-09-07T03:17:53.453655: step 4058, loss 0.0614405, acc 0.98
2016-09-07T03:17:54.134450: step 4059, loss 0.0199237, acc 1
2016-09-07T03:17:54.867812: step 4060, loss 0.00794932, acc 1
2016-09-07T03:17:55.564554: step 4061, loss 0.0254795, acc 0.98
2016-09-07T03:17:56.254822: step 4062, loss 0.122878, acc 0.96
2016-09-07T03:17:56.949915: step 4063, loss 0.0725512, acc 0.98
2016-09-07T03:17:57.631609: step 4064, loss 0.0374182, acc 1
2016-09-07T03:17:58.338028: step 4065, loss 0.00964222, acc 1
2016-09-07T03:17:59.000642: step 4066, loss 0.0225268, acc 1
2016-09-07T03:17:59.716735: step 4067, loss 0.0066118, acc 1
2016-09-07T03:18:00.420066: step 4068, loss 0.00454782, acc 1
2016-09-07T03:18:01.147451: step 4069, loss 0.0195628, acc 1
2016-09-07T03:18:01.834734: step 4070, loss 0.0455837, acc 0.96
2016-09-07T03:18:02.530633: step 4071, loss 0.0263219, acc 0.98
2016-09-07T03:18:03.247273: step 4072, loss 0.00382324, acc 1
2016-09-07T03:18:03.924691: step 4073, loss 0.0678127, acc 0.98
2016-09-07T03:18:04.611910: step 4074, loss 0.00512448, acc 1
2016-09-07T03:18:05.292138: step 4075, loss 0.0235436, acc 1
2016-09-07T03:18:05.992161: step 4076, loss 0.0538467, acc 0.98
2016-09-07T03:18:06.677159: step 4077, loss 0.0628201, acc 0.96
2016-09-07T03:18:07.361791: step 4078, loss 0.0632015, acc 0.98
2016-09-07T03:18:08.058958: step 4079, loss 0.00561641, acc 1
2016-09-07T03:18:08.745356: step 4080, loss 0.00800367, acc 1
2016-09-07T03:18:09.421054: step 4081, loss 0.0554601, acc 0.98
2016-09-07T03:18:10.113964: step 4082, loss 0.0165568, acc 1
2016-09-07T03:18:10.788360: step 4083, loss 0.0275008, acc 1
2016-09-07T03:18:11.469131: step 4084, loss 0.0017029, acc 1
2016-09-07T03:18:12.172678: step 4085, loss 0.0190314, acc 0.98
2016-09-07T03:18:12.911155: step 4086, loss 0.00725855, acc 1
2016-09-07T03:18:13.586145: step 4087, loss 0.0229143, acc 1
2016-09-07T03:18:14.267948: step 4088, loss 0.0197873, acc 0.98
2016-09-07T03:18:14.976608: step 4089, loss 0.047188, acc 0.98
2016-09-07T03:18:15.672730: step 4090, loss 0.0230062, acc 0.98
2016-09-07T03:18:16.371803: step 4091, loss 0.109698, acc 0.94
2016-09-07T03:18:17.070787: step 4092, loss 0.0204756, acc 1
2016-09-07T03:18:17.784500: step 4093, loss 0.0287785, acc 0.98
2016-09-07T03:18:18.491192: step 4094, loss 0.0326995, acc 0.98
2016-09-07T03:18:19.181985: step 4095, loss 0.0298044, acc 0.98
2016-09-07T03:18:19.870797: step 4096, loss 0.0656027, acc 0.96
2016-09-07T03:18:20.564477: step 4097, loss 0.0308445, acc 0.98
2016-09-07T03:18:21.266077: step 4098, loss 0.0341662, acc 0.98
2016-09-07T03:18:21.961674: step 4099, loss 0.0839508, acc 0.94
2016-09-07T03:18:22.651236: step 4100, loss 0.00809252, acc 1

Evaluation:
2016-09-07T03:18:26.213229: step 4100, loss 1.60441, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-4100

2016-09-07T03:18:27.955836: step 4101, loss 0.0106965, acc 1
2016-09-07T03:18:28.655927: step 4102, loss 0.029255, acc 0.98
2016-09-07T03:18:29.344693: step 4103, loss 0.000703251, acc 1
2016-09-07T03:18:30.047719: step 4104, loss 0.0260932, acc 1
2016-09-07T03:18:30.762737: step 4105, loss 0.0146323, acc 1
2016-09-07T03:18:31.451292: step 4106, loss 0.0118886, acc 1
2016-09-07T03:18:32.151299: step 4107, loss 0.0688562, acc 0.98
2016-09-07T03:18:32.826599: step 4108, loss 0.0239252, acc 0.98
2016-09-07T03:18:33.521777: step 4109, loss 0.0154463, acc 1
2016-09-07T03:18:34.226166: step 4110, loss 0.00732044, acc 1
2016-09-07T03:18:34.922000: step 4111, loss 0.0899644, acc 0.98
2016-09-07T03:18:35.625841: step 4112, loss 0.000571768, acc 1
2016-09-07T03:18:36.298149: step 4113, loss 0.00767061, acc 1
2016-09-07T03:18:37.009059: step 4114, loss 0.00199351, acc 1
2016-09-07T03:18:37.712031: step 4115, loss 0.0244167, acc 1
2016-09-07T03:18:38.417548: step 4116, loss 0.0179331, acc 1
2016-09-07T03:18:39.104027: step 4117, loss 0.0315207, acc 1
2016-09-07T03:18:39.794114: step 4118, loss 0.0623125, acc 0.98
2016-09-07T03:18:40.504302: step 4119, loss 0.000218947, acc 1
2016-09-07T03:18:41.193448: step 4120, loss 0.0635858, acc 0.96
2016-09-07T03:18:41.884351: step 4121, loss 0.0623062, acc 0.96
2016-09-07T03:18:42.576434: step 4122, loss 0.125126, acc 0.96
2016-09-07T03:18:43.237619: step 4123, loss 0.0163871, acc 0.98
2016-09-07T03:18:43.950411: step 4124, loss 0.0331606, acc 0.98
2016-09-07T03:18:44.651934: step 4125, loss 0.126763, acc 0.96
2016-09-07T03:18:45.381987: step 4126, loss 0.00355925, acc 1
2016-09-07T03:18:46.068705: step 4127, loss 0.0206924, acc 0.98
2016-09-07T03:18:46.743940: step 4128, loss 0.0654747, acc 0.98
2016-09-07T03:18:47.428254: step 4129, loss 0.0451215, acc 0.98
2016-09-07T03:18:48.109948: step 4130, loss 0.134092, acc 0.96
2016-09-07T03:18:48.815976: step 4131, loss 0.00457453, acc 1
2016-09-07T03:18:49.517339: step 4132, loss 0.0249779, acc 1
2016-09-07T03:18:50.217956: step 4133, loss 0.0242508, acc 0.98
2016-09-07T03:18:50.889957: step 4134, loss 0.0485976, acc 0.98
2016-09-07T03:18:51.559689: step 4135, loss 0.00656455, acc 1
2016-09-07T03:18:52.260946: step 4136, loss 0.0240087, acc 1
2016-09-07T03:18:52.954077: step 4137, loss 0.0377754, acc 0.96
2016-09-07T03:18:53.652754: step 4138, loss 0.0784041, acc 0.96
2016-09-07T03:18:54.320843: step 4139, loss 0.0225126, acc 0.98
2016-09-07T03:18:55.047189: step 4140, loss 0.0402404, acc 0.96
2016-09-07T03:18:55.755384: step 4141, loss 0.0313066, acc 1
2016-09-07T03:18:56.457787: step 4142, loss 0.0971098, acc 0.96
2016-09-07T03:18:57.144832: step 4143, loss 0.0451532, acc 0.98
2016-09-07T03:18:57.815957: step 4144, loss 0.00346446, acc 1
2016-09-07T03:18:58.497856: step 4145, loss 0.0846661, acc 0.98
2016-09-07T03:18:59.161012: step 4146, loss 0.029767, acc 0.98
2016-09-07T03:18:59.857441: step 4147, loss 0.0233252, acc 0.98
2016-09-07T03:19:00.584411: step 4148, loss 0.0544632, acc 0.98
2016-09-07T03:19:01.280351: step 4149, loss 0.0343131, acc 0.98
2016-09-07T03:19:01.964447: step 4150, loss 0.047078, acc 0.98
2016-09-07T03:19:02.663234: step 4151, loss 0.0411005, acc 0.96
2016-09-07T03:19:03.379909: step 4152, loss 0.0125771, acc 1
2016-09-07T03:19:04.054945: step 4153, loss 0.0158047, acc 1
2016-09-07T03:19:04.729161: step 4154, loss 0.0240012, acc 1
2016-09-07T03:19:05.419674: step 4155, loss 0.0304446, acc 0.98
2016-09-07T03:19:06.111345: step 4156, loss 0.0384224, acc 0.98
2016-09-07T03:19:06.803467: step 4157, loss 0.0396612, acc 0.98
2016-09-07T03:19:07.492877: step 4158, loss 0.0319501, acc 0.98
2016-09-07T03:19:08.203272: step 4159, loss 0.0593282, acc 0.94
2016-09-07T03:19:08.883419: step 4160, loss 0.027285, acc 0.98
2016-09-07T03:19:09.581072: step 4161, loss 0.0297145, acc 0.98
2016-09-07T03:19:10.284085: step 4162, loss 0.0891767, acc 0.96
2016-09-07T03:19:10.981493: step 4163, loss 0.0504399, acc 0.96
2016-09-07T03:19:11.680205: step 4164, loss 0.00738185, acc 1
2016-09-07T03:19:12.356915: step 4165, loss 0.0371918, acc 0.98
2016-09-07T03:19:13.087691: step 4166, loss 0.0295769, acc 0.98
2016-09-07T03:19:13.775869: step 4167, loss 0.0215029, acc 1
2016-09-07T03:19:14.453184: step 4168, loss 0.0526277, acc 0.96
2016-09-07T03:19:15.139346: step 4169, loss 0.0141938, acc 1
2016-09-07T03:19:15.830125: step 4170, loss 0.00085939, acc 1
2016-09-07T03:19:16.536254: step 4171, loss 0.03586, acc 0.96
2016-09-07T03:19:17.215682: step 4172, loss 0.0113398, acc 1
2016-09-07T03:19:17.941139: step 4173, loss 0.0747289, acc 0.96
2016-09-07T03:19:18.646219: step 4174, loss 0.0349752, acc 0.96
2016-09-07T03:19:19.340563: step 4175, loss 0.0289379, acc 0.98
2016-09-07T03:19:20.029596: step 4176, loss 0.000255239, acc 1
2016-09-07T03:19:20.715247: step 4177, loss 0.0372608, acc 0.98
2016-09-07T03:19:21.422913: step 4178, loss 0.0332754, acc 0.98
2016-09-07T03:19:22.115787: step 4179, loss 0.0263241, acc 0.98
2016-09-07T03:19:22.794737: step 4180, loss 0.0359822, acc 0.96
2016-09-07T03:19:23.494895: step 4181, loss 0.0225609, acc 0.98
2016-09-07T03:19:24.171132: step 4182, loss 0.0284083, acc 0.98
2016-09-07T03:19:24.891647: step 4183, loss 0.060348, acc 0.98
2016-09-07T03:19:25.553371: step 4184, loss 0.0456698, acc 0.98
2016-09-07T03:19:26.256919: step 4185, loss 0.00199643, acc 1
2016-09-07T03:19:26.921262: step 4186, loss 0.0549999, acc 0.96
2016-09-07T03:19:27.613575: step 4187, loss 0.0215195, acc 0.98
2016-09-07T03:19:28.298424: step 4188, loss 0.000505292, acc 1
2016-09-07T03:19:28.989491: step 4189, loss 0.0102151, acc 1
2016-09-07T03:19:29.687391: step 4190, loss 0.0426765, acc 0.96
2016-09-07T03:19:30.343416: step 4191, loss 0.00626716, acc 1
2016-09-07T03:19:31.036517: step 4192, loss 0.0312479, acc 0.98
2016-09-07T03:19:31.712339: step 4193, loss 0.00107583, acc 1
2016-09-07T03:19:32.415125: step 4194, loss 0.0122282, acc 1
2016-09-07T03:19:33.102239: step 4195, loss 0.0233764, acc 1
2016-09-07T03:19:33.790063: step 4196, loss 0.0304439, acc 0.98
2016-09-07T03:19:34.533023: step 4197, loss 0.0399078, acc 1
2016-09-07T03:19:35.216253: step 4198, loss 0.0333912, acc 0.98
2016-09-07T03:19:35.939129: step 4199, loss 0.0582139, acc 0.98
2016-09-07T03:19:36.650692: step 4200, loss 0.0231476, acc 0.98

Evaluation:
2016-09-07T03:19:40.233795: step 4200, loss 1.73821, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-4200

2016-09-07T03:19:41.962541: step 4201, loss 0.005585, acc 1
2016-09-07T03:19:42.670523: step 4202, loss 0.0370996, acc 0.98
2016-09-07T03:19:43.364221: step 4203, loss 0.0117872, acc 1
2016-09-07T03:19:44.053941: step 4204, loss 0.014129, acc 1
2016-09-07T03:19:44.740208: step 4205, loss 0.0291882, acc 1
2016-09-07T03:19:45.456642: step 4206, loss 0.0401415, acc 1
2016-09-07T03:19:46.129743: step 4207, loss 0.0132248, acc 1
2016-09-07T03:19:46.835050: step 4208, loss 0.0390043, acc 0.98
2016-09-07T03:19:47.546177: step 4209, loss 0.0172432, acc 1
2016-09-07T03:19:48.230391: step 4210, loss 0.0178095, acc 1
2016-09-07T03:19:48.917368: step 4211, loss 0.146087, acc 0.94
2016-09-07T03:19:49.597606: step 4212, loss 0.00451492, acc 1
2016-09-07T03:19:50.301032: step 4213, loss 0.0155878, acc 1
2016-09-07T03:19:50.959710: step 4214, loss 0.000446009, acc 1
2016-09-07T03:19:51.641423: step 4215, loss 0.000694595, acc 1
2016-09-07T03:19:52.330506: step 4216, loss 0.0538031, acc 0.96
2016-09-07T03:19:53.022499: step 4217, loss 0.0203961, acc 0.98
2016-09-07T03:19:53.697748: step 4218, loss 0.0391033, acc 0.98
2016-09-07T03:19:54.395702: step 4219, loss 0.00981256, acc 1
2016-09-07T03:19:55.106934: step 4220, loss 0.00746803, acc 1
2016-09-07T03:19:55.790957: step 4221, loss 0.0528583, acc 0.98
2016-09-07T03:19:56.478944: step 4222, loss 0.0106624, acc 1
2016-09-07T03:19:57.170713: step 4223, loss 0.0972681, acc 0.92
2016-09-07T03:19:57.799968: step 4224, loss 0.0624323, acc 0.977273
2016-09-07T03:19:58.488595: step 4225, loss 0.0058089, acc 1
2016-09-07T03:19:59.175997: step 4226, loss 0.00443833, acc 1
2016-09-07T03:19:59.896151: step 4227, loss 0.0337797, acc 1
2016-09-07T03:20:00.615549: step 4228, loss 0.00188394, acc 1
2016-09-07T03:20:01.313896: step 4229, loss 0.0754905, acc 0.96
2016-09-07T03:20:02.002683: step 4230, loss 0.0540682, acc 0.98
2016-09-07T03:20:02.695238: step 4231, loss 0.0105449, acc 1
2016-09-07T03:20:03.408021: step 4232, loss 0.0274619, acc 0.98
2016-09-07T03:20:04.070158: step 4233, loss 0.113057, acc 0.98
2016-09-07T03:20:04.778795: step 4234, loss 0.0449811, acc 0.96
2016-09-07T03:20:05.456021: step 4235, loss 0.0321769, acc 0.96
2016-09-07T03:20:06.145039: step 4236, loss 0.00824021, acc 1
2016-09-07T03:20:06.856874: step 4237, loss 0.0293554, acc 0.98
2016-09-07T03:20:07.548726: step 4238, loss 0.0249674, acc 0.98
2016-09-07T03:20:08.261798: step 4239, loss 0.0223679, acc 0.98
2016-09-07T03:20:08.927108: step 4240, loss 0.0393953, acc 0.98
2016-09-07T03:20:09.632981: step 4241, loss 0.0251126, acc 1
2016-09-07T03:20:10.358562: step 4242, loss 0.00585948, acc 1
2016-09-07T03:20:11.053477: step 4243, loss 0.0115035, acc 1
2016-09-07T03:20:11.771395: step 4244, loss 0.00588374, acc 1
2016-09-07T03:20:12.440690: step 4245, loss 0.0697344, acc 0.96
2016-09-07T03:20:13.134024: step 4246, loss 0.0184153, acc 1
2016-09-07T03:20:13.833447: step 4247, loss 0.0141657, acc 1
2016-09-07T03:20:14.519930: step 4248, loss 0.00177141, acc 1
2016-09-07T03:20:15.223852: step 4249, loss 0.00776605, acc 1
2016-09-07T03:20:15.913411: step 4250, loss 0.0245794, acc 0.98
2016-09-07T03:20:16.611652: step 4251, loss 0.00165178, acc 1
2016-09-07T03:20:17.285855: step 4252, loss 0.000424803, acc 1
2016-09-07T03:20:17.990888: step 4253, loss 0.0894524, acc 0.98
2016-09-07T03:20:18.687839: step 4254, loss 0.00896889, acc 1
2016-09-07T03:20:19.386971: step 4255, loss 0.0408181, acc 0.98
2016-09-07T03:20:20.076492: step 4256, loss 0.0527704, acc 0.98
2016-09-07T03:20:20.770974: step 4257, loss 0.0474144, acc 0.98
2016-09-07T03:20:21.457255: step 4258, loss 0.0443955, acc 0.96
2016-09-07T03:20:22.149637: step 4259, loss 0.0237924, acc 0.98
2016-09-07T03:20:22.853857: step 4260, loss 0.0822304, acc 0.96
2016-09-07T03:20:23.528273: step 4261, loss 0.0137515, acc 1
2016-09-07T03:20:24.231369: step 4262, loss 0.0649836, acc 0.96
2016-09-07T03:20:24.911062: step 4263, loss 0.00325346, acc 1
2016-09-07T03:20:25.592764: step 4264, loss 0.00103994, acc 1
2016-09-07T03:20:26.289958: step 4265, loss 0.0126119, acc 1
2016-09-07T03:20:26.979415: step 4266, loss 0.00904202, acc 1
2016-09-07T03:20:27.687176: step 4267, loss 0.00571383, acc 1
2016-09-07T03:20:28.367235: step 4268, loss 0.0344064, acc 0.98
2016-09-07T03:20:29.060620: step 4269, loss 0.0287463, acc 0.98
2016-09-07T03:20:29.752469: step 4270, loss 0.0150926, acc 1
2016-09-07T03:20:30.428172: step 4271, loss 0.0164215, acc 0.98
2016-09-07T03:20:31.143871: step 4272, loss 0.096404, acc 0.94
2016-09-07T03:20:31.820625: step 4273, loss 0.00372243, acc 1
2016-09-07T03:20:32.522069: step 4274, loss 0.0448549, acc 0.96
2016-09-07T03:20:33.217734: step 4275, loss 0.0103174, acc 1
2016-09-07T03:20:33.901141: step 4276, loss 0.00811453, acc 1
2016-09-07T03:20:34.606714: step 4277, loss 0.00117844, acc 1
2016-09-07T03:20:35.292157: step 4278, loss 0.00100362, acc 1
2016-09-07T03:20:36.010618: step 4279, loss 0.00626546, acc 1
2016-09-07T03:20:36.705780: step 4280, loss 0.0727801, acc 0.96
2016-09-07T03:20:37.399668: step 4281, loss 0.00686641, acc 1
2016-09-07T03:20:38.109506: step 4282, loss 0.000584978, acc 1
2016-09-07T03:20:38.815387: step 4283, loss 0.209783, acc 0.98
2016-09-07T03:20:39.533114: step 4284, loss 0.00948813, acc 1
2016-09-07T03:20:40.225374: step 4285, loss 0.0343181, acc 0.98
2016-09-07T03:20:40.908156: step 4286, loss 0.0603564, acc 0.96
2016-09-07T03:20:41.629342: step 4287, loss 0.0198039, acc 0.98
2016-09-07T03:20:42.336963: step 4288, loss 0.00208067, acc 1
2016-09-07T03:20:43.039361: step 4289, loss 0.0633294, acc 0.98
2016-09-07T03:20:43.706129: step 4290, loss 0.0316952, acc 0.98
2016-09-07T03:20:44.398597: step 4291, loss 0.0442394, acc 0.98
2016-09-07T03:20:45.083096: step 4292, loss 0.00580297, acc 1
2016-09-07T03:20:45.769906: step 4293, loss 0.00155026, acc 1
2016-09-07T03:20:46.459291: step 4294, loss 0.00388089, acc 1
2016-09-07T03:20:47.140546: step 4295, loss 0.0135851, acc 1
2016-09-07T03:20:47.823773: step 4296, loss 0.00458835, acc 1
2016-09-07T03:20:48.472021: step 4297, loss 0.00147157, acc 1
2016-09-07T03:20:49.173006: step 4298, loss 0.056812, acc 0.98
2016-09-07T03:20:49.850422: step 4299, loss 0.0538326, acc 0.98
2016-09-07T03:20:50.544016: step 4300, loss 0.123839, acc 0.98

Evaluation:
2016-09-07T03:20:54.128041: step 4300, loss 1.70354, acc 0.763602

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-4300

2016-09-07T03:20:55.944857: step 4301, loss 0.0214729, acc 0.98
2016-09-07T03:20:56.631493: step 4302, loss 0.00026372, acc 1
2016-09-07T03:20:57.316126: step 4303, loss 0.0421428, acc 0.98
2016-09-07T03:20:57.996473: step 4304, loss 0.026099, acc 1
2016-09-07T03:20:58.678918: step 4305, loss 0.0331514, acc 0.98
2016-09-07T03:20:59.345557: step 4306, loss 0.00229507, acc 1
2016-09-07T03:21:00.054338: step 4307, loss 0.00855681, acc 1
2016-09-07T03:21:00.784017: step 4308, loss 0.0116181, acc 1
2016-09-07T03:21:01.478723: step 4309, loss 0.00804239, acc 1
2016-09-07T03:21:02.173775: step 4310, loss 0.0487773, acc 0.96
2016-09-07T03:21:02.856438: step 4311, loss 0.00736878, acc 1
2016-09-07T03:21:03.539212: step 4312, loss 0.0363466, acc 0.98
2016-09-07T03:21:04.203677: step 4313, loss 0.0395308, acc 0.98
2016-09-07T03:21:04.915676: step 4314, loss 0.0156718, acc 1
2016-09-07T03:21:05.622885: step 4315, loss 0.0296884, acc 1
2016-09-07T03:21:06.315336: step 4316, loss 0.00199391, acc 1
2016-09-07T03:21:07.017438: step 4317, loss 0.0158327, acc 1
2016-09-07T03:21:07.709375: step 4318, loss 0.0585854, acc 0.96
2016-09-07T03:21:08.424723: step 4319, loss 0.00918522, acc 1
2016-09-07T03:21:09.101632: step 4320, loss 0.0274842, acc 0.98
2016-09-07T03:21:09.799517: step 4321, loss 0.019439, acc 1
2016-09-07T03:21:10.530825: step 4322, loss 0.0322284, acc 0.98
2016-09-07T03:21:11.219304: step 4323, loss 0.068518, acc 0.96
2016-09-07T03:21:11.892897: step 4324, loss 0.02314, acc 0.98
2016-09-07T03:21:12.591615: step 4325, loss 0.0218369, acc 0.98
2016-09-07T03:21:13.309417: step 4326, loss 0.0132455, acc 1
2016-09-07T03:21:14.002204: step 4327, loss 0.0540975, acc 0.96
2016-09-07T03:21:14.689021: step 4328, loss 0.0426877, acc 0.96
2016-09-07T03:21:15.364980: step 4329, loss 0.0170734, acc 1
2016-09-07T03:21:16.045151: step 4330, loss 0.0228664, acc 1
2016-09-07T03:21:16.727189: step 4331, loss 0.159245, acc 0.96
2016-09-07T03:21:17.406810: step 4332, loss 0.180502, acc 0.98
2016-09-07T03:21:18.104474: step 4333, loss 0.0371576, acc 0.98
2016-09-07T03:21:18.786813: step 4334, loss 0.0702683, acc 0.98
2016-09-07T03:21:19.459831: step 4335, loss 0.0866153, acc 0.98
2016-09-07T03:21:20.157951: step 4336, loss 0.0114586, acc 1
2016-09-07T03:21:20.857561: step 4337, loss 0.00843955, acc 1
2016-09-07T03:21:21.564306: step 4338, loss 0.028078, acc 0.98
2016-09-07T03:21:22.228030: step 4339, loss 0.000548193, acc 1
2016-09-07T03:21:22.951460: step 4340, loss 0.0987814, acc 0.92
2016-09-07T03:21:23.642695: step 4341, loss 0.0335672, acc 1
2016-09-07T03:21:24.324724: step 4342, loss 0.0133552, acc 1
2016-09-07T03:21:25.036668: step 4343, loss 0.0244091, acc 0.98
2016-09-07T03:21:25.720026: step 4344, loss 0.0747058, acc 0.96
2016-09-07T03:21:26.406588: step 4345, loss 0.0240688, acc 0.98
2016-09-07T03:21:27.067725: step 4346, loss 0.0267981, acc 1
2016-09-07T03:21:27.767693: step 4347, loss 0.0512736, acc 0.96
2016-09-07T03:21:28.471087: step 4348, loss 0.0159866, acc 1
2016-09-07T03:21:29.146657: step 4349, loss 0.0739284, acc 0.96
2016-09-07T03:21:29.825515: step 4350, loss 0.00520987, acc 1
2016-09-07T03:21:30.510689: step 4351, loss 0.0155509, acc 1
2016-09-07T03:21:31.206396: step 4352, loss 0.062555, acc 0.94
2016-09-07T03:21:31.878315: step 4353, loss 0.0166347, acc 1
2016-09-07T03:21:32.595725: step 4354, loss 0.0352345, acc 0.98
2016-09-07T03:21:33.288259: step 4355, loss 0.010155, acc 1
2016-09-07T03:21:33.973188: step 4356, loss 0.0280919, acc 0.98
2016-09-07T03:21:34.657245: step 4357, loss 0.0118797, acc 1
2016-09-07T03:21:35.347378: step 4358, loss 0.079111, acc 0.94
2016-09-07T03:21:36.036326: step 4359, loss 0.0632525, acc 0.96
2016-09-07T03:21:36.700979: step 4360, loss 0.0191468, acc 1
2016-09-07T03:21:37.409390: step 4361, loss 0.0240397, acc 0.98
2016-09-07T03:21:38.097615: step 4362, loss 0.000285546, acc 1
2016-09-07T03:21:38.787239: step 4363, loss 0.0163977, acc 1
2016-09-07T03:21:39.470645: step 4364, loss 0.0117725, acc 1
2016-09-07T03:21:40.158736: step 4365, loss 0.00468826, acc 1
2016-09-07T03:21:40.837505: step 4366, loss 0.0262136, acc 1
2016-09-07T03:21:41.489531: step 4367, loss 0.0215364, acc 1
2016-09-07T03:21:42.206222: step 4368, loss 0.00550145, acc 1
2016-09-07T03:21:42.906733: step 4369, loss 0.0315637, acc 0.98
2016-09-07T03:21:43.598010: step 4370, loss 0.0468431, acc 0.98
2016-09-07T03:21:44.292227: step 4371, loss 0.00546067, acc 1
2016-09-07T03:21:44.995937: step 4372, loss 0.0517672, acc 0.96
2016-09-07T03:21:45.714693: step 4373, loss 0.0220515, acc 1
2016-09-07T03:21:46.386987: step 4374, loss 0.0648936, acc 0.96
2016-09-07T03:21:47.082177: step 4375, loss 0.00142416, acc 1
2016-09-07T03:21:47.783563: step 4376, loss 0.0965508, acc 0.98
2016-09-07T03:21:48.465493: step 4377, loss 0.00713099, acc 1
2016-09-07T03:21:49.157088: step 4378, loss 0.0078705, acc 1
2016-09-07T03:21:49.840585: step 4379, loss 0.000105807, acc 1
2016-09-07T03:21:50.548523: step 4380, loss 0.00172362, acc 1
2016-09-07T03:21:51.224258: step 4381, loss 0.00815729, acc 1
2016-09-07T03:21:51.936010: step 4382, loss 0.10917, acc 0.98
2016-09-07T03:21:52.625081: step 4383, loss 0.0292654, acc 1
2016-09-07T03:21:53.320974: step 4384, loss 0.0516108, acc 0.98
2016-09-07T03:21:54.005360: step 4385, loss 0.00109995, acc 1
2016-09-07T03:21:54.691806: step 4386, loss 0.0491133, acc 0.98
2016-09-07T03:21:55.421836: step 4387, loss 0.0844304, acc 0.98
2016-09-07T03:21:56.091959: step 4388, loss 0.0137492, acc 1
2016-09-07T03:21:56.779084: step 4389, loss 0.0396284, acc 0.98
2016-09-07T03:21:57.481772: step 4390, loss 0.0221576, acc 1
2016-09-07T03:21:58.179265: step 4391, loss 0.044262, acc 0.96
2016-09-07T03:21:58.883794: step 4392, loss 0.0369973, acc 0.96
2016-09-07T03:21:59.552494: step 4393, loss 0.0173956, acc 1
2016-09-07T03:22:00.293654: step 4394, loss 0.0174482, acc 1
2016-09-07T03:22:00.992424: step 4395, loss 0.0242087, acc 1
2016-09-07T03:22:01.686357: step 4396, loss 0.00625357, acc 1
2016-09-07T03:22:02.369757: step 4397, loss 0.0261771, acc 0.98
2016-09-07T03:22:03.069573: step 4398, loss 0.0501857, acc 0.96
2016-09-07T03:22:03.795224: step 4399, loss 0.00124777, acc 1
2016-09-07T03:22:04.477552: step 4400, loss 0.0345453, acc 0.98

Evaluation:
2016-09-07T03:22:08.036994: step 4400, loss 1.7195, acc 0.760788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-4400

2016-09-07T03:22:09.760682: step 4401, loss 0.0160162, acc 1
2016-09-07T03:22:10.454246: step 4402, loss 0.0487305, acc 0.96
2016-09-07T03:22:11.116854: step 4403, loss 0.00294884, acc 1
2016-09-07T03:22:11.822158: step 4404, loss 0.169645, acc 0.94
2016-09-07T03:22:12.538931: step 4405, loss 0.00916893, acc 1
2016-09-07T03:22:13.248508: step 4406, loss 0.0033465, acc 1
2016-09-07T03:22:13.931649: step 4407, loss 0.00363782, acc 1
2016-09-07T03:22:14.633807: step 4408, loss 0.0438704, acc 0.98
2016-09-07T03:22:15.316502: step 4409, loss 0.00775013, acc 1
2016-09-07T03:22:15.996063: step 4410, loss 0.0600507, acc 0.98
2016-09-07T03:22:16.694200: step 4411, loss 0.0336051, acc 0.98
2016-09-07T03:22:17.384219: step 4412, loss 0.00274252, acc 1
2016-09-07T03:22:18.081495: step 4413, loss 0.0266448, acc 1
2016-09-07T03:22:18.782715: step 4414, loss 0.0416457, acc 0.98
2016-09-07T03:22:19.501581: step 4415, loss 0.00982544, acc 1
2016-09-07T03:22:20.126908: step 4416, loss 0.000882093, acc 1
2016-09-07T03:22:20.798272: step 4417, loss 0.0135893, acc 1
2016-09-07T03:22:21.490001: step 4418, loss 0.0325408, acc 1
2016-09-07T03:22:22.183469: step 4419, loss 0.0143856, acc 1
2016-09-07T03:22:22.903942: step 4420, loss 0.0012832, acc 1
2016-09-07T03:22:23.588456: step 4421, loss 0.0809642, acc 0.96
2016-09-07T03:22:24.289925: step 4422, loss 0.0297607, acc 0.98
2016-09-07T03:22:24.966261: step 4423, loss 0.012014, acc 1
2016-09-07T03:22:25.660481: step 4424, loss 0.0096071, acc 1
2016-09-07T03:22:26.347954: step 4425, loss 0.0120863, acc 1
2016-09-07T03:22:27.024632: step 4426, loss 0.0481321, acc 0.96
2016-09-07T03:22:27.721283: step 4427, loss 0.00336386, acc 1
2016-09-07T03:22:28.396905: step 4428, loss 0.0290446, acc 0.98
2016-09-07T03:22:29.090323: step 4429, loss 0.027697, acc 1
2016-09-07T03:22:29.770935: step 4430, loss 0.0400141, acc 0.98
2016-09-07T03:22:30.466422: step 4431, loss 0.000128989, acc 1
2016-09-07T03:22:31.144915: step 4432, loss 0.00608276, acc 1
2016-09-07T03:22:31.846211: step 4433, loss 0.0452607, acc 0.98
2016-09-07T03:22:32.543830: step 4434, loss 0.0933955, acc 0.98
2016-09-07T03:22:33.199924: step 4435, loss 0.0404197, acc 0.98
2016-09-07T03:22:33.900391: step 4436, loss 0.000649182, acc 1
2016-09-07T03:22:34.577459: step 4437, loss 0.0116232, acc 1
2016-09-07T03:22:35.284610: step 4438, loss 0.0117922, acc 1
2016-09-07T03:22:35.966116: step 4439, loss 0.0368923, acc 0.98
2016-09-07T03:22:36.662749: step 4440, loss 8.46287e-05, acc 1
2016-09-07T03:22:37.363458: step 4441, loss 0.0268239, acc 0.98
2016-09-07T03:22:38.012950: step 4442, loss 0.01097, acc 1
2016-09-07T03:22:38.734043: step 4443, loss 0.0177556, acc 1
2016-09-07T03:22:39.426466: step 4444, loss 0.023288, acc 1
2016-09-07T03:22:40.115662: step 4445, loss 0.0294764, acc 0.98
2016-09-07T03:22:40.803783: step 4446, loss 0.0423852, acc 0.98
2016-09-07T03:22:41.490895: step 4447, loss 0.0227247, acc 0.98
2016-09-07T03:22:42.185937: step 4448, loss 0.0318799, acc 0.98
2016-09-07T03:22:42.874605: step 4449, loss 0.0219105, acc 0.98
2016-09-07T03:22:43.604681: step 4450, loss 0.00255607, acc 1
2016-09-07T03:22:44.285772: step 4451, loss 0.00314953, acc 1
2016-09-07T03:22:44.995578: step 4452, loss 0.0267487, acc 0.98
2016-09-07T03:22:45.711438: step 4453, loss 0.0324929, acc 0.98
2016-09-07T03:22:46.393115: step 4454, loss 0.0552471, acc 0.96
2016-09-07T03:22:47.086429: step 4455, loss 0.0193127, acc 0.98
2016-09-07T03:22:47.767327: step 4456, loss 0.00215935, acc 1
2016-09-07T03:22:48.446417: step 4457, loss 0.0185197, acc 0.98
2016-09-07T03:22:49.133045: step 4458, loss 0.0142651, acc 1
2016-09-07T03:22:49.829398: step 4459, loss 0.00555532, acc 1
2016-09-07T03:22:50.524532: step 4460, loss 0.0181784, acc 1
2016-09-07T03:22:51.205683: step 4461, loss 0.0109238, acc 1
2016-09-07T03:22:51.921591: step 4462, loss 0.0131118, acc 1
2016-09-07T03:22:52.604248: step 4463, loss 0.0126606, acc 1
2016-09-07T03:22:53.297199: step 4464, loss 0.021204, acc 1
2016-09-07T03:22:53.984112: step 4465, loss 0.00344022, acc 1
2016-09-07T03:22:54.682915: step 4466, loss 0.0122521, acc 1
2016-09-07T03:22:55.370621: step 4467, loss 0.0717916, acc 0.96
2016-09-07T03:22:56.057382: step 4468, loss 0.00777064, acc 1
2016-09-07T03:22:56.776840: step 4469, loss 0.000615309, acc 1
2016-09-07T03:22:57.446683: step 4470, loss 0.00888744, acc 1
2016-09-07T03:22:58.138500: step 4471, loss 0.0150356, acc 1
2016-09-07T03:22:58.823484: step 4472, loss 0.0591598, acc 0.98
2016-09-07T03:22:59.512159: step 4473, loss 0.0208835, acc 0.98
2016-09-07T03:23:00.202573: step 4474, loss 0.00011934, acc 1
2016-09-07T03:23:00.841999: step 4475, loss 0.00595029, acc 1
2016-09-07T03:23:01.557921: step 4476, loss 0.0297822, acc 0.98
2016-09-07T03:23:02.225929: step 4477, loss 0.231256, acc 0.96
2016-09-07T03:23:02.915958: step 4478, loss 0.0155044, acc 1
2016-09-07T03:23:03.594032: step 4479, loss 0.0279601, acc 0.98
2016-09-07T03:23:04.278293: step 4480, loss 0.0336824, acc 1
2016-09-07T03:23:04.964186: step 4481, loss 0.0388647, acc 0.98
2016-09-07T03:23:05.657026: step 4482, loss 0.0325849, acc 0.98
2016-09-07T03:23:06.352765: step 4483, loss 0.0987203, acc 0.98
2016-09-07T03:23:07.038734: step 4484, loss 0.00801895, acc 1
2016-09-07T03:23:07.744267: step 4485, loss 0.00687706, acc 1
2016-09-07T03:23:08.452966: step 4486, loss 0.019326, acc 1
2016-09-07T03:23:09.127464: step 4487, loss 0.00119712, acc 1
2016-09-07T03:23:09.825602: step 4488, loss 0.0216221, acc 0.98
2016-09-07T03:23:10.502627: step 4489, loss 0.0015091, acc 1
2016-09-07T03:23:11.220914: step 4490, loss 0.00231636, acc 1
2016-09-07T03:23:11.921852: step 4491, loss 0.00153406, acc 1
2016-09-07T03:23:12.637513: step 4492, loss 0.0365454, acc 0.98
2016-09-07T03:23:13.323431: step 4493, loss 0.00205451, acc 1
2016-09-07T03:23:14.012745: step 4494, loss 0.0256216, acc 0.98
2016-09-07T03:23:14.694575: step 4495, loss 0.147302, acc 0.96
2016-09-07T03:23:15.363556: step 4496, loss 0.102664, acc 0.98
2016-09-07T03:23:16.075366: step 4497, loss 0.00163958, acc 1
2016-09-07T03:23:16.767246: step 4498, loss 0.0384192, acc 0.98
2016-09-07T03:23:17.460311: step 4499, loss 0.0446806, acc 0.96
2016-09-07T03:23:18.149386: step 4500, loss 0.000531461, acc 1

Evaluation:
2016-09-07T03:23:21.728888: step 4500, loss 1.7376, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-4500

2016-09-07T03:23:23.587362: step 4501, loss 0.0263595, acc 0.98
2016-09-07T03:23:24.291826: step 4502, loss 0.0565351, acc 0.96
2016-09-07T03:23:24.986126: step 4503, loss 0.00625779, acc 1
2016-09-07T03:23:25.706181: step 4504, loss 0.00748092, acc 1
2016-09-07T03:23:26.405671: step 4505, loss 0.0196926, acc 1
2016-09-07T03:23:27.096067: step 4506, loss 0.0936867, acc 0.94
2016-09-07T03:23:27.800148: step 4507, loss 0.0354966, acc 0.98
2016-09-07T03:23:28.492729: step 4508, loss 0.106247, acc 0.94
2016-09-07T03:23:29.208350: step 4509, loss 0.0236084, acc 0.98
2016-09-07T03:23:29.872126: step 4510, loss 0.0037853, acc 1
2016-09-07T03:23:30.559256: step 4511, loss 0.0620926, acc 0.96
2016-09-07T03:23:31.278728: step 4512, loss 0.0162185, acc 1
2016-09-07T03:23:31.981101: step 4513, loss 0.143138, acc 0.94
2016-09-07T03:23:32.660654: step 4514, loss 0.0486196, acc 0.98
2016-09-07T03:23:33.338839: step 4515, loss 0.0498086, acc 0.98
2016-09-07T03:23:34.050837: step 4516, loss 0.112876, acc 0.96
2016-09-07T03:23:34.767969: step 4517, loss 0.0246219, acc 1
2016-09-07T03:23:35.466408: step 4518, loss 0.020027, acc 1
2016-09-07T03:23:36.146038: step 4519, loss 0.0208131, acc 1
2016-09-07T03:23:36.829950: step 4520, loss 0.0411793, acc 0.98
2016-09-07T03:23:37.560029: step 4521, loss 0.0119498, acc 1
2016-09-07T03:23:38.231814: step 4522, loss 0.00203263, acc 1
2016-09-07T03:23:38.936981: step 4523, loss 0.0331535, acc 0.98
2016-09-07T03:23:39.612442: step 4524, loss 0.0172143, acc 1
2016-09-07T03:23:40.315457: step 4525, loss 0.0258697, acc 1
2016-09-07T03:23:41.000236: step 4526, loss 0.114227, acc 0.96
2016-09-07T03:23:41.684531: step 4527, loss 0.0419374, acc 0.98
2016-09-07T03:23:42.380337: step 4528, loss 0.00124801, acc 1
2016-09-07T03:23:43.049614: step 4529, loss 0.00940719, acc 1
2016-09-07T03:23:43.755571: step 4530, loss 0.0411895, acc 0.98
2016-09-07T03:23:44.442005: step 4531, loss 0.0226455, acc 0.98
2016-09-07T03:23:45.147614: step 4532, loss 0.211464, acc 0.98
2016-09-07T03:23:45.818010: step 4533, loss 0.0132377, acc 1
2016-09-07T03:23:46.505489: step 4534, loss 0.0311229, acc 0.98
2016-09-07T03:23:47.215448: step 4535, loss 0.111033, acc 0.96
2016-09-07T03:23:47.883030: step 4536, loss 0.0622191, acc 0.96
2016-09-07T03:23:48.581437: step 4537, loss 0.0349013, acc 0.96
2016-09-07T03:23:49.280616: step 4538, loss 0.0469285, acc 1
2016-09-07T03:23:49.979192: step 4539, loss 0.0416503, acc 0.98
2016-09-07T03:23:50.689454: step 4540, loss 0.0836993, acc 0.96
2016-09-07T03:23:51.385573: step 4541, loss 0.0977713, acc 0.92
2016-09-07T03:23:52.120000: step 4542, loss 0.0370218, acc 0.98
2016-09-07T03:23:52.806971: step 4543, loss 0.0670511, acc 0.96
2016-09-07T03:23:53.498364: step 4544, loss 0.0952584, acc 0.98
2016-09-07T03:23:54.194304: step 4545, loss 0.00265812, acc 1
2016-09-07T03:23:54.871425: step 4546, loss 0.0345358, acc 1
2016-09-07T03:23:55.571145: step 4547, loss 0.0658718, acc 0.96
2016-09-07T03:23:56.236282: step 4548, loss 0.0583815, acc 0.98
2016-09-07T03:23:56.951701: step 4549, loss 0.000573795, acc 1
2016-09-07T03:23:57.627310: step 4550, loss 0.0405955, acc 0.96
2016-09-07T03:23:58.324103: step 4551, loss 0.0899858, acc 0.96
2016-09-07T03:23:58.999764: step 4552, loss 0.0113355, acc 1
2016-09-07T03:23:59.675374: step 4553, loss 0.0437177, acc 0.96
2016-09-07T03:24:00.393974: step 4554, loss 0.00170079, acc 1
2016-09-07T03:24:01.068242: step 4555, loss 0.0319829, acc 1
2016-09-07T03:24:01.779867: step 4556, loss 0.0101022, acc 1
2016-09-07T03:24:02.482306: step 4557, loss 0.087597, acc 0.96
2016-09-07T03:24:03.159594: step 4558, loss 0.107653, acc 0.96
2016-09-07T03:24:03.849674: step 4559, loss 0.0583866, acc 0.96
2016-09-07T03:24:04.551307: step 4560, loss 0.0204075, acc 0.98
2016-09-07T03:24:05.254688: step 4561, loss 0.0284233, acc 0.98
2016-09-07T03:24:05.937862: step 4562, loss 0.0294932, acc 1
2016-09-07T03:24:06.630443: step 4563, loss 0.0571239, acc 0.96
2016-09-07T03:24:07.339575: step 4564, loss 0.0196178, acc 1
2016-09-07T03:24:08.019409: step 4565, loss 0.0105777, acc 1
2016-09-07T03:24:08.718976: step 4566, loss 0.0521745, acc 0.96
2016-09-07T03:24:09.396418: step 4567, loss 0.0487179, acc 0.96
2016-09-07T03:24:10.121474: step 4568, loss 0.0223593, acc 0.98
2016-09-07T03:24:10.805142: step 4569, loss 0.0138774, acc 1
2016-09-07T03:24:11.514116: step 4570, loss 0.033, acc 0.98
2016-09-07T03:24:12.189372: step 4571, loss 0.00537617, acc 1
2016-09-07T03:24:12.884467: step 4572, loss 0.000349645, acc 1
2016-09-07T03:24:13.575362: step 4573, loss 0.0200257, acc 0.98
2016-09-07T03:24:14.246003: step 4574, loss 0.0549351, acc 0.96
2016-09-07T03:24:14.968128: step 4575, loss 0.0703697, acc 0.96
2016-09-07T03:24:15.658469: step 4576, loss 0.0175352, acc 1
2016-09-07T03:24:16.349006: step 4577, loss 0.000456116, acc 1
2016-09-07T03:24:17.074008: step 4578, loss 0.0161642, acc 1
2016-09-07T03:24:17.755648: step 4579, loss 0.0198093, acc 1
2016-09-07T03:24:18.458911: step 4580, loss 0.00245022, acc 1
2016-09-07T03:24:19.143101: step 4581, loss 0.00359768, acc 1
2016-09-07T03:24:19.854055: step 4582, loss 0.0292753, acc 0.98
2016-09-07T03:24:20.558405: step 4583, loss 0.0263739, acc 0.98
2016-09-07T03:24:21.250990: step 4584, loss 0.0720119, acc 0.98
2016-09-07T03:24:21.943001: step 4585, loss 0.0574938, acc 0.98
2016-09-07T03:24:22.631898: step 4586, loss 0.0363226, acc 0.98
2016-09-07T03:24:23.339467: step 4587, loss 0.0301404, acc 0.96
2016-09-07T03:24:24.033363: step 4588, loss 0.015861, acc 1
2016-09-07T03:24:24.725837: step 4589, loss 0.0167281, acc 0.98
2016-09-07T03:24:25.408173: step 4590, loss 0.0566159, acc 0.98
2016-09-07T03:24:26.090891: step 4591, loss 0.149952, acc 0.96
2016-09-07T03:24:26.781524: step 4592, loss 0.0303045, acc 0.98
2016-09-07T03:24:27.446574: step 4593, loss 0.106216, acc 0.98
2016-09-07T03:24:28.163516: step 4594, loss 0.124037, acc 0.92
2016-09-07T03:24:28.832637: step 4595, loss 0.0131687, acc 1
2016-09-07T03:24:29.525461: step 4596, loss 0.0233115, acc 0.98
2016-09-07T03:24:30.212058: step 4597, loss 0.0383675, acc 0.98
2016-09-07T03:24:30.888240: step 4598, loss 0.0235897, acc 1
2016-09-07T03:24:31.566216: step 4599, loss 0.00101386, acc 1
2016-09-07T03:24:32.244130: step 4600, loss 0.049278, acc 0.96

Evaluation:
2016-09-07T03:24:35.847974: step 4600, loss 1.61525, acc 0.767355

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-4600

2016-09-07T03:24:37.595183: step 4601, loss 0.0148243, acc 1
2016-09-07T03:24:38.276171: step 4602, loss 0.00134447, acc 1
2016-09-07T03:24:38.983368: step 4603, loss 0.0358651, acc 0.98
2016-09-07T03:24:39.670930: step 4604, loss 0.00118036, acc 1
2016-09-07T03:24:40.365915: step 4605, loss 0.0263232, acc 0.98
2016-09-07T03:24:41.054335: step 4606, loss 0.0213792, acc 0.98
2016-09-07T03:24:41.765805: step 4607, loss 0.0484081, acc 0.96
2016-09-07T03:24:42.416744: step 4608, loss 0.0121967, acc 1
2016-09-07T03:24:43.082867: step 4609, loss 0.0743174, acc 0.96
2016-09-07T03:24:43.795615: step 4610, loss 0.0084038, acc 1
2016-09-07T03:24:44.464614: step 4611, loss 0.0305386, acc 0.98
2016-09-07T03:24:45.167895: step 4612, loss 0.0312077, acc 0.98
2016-09-07T03:24:45.840106: step 4613, loss 0.0415764, acc 0.96
2016-09-07T03:24:46.519220: step 4614, loss 0.0512129, acc 0.98
2016-09-07T03:24:47.229705: step 4615, loss 0.0180086, acc 0.98
2016-09-07T03:24:47.882148: step 4616, loss 0.0116574, acc 1
2016-09-07T03:24:48.576001: step 4617, loss 0.0251075, acc 0.98
2016-09-07T03:24:49.278012: step 4618, loss 0.0174471, acc 0.98
2016-09-07T03:24:49.975096: step 4619, loss 0.000461854, acc 1
2016-09-07T03:24:50.661612: step 4620, loss 0.0232676, acc 1
2016-09-07T03:24:51.342347: step 4621, loss 0.0393418, acc 0.98
2016-09-07T03:24:52.043156: step 4622, loss 0.183078, acc 0.96
2016-09-07T03:24:52.718804: step 4623, loss 0.000488829, acc 1
2016-09-07T03:24:53.419012: step 4624, loss 0.00739269, acc 1
2016-09-07T03:24:54.098630: step 4625, loss 0.00236086, acc 1
2016-09-07T03:24:54.791593: step 4626, loss 0.0362925, acc 0.98
2016-09-07T03:24:55.487580: step 4627, loss 0.0282015, acc 1
2016-09-07T03:24:56.181527: step 4628, loss 0.0716151, acc 0.96
2016-09-07T03:24:56.874015: step 4629, loss 0.0871622, acc 0.94
2016-09-07T03:24:57.539056: step 4630, loss 0.036974, acc 0.98
2016-09-07T03:24:58.244575: step 4631, loss 0.0436406, acc 0.96
2016-09-07T03:24:58.900250: step 4632, loss 0.000647801, acc 1
2016-09-07T03:24:59.611057: step 4633, loss 0.0337659, acc 0.98
2016-09-07T03:25:00.346314: step 4634, loss 0.0135389, acc 1
2016-09-07T03:25:01.021356: step 4635, loss 0.00389272, acc 1
2016-09-07T03:25:01.707441: step 4636, loss 0.0050061, acc 1
2016-09-07T03:25:02.382160: step 4637, loss 0.00392604, acc 1
2016-09-07T03:25:03.107767: step 4638, loss 0.137983, acc 0.96
2016-09-07T03:25:03.804536: step 4639, loss 0.002638, acc 1
2016-09-07T03:25:04.522922: step 4640, loss 0.10798, acc 0.94
2016-09-07T03:25:05.236743: step 4641, loss 0.0330479, acc 1
2016-09-07T03:25:05.930072: step 4642, loss 0.0109614, acc 1
2016-09-07T03:25:06.629368: step 4643, loss 0.0482576, acc 0.96
2016-09-07T03:25:07.314403: step 4644, loss 0.0749444, acc 0.98
2016-09-07T03:25:08.024101: step 4645, loss 0.00461609, acc 1
2016-09-07T03:25:08.726720: step 4646, loss 0.0150197, acc 1
2016-09-07T03:25:09.415094: step 4647, loss 0.00613198, acc 1
2016-09-07T03:25:10.132752: step 4648, loss 0.00808907, acc 1
2016-09-07T03:25:10.793483: step 4649, loss 0.0368523, acc 1
2016-09-07T03:25:11.510576: step 4650, loss 0.0750561, acc 0.96
2016-09-07T03:25:12.207029: step 4651, loss 0.0140502, acc 1
2016-09-07T03:25:12.915732: step 4652, loss 0.00162764, acc 1
2016-09-07T03:25:13.612746: step 4653, loss 0.0352005, acc 0.98
2016-09-07T03:25:14.311395: step 4654, loss 0.0335415, acc 0.98
2016-09-07T03:25:15.006620: step 4655, loss 0.0291362, acc 0.98
2016-09-07T03:25:15.658422: step 4656, loss 0.0376606, acc 0.98
2016-09-07T03:25:16.347618: step 4657, loss 0.0712592, acc 0.98
2016-09-07T03:25:17.050602: step 4658, loss 0.022013, acc 1
2016-09-07T03:25:17.737030: step 4659, loss 0.0267076, acc 1
2016-09-07T03:25:18.430435: step 4660, loss 0.00746624, acc 1
2016-09-07T03:25:19.127708: step 4661, loss 0.0387801, acc 0.98
2016-09-07T03:25:19.839763: step 4662, loss 0.0466649, acc 0.98
2016-09-07T03:25:20.504086: step 4663, loss 0.0273752, acc 0.98
2016-09-07T03:25:21.192963: step 4664, loss 0.0171808, acc 0.98
2016-09-07T03:25:21.906406: step 4665, loss 0.0538791, acc 0.96
2016-09-07T03:25:22.597184: step 4666, loss 0.000889913, acc 1
2016-09-07T03:25:23.297076: step 4667, loss 0.0725101, acc 0.96
2016-09-07T03:25:23.959872: step 4668, loss 0.159212, acc 0.96
2016-09-07T03:25:24.694034: step 4669, loss 0.0442161, acc 0.98
2016-09-07T03:25:25.368186: step 4670, loss 0.0167659, acc 1
2016-09-07T03:25:26.050640: step 4671, loss 0.0473775, acc 0.96
2016-09-07T03:25:26.747028: step 4672, loss 0.06615, acc 0.96
2016-09-07T03:25:27.436630: step 4673, loss 0.00301352, acc 1
2016-09-07T03:25:28.126339: step 4674, loss 0.0507796, acc 0.96
2016-09-07T03:25:28.803858: step 4675, loss 0.00794977, acc 1
2016-09-07T03:25:29.516000: step 4676, loss 0.0082043, acc 1
2016-09-07T03:25:30.203924: step 4677, loss 0.0132072, acc 1
2016-09-07T03:25:30.892229: step 4678, loss 0.0365851, acc 0.98
2016-09-07T03:25:31.566480: step 4679, loss 0.018088, acc 0.98
2016-09-07T03:25:32.237018: step 4680, loss 0.043921, acc 0.96
2016-09-07T03:25:32.965264: step 4681, loss 0.0274543, acc 0.98
2016-09-07T03:25:33.641085: step 4682, loss 0.0183259, acc 0.98
2016-09-07T03:25:34.332598: step 4683, loss 0.0157576, acc 1
2016-09-07T03:25:35.011796: step 4684, loss 0.0582113, acc 0.96
2016-09-07T03:25:35.695879: step 4685, loss 0.0145009, acc 1
2016-09-07T03:25:36.402840: step 4686, loss 0.000247163, acc 1
2016-09-07T03:25:37.087990: step 4687, loss 0.00993451, acc 1
2016-09-07T03:25:37.774056: step 4688, loss 0.0346716, acc 0.98
2016-09-07T03:25:38.460442: step 4689, loss 0.0126916, acc 1
2016-09-07T03:25:39.187025: step 4690, loss 0.00394811, acc 1
2016-09-07T03:25:39.876068: step 4691, loss 0.0855671, acc 0.96
2016-09-07T03:25:40.562592: step 4692, loss 0.0288064, acc 0.98
2016-09-07T03:25:41.255180: step 4693, loss 0.0364882, acc 0.98
2016-09-07T03:25:41.967274: step 4694, loss 0.0233414, acc 0.98
2016-09-07T03:25:42.669039: step 4695, loss 0.00160124, acc 1
2016-09-07T03:25:43.331437: step 4696, loss 0.0342087, acc 0.98
2016-09-07T03:25:44.021935: step 4697, loss 0.0296881, acc 0.98
2016-09-07T03:25:44.714437: step 4698, loss 0.0368117, acc 0.98
2016-09-07T03:25:45.402995: step 4699, loss 0.00933751, acc 1
2016-09-07T03:25:46.105723: step 4700, loss 0.0428088, acc 0.96

Evaluation:
2016-09-07T03:25:49.698663: step 4700, loss 1.85612, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-4700

2016-09-07T03:25:51.562627: step 4701, loss 0.0467389, acc 0.96
2016-09-07T03:25:52.319302: step 4702, loss 0.0621722, acc 0.98
2016-09-07T03:25:53.039754: step 4703, loss 0.0227399, acc 1
2016-09-07T03:25:53.754023: step 4704, loss 0.0484104, acc 0.98
2016-09-07T03:25:54.430734: step 4705, loss 0.0248535, acc 0.98
2016-09-07T03:25:55.141300: step 4706, loss 0.0417795, acc 0.98
2016-09-07T03:25:55.845831: step 4707, loss 0.0154256, acc 1
2016-09-07T03:25:56.523970: step 4708, loss 0.0917034, acc 0.98
2016-09-07T03:25:57.243489: step 4709, loss 0.000446086, acc 1
2016-09-07T03:25:57.926703: step 4710, loss 0.0142913, acc 1
2016-09-07T03:25:58.607827: step 4711, loss 0.0329517, acc 1
2016-09-07T03:25:59.301454: step 4712, loss 0.00133398, acc 1
2016-09-07T03:25:59.990574: step 4713, loss 0.0384612, acc 0.98
2016-09-07T03:26:00.741060: step 4714, loss 0.0594011, acc 0.98
2016-09-07T03:26:01.406991: step 4715, loss 0.0326554, acc 0.98
2016-09-07T03:26:02.095248: step 4716, loss 0.039884, acc 0.96
2016-09-07T03:26:02.789252: step 4717, loss 0.000713749, acc 1
2016-09-07T03:26:03.478381: step 4718, loss 0.0961802, acc 0.98
2016-09-07T03:26:04.185584: step 4719, loss 0.136678, acc 0.96
2016-09-07T03:26:04.854909: step 4720, loss 0.00107286, acc 1
2016-09-07T03:26:05.569730: step 4721, loss 0.0564799, acc 0.98
2016-09-07T03:26:06.259094: step 4722, loss 0.0157235, acc 1
2016-09-07T03:26:06.969962: step 4723, loss 0.0475564, acc 0.98
2016-09-07T03:26:07.666079: step 4724, loss 0.0151005, acc 1
2016-09-07T03:26:08.343900: step 4725, loss 0.0141037, acc 1
2016-09-07T03:26:09.063253: step 4726, loss 0.0397646, acc 0.98
2016-09-07T03:26:09.748347: step 4727, loss 0.00325974, acc 1
2016-09-07T03:26:10.464255: step 4728, loss 0.0179317, acc 0.98
2016-09-07T03:26:11.170027: step 4729, loss 0.104108, acc 0.94
2016-09-07T03:26:11.850283: step 4730, loss 0.0327727, acc 1
2016-09-07T03:26:12.558874: step 4731, loss 0.00606631, acc 1
2016-09-07T03:26:13.245491: step 4732, loss 0.0010452, acc 1
2016-09-07T03:26:13.948844: step 4733, loss 0.0645687, acc 0.94
2016-09-07T03:26:14.624947: step 4734, loss 0.0182535, acc 0.98
2016-09-07T03:26:15.311143: step 4735, loss 0.00226416, acc 1
2016-09-07T03:26:16.017397: step 4736, loss 0.0956558, acc 0.98
2016-09-07T03:26:16.707479: step 4737, loss 0.000346666, acc 1
2016-09-07T03:26:17.410496: step 4738, loss 0.100207, acc 0.96
2016-09-07T03:26:18.111074: step 4739, loss 0.033841, acc 1
2016-09-07T03:26:18.826717: step 4740, loss 0.0699518, acc 0.98
2016-09-07T03:26:19.548146: step 4741, loss 0.017519, acc 1
2016-09-07T03:26:20.247668: step 4742, loss 0.0180829, acc 1
2016-09-07T03:26:20.955913: step 4743, loss 0.187021, acc 0.98
2016-09-07T03:26:21.646154: step 4744, loss 0.0215348, acc 0.98
2016-09-07T03:26:22.381807: step 4745, loss 0.0446583, acc 0.96
2016-09-07T03:26:23.072899: step 4746, loss 0.0856533, acc 0.98
2016-09-07T03:26:23.771659: step 4747, loss 0.0358896, acc 0.98
2016-09-07T03:26:24.472000: step 4748, loss 0.0198168, acc 0.98
2016-09-07T03:26:25.156005: step 4749, loss 0.0136001, acc 1
2016-09-07T03:26:25.859945: step 4750, loss 0.02494, acc 1
2016-09-07T03:26:26.528971: step 4751, loss 0.02379, acc 1
2016-09-07T03:26:27.250895: step 4752, loss 0.000998853, acc 1
2016-09-07T03:26:27.945932: step 4753, loss 0.0578651, acc 0.96
2016-09-07T03:26:28.634023: step 4754, loss 0.0390054, acc 0.98
2016-09-07T03:26:29.334578: step 4755, loss 0.0533427, acc 0.98
2016-09-07T03:26:30.042496: step 4756, loss 0.0326669, acc 0.98
2016-09-07T03:26:30.762089: step 4757, loss 0.0325651, acc 0.98
2016-09-07T03:26:31.446870: step 4758, loss 0.00881548, acc 1
2016-09-07T03:26:32.152899: step 4759, loss 0.00679536, acc 1
2016-09-07T03:26:32.863402: step 4760, loss 0.0291301, acc 0.98
2016-09-07T03:26:33.576229: step 4761, loss 0.028688, acc 0.98
2016-09-07T03:26:34.278550: step 4762, loss 0.049564, acc 0.98
2016-09-07T03:26:34.966128: step 4763, loss 0.00305164, acc 1
2016-09-07T03:26:35.666559: step 4764, loss 0.0164008, acc 0.98
2016-09-07T03:26:36.367358: step 4765, loss 0.00665355, acc 1
2016-09-07T03:26:37.064592: step 4766, loss 0.0475347, acc 0.98
2016-09-07T03:26:37.786837: step 4767, loss 0.0311565, acc 0.98
2016-09-07T03:26:38.487409: step 4768, loss 0.0214432, acc 1
2016-09-07T03:26:39.189385: step 4769, loss 0.00206092, acc 1
2016-09-07T03:26:39.864444: step 4770, loss 0.0144441, acc 0.98
2016-09-07T03:26:40.543781: step 4771, loss 0.00682883, acc 1
2016-09-07T03:26:41.246950: step 4772, loss 0.0138981, acc 1
2016-09-07T03:26:41.966934: step 4773, loss 0.0272896, acc 1
2016-09-07T03:26:42.666090: step 4774, loss 0.0300522, acc 1
2016-09-07T03:26:43.314955: step 4775, loss 0.0714437, acc 0.98
2016-09-07T03:26:44.026379: step 4776, loss 0.0182851, acc 0.98
2016-09-07T03:26:44.723294: step 4777, loss 0.0414783, acc 0.98
2016-09-07T03:26:45.422315: step 4778, loss 0.0660841, acc 0.94
2016-09-07T03:26:46.122729: step 4779, loss 0.00393372, acc 1
2016-09-07T03:26:46.810457: step 4780, loss 0.0445779, acc 0.98
2016-09-07T03:26:47.524985: step 4781, loss 0.0033586, acc 1
2016-09-07T03:26:48.179067: step 4782, loss 0.0405034, acc 1
2016-09-07T03:26:48.866237: step 4783, loss 0.025523, acc 0.98
2016-09-07T03:26:49.566153: step 4784, loss 0.0118859, acc 1
2016-09-07T03:26:50.266250: step 4785, loss 0.0415805, acc 0.98
2016-09-07T03:26:50.967394: step 4786, loss 0.12123, acc 0.96
2016-09-07T03:26:51.677541: step 4787, loss 0.0282379, acc 0.98
2016-09-07T03:26:52.401883: step 4788, loss 0.0188926, acc 1
2016-09-07T03:26:53.086950: step 4789, loss 0.147925, acc 0.98
2016-09-07T03:26:53.761087: step 4790, loss 0.0319941, acc 0.98
2016-09-07T03:26:54.463740: step 4791, loss 0.0137604, acc 1
2016-09-07T03:26:55.163639: step 4792, loss 0.077294, acc 0.98
2016-09-07T03:26:55.869411: step 4793, loss 0.0903062, acc 0.94
2016-09-07T03:26:56.542120: step 4794, loss 0.0315499, acc 0.98
2016-09-07T03:26:57.252333: step 4795, loss 0.0239874, acc 0.98
2016-09-07T03:26:57.950948: step 4796, loss 0.0156062, acc 1
2016-09-07T03:26:58.641097: step 4797, loss 0.0345492, acc 0.98
2016-09-07T03:26:59.330790: step 4798, loss 0.0312727, acc 0.98
2016-09-07T03:26:59.999944: step 4799, loss 0.0372839, acc 0.98
2016-09-07T03:27:00.665502: step 4800, loss 0.000886144, acc 1

Evaluation:
2016-09-07T03:27:04.295821: step 4800, loss 1.82657, acc 0.743902

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-4800

2016-09-07T03:27:06.042854: step 4801, loss 0.0247807, acc 1
2016-09-07T03:27:06.735528: step 4802, loss 0.0145727, acc 1
2016-09-07T03:27:07.395291: step 4803, loss 0.0272635, acc 0.98
2016-09-07T03:27:08.094460: step 4804, loss 0.0141457, acc 1
2016-09-07T03:27:08.792724: step 4805, loss 0.0126628, acc 1
2016-09-07T03:27:09.476701: step 4806, loss 0.0332016, acc 0.98
2016-09-07T03:27:10.175545: step 4807, loss 0.00520031, acc 1
2016-09-07T03:27:10.867669: step 4808, loss 0.00932699, acc 1
2016-09-07T03:27:11.576547: step 4809, loss 0.0374054, acc 0.96
2016-09-07T03:27:12.256627: step 4810, loss 0.0501814, acc 0.96
2016-09-07T03:27:12.977825: step 4811, loss 0.00515386, acc 1
2016-09-07T03:27:13.657163: step 4812, loss 0.00425526, acc 1
2016-09-07T03:27:14.318130: step 4813, loss 0.0495852, acc 0.98
2016-09-07T03:27:15.000981: step 4814, loss 0.0193553, acc 0.98
2016-09-07T03:27:15.704518: step 4815, loss 0.132566, acc 0.96
2016-09-07T03:27:16.400937: step 4816, loss 0.00894227, acc 1
2016-09-07T03:27:17.083244: step 4817, loss 0.161731, acc 0.94
2016-09-07T03:27:17.809704: step 4818, loss 0.00185791, acc 1
2016-09-07T03:27:18.524852: step 4819, loss 0.0235243, acc 0.98
2016-09-07T03:27:19.210489: step 4820, loss 0.0101565, acc 1
2016-09-07T03:27:19.909430: step 4821, loss 0.0180098, acc 0.98
2016-09-07T03:27:20.610596: step 4822, loss 0.0300457, acc 0.98
2016-09-07T03:27:21.336295: step 4823, loss 0.00338544, acc 1
2016-09-07T03:27:22.026475: step 4824, loss 0.00394754, acc 1
2016-09-07T03:27:22.723343: step 4825, loss 0.0872803, acc 0.96
2016-09-07T03:27:23.415791: step 4826, loss 0.0206741, acc 0.98
2016-09-07T03:27:24.118419: step 4827, loss 0.0748082, acc 0.98
2016-09-07T03:27:24.823123: step 4828, loss 0.00966419, acc 1
2016-09-07T03:27:25.497497: step 4829, loss 0.0539837, acc 0.98
2016-09-07T03:27:26.216120: step 4830, loss 0.0224391, acc 1
2016-09-07T03:27:26.905233: step 4831, loss 0.0169575, acc 1
2016-09-07T03:27:27.590928: step 4832, loss 0.102688, acc 0.98
2016-09-07T03:27:28.295631: step 4833, loss 0.0997397, acc 0.96
2016-09-07T03:27:28.992515: step 4834, loss 0.0448775, acc 0.96
2016-09-07T03:27:29.703364: step 4835, loss 0.00715472, acc 1
2016-09-07T03:27:30.392826: step 4836, loss 0.0944366, acc 0.96
2016-09-07T03:27:31.072494: step 4837, loss 0.0233932, acc 0.98
2016-09-07T03:27:31.749383: step 4838, loss 0.0253789, acc 0.98
2016-09-07T03:27:32.414855: step 4839, loss 0.00551589, acc 1
2016-09-07T03:27:33.095345: step 4840, loss 0.0108655, acc 1
2016-09-07T03:27:33.800562: step 4841, loss 0.0391132, acc 0.98
2016-09-07T03:27:34.526542: step 4842, loss 0.0109773, acc 1
2016-09-07T03:27:35.204361: step 4843, loss 0.0358579, acc 0.98
2016-09-07T03:27:35.898024: step 4844, loss 0.0265749, acc 1
2016-09-07T03:27:36.606653: step 4845, loss 0.0307578, acc 1
2016-09-07T03:27:37.311086: step 4846, loss 0.0360931, acc 0.98
2016-09-07T03:27:38.019547: step 4847, loss 0.0116667, acc 1
2016-09-07T03:27:38.694144: step 4848, loss 0.0146942, acc 1
2016-09-07T03:27:39.421516: step 4849, loss 0.0178897, acc 1
2016-09-07T03:27:40.122957: step 4850, loss 0.0462133, acc 0.96
2016-09-07T03:27:40.791895: step 4851, loss 0.033273, acc 0.98
2016-09-07T03:27:41.464444: step 4852, loss 0.0430839, acc 0.96
2016-09-07T03:27:42.138997: step 4853, loss 0.024376, acc 0.98
2016-09-07T03:27:42.832962: step 4854, loss 0.00135361, acc 1
2016-09-07T03:27:43.509210: step 4855, loss 0.0293742, acc 0.98
2016-09-07T03:27:44.228884: step 4856, loss 0.00518577, acc 1
2016-09-07T03:27:44.913652: step 4857, loss 0.053434, acc 0.98
2016-09-07T03:27:45.616789: step 4858, loss 0.00907246, acc 1
2016-09-07T03:27:46.326612: step 4859, loss 0.00537147, acc 1
2016-09-07T03:27:47.019768: step 4860, loss 0.000209775, acc 1
2016-09-07T03:27:47.731382: step 4861, loss 0.0274739, acc 1
2016-09-07T03:27:48.425960: step 4862, loss 0.00889406, acc 1
2016-09-07T03:27:49.106350: step 4863, loss 0.0234324, acc 0.98
2016-09-07T03:27:49.799845: step 4864, loss 0.101186, acc 0.96
2016-09-07T03:27:50.491686: step 4865, loss 0.0111678, acc 1
2016-09-07T03:27:51.172137: step 4866, loss 0.0245176, acc 0.98
2016-09-07T03:27:51.875848: step 4867, loss 0.00815778, acc 1
2016-09-07T03:27:52.583217: step 4868, loss 0.0182642, acc 0.98
2016-09-07T03:27:53.262714: step 4869, loss 0.0244249, acc 1
2016-09-07T03:27:53.946335: step 4870, loss 0.030152, acc 0.98
2016-09-07T03:27:54.639102: step 4871, loss 0.00454886, acc 1
2016-09-07T03:27:55.359195: step 4872, loss 0.025855, acc 0.98
2016-09-07T03:27:56.075348: step 4873, loss 0.00061788, acc 1
2016-09-07T03:27:56.734270: step 4874, loss 0.00637532, acc 1
2016-09-07T03:27:57.436993: step 4875, loss 0.000506345, acc 1
2016-09-07T03:27:58.125337: step 4876, loss 0.0087806, acc 1
2016-09-07T03:27:58.827534: step 4877, loss 0.0165787, acc 1
2016-09-07T03:27:59.522348: step 4878, loss 0.00296747, acc 1
2016-09-07T03:28:00.227366: step 4879, loss 0.00706242, acc 1
2016-09-07T03:28:00.937391: step 4880, loss 0.00667341, acc 1
2016-09-07T03:28:01.600522: step 4881, loss 0.0247353, acc 0.98
2016-09-07T03:28:02.329942: step 4882, loss 0.0234363, acc 1
2016-09-07T03:28:03.034073: step 4883, loss 0.0317071, acc 1
2016-09-07T03:28:03.728873: step 4884, loss 0.00805638, acc 1
2016-09-07T03:28:04.396735: step 4885, loss 0.00500451, acc 1
2016-09-07T03:28:05.097460: step 4886, loss 0.0441553, acc 0.96
2016-09-07T03:28:05.796497: step 4887, loss 0.042156, acc 0.98
2016-09-07T03:28:06.484332: step 4888, loss 0.017146, acc 0.98
2016-09-07T03:28:07.168126: step 4889, loss 0.00898494, acc 1
2016-09-07T03:28:07.857719: step 4890, loss 0.121327, acc 0.98
2016-09-07T03:28:08.538760: step 4891, loss 0.186451, acc 0.96
2016-09-07T03:28:09.241987: step 4892, loss 0.0730277, acc 0.98
2016-09-07T03:28:09.929020: step 4893, loss 0.0203613, acc 0.98
2016-09-07T03:28:10.633051: step 4894, loss 0.0250669, acc 0.98
2016-09-07T03:28:11.360180: step 4895, loss 0.0430202, acc 0.98
2016-09-07T03:28:12.059669: step 4896, loss 0.0412028, acc 0.98
2016-09-07T03:28:12.768588: step 4897, loss 0.0180388, acc 0.98
2016-09-07T03:28:13.487725: step 4898, loss 0.0582703, acc 0.96
2016-09-07T03:28:14.216475: step 4899, loss 0.0162235, acc 1
2016-09-07T03:28:14.930102: step 4900, loss 0.0196847, acc 0.98

Evaluation:
2016-09-07T03:28:18.511226: step 4900, loss 1.99239, acc 0.760788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-4900

2016-09-07T03:28:20.245782: step 4901, loss 0.00986877, acc 1
2016-09-07T03:28:20.929845: step 4902, loss 0.244189, acc 0.92
2016-09-07T03:28:21.614711: step 4903, loss 0.00923591, acc 1
2016-09-07T03:28:22.309814: step 4904, loss 0.00163104, acc 1
2016-09-07T03:28:22.995880: step 4905, loss 0.0190584, acc 1
2016-09-07T03:28:23.702595: step 4906, loss 0.0327385, acc 1
2016-09-07T03:28:24.376807: step 4907, loss 0.000239716, acc 1
2016-09-07T03:28:25.099762: step 4908, loss 0.00295009, acc 1
2016-09-07T03:28:25.802665: step 4909, loss 0.0141373, acc 1
2016-09-07T03:28:26.497510: step 4910, loss 0.0179034, acc 1
2016-09-07T03:28:27.196861: step 4911, loss 0.0363697, acc 0.98
2016-09-07T03:28:27.905261: step 4912, loss 0.0669031, acc 0.96
2016-09-07T03:28:28.623682: step 4913, loss 0.0602772, acc 0.98
2016-09-07T03:28:29.316028: step 4914, loss 0.0310291, acc 0.98
2016-09-07T03:28:29.977911: step 4915, loss 0.0033523, acc 1
2016-09-07T03:28:30.647183: step 4916, loss 0.0635718, acc 0.96
2016-09-07T03:28:31.327987: step 4917, loss 0.0319571, acc 0.98
2016-09-07T03:28:32.022265: step 4918, loss 0.0226787, acc 1
2016-09-07T03:28:32.712133: step 4919, loss 0.0929819, acc 0.96
2016-09-07T03:28:33.420493: step 4920, loss 0.0375373, acc 0.98
2016-09-07T03:28:34.100364: step 4921, loss 0.100256, acc 0.98
2016-09-07T03:28:34.786559: step 4922, loss 0.0198418, acc 0.98
2016-09-07T03:28:35.475395: step 4923, loss 0.00564069, acc 1
2016-09-07T03:28:36.158162: step 4924, loss 0.030702, acc 0.98
2016-09-07T03:28:36.852078: step 4925, loss 0.0380443, acc 1
2016-09-07T03:28:37.548597: step 4926, loss 0.0289407, acc 1
2016-09-07T03:28:38.276660: step 4927, loss 0.0166152, acc 1
2016-09-07T03:28:38.949612: step 4928, loss 0.0169923, acc 0.98
2016-09-07T03:28:39.626434: step 4929, loss 0.00194106, acc 1
2016-09-07T03:28:40.313229: step 4930, loss 0.0513978, acc 0.98
2016-09-07T03:28:41.021556: step 4931, loss 0.0136233, acc 1
2016-09-07T03:28:41.715779: step 4932, loss 0.015651, acc 1
2016-09-07T03:28:42.388207: step 4933, loss 0.0145598, acc 1
2016-09-07T03:28:43.080510: step 4934, loss 0.0774099, acc 0.96
2016-09-07T03:28:43.755706: step 4935, loss 0.0320399, acc 0.98
2016-09-07T03:28:44.432565: step 4936, loss 0.0190836, acc 1
2016-09-07T03:28:45.130453: step 4937, loss 0.0135663, acc 1
2016-09-07T03:28:45.852889: step 4938, loss 0.00598849, acc 1
2016-09-07T03:28:46.558663: step 4939, loss 0.0253853, acc 0.98
2016-09-07T03:28:47.240014: step 4940, loss 0.00957971, acc 1
2016-09-07T03:28:47.949978: step 4941, loss 0.0131485, acc 1
2016-09-07T03:28:48.629346: step 4942, loss 0.0191376, acc 1
2016-09-07T03:28:49.319744: step 4943, loss 0.0259966, acc 1
2016-09-07T03:28:50.006011: step 4944, loss 0.000361393, acc 1
2016-09-07T03:28:50.719088: step 4945, loss 0.0178963, acc 1
2016-09-07T03:28:51.418619: step 4946, loss 0.0662173, acc 0.98
2016-09-07T03:28:52.098449: step 4947, loss 0.0488975, acc 0.96
2016-09-07T03:28:52.805509: step 4948, loss 0.0207275, acc 1
2016-09-07T03:28:53.508028: step 4949, loss 0.0827547, acc 0.96
2016-09-07T03:28:54.199963: step 4950, loss 0.0322092, acc 1
2016-09-07T03:28:54.898586: step 4951, loss 0.0316576, acc 0.98
2016-09-07T03:28:55.586155: step 4952, loss 0.00124327, acc 1
2016-09-07T03:28:56.290894: step 4953, loss 0.0169645, acc 0.98
2016-09-07T03:28:56.952189: step 4954, loss 0.00627711, acc 1
2016-09-07T03:28:57.661469: step 4955, loss 0.0316651, acc 0.98
2016-09-07T03:28:58.343585: step 4956, loss 0.0847209, acc 0.96
2016-09-07T03:28:59.039113: step 4957, loss 0.00132656, acc 1
2016-09-07T03:28:59.750918: step 4958, loss 0.0306338, acc 0.98
2016-09-07T03:29:00.448820: step 4959, loss 0.0299568, acc 0.98
2016-09-07T03:29:01.137802: step 4960, loss 0.0777126, acc 0.96
2016-09-07T03:29:01.805363: step 4961, loss 0.000881197, acc 1
2016-09-07T03:29:02.482513: step 4962, loss 0.00689331, acc 1
2016-09-07T03:29:03.164631: step 4963, loss 0.0094573, acc 1
2016-09-07T03:29:03.850304: step 4964, loss 0.00809075, acc 1
2016-09-07T03:29:04.538976: step 4965, loss 0.0105058, acc 1
2016-09-07T03:29:05.227205: step 4966, loss 0.0313104, acc 0.98
2016-09-07T03:29:05.947479: step 4967, loss 0.0050886, acc 1
2016-09-07T03:29:06.638709: step 4968, loss 0.15343, acc 0.94
2016-09-07T03:29:07.320668: step 4969, loss 0.129634, acc 0.96
2016-09-07T03:29:08.011987: step 4970, loss 0.0316014, acc 0.98
2016-09-07T03:29:08.715714: step 4971, loss 0.0583276, acc 0.98
2016-09-07T03:29:09.408616: step 4972, loss 0.017042, acc 0.98
2016-09-07T03:29:10.086998: step 4973, loss 0.0145251, acc 1
2016-09-07T03:29:10.793607: step 4974, loss 0.0362507, acc 1
2016-09-07T03:29:11.484152: step 4975, loss 0.0259403, acc 0.98
2016-09-07T03:29:12.197968: step 4976, loss 0.0188636, acc 0.98
2016-09-07T03:29:12.884790: step 4977, loss 0.0702865, acc 0.96
2016-09-07T03:29:13.582343: step 4978, loss 0.0505894, acc 0.98
2016-09-07T03:29:14.289699: step 4979, loss 0.012519, acc 1
2016-09-07T03:29:14.964557: step 4980, loss 0.0164346, acc 1
2016-09-07T03:29:15.686779: step 4981, loss 0.0145268, acc 1
2016-09-07T03:29:16.362363: step 4982, loss 0.0382249, acc 0.98
2016-09-07T03:29:17.059831: step 4983, loss 0.00413794, acc 1
2016-09-07T03:29:17.756262: step 4984, loss 0.181815, acc 0.96
2016-09-07T03:29:18.473880: step 4985, loss 0.0187341, acc 0.98
2016-09-07T03:29:19.170724: step 4986, loss 0.0245817, acc 0.98
2016-09-07T03:29:19.853887: step 4987, loss 0.0390109, acc 0.98
2016-09-07T03:29:20.531937: step 4988, loss 0.00145013, acc 1
2016-09-07T03:29:21.216794: step 4989, loss 0.0714218, acc 0.98
2016-09-07T03:29:21.919784: step 4990, loss 0.0417319, acc 0.96
2016-09-07T03:29:22.616452: step 4991, loss 0.0175173, acc 0.98
2016-09-07T03:29:23.257941: step 4992, loss 0.0126566, acc 1
2016-09-07T03:29:23.955367: step 4993, loss 0.0509894, acc 0.98
2016-09-07T03:29:24.636633: step 4994, loss 0.0893699, acc 0.94
2016-09-07T03:29:25.321918: step 4995, loss 0.0435214, acc 0.98
2016-09-07T03:29:26.008920: step 4996, loss 0.00791077, acc 1
2016-09-07T03:29:26.697641: step 4997, loss 0.0387025, acc 0.98
2016-09-07T03:29:27.397943: step 4998, loss 0.0237165, acc 1
2016-09-07T03:29:28.079614: step 4999, loss 0.0119178, acc 1
2016-09-07T03:29:28.807801: step 5000, loss 0.0469047, acc 0.98

Evaluation:
2016-09-07T03:29:32.415833: step 5000, loss 1.84991, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-5000

2016-09-07T03:29:34.238033: step 5001, loss 0.0157034, acc 1
2016-09-07T03:29:34.932293: step 5002, loss 0.00175138, acc 1
2016-09-07T03:29:35.609800: step 5003, loss 0.0527417, acc 0.98
2016-09-07T03:29:36.278204: step 5004, loss 0.00288257, acc 1
2016-09-07T03:29:36.958327: step 5005, loss 0.0417074, acc 0.96
2016-09-07T03:29:37.635805: step 5006, loss 0.0150603, acc 1
2016-09-07T03:29:38.354253: step 5007, loss 0.0417745, acc 0.98
2016-09-07T03:29:39.058203: step 5008, loss 0.000276178, acc 1
2016-09-07T03:29:39.740041: step 5009, loss 0.0490278, acc 0.98
2016-09-07T03:29:40.427835: step 5010, loss 0.136994, acc 0.94
2016-09-07T03:29:41.114048: step 5011, loss 0.122017, acc 0.94
2016-09-07T03:29:41.800471: step 5012, loss 0.0120413, acc 1
2016-09-07T03:29:42.500695: step 5013, loss 0.0154599, acc 1
2016-09-07T03:29:43.205790: step 5014, loss 0.0181569, acc 0.98
2016-09-07T03:29:43.890293: step 5015, loss 0.022251, acc 0.98
2016-09-07T03:29:44.593451: step 5016, loss 0.0579894, acc 0.98
2016-09-07T03:29:45.290711: step 5017, loss 0.0412086, acc 0.98
2016-09-07T03:29:45.965005: step 5018, loss 0.0141699, acc 1
2016-09-07T03:29:46.672552: step 5019, loss 0.176063, acc 0.98
2016-09-07T03:29:47.389258: step 5020, loss 0.0134759, acc 1
2016-09-07T03:29:48.107518: step 5021, loss 0.0526553, acc 0.96
2016-09-07T03:29:48.815438: step 5022, loss 0.0219843, acc 1
2016-09-07T03:29:49.513739: step 5023, loss 0.0387764, acc 0.98
2016-09-07T03:29:50.222509: step 5024, loss 0.0544661, acc 0.96
2016-09-07T03:29:50.920259: step 5025, loss 0.0392441, acc 0.98
2016-09-07T03:29:51.633390: step 5026, loss 0.0162651, acc 1
2016-09-07T03:29:52.295565: step 5027, loss 0.0392467, acc 0.98
2016-09-07T03:29:52.978712: step 5028, loss 0.0299259, acc 0.98
2016-09-07T03:29:53.675319: step 5029, loss 0.038152, acc 0.98
2016-09-07T03:29:54.383517: step 5030, loss 0.0358425, acc 0.98
2016-09-07T03:29:55.086956: step 5031, loss 0.008937, acc 1
2016-09-07T03:29:55.744298: step 5032, loss 0.018008, acc 1
2016-09-07T03:29:56.442760: step 5033, loss 0.0316173, acc 1
2016-09-07T03:29:57.109713: step 5034, loss 0.047552, acc 0.98
2016-09-07T03:29:57.802066: step 5035, loss 0.0363213, acc 0.98
2016-09-07T03:29:58.497693: step 5036, loss 0.00802535, acc 1
2016-09-07T03:29:59.191585: step 5037, loss 0.0188493, acc 1
2016-09-07T03:29:59.891370: step 5038, loss 0.0294334, acc 1
2016-09-07T03:30:00.594499: step 5039, loss 0.0301399, acc 1
2016-09-07T03:30:01.301699: step 5040, loss 0.0346527, acc 1
2016-09-07T03:30:01.978794: step 5041, loss 0.0582174, acc 0.96
2016-09-07T03:30:02.644821: step 5042, loss 0.0227585, acc 1
2016-09-07T03:30:03.338005: step 5043, loss 0.0558628, acc 0.98
2016-09-07T03:30:04.035391: step 5044, loss 0.0287404, acc 0.98
2016-09-07T03:30:04.714564: step 5045, loss 0.0161208, acc 0.98
2016-09-07T03:30:05.373250: step 5046, loss 0.000124878, acc 1
2016-09-07T03:30:06.085918: step 5047, loss 0.0148879, acc 0.98
2016-09-07T03:30:06.747222: step 5048, loss 0.0228376, acc 1
2016-09-07T03:30:07.428647: step 5049, loss 0.0285055, acc 0.98
2016-09-07T03:30:08.100943: step 5050, loss 0.0063346, acc 1
2016-09-07T03:30:08.811484: step 5051, loss 0.0336125, acc 0.98
2016-09-07T03:30:09.519841: step 5052, loss 0.00270389, acc 1
2016-09-07T03:30:10.181278: step 5053, loss 0.0332161, acc 0.98
2016-09-07T03:30:10.875587: step 5054, loss 0.026901, acc 1
2016-09-07T03:30:11.559862: step 5055, loss 0.0127355, acc 1
2016-09-07T03:30:12.234059: step 5056, loss 0.017215, acc 1
2016-09-07T03:30:12.932459: step 5057, loss 0.01132, acc 1
2016-09-07T03:30:13.602393: step 5058, loss 0.0371402, acc 0.96
2016-09-07T03:30:14.301894: step 5059, loss 0.0694485, acc 0.98
2016-09-07T03:30:14.996264: step 5060, loss 0.0293908, acc 0.98
2016-09-07T03:30:15.713409: step 5061, loss 0.0115308, acc 1
2016-09-07T03:30:16.379788: step 5062, loss 0.141042, acc 0.96
2016-09-07T03:30:17.085815: step 5063, loss 0.126611, acc 0.96
2016-09-07T03:30:17.777793: step 5064, loss 0.0324907, acc 0.98
2016-09-07T03:30:18.486487: step 5065, loss 0.000600201, acc 1
2016-09-07T03:30:19.187357: step 5066, loss 0.00193915, acc 1
2016-09-07T03:30:19.884174: step 5067, loss 0.0589679, acc 0.98
2016-09-07T03:30:20.598576: step 5068, loss 0.0220691, acc 1
2016-09-07T03:30:21.286443: step 5069, loss 0.0422883, acc 0.96
2016-09-07T03:30:21.975077: step 5070, loss 0.000375604, acc 1
2016-09-07T03:30:22.643991: step 5071, loss 0.0397523, acc 0.96
2016-09-07T03:30:23.336508: step 5072, loss 0.0029348, acc 1
2016-09-07T03:30:24.036619: step 5073, loss 0.0234327, acc 0.98
2016-09-07T03:30:24.723534: step 5074, loss 0.0182027, acc 0.98
2016-09-07T03:30:25.435845: step 5075, loss 0.0157074, acc 1
2016-09-07T03:30:26.140493: step 5076, loss 0.00836272, acc 1
2016-09-07T03:30:26.832371: step 5077, loss 0.00660318, acc 1
2016-09-07T03:30:27.528639: step 5078, loss 0.0191042, acc 1
2016-09-07T03:30:28.223794: step 5079, loss 0.0182106, acc 1
2016-09-07T03:30:28.928833: step 5080, loss 0.00951632, acc 1
2016-09-07T03:30:29.618001: step 5081, loss 0.108991, acc 0.98
2016-09-07T03:30:30.324118: step 5082, loss 0.0280941, acc 0.98
2016-09-07T03:30:31.004279: step 5083, loss 0.0470232, acc 0.96
2016-09-07T03:30:31.706110: step 5084, loss 0.0205222, acc 0.98
2016-09-07T03:30:32.409730: step 5085, loss 0.00270294, acc 1
2016-09-07T03:30:33.105127: step 5086, loss 0.0602329, acc 0.96
2016-09-07T03:30:33.807418: step 5087, loss 0.0528551, acc 0.98
2016-09-07T03:30:34.481464: step 5088, loss 0.0111858, acc 1
2016-09-07T03:30:35.175590: step 5089, loss 0.00273882, acc 1
2016-09-07T03:30:35.874632: step 5090, loss 0.0743215, acc 0.98
2016-09-07T03:30:36.580073: step 5091, loss 0.0167664, acc 1
2016-09-07T03:30:37.294956: step 5092, loss 0.175275, acc 0.96
2016-09-07T03:30:37.961761: step 5093, loss 0.00609198, acc 1
2016-09-07T03:30:38.646680: step 5094, loss 0.0221695, acc 1
2016-09-07T03:30:39.346523: step 5095, loss 0.0750456, acc 0.98
2016-09-07T03:30:40.037103: step 5096, loss 0.0127188, acc 1
2016-09-07T03:30:40.726132: step 5097, loss 0.091318, acc 0.98
2016-09-07T03:30:41.417015: step 5098, loss 0.0510904, acc 0.98
2016-09-07T03:30:42.134000: step 5099, loss 0.127008, acc 0.94
2016-09-07T03:30:42.809080: step 5100, loss 0.0043187, acc 1

Evaluation:
2016-09-07T03:30:46.392953: step 5100, loss 2.0582, acc 0.730769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-5100

2016-09-07T03:30:48.122152: step 5101, loss 0.00341047, acc 1
2016-09-07T03:30:48.824549: step 5102, loss 0.0140309, acc 1
2016-09-07T03:30:49.515903: step 5103, loss 0.0448287, acc 0.96
2016-09-07T03:30:50.226895: step 5104, loss 0.083075, acc 0.98
2016-09-07T03:30:50.934795: step 5105, loss 0.0724336, acc 0.96
2016-09-07T03:30:51.641695: step 5106, loss 0.0546202, acc 0.96
2016-09-07T03:30:52.291620: step 5107, loss 0.0149495, acc 1
2016-09-07T03:30:52.995666: step 5108, loss 0.0964893, acc 0.96
2016-09-07T03:30:53.691745: step 5109, loss 0.026153, acc 1
2016-09-07T03:30:54.425270: step 5110, loss 0.0139198, acc 1
2016-09-07T03:30:55.127716: step 5111, loss 0.0212139, acc 1
2016-09-07T03:30:55.820894: step 5112, loss 0.182844, acc 0.96
2016-09-07T03:30:56.543070: step 5113, loss 0.0304652, acc 0.98
2016-09-07T03:30:57.219749: step 5114, loss 0.0896787, acc 0.96
2016-09-07T03:30:57.908393: step 5115, loss 0.0642605, acc 0.98
2016-09-07T03:30:58.602429: step 5116, loss 0.0158817, acc 1
2016-09-07T03:30:59.302672: step 5117, loss 0.114292, acc 0.96
2016-09-07T03:31:00.003775: step 5118, loss 0.0190236, acc 1
2016-09-07T03:31:00.733072: step 5119, loss 0.0074905, acc 1
2016-09-07T03:31:01.429288: step 5120, loss 0.0192644, acc 0.98
2016-09-07T03:31:02.127759: step 5121, loss 0.0204336, acc 0.98
2016-09-07T03:31:02.810694: step 5122, loss 0.028689, acc 1
2016-09-07T03:31:03.502361: step 5123, loss 0.0142004, acc 1
2016-09-07T03:31:04.193003: step 5124, loss 0.0209111, acc 1
2016-09-07T03:31:04.916787: step 5125, loss 0.0281181, acc 0.98
2016-09-07T03:31:05.592463: step 5126, loss 0.054681, acc 0.98
2016-09-07T03:31:06.307618: step 5127, loss 0.0490454, acc 0.98
2016-09-07T03:31:07.007189: step 5128, loss 0.0332398, acc 0.98
2016-09-07T03:31:07.708161: step 5129, loss 0.0283537, acc 0.98
2016-09-07T03:31:08.385782: step 5130, loss 0.005808, acc 1
2016-09-07T03:31:09.072771: step 5131, loss 0.0256097, acc 0.98
2016-09-07T03:31:09.779343: step 5132, loss 0.00483705, acc 1
2016-09-07T03:31:10.459416: step 5133, loss 0.00105025, acc 1
2016-09-07T03:31:11.143225: step 5134, loss 0.0281945, acc 0.98
2016-09-07T03:31:11.818507: step 5135, loss 0.040464, acc 0.96
2016-09-07T03:31:12.538953: step 5136, loss 0.0510541, acc 0.96
2016-09-07T03:31:13.231297: step 5137, loss 0.00305909, acc 1
2016-09-07T03:31:13.919084: step 5138, loss 0.0394089, acc 0.98
2016-09-07T03:31:14.625191: step 5139, loss 0.101569, acc 0.96
2016-09-07T03:31:15.305653: step 5140, loss 0.102542, acc 0.98
2016-09-07T03:31:16.010480: step 5141, loss 0.027983, acc 1
2016-09-07T03:31:16.689811: step 5142, loss 0.00133053, acc 1
2016-09-07T03:31:17.382036: step 5143, loss 0.00581137, acc 1
2016-09-07T03:31:18.069982: step 5144, loss 0.030459, acc 0.98
2016-09-07T03:31:18.729713: step 5145, loss 0.000121395, acc 1
2016-09-07T03:31:19.438702: step 5146, loss 0.00868291, acc 1
2016-09-07T03:31:20.136264: step 5147, loss 0.038424, acc 0.98
2016-09-07T03:31:20.860820: step 5148, loss 0.00406466, acc 1
2016-09-07T03:31:21.563424: step 5149, loss 0.0202992, acc 0.98
2016-09-07T03:31:22.251561: step 5150, loss 0.0186932, acc 0.98
2016-09-07T03:31:22.948190: step 5151, loss 0.00868068, acc 1
2016-09-07T03:31:23.661424: step 5152, loss 0.0370895, acc 0.98
2016-09-07T03:31:24.355844: step 5153, loss 0.0977618, acc 0.94
2016-09-07T03:31:25.051571: step 5154, loss 0.0385137, acc 0.98
2016-09-07T03:31:25.742673: step 5155, loss 0.0172358, acc 1
2016-09-07T03:31:26.455853: step 5156, loss 0.0462835, acc 0.96
2016-09-07T03:31:27.135221: step 5157, loss 0.00553875, acc 1
2016-09-07T03:31:27.844902: step 5158, loss 0.0221594, acc 1
2016-09-07T03:31:28.529591: step 5159, loss 0.0192486, acc 1
2016-09-07T03:31:29.237164: step 5160, loss 0.0731869, acc 0.96
2016-09-07T03:31:29.922349: step 5161, loss 0.0061822, acc 1
2016-09-07T03:31:30.610932: step 5162, loss 0.128703, acc 0.94
2016-09-07T03:31:31.296685: step 5163, loss 0.126413, acc 0.98
2016-09-07T03:31:31.988998: step 5164, loss 0.0456978, acc 0.98
2016-09-07T03:31:32.682638: step 5165, loss 0.0610928, acc 0.98
2016-09-07T03:31:33.357685: step 5166, loss 0.0317399, acc 0.98
2016-09-07T03:31:34.039721: step 5167, loss 0.0280839, acc 0.98
2016-09-07T03:31:34.736765: step 5168, loss 0.0228035, acc 1
2016-09-07T03:31:35.434729: step 5169, loss 0.0273724, acc 0.98
2016-09-07T03:31:36.131517: step 5170, loss 0.0295417, acc 0.98
2016-09-07T03:31:36.794743: step 5171, loss 0.0126001, acc 1
2016-09-07T03:31:37.509805: step 5172, loss 0.0184053, acc 1
2016-09-07T03:31:38.217092: step 5173, loss 0.00580822, acc 1
2016-09-07T03:31:38.902028: step 5174, loss 0.101506, acc 0.98
2016-09-07T03:31:39.617589: step 5175, loss 0.0351395, acc 0.98
2016-09-07T03:31:40.317831: step 5176, loss 0.0623642, acc 0.94
2016-09-07T03:31:41.029414: step 5177, loss 0.00816947, acc 1
2016-09-07T03:31:41.714147: step 5178, loss 0.0134915, acc 1
2016-09-07T03:31:42.395373: step 5179, loss 0.042243, acc 0.98
2016-09-07T03:31:43.072082: step 5180, loss 0.00229351, acc 1
2016-09-07T03:31:43.769354: step 5181, loss 0.0351773, acc 0.98
2016-09-07T03:31:44.479964: step 5182, loss 0.00697523, acc 1
2016-09-07T03:31:45.169762: step 5183, loss 0.0270339, acc 1
2016-09-07T03:31:45.814183: step 5184, loss 0.000704246, acc 1
2016-09-07T03:31:46.505349: step 5185, loss 0.0323376, acc 1
2016-09-07T03:31:47.191481: step 5186, loss 0.0176619, acc 0.98
2016-09-07T03:31:47.885506: step 5187, loss 0.0157521, acc 1
2016-09-07T03:31:48.572821: step 5188, loss 0.01658, acc 0.98
2016-09-07T03:31:49.261100: step 5189, loss 0.0292301, acc 1
2016-09-07T03:31:49.952375: step 5190, loss 0.146022, acc 0.96
2016-09-07T03:31:50.653788: step 5191, loss 0.0101756, acc 1
2016-09-07T03:31:51.322688: step 5192, loss 0.0226802, acc 0.98
2016-09-07T03:31:52.008814: step 5193, loss 0.0215313, acc 0.98
2016-09-07T03:31:52.735258: step 5194, loss 0.143044, acc 0.94
2016-09-07T03:31:53.439854: step 5195, loss 0.0068828, acc 1
2016-09-07T03:31:54.145550: step 5196, loss 0.0292131, acc 1
2016-09-07T03:31:54.806880: step 5197, loss 0.0294033, acc 1
2016-09-07T03:31:55.494974: step 5198, loss 0.125349, acc 0.94
2016-09-07T03:31:56.161561: step 5199, loss 0.0243136, acc 0.98
2016-09-07T03:31:56.824176: step 5200, loss 0.000910114, acc 1

Evaluation:
2016-09-07T03:32:00.487293: step 5200, loss 1.56295, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-5200

2016-09-07T03:32:02.282555: step 5201, loss 0.0290181, acc 1
2016-09-07T03:32:02.993892: step 5202, loss 0.0282517, acc 0.98
2016-09-07T03:32:03.686409: step 5203, loss 0.0268115, acc 0.98
2016-09-07T03:32:04.376965: step 5204, loss 0.0129654, acc 1
2016-09-07T03:32:05.090591: step 5205, loss 0.0307806, acc 0.98
2016-09-07T03:32:05.781722: step 5206, loss 0.0334791, acc 0.96
2016-09-07T03:32:06.467710: step 5207, loss 0.00779959, acc 1
2016-09-07T03:32:07.158304: step 5208, loss 0.0147412, acc 1
2016-09-07T03:32:07.853081: step 5209, loss 0.006053, acc 1
2016-09-07T03:32:08.544583: step 5210, loss 0.000608205, acc 1
2016-09-07T03:32:09.246539: step 5211, loss 0.0170045, acc 1
2016-09-07T03:32:09.952048: step 5212, loss 0.0195548, acc 0.98
2016-09-07T03:32:10.631098: step 5213, loss 0.0383857, acc 0.98
2016-09-07T03:32:11.332203: step 5214, loss 0.0465304, acc 0.98
2016-09-07T03:32:12.017157: step 5215, loss 0.0187644, acc 0.98
2016-09-07T03:32:12.692946: step 5216, loss 0.019739, acc 0.98
2016-09-07T03:32:13.374417: step 5217, loss 0.0178522, acc 1
2016-09-07T03:32:14.047442: step 5218, loss 0.00861518, acc 1
2016-09-07T03:32:14.758570: step 5219, loss 0.0105006, acc 1
2016-09-07T03:32:15.424000: step 5220, loss 0.0322631, acc 0.98
2016-09-07T03:32:16.166048: step 5221, loss 0.00365413, acc 1
2016-09-07T03:32:16.863430: step 5222, loss 0.0157222, acc 1
2016-09-07T03:32:17.570208: step 5223, loss 0.00425106, acc 1
2016-09-07T03:32:18.268370: step 5224, loss 0.000169717, acc 1
2016-09-07T03:32:18.937005: step 5225, loss 0.0321556, acc 1
2016-09-07T03:32:19.629253: step 5226, loss 0.0166909, acc 0.98
2016-09-07T03:32:20.316515: step 5227, loss 0.000601056, acc 1
2016-09-07T03:32:20.994234: step 5228, loss 0.00236288, acc 1
2016-09-07T03:32:21.683131: step 5229, loss 0.0231703, acc 1
2016-09-07T03:32:22.385091: step 5230, loss 0.0585175, acc 0.98
2016-09-07T03:32:23.113247: step 5231, loss 0.0696369, acc 0.96
2016-09-07T03:32:23.785094: step 5232, loss 0.000210688, acc 1
2016-09-07T03:32:24.474067: step 5233, loss 0.0307726, acc 0.98
2016-09-07T03:32:25.159138: step 5234, loss 0.00582288, acc 1
2016-09-07T03:32:25.844675: step 5235, loss 0.027313, acc 0.98
2016-09-07T03:32:26.511206: step 5236, loss 0.032039, acc 0.96
2016-09-07T03:32:27.200113: step 5237, loss 0.0671362, acc 0.96
2016-09-07T03:32:27.910890: step 5238, loss 0.024405, acc 0.98
2016-09-07T03:32:28.593097: step 5239, loss 0.00397827, acc 1
2016-09-07T03:32:29.291262: step 5240, loss 0.010039, acc 1
2016-09-07T03:32:29.978029: step 5241, loss 0.0360457, acc 0.98
2016-09-07T03:32:30.671457: step 5242, loss 0.00424105, acc 1
2016-09-07T03:32:31.363557: step 5243, loss 0.00941862, acc 1
2016-09-07T03:32:32.054031: step 5244, loss 0.000617282, acc 1
2016-09-07T03:32:32.769845: step 5245, loss 0.0129343, acc 1
2016-09-07T03:32:33.466645: step 5246, loss 0.0622912, acc 0.98
2016-09-07T03:32:34.163274: step 5247, loss 0.00549865, acc 1
2016-09-07T03:32:34.851594: step 5248, loss 0.0287706, acc 0.98
2016-09-07T03:32:35.542379: step 5249, loss 0.0863677, acc 0.96
2016-09-07T03:32:36.244624: step 5250, loss 0.0453353, acc 0.96
2016-09-07T03:32:36.926073: step 5251, loss 0.0215952, acc 1
2016-09-07T03:32:37.633123: step 5252, loss 0.0435122, acc 0.98
2016-09-07T03:32:38.300555: step 5253, loss 0.0489884, acc 0.98
2016-09-07T03:32:38.974360: step 5254, loss 0.144048, acc 0.98
2016-09-07T03:32:39.674684: step 5255, loss 0.000814815, acc 1
2016-09-07T03:32:40.373872: step 5256, loss 0.0168484, acc 0.98
2016-09-07T03:32:41.074047: step 5257, loss 0.00108961, acc 1
2016-09-07T03:32:41.738549: step 5258, loss 0.0168571, acc 1
2016-09-07T03:32:42.449790: step 5259, loss 0.0135449, acc 1
2016-09-07T03:32:43.136439: step 5260, loss 0.0798027, acc 0.94
2016-09-07T03:32:43.834713: step 5261, loss 0.0168245, acc 0.98
2016-09-07T03:32:44.520068: step 5262, loss 0.00431816, acc 1
2016-09-07T03:32:45.212593: step 5263, loss 0.0212751, acc 0.98
2016-09-07T03:32:45.909259: step 5264, loss 0.0435535, acc 0.98
2016-09-07T03:32:46.583198: step 5265, loss 0.0332138, acc 0.98
2016-09-07T03:32:47.299965: step 5266, loss 0.0113708, acc 1
2016-09-07T03:32:47.983101: step 5267, loss 0.0329206, acc 0.98
2016-09-07T03:32:48.687159: step 5268, loss 0.00859627, acc 1
2016-09-07T03:32:49.377054: step 5269, loss 0.00144794, acc 1
2016-09-07T03:32:50.085803: step 5270, loss 0.0938011, acc 0.92
2016-09-07T03:32:50.805265: step 5271, loss 0.0123854, acc 1
2016-09-07T03:32:51.483837: step 5272, loss 0.0306085, acc 0.98
2016-09-07T03:32:52.181341: step 5273, loss 0.0273584, acc 0.98
2016-09-07T03:32:52.878660: step 5274, loss 0.0285435, acc 1
2016-09-07T03:32:53.573031: step 5275, loss 0.0593653, acc 0.98
2016-09-07T03:32:54.309874: step 5276, loss 0.00950231, acc 1
2016-09-07T03:32:54.987217: step 5277, loss 0.00239709, acc 1
2016-09-07T03:32:55.699392: step 5278, loss 0.0271815, acc 1
2016-09-07T03:32:56.402358: step 5279, loss 0.0200114, acc 1
2016-09-07T03:32:57.102735: step 5280, loss 0.038706, acc 0.98
2016-09-07T03:32:57.823253: step 5281, loss 0.0191556, acc 1
2016-09-07T03:32:58.517097: step 5282, loss 0.046019, acc 0.98
2016-09-07T03:32:59.245710: step 5283, loss 0.0848907, acc 0.98
2016-09-07T03:32:59.927442: step 5284, loss 0.00950778, acc 1
2016-09-07T03:33:00.627093: step 5285, loss 0.0236617, acc 0.98
2016-09-07T03:33:01.324379: step 5286, loss 0.0661017, acc 0.96
2016-09-07T03:33:02.014335: step 5287, loss 0.016479, acc 1
2016-09-07T03:33:02.726584: step 5288, loss 0.00731849, acc 1
2016-09-07T03:33:03.396317: step 5289, loss 0.0108153, acc 1
2016-09-07T03:33:04.116500: step 5290, loss 0.0313371, acc 0.98
2016-09-07T03:33:04.807246: step 5291, loss 0.0116464, acc 1
2016-09-07T03:33:05.487202: step 5292, loss 0.0690821, acc 0.98
2016-09-07T03:33:06.173164: step 5293, loss 0.0108763, acc 1
2016-09-07T03:33:06.902528: step 5294, loss 0.00963037, acc 1
2016-09-07T03:33:07.619997: step 5295, loss 0.0259294, acc 0.98
2016-09-07T03:33:08.309431: step 5296, loss 0.029726, acc 0.98
2016-09-07T03:33:09.031924: step 5297, loss 0.00141204, acc 1
2016-09-07T03:33:09.722864: step 5298, loss 0.0307682, acc 1
2016-09-07T03:33:10.402699: step 5299, loss 0.000452667, acc 1
2016-09-07T03:33:11.091202: step 5300, loss 0.0824333, acc 0.98

Evaluation:
2016-09-07T03:33:14.739174: step 5300, loss 1.97662, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-5300

2016-09-07T03:33:16.558880: step 5301, loss 0.0269859, acc 1
2016-09-07T03:33:17.269652: step 5302, loss 0.023955, acc 0.98
2016-09-07T03:33:17.969500: step 5303, loss 0.000985557, acc 1
2016-09-07T03:33:18.679896: step 5304, loss 0.0650615, acc 0.96
2016-09-07T03:33:19.370561: step 5305, loss 0.0171562, acc 0.98
2016-09-07T03:33:20.066878: step 5306, loss 0.0263501, acc 0.98
2016-09-07T03:33:20.765664: step 5307, loss 0.00137334, acc 1
2016-09-07T03:33:21.426843: step 5308, loss 0.00878031, acc 1
2016-09-07T03:33:22.139355: step 5309, loss 0.0518134, acc 0.96
2016-09-07T03:33:22.835066: step 5310, loss 0.0204508, acc 1
2016-09-07T03:33:23.564710: step 5311, loss 0.18085, acc 0.94
2016-09-07T03:33:24.274474: step 5312, loss 0.0377594, acc 0.98
2016-09-07T03:33:24.951977: step 5313, loss 0.00605809, acc 1
2016-09-07T03:33:25.680902: step 5314, loss 0.0270129, acc 1
2016-09-07T03:33:26.379992: step 5315, loss 0.0262472, acc 0.98
2016-09-07T03:33:27.073142: step 5316, loss 0.118375, acc 0.94
2016-09-07T03:33:27.780442: step 5317, loss 0.0416516, acc 0.98
2016-09-07T03:33:28.484416: step 5318, loss 0.0238108, acc 0.98
2016-09-07T03:33:29.192655: step 5319, loss 0.0729418, acc 0.98
2016-09-07T03:33:29.849064: step 5320, loss 0.0278778, acc 1
2016-09-07T03:33:30.546012: step 5321, loss 0.138698, acc 0.98
2016-09-07T03:33:31.233514: step 5322, loss 0.00175295, acc 1
2016-09-07T03:33:31.927818: step 5323, loss 0.000235026, acc 1
2016-09-07T03:33:32.613807: step 5324, loss 0.0807323, acc 0.96
2016-09-07T03:33:33.312669: step 5325, loss 0.0667001, acc 0.98
2016-09-07T03:33:34.035090: step 5326, loss 0.0492947, acc 0.98
2016-09-07T03:33:34.701010: step 5327, loss 0.0582339, acc 0.96
2016-09-07T03:33:35.409013: step 5328, loss 0.0220089, acc 0.98
2016-09-07T03:33:36.107804: step 5329, loss 0.0703793, acc 0.98
2016-09-07T03:33:36.802205: step 5330, loss 0.00701193, acc 1
2016-09-07T03:33:37.494625: step 5331, loss 0.166277, acc 0.96
2016-09-07T03:33:38.177348: step 5332, loss 0.084437, acc 0.96
2016-09-07T03:33:38.892969: step 5333, loss 0.0165577, acc 0.98
2016-09-07T03:33:39.586900: step 5334, loss 0.00427468, acc 1
2016-09-07T03:33:40.286262: step 5335, loss 0.0370643, acc 0.98
2016-09-07T03:33:40.981591: step 5336, loss 0.0279616, acc 1
2016-09-07T03:33:41.670774: step 5337, loss 0.0318153, acc 1
2016-09-07T03:33:42.375694: step 5338, loss 0.039812, acc 0.98
2016-09-07T03:33:43.071840: step 5339, loss 0.0113992, acc 1
2016-09-07T03:33:43.777875: step 5340, loss 0.000992366, acc 1
2016-09-07T03:33:44.473561: step 5341, loss 0.0109219, acc 1
2016-09-07T03:33:45.163751: step 5342, loss 0.0243236, acc 0.98
2016-09-07T03:33:45.849350: step 5343, loss 0.0200594, acc 1
2016-09-07T03:33:46.533691: step 5344, loss 0.019137, acc 0.98
2016-09-07T03:33:47.210988: step 5345, loss 0.00336932, acc 1
2016-09-07T03:33:47.872974: step 5346, loss 0.030986, acc 0.98
2016-09-07T03:33:48.602904: step 5347, loss 0.0340952, acc 0.98
2016-09-07T03:33:49.296375: step 5348, loss 0.0423336, acc 0.98
2016-09-07T03:33:49.971367: step 5349, loss 0.0395209, acc 0.98
2016-09-07T03:33:50.648159: step 5350, loss 0.0868533, acc 0.94
2016-09-07T03:33:51.352032: step 5351, loss 0.00987886, acc 1
2016-09-07T03:33:52.077439: step 5352, loss 0.00744482, acc 1
2016-09-07T03:33:52.773619: step 5353, loss 0.0627324, acc 0.96
2016-09-07T03:33:53.458396: step 5354, loss 0.00984351, acc 1
2016-09-07T03:33:54.144469: step 5355, loss 0.0179758, acc 0.98
2016-09-07T03:33:54.830549: step 5356, loss 0.0052491, acc 1
2016-09-07T03:33:55.529616: step 5357, loss 0.0244144, acc 1
2016-09-07T03:33:56.253735: step 5358, loss 0.0359035, acc 0.98
2016-09-07T03:33:56.958800: step 5359, loss 0.0122779, acc 1
2016-09-07T03:33:57.646093: step 5360, loss 0.0255917, acc 0.98
2016-09-07T03:33:58.321700: step 5361, loss 0.0419442, acc 0.98
2016-09-07T03:33:59.005075: step 5362, loss 0.0715997, acc 0.98
2016-09-07T03:33:59.701448: step 5363, loss 0.0104775, acc 1
2016-09-07T03:34:00.434532: step 5364, loss 0.00447869, acc 1
2016-09-07T03:34:01.124444: step 5365, loss 0.173791, acc 0.98
2016-09-07T03:34:01.823002: step 5366, loss 0.0135686, acc 1
2016-09-07T03:34:02.512421: step 5367, loss 0.00692515, acc 1
2016-09-07T03:34:03.190352: step 5368, loss 0.0326705, acc 1
2016-09-07T03:34:03.864295: step 5369, loss 0.0248481, acc 0.98
2016-09-07T03:34:04.557271: step 5370, loss 0.000408017, acc 1
2016-09-07T03:34:05.256461: step 5371, loss 0.01952, acc 1
2016-09-07T03:34:05.935501: step 5372, loss 0.00280331, acc 1
2016-09-07T03:34:06.657405: step 5373, loss 0.0169478, acc 1
2016-09-07T03:34:07.357494: step 5374, loss 0.0493859, acc 0.96
2016-09-07T03:34:08.050198: step 5375, loss 0.0340058, acc 0.98
2016-09-07T03:34:08.704424: step 5376, loss 0.0312196, acc 0.977273
2016-09-07T03:34:09.410141: step 5377, loss 0.104132, acc 0.98
2016-09-07T03:34:10.115983: step 5378, loss 0.0238378, acc 1
2016-09-07T03:34:10.783175: step 5379, loss 0.0302796, acc 1
2016-09-07T03:34:11.496311: step 5380, loss 0.0448616, acc 0.98
2016-09-07T03:34:12.178269: step 5381, loss 0.00943188, acc 1
2016-09-07T03:34:12.857075: step 5382, loss 0.0539992, acc 0.96
2016-09-07T03:34:13.556117: step 5383, loss 0.0720312, acc 0.96
2016-09-07T03:34:14.246982: step 5384, loss 0.0225018, acc 0.98
2016-09-07T03:34:14.961382: step 5385, loss 0.0237751, acc 0.98
2016-09-07T03:34:15.633147: step 5386, loss 0.00261523, acc 1
2016-09-07T03:34:16.327399: step 5387, loss 0.00583221, acc 1
2016-09-07T03:34:17.026493: step 5388, loss 0.0462518, acc 0.96
2016-09-07T03:34:17.716490: step 5389, loss 0.0487024, acc 0.98
2016-09-07T03:34:18.418115: step 5390, loss 0.0737291, acc 0.98
2016-09-07T03:34:19.097936: step 5391, loss 0.010304, acc 1
2016-09-07T03:34:19.805135: step 5392, loss 0.0100863, acc 1
2016-09-07T03:34:20.461880: step 5393, loss 0.0156532, acc 1
2016-09-07T03:34:21.163883: step 5394, loss 0.0212237, acc 0.98
2016-09-07T03:34:21.856549: step 5395, loss 0.0497498, acc 0.98
2016-09-07T03:34:22.562142: step 5396, loss 0.0429271, acc 0.98
2016-09-07T03:34:23.247939: step 5397, loss 0.045515, acc 0.98
2016-09-07T03:34:23.919475: step 5398, loss 0.043665, acc 0.98
2016-09-07T03:34:24.645113: step 5399, loss 0.0125458, acc 1
2016-09-07T03:34:25.343239: step 5400, loss 0.0304413, acc 0.98

Evaluation:
2016-09-07T03:34:29.046541: step 5400, loss 1.89752, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-5400

2016-09-07T03:34:30.908667: step 5401, loss 0.0191836, acc 1
2016-09-07T03:34:31.587202: step 5402, loss 0.0110276, acc 1
2016-09-07T03:34:32.279836: step 5403, loss 0.0193004, acc 0.98
2016-09-07T03:34:33.003284: step 5404, loss 0.00391626, acc 1
2016-09-07T03:34:33.688040: step 5405, loss 0.0204891, acc 1
2016-09-07T03:34:34.373332: step 5406, loss 0.0103349, acc 1
2016-09-07T03:34:35.058465: step 5407, loss 0.000421668, acc 1
2016-09-07T03:34:35.770254: step 5408, loss 0.0629473, acc 0.94
2016-09-07T03:34:36.473047: step 5409, loss 0.0434766, acc 1
2016-09-07T03:34:37.146675: step 5410, loss 0.00326758, acc 1
2016-09-07T03:34:37.859801: step 5411, loss 0.00377302, acc 1
2016-09-07T03:34:38.549945: step 5412, loss 0.0303554, acc 0.98
2016-09-07T03:34:39.244326: step 5413, loss 0.000917199, acc 1
2016-09-07T03:34:39.927285: step 5414, loss 0.00122244, acc 1
2016-09-07T03:34:40.598624: step 5415, loss 0.0202979, acc 0.98
2016-09-07T03:34:41.310874: step 5416, loss 0.000220663, acc 1
2016-09-07T03:34:41.984900: step 5417, loss 0.0213218, acc 1
2016-09-07T03:34:42.726240: step 5418, loss 0.0527949, acc 0.98
2016-09-07T03:34:43.410954: step 5419, loss 0.240786, acc 0.98
2016-09-07T03:34:44.106177: step 5420, loss 0.00641614, acc 1
2016-09-07T03:34:44.803295: step 5421, loss 0.0331077, acc 0.98
2016-09-07T03:34:45.498055: step 5422, loss 0.0301658, acc 0.98
2016-09-07T03:34:46.200749: step 5423, loss 0.0250067, acc 1
2016-09-07T03:34:46.903193: step 5424, loss 0.0564141, acc 0.96
2016-09-07T03:34:47.596686: step 5425, loss 0.006614, acc 1
2016-09-07T03:34:48.300687: step 5426, loss 0.0663111, acc 0.96
2016-09-07T03:34:48.998142: step 5427, loss 0.00548563, acc 1
2016-09-07T03:34:49.704782: step 5428, loss 0.00173842, acc 1
2016-09-07T03:34:50.390139: step 5429, loss 0.0121338, acc 1
2016-09-07T03:34:51.102702: step 5430, loss 0.0429463, acc 0.98
2016-09-07T03:34:51.794978: step 5431, loss 0.0377436, acc 0.98
2016-09-07T03:34:52.487790: step 5432, loss 0.0255518, acc 1
2016-09-07T03:34:53.173021: step 5433, loss 0.0112603, acc 1
2016-09-07T03:34:53.870427: step 5434, loss 0.00225257, acc 1
2016-09-07T03:34:54.570212: step 5435, loss 0.00904453, acc 1
2016-09-07T03:34:55.248692: step 5436, loss 0.000340694, acc 1
2016-09-07T03:34:55.967631: step 5437, loss 0.0358947, acc 0.98
2016-09-07T03:34:56.647399: step 5438, loss 0.0565736, acc 0.98
2016-09-07T03:34:57.347792: step 5439, loss 0.0622345, acc 0.98
2016-09-07T03:34:58.050793: step 5440, loss 0.0130252, acc 1
2016-09-07T03:34:58.735872: step 5441, loss 0.0210589, acc 0.98
2016-09-07T03:34:59.463033: step 5442, loss 0.0380325, acc 0.96
2016-09-07T03:35:00.155769: step 5443, loss 0.0271596, acc 1
2016-09-07T03:35:00.879454: step 5444, loss 0.00105533, acc 1
2016-09-07T03:35:01.589477: step 5445, loss 0.00179816, acc 1
2016-09-07T03:35:02.304172: step 5446, loss 0.00593974, acc 1
2016-09-07T03:35:03.024308: step 5447, loss 0.00373759, acc 1
2016-09-07T03:35:03.701569: step 5448, loss 0.0165212, acc 0.98
2016-09-07T03:35:04.394053: step 5449, loss 0.000192938, acc 1
2016-09-07T03:35:05.090657: step 5450, loss 0.00787484, acc 1
2016-09-07T03:35:05.782775: step 5451, loss 0.00309286, acc 1
2016-09-07T03:35:06.474011: step 5452, loss 0.028729, acc 0.98
2016-09-07T03:35:07.142136: step 5453, loss 0.0243145, acc 0.98
2016-09-07T03:35:07.835060: step 5454, loss 0.000722277, acc 1
2016-09-07T03:35:08.494231: step 5455, loss 0.0118354, acc 1
2016-09-07T03:35:09.205503: step 5456, loss 0.00501083, acc 1
2016-09-07T03:35:09.899519: step 5457, loss 0.0623601, acc 0.98
2016-09-07T03:35:10.586966: step 5458, loss 0.0196513, acc 1
2016-09-07T03:35:11.276869: step 5459, loss 0.00409129, acc 1
2016-09-07T03:35:11.953326: step 5460, loss 0.00577619, acc 1
2016-09-07T03:35:12.669585: step 5461, loss 0.0590975, acc 0.98
2016-09-07T03:35:13.349359: step 5462, loss 0.0151604, acc 0.98
2016-09-07T03:35:14.032255: step 5463, loss 0.00022064, acc 1
2016-09-07T03:35:14.721536: step 5464, loss 0.00369292, acc 1
2016-09-07T03:35:15.413157: step 5465, loss 0.0332449, acc 0.98
2016-09-07T03:35:16.126365: step 5466, loss 0.0505697, acc 0.98
2016-09-07T03:35:16.796373: step 5467, loss 0.0860686, acc 0.96
2016-09-07T03:35:17.483480: step 5468, loss 0.0340406, acc 0.98
2016-09-07T03:35:18.154263: step 5469, loss 0.00476654, acc 1
2016-09-07T03:35:18.866354: step 5470, loss 0.000199736, acc 1
2016-09-07T03:35:19.552918: step 5471, loss 0.0704053, acc 0.96
2016-09-07T03:35:20.265247: step 5472, loss 0.00895005, acc 1
2016-09-07T03:35:20.979808: step 5473, loss 0.0194721, acc 0.98
2016-09-07T03:35:21.671950: step 5474, loss 0.0550959, acc 0.96
2016-09-07T03:35:22.387635: step 5475, loss 0.00108888, acc 1
2016-09-07T03:35:23.097802: step 5476, loss 0.0436418, acc 0.96
2016-09-07T03:35:23.803868: step 5477, loss 0.00091674, acc 1
2016-09-07T03:35:24.506186: step 5478, loss 0.012832, acc 1
2016-09-07T03:35:25.192142: step 5479, loss 0.0170811, acc 1
2016-09-07T03:35:25.889518: step 5480, loss 0.0889876, acc 0.94
2016-09-07T03:35:26.584900: step 5481, loss 0.110528, acc 0.94
2016-09-07T03:35:27.296537: step 5482, loss 0.00689705, acc 1
2016-09-07T03:35:28.000616: step 5483, loss 0.0947206, acc 0.96
2016-09-07T03:35:28.687802: step 5484, loss 0.0303457, acc 1
2016-09-07T03:35:29.378087: step 5485, loss 0.0209922, acc 1
2016-09-07T03:35:30.044212: step 5486, loss 0.0142145, acc 1
2016-09-07T03:35:30.749362: step 5487, loss 0.0226938, acc 0.98
2016-09-07T03:35:31.441291: step 5488, loss 0.00229283, acc 1
2016-09-07T03:35:32.130110: step 5489, loss 0.0385035, acc 0.96
2016-09-07T03:35:32.835436: step 5490, loss 0.0150234, acc 1
2016-09-07T03:35:33.520988: step 5491, loss 0.00269104, acc 1
2016-09-07T03:35:34.226282: step 5492, loss 0.0614812, acc 0.94
2016-09-07T03:35:34.922683: step 5493, loss 0.0259096, acc 0.98
2016-09-07T03:35:35.611908: step 5494, loss 0.0262291, acc 0.98
2016-09-07T03:35:36.326454: step 5495, loss 0.0533944, acc 0.98
2016-09-07T03:35:37.028283: step 5496, loss 0.0306449, acc 0.98
2016-09-07T03:35:37.726882: step 5497, loss 0.0469648, acc 0.96
2016-09-07T03:35:38.411688: step 5498, loss 0.0444669, acc 0.96
2016-09-07T03:35:39.139176: step 5499, loss 0.00420321, acc 1
2016-09-07T03:35:39.838233: step 5500, loss 0.0044186, acc 1

Evaluation:
2016-09-07T03:35:43.422089: step 5500, loss 1.94091, acc 0.727017

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-5500

2016-09-07T03:35:45.206053: step 5501, loss 0.00983581, acc 1
2016-09-07T03:35:45.888432: step 5502, loss 0.0394869, acc 0.98
2016-09-07T03:35:46.592604: step 5503, loss 0.0569609, acc 0.98
2016-09-07T03:35:47.305577: step 5504, loss 0.00889936, acc 1
2016-09-07T03:35:48.004317: step 5505, loss 0.0164797, acc 0.98
2016-09-07T03:35:48.719695: step 5506, loss 0.0266949, acc 1
2016-09-07T03:35:49.400270: step 5507, loss 0.0465765, acc 0.96
2016-09-07T03:35:50.098841: step 5508, loss 0.0141137, acc 1
2016-09-07T03:35:50.792585: step 5509, loss 0.0389494, acc 0.98
2016-09-07T03:35:51.482355: step 5510, loss 0.0198902, acc 0.98
2016-09-07T03:35:52.179759: step 5511, loss 0.0252324, acc 0.98
2016-09-07T03:35:52.875493: step 5512, loss 0.0134166, acc 1
2016-09-07T03:35:53.578061: step 5513, loss 0.0077596, acc 1
2016-09-07T03:35:54.265201: step 5514, loss 0.000470698, acc 1
2016-09-07T03:35:54.949851: step 5515, loss 0.00427711, acc 1
2016-09-07T03:35:55.632411: step 5516, loss 0.0163181, acc 0.98
2016-09-07T03:35:56.325082: step 5517, loss 0.0198892, acc 1
2016-09-07T03:35:57.027108: step 5518, loss 0.0312079, acc 0.98
2016-09-07T03:35:57.684959: step 5519, loss 0.0283269, acc 0.98
2016-09-07T03:35:58.400690: step 5520, loss 0.0369497, acc 0.98
2016-09-07T03:35:59.088060: step 5521, loss 0.0164516, acc 0.98
2016-09-07T03:35:59.781507: step 5522, loss 0.00325927, acc 1
2016-09-07T03:36:00.507141: step 5523, loss 0.00471604, acc 1
2016-09-07T03:36:01.187740: step 5524, loss 0.004451, acc 1
2016-09-07T03:36:01.885473: step 5525, loss 0.00702406, acc 1
2016-09-07T03:36:02.585187: step 5526, loss 0.0204497, acc 1
2016-09-07T03:36:03.263060: step 5527, loss 0.0102853, acc 1
2016-09-07T03:36:03.956553: step 5528, loss 0.00283259, acc 1
2016-09-07T03:36:04.658790: step 5529, loss 0.000737443, acc 1
2016-09-07T03:36:05.366681: step 5530, loss 0.0731549, acc 0.94
2016-09-07T03:36:06.069894: step 5531, loss 0.060812, acc 0.98
2016-09-07T03:36:06.775923: step 5532, loss 0.0410148, acc 0.98
2016-09-07T03:36:07.474150: step 5533, loss 0.00169858, acc 1
2016-09-07T03:36:08.157688: step 5534, loss 0.0164698, acc 0.98
2016-09-07T03:36:08.844216: step 5535, loss 0.039662, acc 0.98
2016-09-07T03:36:09.535937: step 5536, loss 0.016005, acc 1
2016-09-07T03:36:10.219267: step 5537, loss 7.93151e-05, acc 1
2016-09-07T03:36:10.898291: step 5538, loss 0.00527595, acc 1
2016-09-07T03:36:11.622748: step 5539, loss 0.0126056, acc 1
2016-09-07T03:36:12.306591: step 5540, loss 0.076593, acc 0.98
2016-09-07T03:36:12.993951: step 5541, loss 0.0178862, acc 0.98
2016-09-07T03:36:13.679060: step 5542, loss 0.1347, acc 0.96
2016-09-07T03:36:14.368392: step 5543, loss 0.00197104, acc 1
2016-09-07T03:36:15.073973: step 5544, loss 0.0314759, acc 1
2016-09-07T03:36:15.744382: step 5545, loss 0.037405, acc 0.98
2016-09-07T03:36:16.448029: step 5546, loss 0.0303756, acc 0.98
2016-09-07T03:36:17.132541: step 5547, loss 0.0196262, acc 0.98
2016-09-07T03:36:17.815304: step 5548, loss 0.00509776, acc 1
2016-09-07T03:36:18.502032: step 5549, loss 0.0256558, acc 1
2016-09-07T03:36:19.187150: step 5550, loss 0.0020022, acc 1
2016-09-07T03:36:19.873776: step 5551, loss 0.00093095, acc 1
2016-09-07T03:36:20.579891: step 5552, loss 0.0387907, acc 1
2016-09-07T03:36:21.292394: step 5553, loss 0.00115921, acc 1
2016-09-07T03:36:21.989322: step 5554, loss 0.008369, acc 1
2016-09-07T03:36:22.673782: step 5555, loss 0.000267442, acc 1
2016-09-07T03:36:23.363918: step 5556, loss 0.00322777, acc 1
2016-09-07T03:36:24.052339: step 5557, loss 0.00753875, acc 1
2016-09-07T03:36:24.751784: step 5558, loss 0.000254165, acc 1
2016-09-07T03:36:25.417153: step 5559, loss 2.60928e-05, acc 1
2016-09-07T03:36:26.125919: step 5560, loss 0.0331587, acc 0.98
2016-09-07T03:36:26.818536: step 5561, loss 0.0218287, acc 0.98
2016-09-07T03:36:27.515504: step 5562, loss 0.0104011, acc 1
2016-09-07T03:36:28.221762: step 5563, loss 0.0512193, acc 0.98
2016-09-07T03:36:28.906009: step 5564, loss 0.00374968, acc 1
2016-09-07T03:36:29.612203: step 5565, loss 0.0263066, acc 0.98
2016-09-07T03:36:30.298389: step 5566, loss 0.0869139, acc 0.96
2016-09-07T03:36:30.997662: step 5567, loss 0.0219582, acc 0.98
2016-09-07T03:36:31.646166: step 5568, loss 0.00258278, acc 1
2016-09-07T03:36:32.344965: step 5569, loss 0.0281694, acc 0.98
2016-09-07T03:36:33.041466: step 5570, loss 0.0268441, acc 0.98
2016-09-07T03:36:33.740462: step 5571, loss 0.021404, acc 1
2016-09-07T03:36:34.455086: step 5572, loss 0.0339972, acc 0.98
2016-09-07T03:36:35.131654: step 5573, loss 0.0246059, acc 0.98
2016-09-07T03:36:35.808333: step 5574, loss 0.00326892, acc 1
2016-09-07T03:36:36.503575: step 5575, loss 0.00294831, acc 1
2016-09-07T03:36:37.176614: step 5576, loss 0.0431607, acc 0.98
2016-09-07T03:36:37.894780: step 5577, loss 0.0092732, acc 1
2016-09-07T03:36:38.561272: step 5578, loss 0.0183673, acc 1
2016-09-07T03:36:39.298545: step 5579, loss 0.0575469, acc 0.96
2016-09-07T03:36:39.988420: step 5580, loss 0.0048684, acc 1
2016-09-07T03:36:40.677372: step 5581, loss 0.0179998, acc 1
2016-09-07T03:36:41.378486: step 5582, loss 0.0519601, acc 0.96
2016-09-07T03:36:42.057490: step 5583, loss 6.32402e-05, acc 1
2016-09-07T03:36:42.761072: step 5584, loss 0.00360442, acc 1
2016-09-07T03:36:43.433862: step 5585, loss 0.011142, acc 1
2016-09-07T03:36:44.141891: step 5586, loss 0.0953866, acc 0.98
2016-09-07T03:36:44.849191: step 5587, loss 0.00345072, acc 1
2016-09-07T03:36:45.556706: step 5588, loss 0.0890806, acc 0.96
2016-09-07T03:36:46.241968: step 5589, loss 0.022839, acc 1
2016-09-07T03:36:46.947124: step 5590, loss 0.0194985, acc 0.98
2016-09-07T03:36:47.653927: step 5591, loss 0.0021105, acc 1
2016-09-07T03:36:48.344952: step 5592, loss 0.000510403, acc 1
2016-09-07T03:36:49.027302: step 5593, loss 0.00190759, acc 1
2016-09-07T03:36:49.753062: step 5594, loss 0.0254093, acc 0.98
2016-09-07T03:36:50.443570: step 5595, loss 0.00439524, acc 1
2016-09-07T03:36:51.134242: step 5596, loss 0.0070731, acc 1
2016-09-07T03:36:51.811461: step 5597, loss 0.0257342, acc 0.98
2016-09-07T03:36:52.520645: step 5598, loss 0.0207426, acc 0.98
2016-09-07T03:36:53.203217: step 5599, loss 0.0171343, acc 1
2016-09-07T03:36:53.886811: step 5600, loss 0.0457938, acc 0.96

Evaluation:
2016-09-07T03:36:57.412698: step 5600, loss 1.84884, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-5600

2016-09-07T03:36:59.232036: step 5601, loss 0.0286095, acc 0.98
2016-09-07T03:36:59.927548: step 5602, loss 0.0258786, acc 0.98
2016-09-07T03:37:00.662434: step 5603, loss 0.0642854, acc 0.98
2016-09-07T03:37:01.358515: step 5604, loss 0.000871963, acc 1
2016-09-07T03:37:02.066839: step 5605, loss 0.00542252, acc 1
2016-09-07T03:37:02.745261: step 5606, loss 0.0441028, acc 0.98
2016-09-07T03:37:03.434330: step 5607, loss 0.000906675, acc 1
2016-09-07T03:37:04.133061: step 5608, loss 0.000500756, acc 1
2016-09-07T03:37:04.810938: step 5609, loss 0.0379203, acc 0.98
2016-09-07T03:37:05.500604: step 5610, loss 0.0657757, acc 0.98
2016-09-07T03:37:06.181555: step 5611, loss 0.0766281, acc 0.96
2016-09-07T03:37:06.882766: step 5612, loss 0.0276011, acc 0.98
2016-09-07T03:37:07.564240: step 5613, loss 0.0245207, acc 0.98
2016-09-07T03:37:08.248785: step 5614, loss 0.0739115, acc 0.98
2016-09-07T03:37:08.945183: step 5615, loss 0.0193355, acc 0.98
2016-09-07T03:37:09.635660: step 5616, loss 0.0477116, acc 0.98
2016-09-07T03:37:10.322596: step 5617, loss 0.0150105, acc 1
2016-09-07T03:37:10.998331: step 5618, loss 0.0934567, acc 0.98
2016-09-07T03:37:11.709641: step 5619, loss 0.0117404, acc 1
2016-09-07T03:37:12.402739: step 5620, loss 0.00368176, acc 1
2016-09-07T03:37:13.079310: step 5621, loss 0.0465787, acc 0.98
2016-09-07T03:37:13.779080: step 5622, loss 0.00107274, acc 1
2016-09-07T03:37:14.467969: step 5623, loss 0.0170244, acc 1
2016-09-07T03:37:15.165566: step 5624, loss 0.0494768, acc 0.96
2016-09-07T03:37:15.839070: step 5625, loss 0.0311294, acc 0.98
2016-09-07T03:37:16.564117: step 5626, loss 0.000381679, acc 1
2016-09-07T03:37:17.262643: step 5627, loss 0.173984, acc 0.98
2016-09-07T03:37:17.972578: step 5628, loss 0.0169755, acc 0.98
2016-09-07T03:37:18.656173: step 5629, loss 0.0528602, acc 0.98
2016-09-07T03:37:19.370726: step 5630, loss 0.00704693, acc 1
2016-09-07T03:37:20.102187: step 5631, loss 0.0269065, acc 1
2016-09-07T03:37:20.797763: step 5632, loss 0.0219126, acc 0.98
2016-09-07T03:37:21.481676: step 5633, loss 0.0360869, acc 0.98
2016-09-07T03:37:22.184671: step 5634, loss 0.0105923, acc 1
2016-09-07T03:37:22.881688: step 5635, loss 0.0358267, acc 0.98
2016-09-07T03:37:23.569815: step 5636, loss 0.009323, acc 1
2016-09-07T03:37:24.244980: step 5637, loss 0.0205365, acc 1
2016-09-07T03:37:24.975117: step 5638, loss 0.0420287, acc 0.98
2016-09-07T03:37:25.655985: step 5639, loss 0.010476, acc 1
2016-09-07T03:37:26.348121: step 5640, loss 0.000786851, acc 1
2016-09-07T03:37:27.026394: step 5641, loss 0.00847521, acc 1
2016-09-07T03:37:27.731405: step 5642, loss 0.00261374, acc 1
2016-09-07T03:37:28.444634: step 5643, loss 0.00755075, acc 1
2016-09-07T03:37:29.129069: step 5644, loss 0.0344259, acc 1
2016-09-07T03:37:29.852844: step 5645, loss 0.000466381, acc 1
2016-09-07T03:37:30.578977: step 5646, loss 0.015022, acc 1
2016-09-07T03:37:31.275215: step 5647, loss 0.0174292, acc 1
2016-09-07T03:37:31.962530: step 5648, loss 0.0122554, acc 1
2016-09-07T03:37:32.650406: step 5649, loss 0.0027315, acc 1
2016-09-07T03:37:33.364214: step 5650, loss 0.00366126, acc 1
2016-09-07T03:37:34.053145: step 5651, loss 0.0211267, acc 1
2016-09-07T03:37:34.735844: step 5652, loss 0.00276432, acc 1
2016-09-07T03:37:35.426104: step 5653, loss 0.011364, acc 1
2016-09-07T03:37:36.141054: step 5654, loss 0.0556642, acc 0.98
2016-09-07T03:37:36.823892: step 5655, loss 0.0282931, acc 0.98
2016-09-07T03:37:37.503176: step 5656, loss 0.00983721, acc 1
2016-09-07T03:37:38.255793: step 5657, loss 0.0170935, acc 0.98
2016-09-07T03:37:38.944946: step 5658, loss 0.0317486, acc 0.98
2016-09-07T03:37:39.644076: step 5659, loss 0.0377915, acc 0.98
2016-09-07T03:37:40.329551: step 5660, loss 0.0738454, acc 0.98
2016-09-07T03:37:41.054590: step 5661, loss 0.018052, acc 0.98
2016-09-07T03:37:41.765787: step 5662, loss 0.0177048, acc 1
2016-09-07T03:37:42.455160: step 5663, loss 0.114933, acc 0.94
2016-09-07T03:37:43.175576: step 5664, loss 0.0135145, acc 1
2016-09-07T03:37:43.883728: step 5665, loss 0.0195757, acc 0.98
2016-09-07T03:37:44.580440: step 5666, loss 0.0550743, acc 0.96
2016-09-07T03:37:45.301859: step 5667, loss 0.00443992, acc 1
2016-09-07T03:37:46.004453: step 5668, loss 0.0395978, acc 0.96
2016-09-07T03:37:46.717598: step 5669, loss 0.04903, acc 0.98
2016-09-07T03:37:47.411275: step 5670, loss 0.0454049, acc 0.98
2016-09-07T03:37:48.123022: step 5671, loss 0.0141301, acc 1
2016-09-07T03:37:48.825406: step 5672, loss 0.00615924, acc 1
2016-09-07T03:37:49.513228: step 5673, loss 0.0527849, acc 0.96
2016-09-07T03:37:50.243093: step 5674, loss 0.102165, acc 0.94
2016-09-07T03:37:50.943096: step 5675, loss 0.00528406, acc 1
2016-09-07T03:37:51.659135: step 5676, loss 0.0122356, acc 1
2016-09-07T03:37:52.361455: step 5677, loss 0.00390983, acc 1
2016-09-07T03:37:53.049802: step 5678, loss 0.00615585, acc 1
2016-09-07T03:37:53.761936: step 5679, loss 0.0376231, acc 0.98
2016-09-07T03:37:54.455085: step 5680, loss 0.000904863, acc 1
2016-09-07T03:37:55.133903: step 5681, loss 0.0406137, acc 0.98
2016-09-07T03:37:55.827967: step 5682, loss 0.125831, acc 0.98
2016-09-07T03:37:56.540921: step 5683, loss 0.0103491, acc 1
2016-09-07T03:37:57.233382: step 5684, loss 0.0458783, acc 0.98
2016-09-07T03:37:57.918118: step 5685, loss 0.0342506, acc 0.98
2016-09-07T03:37:58.614966: step 5686, loss 0.0283456, acc 0.98
2016-09-07T03:37:59.314966: step 5687, loss 0.0211477, acc 0.98
2016-09-07T03:38:00.010487: step 5688, loss 0.0253275, acc 1
2016-09-07T03:38:00.750605: step 5689, loss 0.000739806, acc 1
2016-09-07T03:38:01.454968: step 5690, loss 0.0269765, acc 0.98
2016-09-07T03:38:02.132486: step 5691, loss 0.00377069, acc 1
2016-09-07T03:38:02.816849: step 5692, loss 0.000282836, acc 1
2016-09-07T03:38:03.538346: step 5693, loss 0.0170286, acc 1
2016-09-07T03:38:04.222938: step 5694, loss 0.0152682, acc 0.98
2016-09-07T03:38:04.903375: step 5695, loss 0.0441658, acc 0.98
2016-09-07T03:38:05.596237: step 5696, loss 0.0512644, acc 0.94
2016-09-07T03:38:06.274920: step 5697, loss 0.00284544, acc 1
2016-09-07T03:38:06.977410: step 5698, loss 0.0107082, acc 1
2016-09-07T03:38:07.657017: step 5699, loss 0.017663, acc 0.98
2016-09-07T03:38:08.375181: step 5700, loss 0.013697, acc 1

Evaluation:
2016-09-07T03:38:11.995175: step 5700, loss 1.97726, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473186470/checkpoints/model-5700

2016-09-07T03:38:13.807663: step 5701, loss 0.00147046, acc 1
2016-09-07T03:38:14.483869: step 5702, loss 0.0419725, acc 0.98
2016-09-07T03:38:15.170814: step 5703, loss 0.0729038, acc 0.96
2016-09-07T03:38:15.845877: step 5704, loss 0.00900664, acc 1
2016-09-07T03:38:16.562976: step 5705, loss 0.00328544, acc 1
2016-09-07T03:38:17.233010: step 5706, loss 0.142102, acc 0.98
2016-09-07T03:38:17.960026: step 5707, loss 0.0156131, acc 1
2016-09-07T03:38:18.657374: step 5708, loss 0.0456644, acc 0.96
2016-09-07T03:38:19.358288: step 5709, loss 0.000644125, acc 1
2016-09-07T03:38:20.035434: step 5710, loss 0.0330016, acc 0.98
2016-09-07T03:38:20.719644: step 5711, loss 0.00480147, acc 1
2016-09-07T03:38:21.433256: step 5712, loss 0.0105685, acc 1
2016-09-07T03:38:22.113424: step 5713, loss 0.0311389, acc 0.98
2016-09-07T03:38:22.801410: step 5714, loss 0.000462834, acc 1
2016-09-07T03:38:23.502105: step 5715, loss 0.0348278, acc 0.98
2016-09-07T03:38:24.218203: step 5716, loss 0.045704, acc 0.96
2016-09-07T03:38:24.927649: step 5717, loss 0.0542216, acc 0.98
2016-09-07T03:38:25.584241: step 5718, loss 0.0495067, acc 0.96
2016-09-07T03:38:26.312480: step 5719, loss 0.0129947, acc 1
2016-09-07T03:38:26.998819: step 5720, loss 0.0901635, acc 0.98
2016-09-07T03:38:27.692279: step 5721, loss 0.0109336, acc 1
2016-09-07T03:38:28.390184: step 5722, loss 0.23249, acc 0.96
2016-09-07T03:38:29.088287: step 5723, loss 0.0292158, acc 1
2016-09-07T03:38:29.792256: step 5724, loss 0.00111241, acc 1
2016-09-07T03:38:30.483652: step 5725, loss 0.004781, acc 1
2016-09-07T03:38:31.193930: step 5726, loss 0.0180426, acc 1
2016-09-07T03:38:31.876636: step 5727, loss 0.00860212, acc 1
2016-09-07T03:38:32.544405: step 5728, loss 0.0442512, acc 0.98
2016-09-07T03:38:33.227955: step 5729, loss 4.04028e-05, acc 1
2016-09-07T03:38:33.918208: step 5730, loss 0.0163785, acc 0.98
2016-09-07T03:38:34.641669: step 5731, loss 0.0383522, acc 0.98
2016-09-07T03:38:35.336506: step 5732, loss 0.00843524, acc 1
2016-09-07T03:38:36.038384: step 5733, loss 0.0226046, acc 1
2016-09-07T03:38:36.719081: step 5734, loss 0.0220998, acc 0.98
2016-09-07T03:38:37.385567: step 5735, loss 0.0251318, acc 0.98
2016-09-07T03:38:38.080922: step 5736, loss 0.0250984, acc 0.98
2016-09-07T03:38:38.788667: step 5737, loss 0.0346186, acc 0.98
2016-09-07T03:38:39.487495: step 5738, loss 0.193149, acc 0.96
2016-09-07T03:38:40.168618: step 5739, loss 0.0199539, acc 0.98
2016-09-07T03:38:40.853048: step 5740, loss 0.0048256, acc 1
2016-09-07T03:38:41.550318: step 5741, loss 0.0136432, acc 1
2016-09-07T03:38:42.249729: step 5742, loss 0.0211372, acc 1
2016-09-07T03:38:42.958443: step 5743, loss 0.00297749, acc 1
2016-09-07T03:38:43.646281: step 5744, loss 0.0185332, acc 1
2016-09-07T03:38:44.365151: step 5745, loss 0.0136929, acc 1
2016-09-07T03:38:45.028752: step 5746, loss 0.023353, acc 1
2016-09-07T03:38:45.714975: step 5747, loss 0.0658323, acc 0.96
2016-09-07T03:38:46.402740: step 5748, loss 0.0718006, acc 0.94
2016-09-07T03:38:47.088120: step 5749, loss 0.0470717, acc 0.98
2016-09-07T03:38:47.782984: step 5750, loss 0.00162726, acc 1
2016-09-07T03:38:48.444896: step 5751, loss 0.00464002, acc 1
2016-09-07T03:38:49.142878: step 5752, loss 0.0187764, acc 0.98
2016-09-07T03:38:49.805239: step 5753, loss 0.0199721, acc 1
2016-09-07T03:38:50.485047: step 5754, loss 0.00224809, acc 1
2016-09-07T03:38:51.173255: step 5755, loss 0.0463618, acc 0.96
2016-09-07T03:38:51.866362: step 5756, loss 0.000157565, acc 1
2016-09-07T03:38:52.570229: step 5757, loss 0.0257359, acc 1
2016-09-07T03:38:53.253578: step 5758, loss 0.0100791, acc 1
2016-09-07T03:38:53.942353: step 5759, loss 0.00277131, acc 1
2016-09-07T03:38:54.589099: step 5760, loss 0.00206709, acc 1

WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fa624312e90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fa624312e50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
CV_INDEX=6
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=30
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9594/1066
Writing to /home/cil/lstm-context-embeddings/runs/1473182186

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-07T01:16:46.643169: step 1, loss 0.693147, acc 0.38
2016-09-07T01:16:47.346977: step 2, loss 0.721352, acc 0.44
2016-09-07T01:16:48.046146: step 3, loss 0.693531, acc 0.52
2016-09-07T01:16:48.740966: step 4, loss 0.700985, acc 0.5
2016-09-07T01:16:49.412367: step 5, loss 0.689581, acc 0.52
2016-09-07T01:16:50.087175: step 6, loss 0.690751, acc 0.58
2016-09-07T01:16:50.790208: step 7, loss 0.694604, acc 0.46
2016-09-07T01:16:51.468157: step 8, loss 0.692941, acc 0.52
2016-09-07T01:16:52.157101: step 9, loss 0.690529, acc 0.54
2016-09-07T01:16:52.851277: step 10, loss 0.708003, acc 0.54
2016-09-07T01:16:53.551689: step 11, loss 0.696006, acc 0.48
2016-09-07T01:16:54.235518: step 12, loss 0.706227, acc 0.44
2016-09-07T01:16:54.921886: step 13, loss 0.686873, acc 0.56
2016-09-07T01:16:55.608303: step 14, loss 0.697398, acc 0.4
2016-09-07T01:16:56.299498: step 15, loss 0.672486, acc 0.6
2016-09-07T01:16:57.013897: step 16, loss 0.764095, acc 0.44
2016-09-07T01:16:57.687823: step 17, loss 0.686606, acc 0.6
2016-09-07T01:16:58.384820: step 18, loss 0.706293, acc 0.52
2016-09-07T01:16:59.063182: step 19, loss 0.687013, acc 0.54
2016-09-07T01:16:59.740205: step 20, loss 0.675541, acc 0.6
2016-09-07T01:17:00.468244: step 21, loss 0.695026, acc 0.56
2016-09-07T01:17:01.156240: step 22, loss 0.68976, acc 0.52
2016-09-07T01:17:01.844890: step 23, loss 0.668756, acc 0.66
2016-09-07T01:17:02.508395: step 24, loss 0.643857, acc 0.66
2016-09-07T01:17:03.219227: step 25, loss 0.750689, acc 0.46
2016-09-07T01:17:03.912431: step 26, loss 0.656014, acc 0.66
2016-09-07T01:17:04.629993: step 27, loss 0.656382, acc 0.64
2016-09-07T01:17:05.313925: step 28, loss 0.61585, acc 0.72
2016-09-07T01:17:06.008526: step 29, loss 0.732841, acc 0.42
2016-09-07T01:17:06.687655: step 30, loss 0.650133, acc 0.46
2016-09-07T01:17:07.352705: step 31, loss 0.693266, acc 0.56
2016-09-07T01:17:08.068948: step 32, loss 0.628094, acc 0.7
2016-09-07T01:17:08.753194: step 33, loss 0.696854, acc 0.6
2016-09-07T01:17:09.441847: step 34, loss 0.682659, acc 0.58
2016-09-07T01:17:10.161735: step 35, loss 0.620413, acc 0.64
2016-09-07T01:17:10.866903: step 36, loss 0.602961, acc 0.7
2016-09-07T01:17:11.596833: step 37, loss 0.651413, acc 0.58
2016-09-07T01:17:12.263490: step 38, loss 0.582288, acc 0.68
2016-09-07T01:17:12.960338: step 39, loss 0.527072, acc 0.68
2016-09-07T01:17:13.648516: step 40, loss 0.704057, acc 0.64
2016-09-07T01:17:14.316215: step 41, loss 0.695698, acc 0.62
2016-09-07T01:17:14.986741: step 42, loss 0.606552, acc 0.76
2016-09-07T01:17:15.669454: step 43, loss 0.562976, acc 0.74
2016-09-07T01:17:16.387639: step 44, loss 0.831196, acc 0.52
2016-09-07T01:17:17.055450: step 45, loss 0.737968, acc 0.58
2016-09-07T01:17:17.753450: step 46, loss 0.585125, acc 0.68
2016-09-07T01:17:18.447966: step 47, loss 0.666657, acc 0.6
2016-09-07T01:17:19.119721: step 48, loss 0.701488, acc 0.7
2016-09-07T01:17:19.794177: step 49, loss 0.785769, acc 0.62
2016-09-07T01:17:20.487647: step 50, loss 0.571611, acc 0.68
2016-09-07T01:17:21.187545: step 51, loss 0.602913, acc 0.72
2016-09-07T01:17:21.848389: step 52, loss 0.647038, acc 0.68
2016-09-07T01:17:22.544913: step 53, loss 0.595215, acc 0.66
2016-09-07T01:17:23.249144: step 54, loss 0.560006, acc 0.72
2016-09-07T01:17:23.947469: step 55, loss 0.627467, acc 0.64
2016-09-07T01:17:24.628645: step 56, loss 0.633697, acc 0.72
2016-09-07T01:17:25.312962: step 57, loss 0.604444, acc 0.66
2016-09-07T01:17:26.020542: step 58, loss 0.701881, acc 0.54
2016-09-07T01:17:26.675757: step 59, loss 0.621908, acc 0.58
2016-09-07T01:17:27.373768: step 60, loss 0.600727, acc 0.68
2016-09-07T01:17:28.091643: step 61, loss 0.59361, acc 0.7
2016-09-07T01:17:28.777024: step 62, loss 0.588233, acc 0.72
2016-09-07T01:17:29.451918: step 63, loss 0.525108, acc 0.76
2016-09-07T01:17:30.141638: step 64, loss 0.589144, acc 0.68
2016-09-07T01:17:30.847762: step 65, loss 0.635982, acc 0.66
2016-09-07T01:17:31.529156: step 66, loss 0.604536, acc 0.62
2016-09-07T01:17:32.218834: step 67, loss 0.686723, acc 0.62
2016-09-07T01:17:32.912915: step 68, loss 0.563276, acc 0.72
2016-09-07T01:17:33.603172: step 69, loss 0.675818, acc 0.58
2016-09-07T01:17:34.315709: step 70, loss 0.481063, acc 0.86
2016-09-07T01:17:34.993391: step 71, loss 0.55404, acc 0.76
2016-09-07T01:17:35.693844: step 72, loss 0.465316, acc 0.78
2016-09-07T01:17:36.386862: step 73, loss 0.558024, acc 0.74
2016-09-07T01:17:37.072924: step 74, loss 0.554333, acc 0.68
2016-09-07T01:17:37.766267: step 75, loss 0.500046, acc 0.78
2016-09-07T01:17:38.450170: step 76, loss 0.630352, acc 0.68
2016-09-07T01:17:39.157553: step 77, loss 0.482595, acc 0.8
2016-09-07T01:17:39.844490: step 78, loss 0.594415, acc 0.7
2016-09-07T01:17:40.545876: step 79, loss 0.604231, acc 0.7
2016-09-07T01:17:41.253888: step 80, loss 0.406055, acc 0.86
2016-09-07T01:17:41.953997: step 81, loss 0.494679, acc 0.76
2016-09-07T01:17:42.643119: step 82, loss 0.474661, acc 0.74
2016-09-07T01:17:43.353206: step 83, loss 0.4318, acc 0.76
2016-09-07T01:17:44.046112: step 84, loss 0.628132, acc 0.68
2016-09-07T01:17:44.736113: step 85, loss 0.48678, acc 0.76
2016-09-07T01:17:45.424233: step 86, loss 0.601473, acc 0.68
2016-09-07T01:17:46.123867: step 87, loss 0.458634, acc 0.78
2016-09-07T01:17:46.800937: step 88, loss 0.571939, acc 0.72
2016-09-07T01:17:47.500490: step 89, loss 0.682419, acc 0.68
2016-09-07T01:17:48.195896: step 90, loss 0.524503, acc 0.68
2016-09-07T01:17:48.919355: step 91, loss 0.623866, acc 0.68
2016-09-07T01:17:49.597621: step 92, loss 0.627388, acc 0.72
2016-09-07T01:17:50.282315: step 93, loss 0.662102, acc 0.6
2016-09-07T01:17:50.950017: step 94, loss 0.589843, acc 0.62
2016-09-07T01:17:51.635050: step 95, loss 0.74907, acc 0.58
2016-09-07T01:17:52.330172: step 96, loss 0.532242, acc 0.8
2016-09-07T01:17:53.039541: step 97, loss 0.503428, acc 0.78
2016-09-07T01:17:53.749686: step 98, loss 0.510053, acc 0.72
2016-09-07T01:17:54.449576: step 99, loss 0.545959, acc 0.7
2016-09-07T01:17:55.152563: step 100, loss 0.584293, acc 0.64

Evaluation:
2016-09-07T01:17:58.363334: step 100, loss 0.508617, acc 0.765478

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-100

2016-09-07T01:18:00.116312: step 101, loss 0.500739, acc 0.82
2016-09-07T01:18:00.841886: step 102, loss 0.590389, acc 0.74
2016-09-07T01:18:01.519891: step 103, loss 0.544542, acc 0.7
2016-09-07T01:18:02.237267: step 104, loss 0.467572, acc 0.8
2016-09-07T01:18:02.931858: step 105, loss 0.585844, acc 0.7
2016-09-07T01:18:03.620041: step 106, loss 0.533853, acc 0.74
2016-09-07T01:18:04.309791: step 107, loss 0.403937, acc 0.82
2016-09-07T01:18:05.002858: step 108, loss 0.523519, acc 0.72
2016-09-07T01:18:05.712991: step 109, loss 0.521192, acc 0.74
2016-09-07T01:18:06.374938: step 110, loss 0.441194, acc 0.82
2016-09-07T01:18:07.046493: step 111, loss 0.437142, acc 0.8
2016-09-07T01:18:07.741919: step 112, loss 0.438693, acc 0.78
2016-09-07T01:18:08.409227: step 113, loss 0.507174, acc 0.78
2016-09-07T01:18:09.092913: step 114, loss 0.481711, acc 0.74
2016-09-07T01:18:09.800675: step 115, loss 0.454489, acc 0.72
2016-09-07T01:18:10.518627: step 116, loss 0.443387, acc 0.78
2016-09-07T01:18:11.207309: step 117, loss 0.584531, acc 0.72
2016-09-07T01:18:11.905877: step 118, loss 0.603423, acc 0.7
2016-09-07T01:18:12.606846: step 119, loss 0.451853, acc 0.82
2016-09-07T01:18:13.298478: step 120, loss 0.469078, acc 0.76
2016-09-07T01:18:13.990916: step 121, loss 0.568488, acc 0.68
2016-09-07T01:18:14.652365: step 122, loss 0.634517, acc 0.64
2016-09-07T01:18:15.339576: step 123, loss 0.684435, acc 0.52
2016-09-07T01:18:16.016165: step 124, loss 0.480663, acc 0.76
2016-09-07T01:18:16.732598: step 125, loss 0.543321, acc 0.68
2016-09-07T01:18:17.435875: step 126, loss 0.448135, acc 0.74
2016-09-07T01:18:18.128472: step 127, loss 0.489824, acc 0.76
2016-09-07T01:18:18.828941: step 128, loss 0.505672, acc 0.76
2016-09-07T01:18:19.518603: step 129, loss 0.482452, acc 0.78
2016-09-07T01:18:20.222228: step 130, loss 0.566974, acc 0.78
2016-09-07T01:18:20.891471: step 131, loss 0.539704, acc 0.66
2016-09-07T01:18:21.569155: step 132, loss 0.586739, acc 0.72
2016-09-07T01:18:22.253336: step 133, loss 0.48889, acc 0.84
2016-09-07T01:18:22.937226: step 134, loss 0.550645, acc 0.76
2016-09-07T01:18:23.628013: step 135, loss 0.506757, acc 0.76
2016-09-07T01:18:24.305041: step 136, loss 0.498221, acc 0.82
2016-09-07T01:18:25.005550: step 137, loss 0.543391, acc 0.76
2016-09-07T01:18:25.707972: step 138, loss 0.468914, acc 0.82
2016-09-07T01:18:26.384253: step 139, loss 0.521654, acc 0.8
2016-09-07T01:18:27.075578: step 140, loss 0.491743, acc 0.76
2016-09-07T01:18:27.758747: step 141, loss 0.484063, acc 0.82
2016-09-07T01:18:28.459579: step 142, loss 0.490705, acc 0.68
2016-09-07T01:18:29.126957: step 143, loss 0.37492, acc 0.88
2016-09-07T01:18:29.826352: step 144, loss 0.465139, acc 0.78
2016-09-07T01:18:30.519049: step 145, loss 0.674288, acc 0.62
2016-09-07T01:18:31.199127: step 146, loss 0.485548, acc 0.78
2016-09-07T01:18:31.884295: step 147, loss 0.424814, acc 0.72
2016-09-07T01:18:32.564816: step 148, loss 0.423335, acc 0.82
2016-09-07T01:18:33.256847: step 149, loss 0.555027, acc 0.68
2016-09-07T01:18:33.933478: step 150, loss 0.56488, acc 0.78
2016-09-07T01:18:34.626318: step 151, loss 0.494434, acc 0.78
2016-09-07T01:18:35.320419: step 152, loss 0.407618, acc 0.84
2016-09-07T01:18:36.029813: step 153, loss 0.61257, acc 0.68
2016-09-07T01:18:36.712615: step 154, loss 0.455891, acc 0.78
2016-09-07T01:18:37.390099: step 155, loss 0.504067, acc 0.78
2016-09-07T01:18:38.075347: step 156, loss 0.495931, acc 0.78
2016-09-07T01:18:38.727161: step 157, loss 0.4715, acc 0.76
2016-09-07T01:18:39.426777: step 158, loss 0.372054, acc 0.86
2016-09-07T01:18:40.094690: step 159, loss 0.420112, acc 0.8
2016-09-07T01:18:40.798403: step 160, loss 0.532708, acc 0.66
2016-09-07T01:18:41.495728: step 161, loss 0.609262, acc 0.7
2016-09-07T01:18:42.191105: step 162, loss 0.561364, acc 0.68
2016-09-07T01:18:42.892523: step 163, loss 0.409707, acc 0.78
2016-09-07T01:18:43.582190: step 164, loss 0.550501, acc 0.68
2016-09-07T01:18:44.288717: step 165, loss 0.446022, acc 0.78
2016-09-07T01:18:44.984345: step 166, loss 0.413583, acc 0.86
2016-09-07T01:18:45.660687: step 167, loss 0.584194, acc 0.66
2016-09-07T01:18:46.341444: step 168, loss 0.481717, acc 0.74
2016-09-07T01:18:47.011728: step 169, loss 0.482502, acc 0.74
2016-09-07T01:18:47.690061: step 170, loss 0.48095, acc 0.64
2016-09-07T01:18:48.354845: step 171, loss 0.576262, acc 0.68
2016-09-07T01:18:49.074459: step 172, loss 0.467786, acc 0.78
2016-09-07T01:18:49.782658: step 173, loss 0.548187, acc 0.8
2016-09-07T01:18:50.472272: step 174, loss 0.474728, acc 0.76
2016-09-07T01:18:51.184620: step 175, loss 0.519688, acc 0.78
2016-09-07T01:18:51.869354: step 176, loss 0.482915, acc 0.74
2016-09-07T01:18:52.584862: step 177, loss 0.502094, acc 0.72
2016-09-07T01:18:53.251839: step 178, loss 0.448459, acc 0.78
2016-09-07T01:18:53.936466: step 179, loss 0.475155, acc 0.76
2016-09-07T01:18:54.638872: step 180, loss 0.544098, acc 0.72
2016-09-07T01:18:55.325272: step 181, loss 0.478913, acc 0.74
2016-09-07T01:18:56.022919: step 182, loss 0.626502, acc 0.66
2016-09-07T01:18:56.738947: step 183, loss 0.376684, acc 0.84
2016-09-07T01:18:57.439679: step 184, loss 0.663766, acc 0.62
2016-09-07T01:18:58.149110: step 185, loss 0.520767, acc 0.74
2016-09-07T01:18:58.828458: step 186, loss 0.483421, acc 0.76
2016-09-07T01:18:59.532842: step 187, loss 0.453393, acc 0.8
2016-09-07T01:19:00.237435: step 188, loss 0.594224, acc 0.74
2016-09-07T01:19:00.940116: step 189, loss 0.402248, acc 0.78
2016-09-07T01:19:01.613939: step 190, loss 0.48137, acc 0.74
2016-09-07T01:19:02.325877: step 191, loss 0.424504, acc 0.8
2016-09-07T01:19:02.977157: step 192, loss 0.4635, acc 0.818182
2016-09-07T01:19:03.674568: step 193, loss 0.336648, acc 0.84
2016-09-07T01:19:04.356743: step 194, loss 0.40102, acc 0.8
2016-09-07T01:19:05.057405: step 195, loss 0.386735, acc 0.84
2016-09-07T01:19:05.770271: step 196, loss 0.442774, acc 0.76
2016-09-07T01:19:06.459704: step 197, loss 0.29869, acc 0.88
2016-09-07T01:19:07.163199: step 198, loss 0.39548, acc 0.8
2016-09-07T01:19:07.848913: step 199, loss 0.47923, acc 0.8
2016-09-07T01:19:08.521007: step 200, loss 0.386782, acc 0.8

Evaluation:
2016-09-07T01:19:11.722445: step 200, loss 0.493767, acc 0.786116

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-200

2016-09-07T01:19:13.364121: step 201, loss 0.397647, acc 0.88
2016-09-07T01:19:14.065024: step 202, loss 0.26401, acc 0.9
2016-09-07T01:19:14.774337: step 203, loss 0.330907, acc 0.86
2016-09-07T01:19:15.488194: step 204, loss 0.35673, acc 0.84
2016-09-07T01:19:16.178018: step 205, loss 0.389246, acc 0.82
2016-09-07T01:19:16.862457: step 206, loss 0.334386, acc 0.82
2016-09-07T01:19:17.575908: step 207, loss 0.417777, acc 0.74
2016-09-07T01:19:18.281956: step 208, loss 0.216708, acc 0.92
2016-09-07T01:19:18.999775: step 209, loss 0.404358, acc 0.84
2016-09-07T01:19:19.675354: step 210, loss 0.331723, acc 0.84
2016-09-07T01:19:20.376101: step 211, loss 0.279791, acc 0.88
2016-09-07T01:19:21.077969: step 212, loss 0.312457, acc 0.84
2016-09-07T01:19:21.789340: step 213, loss 0.400931, acc 0.8
2016-09-07T01:19:22.490613: step 214, loss 0.434256, acc 0.8
2016-09-07T01:19:23.185710: step 215, loss 0.244242, acc 0.88
2016-09-07T01:19:23.894534: step 216, loss 0.408017, acc 0.84
2016-09-07T01:19:24.583040: step 217, loss 0.44146, acc 0.82
2016-09-07T01:19:25.292615: step 218, loss 0.28652, acc 0.86
2016-09-07T01:19:25.984835: step 219, loss 0.256729, acc 0.88
2016-09-07T01:19:26.685035: step 220, loss 0.382188, acc 0.86
2016-09-07T01:19:27.394278: step 221, loss 0.324591, acc 0.8
2016-09-07T01:19:28.058213: step 222, loss 0.405758, acc 0.82
2016-09-07T01:19:28.779874: step 223, loss 0.187777, acc 0.94
2016-09-07T01:19:29.470144: step 224, loss 0.443439, acc 0.84
2016-09-07T01:19:30.148890: step 225, loss 0.363133, acc 0.82
2016-09-07T01:19:30.858647: step 226, loss 0.272089, acc 0.82
2016-09-07T01:19:31.529210: step 227, loss 0.27756, acc 0.84
2016-09-07T01:19:32.233903: step 228, loss 0.377043, acc 0.86
2016-09-07T01:19:32.922910: step 229, loss 0.274316, acc 0.92
2016-09-07T01:19:33.619712: step 230, loss 0.448748, acc 0.78
2016-09-07T01:19:34.307909: step 231, loss 0.284151, acc 0.82
2016-09-07T01:19:34.991661: step 232, loss 0.255667, acc 0.86
2016-09-07T01:19:35.671156: step 233, loss 0.501441, acc 0.72
2016-09-07T01:19:36.371025: step 234, loss 0.363669, acc 0.84
2016-09-07T01:19:37.080854: step 235, loss 0.41419, acc 0.8
2016-09-07T01:19:37.746930: step 236, loss 0.180519, acc 0.9
2016-09-07T01:19:38.461611: step 237, loss 0.303497, acc 0.84
2016-09-07T01:19:39.144968: step 238, loss 0.279685, acc 0.9
2016-09-07T01:19:39.851970: step 239, loss 0.306103, acc 0.84
2016-09-07T01:19:40.558454: step 240, loss 0.44132, acc 0.84
2016-09-07T01:19:41.256621: step 241, loss 0.474639, acc 0.72
2016-09-07T01:19:41.961849: step 242, loss 0.487249, acc 0.72
2016-09-07T01:19:42.649118: step 243, loss 0.383329, acc 0.86
2016-09-07T01:19:43.343825: step 244, loss 0.310384, acc 0.84
2016-09-07T01:19:44.033373: step 245, loss 0.378752, acc 0.86
2016-09-07T01:19:44.730830: step 246, loss 0.414769, acc 0.86
2016-09-07T01:19:45.460370: step 247, loss 0.557618, acc 0.7
2016-09-07T01:19:46.134185: step 248, loss 0.24236, acc 0.9
2016-09-07T01:19:46.835780: step 249, loss 0.235067, acc 0.88
2016-09-07T01:19:47.527940: step 250, loss 0.342424, acc 0.82
2016-09-07T01:19:48.227080: step 251, loss 0.471685, acc 0.78
2016-09-07T01:19:48.937745: step 252, loss 0.379991, acc 0.86
2016-09-07T01:19:49.629550: step 253, loss 0.337575, acc 0.84
2016-09-07T01:19:50.327170: step 254, loss 0.472655, acc 0.76
2016-09-07T01:19:51.040514: step 255, loss 0.488844, acc 0.78
2016-09-07T01:19:51.725665: step 256, loss 0.292209, acc 0.92
2016-09-07T01:19:52.404881: step 257, loss 0.471992, acc 0.72
2016-09-07T01:19:53.092912: step 258, loss 0.412727, acc 0.8
2016-09-07T01:19:53.793808: step 259, loss 0.421091, acc 0.84
2016-09-07T01:19:54.496713: step 260, loss 0.437095, acc 0.84
2016-09-07T01:19:55.211373: step 261, loss 0.310663, acc 0.88
2016-09-07T01:19:55.927579: step 262, loss 0.328663, acc 0.88
2016-09-07T01:19:56.612441: step 263, loss 0.407913, acc 0.8
2016-09-07T01:19:57.301258: step 264, loss 0.49266, acc 0.74
2016-09-07T01:19:57.996364: step 265, loss 0.356448, acc 0.88
2016-09-07T01:19:58.707253: step 266, loss 0.324797, acc 0.88
2016-09-07T01:19:59.394513: step 267, loss 0.369211, acc 0.82
2016-09-07T01:20:00.117276: step 268, loss 0.494828, acc 0.76
2016-09-07T01:20:00.838338: step 269, loss 0.463376, acc 0.82
2016-09-07T01:20:01.522574: step 270, loss 0.406256, acc 0.8
2016-09-07T01:20:02.214553: step 271, loss 0.482941, acc 0.74
2016-09-07T01:20:02.894937: step 272, loss 0.46924, acc 0.76
2016-09-07T01:20:03.592665: step 273, loss 0.544592, acc 0.78
2016-09-07T01:20:04.280798: step 274, loss 0.399854, acc 0.8
2016-09-07T01:20:04.967954: step 275, loss 0.336364, acc 0.86
2016-09-07T01:20:05.659795: step 276, loss 0.347006, acc 0.84
2016-09-07T01:20:06.334472: step 277, loss 0.32224, acc 0.88
2016-09-07T01:20:07.020517: step 278, loss 0.431613, acc 0.78
2016-09-07T01:20:07.689485: step 279, loss 0.326741, acc 0.86
2016-09-07T01:20:08.403857: step 280, loss 0.304345, acc 0.92
2016-09-07T01:20:09.090571: step 281, loss 0.50255, acc 0.74
2016-09-07T01:20:09.790406: step 282, loss 0.423256, acc 0.8
2016-09-07T01:20:10.489749: step 283, loss 0.289837, acc 0.88
2016-09-07T01:20:11.177921: step 284, loss 0.278951, acc 0.92
2016-09-07T01:20:11.877831: step 285, loss 0.366572, acc 0.8
2016-09-07T01:20:12.558929: step 286, loss 0.249471, acc 0.92
2016-09-07T01:20:13.309078: step 287, loss 0.318911, acc 0.84
2016-09-07T01:20:14.002685: step 288, loss 0.331624, acc 0.86
2016-09-07T01:20:14.689764: step 289, loss 0.35446, acc 0.78
2016-09-07T01:20:15.359787: step 290, loss 0.341509, acc 0.78
2016-09-07T01:20:16.032265: step 291, loss 0.357376, acc 0.88
2016-09-07T01:20:16.759988: step 292, loss 0.385157, acc 0.82
2016-09-07T01:20:17.436042: step 293, loss 0.43354, acc 0.78
2016-09-07T01:20:18.141039: step 294, loss 0.350409, acc 0.88
2016-09-07T01:20:18.818604: step 295, loss 0.430584, acc 0.78
2016-09-07T01:20:19.495571: step 296, loss 0.532831, acc 0.74
2016-09-07T01:20:20.164164: step 297, loss 0.351486, acc 0.86
2016-09-07T01:20:20.857755: step 298, loss 0.435088, acc 0.74
2016-09-07T01:20:21.557522: step 299, loss 0.349127, acc 0.82
2016-09-07T01:20:22.241386: step 300, loss 0.442687, acc 0.72

Evaluation:
2016-09-07T01:20:25.434884: step 300, loss 0.437259, acc 0.792683

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-300

2016-09-07T01:20:27.194112: step 301, loss 0.502579, acc 0.76
2016-09-07T01:20:27.882307: step 302, loss 0.302847, acc 0.86
2016-09-07T01:20:28.596563: step 303, loss 0.443089, acc 0.78
2016-09-07T01:20:29.279241: step 304, loss 0.324161, acc 0.86
2016-09-07T01:20:29.978179: step 305, loss 0.409673, acc 0.8
2016-09-07T01:20:30.655510: step 306, loss 0.402186, acc 0.8
2016-09-07T01:20:31.326947: step 307, loss 0.428928, acc 0.78
2016-09-07T01:20:32.021906: step 308, loss 0.37576, acc 0.8
2016-09-07T01:20:32.729666: step 309, loss 0.441834, acc 0.78
2016-09-07T01:20:33.411544: step 310, loss 0.295956, acc 0.86
2016-09-07T01:20:34.111249: step 311, loss 0.495869, acc 0.86
2016-09-07T01:20:34.807636: step 312, loss 0.332531, acc 0.82
2016-09-07T01:20:35.485050: step 313, loss 0.290553, acc 0.86
2016-09-07T01:20:36.183494: step 314, loss 0.368688, acc 0.84
2016-09-07T01:20:36.886540: step 315, loss 0.372514, acc 0.82
2016-09-07T01:20:37.585618: step 316, loss 0.348332, acc 0.84
2016-09-07T01:20:38.276707: step 317, loss 0.204934, acc 0.94
2016-09-07T01:20:38.955144: step 318, loss 0.419045, acc 0.8
2016-09-07T01:20:39.701979: step 319, loss 0.468431, acc 0.76
2016-09-07T01:20:40.395036: step 320, loss 0.289011, acc 0.9
2016-09-07T01:20:41.099414: step 321, loss 0.418698, acc 0.8
2016-09-07T01:20:41.817566: step 322, loss 0.239769, acc 0.9
2016-09-07T01:20:42.520037: step 323, loss 0.456605, acc 0.78
2016-09-07T01:20:43.222573: step 324, loss 0.328543, acc 0.86
2016-09-07T01:20:43.899477: step 325, loss 0.32855, acc 0.88
2016-09-07T01:20:44.589538: step 326, loss 0.268833, acc 0.86
2016-09-07T01:20:45.296363: step 327, loss 0.380523, acc 0.8
2016-09-07T01:20:45.972440: step 328, loss 0.455478, acc 0.78
2016-09-07T01:20:46.650185: step 329, loss 0.360804, acc 0.84
2016-09-07T01:20:47.341385: step 330, loss 0.278977, acc 0.86
2016-09-07T01:20:48.049002: step 331, loss 0.347289, acc 0.88
2016-09-07T01:20:48.725219: step 332, loss 0.358869, acc 0.8
2016-09-07T01:20:49.435041: step 333, loss 0.313262, acc 0.86
2016-09-07T01:20:50.124852: step 334, loss 0.36457, acc 0.86
2016-09-07T01:20:50.819330: step 335, loss 0.483556, acc 0.82
2016-09-07T01:20:51.489974: step 336, loss 0.379968, acc 0.88
2016-09-07T01:20:52.164438: step 337, loss 0.248618, acc 0.94
2016-09-07T01:20:52.873871: step 338, loss 0.627667, acc 0.72
2016-09-07T01:20:53.548279: step 339, loss 0.378177, acc 0.8
2016-09-07T01:20:54.241428: step 340, loss 0.524553, acc 0.76
2016-09-07T01:20:54.937206: step 341, loss 0.256416, acc 0.94
2016-09-07T01:20:55.629782: step 342, loss 0.412819, acc 0.76
2016-09-07T01:20:56.328810: step 343, loss 0.338041, acc 0.84
2016-09-07T01:20:56.990104: step 344, loss 0.482094, acc 0.74
2016-09-07T01:20:57.696128: step 345, loss 0.312281, acc 0.9
2016-09-07T01:20:58.369960: step 346, loss 0.419609, acc 0.8
2016-09-07T01:20:59.073775: step 347, loss 0.267203, acc 0.88
2016-09-07T01:20:59.768273: step 348, loss 0.339715, acc 0.9
2016-09-07T01:21:00.494087: step 349, loss 0.438549, acc 0.84
2016-09-07T01:21:01.186578: step 350, loss 0.327964, acc 0.84
2016-09-07T01:21:01.857310: step 351, loss 0.440681, acc 0.84
2016-09-07T01:21:02.560132: step 352, loss 0.379808, acc 0.86
2016-09-07T01:21:03.312839: step 353, loss 0.416839, acc 0.78
2016-09-07T01:21:04.005903: step 354, loss 0.464458, acc 0.84
2016-09-07T01:21:04.703335: step 355, loss 0.343484, acc 0.8
2016-09-07T01:21:05.386633: step 356, loss 0.268311, acc 0.92
2016-09-07T01:21:06.084277: step 357, loss 0.499063, acc 0.72
2016-09-07T01:21:06.761878: step 358, loss 0.491774, acc 0.78
2016-09-07T01:21:07.469734: step 359, loss 0.290095, acc 0.9
2016-09-07T01:21:08.160113: step 360, loss 0.497721, acc 0.78
2016-09-07T01:21:08.860292: step 361, loss 0.36769, acc 0.78
2016-09-07T01:21:09.545751: step 362, loss 0.246998, acc 0.88
2016-09-07T01:21:10.227435: step 363, loss 0.261312, acc 0.88
2016-09-07T01:21:10.938823: step 364, loss 0.375992, acc 0.82
2016-09-07T01:21:11.627144: step 365, loss 0.417339, acc 0.84
2016-09-07T01:21:12.320460: step 366, loss 0.499603, acc 0.76
2016-09-07T01:21:13.012502: step 367, loss 0.439473, acc 0.78
2016-09-07T01:21:13.704720: step 368, loss 0.460951, acc 0.72
2016-09-07T01:21:14.394763: step 369, loss 0.453803, acc 0.82
2016-09-07T01:21:15.060755: step 370, loss 0.368543, acc 0.84
2016-09-07T01:21:15.755035: step 371, loss 0.402526, acc 0.82
2016-09-07T01:21:16.442154: step 372, loss 0.498673, acc 0.76
2016-09-07T01:21:17.113458: step 373, loss 0.483422, acc 0.7
2016-09-07T01:21:17.825144: step 374, loss 0.494305, acc 0.76
2016-09-07T01:21:18.510364: step 375, loss 0.364862, acc 0.82
2016-09-07T01:21:19.207540: step 376, loss 0.336176, acc 0.8
2016-09-07T01:21:19.904690: step 377, loss 0.362965, acc 0.86
2016-09-07T01:21:20.618925: step 378, loss 0.266452, acc 0.88
2016-09-07T01:21:21.337744: step 379, loss 0.319399, acc 0.86
2016-09-07T01:21:22.039491: step 380, loss 0.431982, acc 0.78
2016-09-07T01:21:22.739899: step 381, loss 0.52588, acc 0.76
2016-09-07T01:21:23.424819: step 382, loss 0.383711, acc 0.84
2016-09-07T01:21:24.138894: step 383, loss 0.349225, acc 0.82
2016-09-07T01:21:24.760993: step 384, loss 0.322036, acc 0.863636
2016-09-07T01:21:25.491396: step 385, loss 0.364872, acc 0.84
2016-09-07T01:21:26.176790: step 386, loss 0.206432, acc 0.92
2016-09-07T01:21:26.864115: step 387, loss 0.336013, acc 0.84
2016-09-07T01:21:27.547570: step 388, loss 0.181941, acc 0.94
2016-09-07T01:21:28.225535: step 389, loss 0.228434, acc 0.84
2016-09-07T01:21:28.947867: step 390, loss 0.327411, acc 0.9
2016-09-07T01:21:29.617079: step 391, loss 0.245517, acc 0.88
2016-09-07T01:21:30.286544: step 392, loss 0.264959, acc 0.9
2016-09-07T01:21:30.982929: step 393, loss 0.221047, acc 0.9
2016-09-07T01:21:31.670676: step 394, loss 0.146617, acc 0.9
2016-09-07T01:21:32.371853: step 395, loss 0.0547987, acc 1
2016-09-07T01:21:33.057431: step 396, loss 0.210459, acc 0.92
2016-09-07T01:21:33.762915: step 397, loss 0.253249, acc 0.9
2016-09-07T01:21:34.472434: step 398, loss 0.145271, acc 0.94
2016-09-07T01:21:35.156133: step 399, loss 0.387381, acc 0.76
2016-09-07T01:21:35.856172: step 400, loss 0.193471, acc 0.9

Evaluation:
2016-09-07T01:21:39.053074: step 400, loss 0.540385, acc 0.792683

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-400

2016-09-07T01:21:40.754687: step 401, loss 0.139704, acc 0.9
2016-09-07T01:21:41.448101: step 402, loss 0.216011, acc 0.92
2016-09-07T01:21:42.152269: step 403, loss 0.235244, acc 0.86
2016-09-07T01:21:42.838500: step 404, loss 0.308137, acc 0.86
2016-09-07T01:21:43.519064: step 405, loss 0.272335, acc 0.9
2016-09-07T01:21:44.215427: step 406, loss 0.304594, acc 0.86
2016-09-07T01:21:44.934282: step 407, loss 0.220637, acc 0.88
2016-09-07T01:21:45.633846: step 408, loss 0.249873, acc 0.9
2016-09-07T01:21:46.314432: step 409, loss 0.443258, acc 0.88
2016-09-07T01:21:47.027832: step 410, loss 0.174658, acc 0.9
2016-09-07T01:21:47.734019: step 411, loss 0.09959, acc 0.96
2016-09-07T01:21:48.425633: step 412, loss 0.247399, acc 0.86
2016-09-07T01:21:49.106663: step 413, loss 0.260079, acc 0.92
2016-09-07T01:21:49.792067: step 414, loss 0.282529, acc 0.86
2016-09-07T01:21:50.483798: step 415, loss 0.204469, acc 0.88
2016-09-07T01:21:51.143970: step 416, loss 0.188923, acc 0.92
2016-09-07T01:21:51.844477: step 417, loss 0.309868, acc 0.82
2016-09-07T01:21:52.542630: step 418, loss 0.278082, acc 0.8
2016-09-07T01:21:53.222881: step 419, loss 0.162633, acc 0.92
2016-09-07T01:21:53.918142: step 420, loss 0.231943, acc 0.9
2016-09-07T01:21:54.596266: step 421, loss 0.22004, acc 0.92
2016-09-07T01:21:55.293595: step 422, loss 0.167454, acc 0.92
2016-09-07T01:21:55.987202: step 423, loss 0.240993, acc 0.9
2016-09-07T01:21:56.686081: step 424, loss 0.374634, acc 0.84
2016-09-07T01:21:57.374190: step 425, loss 0.277032, acc 0.82
2016-09-07T01:21:58.052492: step 426, loss 0.255229, acc 0.86
2016-09-07T01:21:58.749588: step 427, loss 0.290857, acc 0.9
2016-09-07T01:21:59.422958: step 428, loss 0.184013, acc 0.92
2016-09-07T01:22:00.105990: step 429, loss 0.181275, acc 0.94
2016-09-07T01:22:00.824866: step 430, loss 0.252297, acc 0.86
2016-09-07T01:22:01.530940: step 431, loss 0.194636, acc 0.94
2016-09-07T01:22:02.214433: step 432, loss 0.198484, acc 0.94
2016-09-07T01:22:02.892517: step 433, loss 0.341499, acc 0.88
2016-09-07T01:22:03.573726: step 434, loss 0.2446, acc 0.9
2016-09-07T01:22:04.262871: step 435, loss 0.2208, acc 0.88
2016-09-07T01:22:04.979905: step 436, loss 0.163879, acc 0.92
2016-09-07T01:22:05.668112: step 437, loss 0.372028, acc 0.76
2016-09-07T01:22:06.350841: step 438, loss 0.270395, acc 0.86
2016-09-07T01:22:07.044234: step 439, loss 0.214849, acc 0.88
2016-09-07T01:22:07.704567: step 440, loss 0.213349, acc 0.92
2016-09-07T01:22:08.404086: step 441, loss 0.308529, acc 0.88
2016-09-07T01:22:09.082534: step 442, loss 0.21442, acc 0.92
2016-09-07T01:22:09.790735: step 443, loss 0.236951, acc 0.88
2016-09-07T01:22:10.468461: step 444, loss 0.249882, acc 0.9
2016-09-07T01:22:11.138627: step 445, loss 0.243099, acc 0.84
2016-09-07T01:22:11.816660: step 446, loss 0.107853, acc 0.96
2016-09-07T01:22:12.505888: step 447, loss 0.14598, acc 0.94
2016-09-07T01:22:13.219383: step 448, loss 0.364621, acc 0.88
2016-09-07T01:22:13.891848: step 449, loss 0.142176, acc 0.94
2016-09-07T01:22:14.593514: step 450, loss 0.230074, acc 0.9
2016-09-07T01:22:15.278271: step 451, loss 0.249428, acc 0.9
2016-09-07T01:22:15.970405: step 452, loss 0.402448, acc 0.82
2016-09-07T01:22:16.670460: step 453, loss 0.271259, acc 0.94
2016-09-07T01:22:17.357660: step 454, loss 0.2758, acc 0.88
2016-09-07T01:22:18.043887: step 455, loss 0.381478, acc 0.86
2016-09-07T01:22:18.729285: step 456, loss 0.244664, acc 0.88
2016-09-07T01:22:19.447820: step 457, loss 0.17262, acc 0.88
2016-09-07T01:22:20.120391: step 458, loss 0.125313, acc 0.96
2016-09-07T01:22:20.807880: step 459, loss 0.246731, acc 0.9
2016-09-07T01:22:21.493771: step 460, loss 0.264401, acc 0.88
2016-09-07T01:22:22.167422: step 461, loss 0.221566, acc 0.88
2016-09-07T01:22:22.874019: step 462, loss 0.266054, acc 0.88
2016-09-07T01:22:23.565474: step 463, loss 0.259841, acc 0.86
2016-09-07T01:22:24.275631: step 464, loss 0.201975, acc 0.94
2016-09-07T01:22:24.941845: step 465, loss 0.276892, acc 0.84
2016-09-07T01:22:25.622288: step 466, loss 0.220191, acc 0.88
2016-09-07T01:22:26.318364: step 467, loss 0.186686, acc 0.92
2016-09-07T01:22:27.021368: step 468, loss 0.158735, acc 0.94
2016-09-07T01:22:27.730398: step 469, loss 0.29111, acc 0.84
2016-09-07T01:22:28.460088: step 470, loss 0.188117, acc 0.92
2016-09-07T01:22:29.164319: step 471, loss 0.172231, acc 0.96
2016-09-07T01:22:29.862628: step 472, loss 0.200735, acc 0.92
2016-09-07T01:22:30.556199: step 473, loss 0.254497, acc 0.88
2016-09-07T01:22:31.267423: step 474, loss 0.216752, acc 0.92
2016-09-07T01:22:31.965610: step 475, loss 0.202327, acc 0.94
2016-09-07T01:22:32.669380: step 476, loss 0.222986, acc 0.92
2016-09-07T01:22:33.339296: step 477, loss 0.414861, acc 0.82
2016-09-07T01:22:34.028854: step 478, loss 0.424801, acc 0.88
2016-09-07T01:22:34.720495: step 479, loss 0.239261, acc 0.88
2016-09-07T01:22:35.410432: step 480, loss 0.194764, acc 0.9
2016-09-07T01:22:36.091544: step 481, loss 0.164789, acc 0.92
2016-09-07T01:22:36.770724: step 482, loss 0.259338, acc 0.9
2016-09-07T01:22:37.458395: step 483, loss 0.249769, acc 0.9
2016-09-07T01:22:38.145750: step 484, loss 0.184107, acc 0.9
2016-09-07T01:22:38.852971: step 485, loss 0.195127, acc 0.88
2016-09-07T01:22:39.551344: step 486, loss 0.350748, acc 0.86
2016-09-07T01:22:40.243550: step 487, loss 0.239172, acc 0.9
2016-09-07T01:22:40.931348: step 488, loss 0.223805, acc 0.88
2016-09-07T01:22:41.600774: step 489, loss 0.215548, acc 0.9
2016-09-07T01:22:42.317359: step 490, loss 0.255977, acc 0.88
2016-09-07T01:22:42.990808: step 491, loss 0.241356, acc 0.86
2016-09-07T01:22:43.683080: step 492, loss 0.476263, acc 0.82
2016-09-07T01:22:44.356601: step 493, loss 0.337658, acc 0.86
2016-09-07T01:22:45.057535: step 494, loss 0.273361, acc 0.94
2016-09-07T01:22:45.747311: step 495, loss 0.40063, acc 0.78
2016-09-07T01:22:46.451229: step 496, loss 0.163686, acc 0.98
2016-09-07T01:22:47.140290: step 497, loss 0.152025, acc 0.94
2016-09-07T01:22:47.843353: step 498, loss 0.339319, acc 0.82
2016-09-07T01:22:48.522218: step 499, loss 0.10851, acc 0.98
2016-09-07T01:22:49.218215: step 500, loss 0.195635, acc 0.9

Evaluation:
2016-09-07T01:22:52.465193: step 500, loss 0.478266, acc 0.787992

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-500

2016-09-07T01:22:54.167172: step 501, loss 0.17975, acc 0.94
2016-09-07T01:22:54.843510: step 502, loss 0.154078, acc 0.94
2016-09-07T01:22:55.540039: step 503, loss 0.370767, acc 0.88
2016-09-07T01:22:56.228470: step 504, loss 0.339849, acc 0.88
2016-09-07T01:22:56.923610: step 505, loss 0.19932, acc 0.92
2016-09-07T01:22:57.608449: step 506, loss 0.341047, acc 0.88
2016-09-07T01:22:58.306562: step 507, loss 0.334708, acc 0.86
2016-09-07T01:22:59.014131: step 508, loss 0.243577, acc 0.9
2016-09-07T01:22:59.707485: step 509, loss 0.0957987, acc 1
2016-09-07T01:23:00.445381: step 510, loss 0.158492, acc 0.92
2016-09-07T01:23:01.136731: step 511, loss 0.18276, acc 0.9
2016-09-07T01:23:01.856413: step 512, loss 0.142094, acc 0.92
2016-09-07T01:23:02.526384: step 513, loss 0.315377, acc 0.78
2016-09-07T01:23:03.212951: step 514, loss 0.379563, acc 0.84
2016-09-07T01:23:03.903750: step 515, loss 0.213709, acc 0.92
2016-09-07T01:23:04.580571: step 516, loss 0.247233, acc 0.88
2016-09-07T01:23:05.260148: step 517, loss 0.305142, acc 0.86
2016-09-07T01:23:05.973706: step 518, loss 0.158323, acc 0.88
2016-09-07T01:23:06.669924: step 519, loss 0.25715, acc 0.86
2016-09-07T01:23:07.378471: step 520, loss 0.269972, acc 0.9
2016-09-07T01:23:08.050672: step 521, loss 0.333603, acc 0.84
2016-09-07T01:23:08.745315: step 522, loss 0.241405, acc 0.9
2016-09-07T01:23:09.429377: step 523, loss 0.350017, acc 0.88
2016-09-07T01:23:10.096231: step 524, loss 0.352874, acc 0.9
2016-09-07T01:23:10.784544: step 525, loss 0.333111, acc 0.82
2016-09-07T01:23:11.483203: step 526, loss 0.469412, acc 0.82
2016-09-07T01:23:12.181030: step 527, loss 0.147991, acc 0.96
2016-09-07T01:23:12.851876: step 528, loss 0.274103, acc 0.94
2016-09-07T01:23:13.549733: step 529, loss 0.303672, acc 0.92
2016-09-07T01:23:14.217116: step 530, loss 0.341769, acc 0.84
2016-09-07T01:23:14.919335: step 531, loss 0.224838, acc 0.9
2016-09-07T01:23:15.595479: step 532, loss 0.285326, acc 0.88
2016-09-07T01:23:16.292562: step 533, loss 0.313402, acc 0.86
2016-09-07T01:23:16.984336: step 534, loss 0.209492, acc 0.9
2016-09-07T01:23:17.647094: step 535, loss 0.27456, acc 0.86
2016-09-07T01:23:18.349613: step 536, loss 0.261081, acc 0.86
2016-09-07T01:23:19.045514: step 537, loss 0.266454, acc 0.9
2016-09-07T01:23:19.737405: step 538, loss 0.256505, acc 0.94
2016-09-07T01:23:20.461815: step 539, loss 0.164883, acc 0.96
2016-09-07T01:23:21.148016: step 540, loss 0.186607, acc 0.9
2016-09-07T01:23:21.846541: step 541, loss 0.239059, acc 0.86
2016-09-07T01:23:22.516026: step 542, loss 0.265743, acc 0.9
2016-09-07T01:23:23.204425: step 543, loss 0.249341, acc 0.88
2016-09-07T01:23:23.879153: step 544, loss 0.526781, acc 0.8
2016-09-07T01:23:24.554713: step 545, loss 0.152407, acc 0.92
2016-09-07T01:23:25.238135: step 546, loss 0.26689, acc 0.9
2016-09-07T01:23:25.934411: step 547, loss 0.178281, acc 0.94
2016-09-07T01:23:26.634506: step 548, loss 0.243336, acc 0.84
2016-09-07T01:23:27.308172: step 549, loss 0.34045, acc 0.88
2016-09-07T01:23:28.021162: step 550, loss 0.17662, acc 0.94
2016-09-07T01:23:28.715528: step 551, loss 0.340427, acc 0.88
2016-09-07T01:23:29.410012: step 552, loss 0.192768, acc 0.92
2016-09-07T01:23:30.096923: step 553, loss 0.259183, acc 0.94
2016-09-07T01:23:30.786597: step 554, loss 0.445708, acc 0.82
2016-09-07T01:23:31.488036: step 555, loss 0.256038, acc 0.88
2016-09-07T01:23:32.165976: step 556, loss 0.416155, acc 0.84
2016-09-07T01:23:32.893716: step 557, loss 0.308063, acc 0.84
2016-09-07T01:23:33.610372: step 558, loss 0.281577, acc 0.9
2016-09-07T01:23:34.315638: step 559, loss 0.337984, acc 0.86
2016-09-07T01:23:35.013200: step 560, loss 0.271444, acc 0.86
2016-09-07T01:23:35.696326: step 561, loss 0.264238, acc 0.9
2016-09-07T01:23:36.406924: step 562, loss 0.249352, acc 0.88
2016-09-07T01:23:37.095006: step 563, loss 0.251016, acc 0.92
2016-09-07T01:23:37.782267: step 564, loss 0.327252, acc 0.86
2016-09-07T01:23:38.469093: step 565, loss 0.16355, acc 0.9
2016-09-07T01:23:39.167926: step 566, loss 0.145863, acc 0.98
2016-09-07T01:23:39.869055: step 567, loss 0.169019, acc 0.96
2016-09-07T01:23:40.562313: step 568, loss 0.271539, acc 0.9
2016-09-07T01:23:41.262882: step 569, loss 0.267978, acc 0.88
2016-09-07T01:23:41.959441: step 570, loss 0.308271, acc 0.8
2016-09-07T01:23:42.654943: step 571, loss 0.187586, acc 0.9
2016-09-07T01:23:43.336636: step 572, loss 0.128807, acc 0.92
2016-09-07T01:23:44.014052: step 573, loss 0.409468, acc 0.88
2016-09-07T01:23:44.698408: step 574, loss 0.301766, acc 0.84
2016-09-07T01:23:45.363385: step 575, loss 0.269127, acc 0.86
2016-09-07T01:23:46.032326: step 576, loss 0.177501, acc 0.931818
2016-09-07T01:23:46.724483: step 577, loss 0.208532, acc 0.9
2016-09-07T01:23:47.448013: step 578, loss 0.189076, acc 0.92
2016-09-07T01:23:48.146109: step 579, loss 0.257248, acc 0.88
2016-09-07T01:23:48.830795: step 580, loss 0.171711, acc 0.88
2016-09-07T01:23:49.536378: step 581, loss 0.154463, acc 0.92
2016-09-07T01:23:50.213434: step 582, loss 0.217749, acc 0.88
2016-09-07T01:23:50.931340: step 583, loss 0.195761, acc 0.96
2016-09-07T01:23:51.622314: step 584, loss 0.102591, acc 0.98
2016-09-07T01:23:52.305540: step 585, loss 0.146552, acc 0.92
2016-09-07T01:23:52.971413: step 586, loss 0.29749, acc 0.82
2016-09-07T01:23:53.648793: step 587, loss 0.175134, acc 0.96
2016-09-07T01:23:54.368574: step 588, loss 0.117332, acc 0.96
2016-09-07T01:23:55.064991: step 589, loss 0.268404, acc 0.9
2016-09-07T01:23:55.763615: step 590, loss 0.098156, acc 0.98
2016-09-07T01:23:56.427318: step 591, loss 0.177945, acc 0.92
2016-09-07T01:23:57.120743: step 592, loss 0.173528, acc 0.94
2016-09-07T01:23:57.805084: step 593, loss 0.17146, acc 0.92
2016-09-07T01:23:58.517951: step 594, loss 0.248647, acc 0.9
2016-09-07T01:23:59.231985: step 595, loss 0.100416, acc 0.96
2016-09-07T01:23:59.928125: step 596, loss 0.127565, acc 0.94
2016-09-07T01:24:00.641036: step 597, loss 0.119591, acc 0.94
2016-09-07T01:24:01.327793: step 598, loss 0.165855, acc 0.94
2016-09-07T01:24:02.011805: step 599, loss 0.178756, acc 0.9
2016-09-07T01:24:02.709237: step 600, loss 0.0826715, acc 0.98

Evaluation:
2016-09-07T01:24:05.987379: step 600, loss 0.559526, acc 0.777674

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-600

2016-09-07T01:24:07.666758: step 601, loss 0.193835, acc 0.94
2016-09-07T01:24:08.344330: step 602, loss 0.21816, acc 0.94
2016-09-07T01:24:09.034442: step 603, loss 0.11621, acc 0.96
2016-09-07T01:24:09.725741: step 604, loss 0.0717582, acc 0.96
2016-09-07T01:24:10.428642: step 605, loss 0.180063, acc 0.94
2016-09-07T01:24:11.116137: step 606, loss 0.0781983, acc 1
2016-09-07T01:24:11.770689: step 607, loss 0.133949, acc 0.92
2016-09-07T01:24:12.473703: step 608, loss 0.219137, acc 0.88
2016-09-07T01:24:13.154819: step 609, loss 0.302127, acc 0.86
2016-09-07T01:24:13.841635: step 610, loss 0.0868685, acc 0.96
2016-09-07T01:24:14.543707: step 611, loss 0.147411, acc 0.94
2016-09-07T01:24:15.236962: step 612, loss 0.144449, acc 0.92
2016-09-07T01:24:15.911892: step 613, loss 0.201957, acc 0.9
2016-09-07T01:24:16.586300: step 614, loss 0.153502, acc 0.96
2016-09-07T01:24:17.295418: step 615, loss 0.258301, acc 0.88
2016-09-07T01:24:17.988537: step 616, loss 0.133702, acc 0.96
2016-09-07T01:24:18.665949: step 617, loss 0.112067, acc 0.96
2016-09-07T01:24:19.381133: step 618, loss 0.141364, acc 0.94
2016-09-07T01:24:20.077337: step 619, loss 0.098498, acc 0.94
2016-09-07T01:24:20.765634: step 620, loss 0.205121, acc 0.92
2016-09-07T01:24:21.433383: step 621, loss 0.405221, acc 0.84
2016-09-07T01:24:22.132365: step 622, loss 0.189096, acc 0.92
2016-09-07T01:24:22.825662: step 623, loss 0.141625, acc 0.94
2016-09-07T01:24:23.525891: step 624, loss 0.114127, acc 0.94
2016-09-07T01:24:24.199363: step 625, loss 0.191311, acc 0.94
2016-09-07T01:24:24.904001: step 626, loss 0.167151, acc 0.98
2016-09-07T01:24:25.611822: step 627, loss 0.119059, acc 0.96
2016-09-07T01:24:26.290478: step 628, loss 0.130989, acc 0.94
2016-09-07T01:24:27.008936: step 629, loss 0.151379, acc 0.92
2016-09-07T01:24:27.716005: step 630, loss 0.251311, acc 0.92
2016-09-07T01:24:28.417000: step 631, loss 0.0816498, acc 1
2016-09-07T01:24:29.106362: step 632, loss 0.123548, acc 0.96
2016-09-07T01:24:29.818567: step 633, loss 0.146231, acc 0.96
2016-09-07T01:24:30.530661: step 634, loss 0.127172, acc 0.94
2016-09-07T01:24:31.213163: step 635, loss 0.119294, acc 0.96
2016-09-07T01:24:31.925470: step 636, loss 0.159166, acc 0.94
2016-09-07T01:24:32.612162: step 637, loss 0.271492, acc 0.84
2016-09-07T01:24:33.320246: step 638, loss 0.0953698, acc 0.96
2016-09-07T01:24:34.011374: step 639, loss 0.210914, acc 0.9
2016-09-07T01:24:34.671618: step 640, loss 0.173349, acc 0.94
2016-09-07T01:24:35.371398: step 641, loss 0.120215, acc 0.98
2016-09-07T01:24:36.099157: step 642, loss 0.0795932, acc 0.98
2016-09-07T01:24:36.787367: step 643, loss 0.0650067, acc 0.98
2016-09-07T01:24:37.476840: step 644, loss 0.266523, acc 0.94
2016-09-07T01:24:38.152930: step 645, loss 0.0652697, acc 0.98
2016-09-07T01:24:38.841218: step 646, loss 0.102186, acc 0.98
2016-09-07T01:24:39.521948: step 647, loss 0.261602, acc 0.88
2016-09-07T01:24:40.239682: step 648, loss 0.247837, acc 0.9
2016-09-07T01:24:40.943861: step 649, loss 0.111973, acc 0.94
2016-09-07T01:24:41.628624: step 650, loss 0.264079, acc 0.92
2016-09-07T01:24:42.312745: step 651, loss 0.425715, acc 0.88
2016-09-07T01:24:43.012507: step 652, loss 0.212926, acc 0.88
2016-09-07T01:24:43.716129: step 653, loss 0.118742, acc 0.92
2016-09-07T01:24:44.390537: step 654, loss 0.246232, acc 0.9
2016-09-07T01:24:45.069869: step 655, loss 0.175054, acc 0.94
2016-09-07T01:24:45.763330: step 656, loss 0.138924, acc 0.94
2016-09-07T01:24:46.470721: step 657, loss 0.285227, acc 0.92
2016-09-07T01:24:47.157905: step 658, loss 0.126416, acc 0.94
2016-09-07T01:24:47.843640: step 659, loss 0.100861, acc 0.98
2016-09-07T01:24:48.557817: step 660, loss 0.315753, acc 0.9
2016-09-07T01:24:49.236912: step 661, loss 0.173913, acc 0.94
2016-09-07T01:24:49.937097: step 662, loss 0.311411, acc 0.82
2016-09-07T01:24:50.634021: step 663, loss 0.148153, acc 0.94
2016-09-07T01:24:51.314788: step 664, loss 0.121064, acc 0.94
2016-09-07T01:24:52.014894: step 665, loss 0.129406, acc 0.96
2016-09-07T01:24:52.693551: step 666, loss 0.189838, acc 0.92
2016-09-07T01:24:53.403533: step 667, loss 0.200043, acc 0.92
2016-09-07T01:24:54.103273: step 668, loss 0.0980628, acc 0.98
2016-09-07T01:24:54.783134: step 669, loss 0.169034, acc 0.9
2016-09-07T01:24:55.473279: step 670, loss 0.148916, acc 0.94
2016-09-07T01:24:56.156360: step 671, loss 0.130947, acc 0.92
2016-09-07T01:24:56.864142: step 672, loss 0.163611, acc 0.96
2016-09-07T01:24:57.540987: step 673, loss 0.118208, acc 0.98
2016-09-07T01:24:58.252191: step 674, loss 0.253889, acc 0.9
2016-09-07T01:24:58.936010: step 675, loss 0.32578, acc 0.9
2016-09-07T01:24:59.621846: step 676, loss 0.296893, acc 0.88
2016-09-07T01:25:00.370768: step 677, loss 0.0596097, acc 0.98
2016-09-07T01:25:01.057697: step 678, loss 0.133494, acc 0.94
2016-09-07T01:25:01.764473: step 679, loss 0.238251, acc 0.9
2016-09-07T01:25:02.439579: step 680, loss 0.14651, acc 0.96
2016-09-07T01:25:03.131863: step 681, loss 0.187602, acc 0.92
2016-09-07T01:25:03.844355: step 682, loss 0.256342, acc 0.86
2016-09-07T01:25:04.557284: step 683, loss 0.243831, acc 0.9
2016-09-07T01:25:05.270266: step 684, loss 0.33369, acc 0.88
2016-09-07T01:25:05.943207: step 685, loss 0.398363, acc 0.84
2016-09-07T01:25:06.677389: step 686, loss 0.105934, acc 0.96
2016-09-07T01:25:07.366114: step 687, loss 0.238654, acc 0.9
2016-09-07T01:25:08.054020: step 688, loss 0.120414, acc 0.96
2016-09-07T01:25:08.747028: step 689, loss 0.196612, acc 0.9
2016-09-07T01:25:09.450851: step 690, loss 0.143817, acc 0.96
2016-09-07T01:25:10.157595: step 691, loss 0.186919, acc 0.92
2016-09-07T01:25:10.830238: step 692, loss 0.163468, acc 0.96
2016-09-07T01:25:11.510393: step 693, loss 0.215475, acc 0.9
2016-09-07T01:25:12.204643: step 694, loss 0.168255, acc 0.92
2016-09-07T01:25:12.907186: step 695, loss 0.236292, acc 0.92
2016-09-07T01:25:13.629140: step 696, loss 0.17155, acc 0.88
2016-09-07T01:25:14.309018: step 697, loss 0.296223, acc 0.86
2016-09-07T01:25:15.042562: step 698, loss 0.195174, acc 0.9
2016-09-07T01:25:15.733054: step 699, loss 0.0885204, acc 0.98
2016-09-07T01:25:16.429323: step 700, loss 0.169331, acc 0.96

Evaluation:
2016-09-07T01:25:19.788078: step 700, loss 0.544631, acc 0.772983

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-700

2016-09-07T01:25:21.534647: step 701, loss 0.257912, acc 0.88
2016-09-07T01:25:22.238475: step 702, loss 0.129193, acc 0.94
2016-09-07T01:25:22.918079: step 703, loss 0.226778, acc 0.92
2016-09-07T01:25:23.607000: step 704, loss 0.148256, acc 0.92
2016-09-07T01:25:24.323080: step 705, loss 0.209826, acc 0.92
2016-09-07T01:25:25.020469: step 706, loss 0.245915, acc 0.92
2016-09-07T01:25:25.722307: step 707, loss 0.248121, acc 0.9
2016-09-07T01:25:26.391634: step 708, loss 0.103152, acc 0.94
2016-09-07T01:25:27.102565: step 709, loss 0.171729, acc 0.9
2016-09-07T01:25:27.797136: step 710, loss 0.221069, acc 0.94
2016-09-07T01:25:28.503229: step 711, loss 0.0719261, acc 0.96
2016-09-07T01:25:29.193907: step 712, loss 0.0926313, acc 0.98
2016-09-07T01:25:29.875541: step 713, loss 0.190101, acc 0.92
2016-09-07T01:25:30.556885: step 714, loss 0.146668, acc 0.92
2016-09-07T01:25:31.218969: step 715, loss 0.304802, acc 0.92
2016-09-07T01:25:31.935155: step 716, loss 0.175106, acc 0.92
2016-09-07T01:25:32.606996: step 717, loss 0.20321, acc 0.94
2016-09-07T01:25:33.299197: step 718, loss 0.144929, acc 0.92
2016-09-07T01:25:33.982696: step 719, loss 0.172894, acc 0.88
2016-09-07T01:25:34.662433: step 720, loss 0.391126, acc 0.8
2016-09-07T01:25:35.358973: step 721, loss 0.337867, acc 0.82
2016-09-07T01:25:36.012548: step 722, loss 0.265276, acc 0.88
2016-09-07T01:25:36.736743: step 723, loss 0.234432, acc 0.88
2016-09-07T01:25:37.442654: step 724, loss 0.136499, acc 0.92
2016-09-07T01:25:38.123756: step 725, loss 0.215614, acc 0.94
2016-09-07T01:25:38.814357: step 726, loss 0.217796, acc 0.92
2016-09-07T01:25:39.503397: step 727, loss 0.129578, acc 0.94
2016-09-07T01:25:40.209545: step 728, loss 0.0881452, acc 0.96
2016-09-07T01:25:40.883735: step 729, loss 0.0879134, acc 0.98
2016-09-07T01:25:41.582953: step 730, loss 0.144996, acc 0.94
2016-09-07T01:25:42.271772: step 731, loss 0.357878, acc 0.84
2016-09-07T01:25:42.944824: step 732, loss 0.155865, acc 0.92
2016-09-07T01:25:43.654874: step 733, loss 0.165056, acc 0.94
2016-09-07T01:25:44.331467: step 734, loss 0.182418, acc 0.9
2016-09-07T01:25:45.025739: step 735, loss 0.228678, acc 0.86
2016-09-07T01:25:45.693872: step 736, loss 0.151472, acc 0.96
2016-09-07T01:25:46.403614: step 737, loss 0.12284, acc 0.94
2016-09-07T01:25:47.087207: step 738, loss 0.121007, acc 0.94
2016-09-07T01:25:47.780599: step 739, loss 0.148079, acc 0.94
2016-09-07T01:25:48.474335: step 740, loss 0.108644, acc 0.98
2016-09-07T01:25:49.172968: step 741, loss 0.116105, acc 0.94
2016-09-07T01:25:49.883334: step 742, loss 0.245344, acc 0.94
2016-09-07T01:25:50.559161: step 743, loss 0.177324, acc 0.94
2016-09-07T01:25:51.245209: step 744, loss 0.15967, acc 0.92
2016-09-07T01:25:51.924751: step 745, loss 0.299272, acc 0.86
2016-09-07T01:25:52.629429: step 746, loss 0.104579, acc 0.96
2016-09-07T01:25:53.312518: step 747, loss 0.221305, acc 0.94
2016-09-07T01:25:54.014278: step 748, loss 0.0806873, acc 0.96
2016-09-07T01:25:54.721750: step 749, loss 0.136559, acc 0.94
2016-09-07T01:25:55.374892: step 750, loss 0.172243, acc 0.94
2016-09-07T01:25:56.059561: step 751, loss 0.220382, acc 0.92
2016-09-07T01:25:56.736564: step 752, loss 0.230709, acc 0.92
2016-09-07T01:25:57.415445: step 753, loss 0.416335, acc 0.82
2016-09-07T01:25:58.108418: step 754, loss 0.101052, acc 0.94
2016-09-07T01:25:58.811473: step 755, loss 0.14613, acc 0.92
2016-09-07T01:25:59.520343: step 756, loss 0.220812, acc 0.88
2016-09-07T01:26:00.216903: step 757, loss 0.0323515, acc 0.98
2016-09-07T01:26:00.906454: step 758, loss 0.0799626, acc 1
2016-09-07T01:26:01.587483: step 759, loss 0.115711, acc 0.96
2016-09-07T01:26:02.284467: step 760, loss 0.363711, acc 0.86
2016-09-07T01:26:02.981436: step 761, loss 0.217252, acc 0.88
2016-09-07T01:26:03.645416: step 762, loss 0.176553, acc 0.94
2016-09-07T01:26:04.339874: step 763, loss 0.191569, acc 0.92
2016-09-07T01:26:05.013762: step 764, loss 0.211633, acc 0.92
2016-09-07T01:26:05.704616: step 765, loss 0.127105, acc 0.96
2016-09-07T01:26:06.383458: step 766, loss 0.262595, acc 0.9
2016-09-07T01:26:07.071177: step 767, loss 0.167246, acc 0.92
2016-09-07T01:26:07.688067: step 768, loss 0.131447, acc 0.909091
2016-09-07T01:26:08.388296: step 769, loss 0.0967174, acc 0.96
2016-09-07T01:26:09.101356: step 770, loss 0.197931, acc 0.92
2016-09-07T01:26:09.818575: step 771, loss 0.170351, acc 0.94
2016-09-07T01:26:10.511354: step 772, loss 0.115119, acc 0.96
2016-09-07T01:26:11.202470: step 773, loss 0.08951, acc 0.96
2016-09-07T01:26:11.880952: step 774, loss 0.195392, acc 0.9
2016-09-07T01:26:12.579518: step 775, loss 0.0849524, acc 0.96
2016-09-07T01:26:13.273067: step 776, loss 0.150026, acc 0.94
2016-09-07T01:26:13.977274: step 777, loss 0.1065, acc 0.94
2016-09-07T01:26:14.677781: step 778, loss 0.0677868, acc 0.98
2016-09-07T01:26:15.371535: step 779, loss 0.116821, acc 0.94
2016-09-07T01:26:16.077652: step 780, loss 0.172266, acc 0.9
2016-09-07T01:26:16.778974: step 781, loss 0.124163, acc 0.96
2016-09-07T01:26:17.452286: step 782, loss 0.209974, acc 0.94
2016-09-07T01:26:18.129514: step 783, loss 0.0528119, acc 0.98
2016-09-07T01:26:18.823989: step 784, loss 0.106024, acc 0.94
2016-09-07T01:26:19.517301: step 785, loss 0.105935, acc 0.96
2016-09-07T01:26:20.199708: step 786, loss 0.248231, acc 0.94
2016-09-07T01:26:20.910721: step 787, loss 0.174079, acc 0.92
2016-09-07T01:26:21.598408: step 788, loss 0.0767182, acc 0.96
2016-09-07T01:26:22.310476: step 789, loss 0.034692, acc 1
2016-09-07T01:26:22.974232: step 790, loss 0.136418, acc 0.92
2016-09-07T01:26:23.680188: step 791, loss 0.0705188, acc 0.98
2016-09-07T01:26:24.392612: step 792, loss 0.0338938, acc 0.98
2016-09-07T01:26:25.082755: step 793, loss 0.117487, acc 0.94
2016-09-07T01:26:25.776157: step 794, loss 0.139526, acc 0.96
2016-09-07T01:26:26.465806: step 795, loss 0.0571715, acc 0.98
2016-09-07T01:26:27.180175: step 796, loss 0.192806, acc 0.94
2016-09-07T01:26:27.865715: step 797, loss 0.129966, acc 0.94
2016-09-07T01:26:28.544259: step 798, loss 0.0673657, acc 0.98
2016-09-07T01:26:29.248930: step 799, loss 0.100192, acc 0.98
2016-09-07T01:26:29.939122: step 800, loss 0.154782, acc 0.92

Evaluation:
2016-09-07T01:26:33.210853: step 800, loss 0.606283, acc 0.786116

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-800

2016-09-07T01:26:34.967968: step 801, loss 0.0785279, acc 0.98
2016-09-07T01:26:35.667779: step 802, loss 0.0704319, acc 0.98
2016-09-07T01:26:36.362057: step 803, loss 0.157887, acc 0.92
2016-09-07T01:26:37.050116: step 804, loss 0.150674, acc 0.96
2016-09-07T01:26:37.744976: step 805, loss 0.139478, acc 0.92
2016-09-07T01:26:38.418422: step 806, loss 0.165658, acc 0.94
2016-09-07T01:26:39.128915: step 807, loss 0.130429, acc 0.9
2016-09-07T01:26:39.796251: step 808, loss 0.139282, acc 0.96
2016-09-07T01:26:40.510289: step 809, loss 0.188644, acc 0.9
2016-09-07T01:26:41.221226: step 810, loss 0.348953, acc 0.82
2016-09-07T01:26:41.930748: step 811, loss 0.178418, acc 0.94
2016-09-07T01:26:42.631839: step 812, loss 0.0862386, acc 0.98
2016-09-07T01:26:43.311429: step 813, loss 0.0767204, acc 0.96
2016-09-07T01:26:44.038263: step 814, loss 0.0769418, acc 0.98
2016-09-07T01:26:44.744519: step 815, loss 0.154785, acc 0.96
2016-09-07T01:26:45.444868: step 816, loss 0.11527, acc 0.94
2016-09-07T01:26:46.136963: step 817, loss 0.104181, acc 0.94
2016-09-07T01:26:46.819436: step 818, loss 0.181511, acc 0.92
2016-09-07T01:26:47.541729: step 819, loss 0.0796745, acc 0.98
2016-09-07T01:26:48.218138: step 820, loss 0.134999, acc 0.92
2016-09-07T01:26:48.919688: step 821, loss 0.155292, acc 0.9
2016-09-07T01:26:49.624573: step 822, loss 0.0560842, acc 0.98
2016-09-07T01:26:50.323291: step 823, loss 0.121055, acc 0.92
2016-09-07T01:26:51.013846: step 824, loss 0.0818111, acc 0.96
2016-09-07T01:26:51.686994: step 825, loss 0.177785, acc 0.92
2016-09-07T01:26:52.397587: step 826, loss 0.0931243, acc 0.96
2016-09-07T01:26:53.098706: step 827, loss 0.0534681, acc 0.98
2016-09-07T01:26:53.796361: step 828, loss 0.105121, acc 0.96
2016-09-07T01:26:54.483029: step 829, loss 0.13069, acc 0.92
2016-09-07T01:26:55.179075: step 830, loss 0.0883672, acc 0.96
2016-09-07T01:26:55.884638: step 831, loss 0.238435, acc 0.9
2016-09-07T01:26:56.581976: step 832, loss 0.212547, acc 0.88
2016-09-07T01:26:57.307336: step 833, loss 0.0791444, acc 0.98
2016-09-07T01:26:58.041564: step 834, loss 0.220141, acc 0.9
2016-09-07T01:26:58.727121: step 835, loss 0.100273, acc 0.96
2016-09-07T01:26:59.417779: step 836, loss 0.15087, acc 0.96
2016-09-07T01:27:00.132221: step 837, loss 0.068858, acc 1
2016-09-07T01:27:00.877937: step 838, loss 0.175786, acc 0.9
2016-09-07T01:27:01.577134: step 839, loss 0.160616, acc 0.92
2016-09-07T01:27:02.286924: step 840, loss 0.220596, acc 0.86
2016-09-07T01:27:02.955435: step 841, loss 0.0872203, acc 0.96
2016-09-07T01:27:03.664756: step 842, loss 0.12372, acc 0.94
2016-09-07T01:27:04.381581: step 843, loss 0.141749, acc 0.96
2016-09-07T01:27:05.049593: step 844, loss 0.195398, acc 0.9
2016-09-07T01:27:05.764321: step 845, loss 0.112414, acc 0.96
2016-09-07T01:27:06.461590: step 846, loss 0.152023, acc 0.96
2016-09-07T01:27:07.162973: step 847, loss 0.115711, acc 0.92
2016-09-07T01:27:07.832120: step 848, loss 0.130796, acc 0.92
2016-09-07T01:27:08.516682: step 849, loss 0.0872689, acc 0.94
2016-09-07T01:27:09.231252: step 850, loss 0.185547, acc 0.92
2016-09-07T01:27:09.916030: step 851, loss 0.138414, acc 0.94
2016-09-07T01:27:10.614380: step 852, loss 0.180772, acc 0.94
2016-09-07T01:27:11.320667: step 853, loss 0.0775119, acc 0.96
2016-09-07T01:27:12.017684: step 854, loss 0.0908464, acc 0.96
2016-09-07T01:27:12.730748: step 855, loss 0.166951, acc 0.94
2016-09-07T01:27:13.426493: step 856, loss 0.0823922, acc 0.94
2016-09-07T01:27:14.134712: step 857, loss 0.223714, acc 0.94
2016-09-07T01:27:14.834060: step 858, loss 0.172554, acc 0.9
2016-09-07T01:27:15.526969: step 859, loss 0.168992, acc 0.94
2016-09-07T01:27:16.229399: step 860, loss 0.0802785, acc 0.98
2016-09-07T01:27:16.912242: step 861, loss 0.0763879, acc 0.98
2016-09-07T01:27:17.622448: step 862, loss 0.121049, acc 0.96
2016-09-07T01:27:18.292777: step 863, loss 0.186199, acc 0.88
2016-09-07T01:27:18.999109: step 864, loss 0.203252, acc 0.9
2016-09-07T01:27:19.696145: step 865, loss 0.147262, acc 0.94
2016-09-07T01:27:20.382293: step 866, loss 0.214434, acc 0.9
2016-09-07T01:27:21.080521: step 867, loss 0.206708, acc 0.92
2016-09-07T01:27:21.774940: step 868, loss 0.101812, acc 0.94
2016-09-07T01:27:22.486594: step 869, loss 0.0450304, acc 0.98
2016-09-07T01:27:23.174773: step 870, loss 0.0266698, acc 1
2016-09-07T01:27:23.871461: step 871, loss 0.0687601, acc 0.98
2016-09-07T01:27:24.565027: step 872, loss 0.0799896, acc 0.96
2016-09-07T01:27:25.256986: step 873, loss 0.0460718, acc 0.98
2016-09-07T01:27:25.961661: step 874, loss 0.146576, acc 0.92
2016-09-07T01:27:26.662143: step 875, loss 0.175481, acc 0.92
2016-09-07T01:27:27.373617: step 876, loss 0.139799, acc 0.94
2016-09-07T01:27:28.070093: step 877, loss 0.157131, acc 0.98
2016-09-07T01:27:28.768602: step 878, loss 0.0523761, acc 0.96
2016-09-07T01:27:29.472082: step 879, loss 0.226404, acc 0.9
2016-09-07T01:27:30.162814: step 880, loss 0.104081, acc 0.96
2016-09-07T01:27:30.869547: step 881, loss 0.148119, acc 0.9
2016-09-07T01:27:31.547959: step 882, loss 0.100947, acc 0.96
2016-09-07T01:27:32.233963: step 883, loss 0.122271, acc 0.96
2016-09-07T01:27:32.907939: step 884, loss 0.241743, acc 0.94
2016-09-07T01:27:33.594519: step 885, loss 0.0791191, acc 0.98
2016-09-07T01:27:34.305499: step 886, loss 0.0566913, acc 0.98
2016-09-07T01:27:34.984992: step 887, loss 0.126793, acc 0.98
2016-09-07T01:27:35.694846: step 888, loss 0.267867, acc 0.92
2016-09-07T01:27:36.373297: step 889, loss 0.151923, acc 0.92
2016-09-07T01:27:37.050536: step 890, loss 0.12525, acc 0.94
2016-09-07T01:27:37.769556: step 891, loss 0.0481334, acc 0.98
2016-09-07T01:27:38.453024: step 892, loss 0.0722037, acc 0.94
2016-09-07T01:27:39.141780: step 893, loss 0.142696, acc 0.92
2016-09-07T01:27:39.831483: step 894, loss 0.0660661, acc 0.98
2016-09-07T01:27:40.545498: step 895, loss 0.0379945, acc 0.98
2016-09-07T01:27:41.228018: step 896, loss 0.165081, acc 0.94
2016-09-07T01:27:41.923602: step 897, loss 0.140794, acc 0.96
2016-09-07T01:27:42.624346: step 898, loss 0.108118, acc 0.96
2016-09-07T01:27:43.317425: step 899, loss 0.360219, acc 0.88
2016-09-07T01:27:44.030779: step 900, loss 0.171377, acc 0.9

Evaluation:
2016-09-07T01:27:47.367660: step 900, loss 0.695389, acc 0.775797

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-900

2016-09-07T01:27:49.022885: step 901, loss 0.0948404, acc 0.96
2016-09-07T01:27:49.721368: step 902, loss 0.0758959, acc 0.96
2016-09-07T01:27:50.406716: step 903, loss 0.0265048, acc 1
2016-09-07T01:27:51.102069: step 904, loss 0.0685528, acc 0.98
2016-09-07T01:27:51.802496: step 905, loss 0.177005, acc 0.92
2016-09-07T01:27:52.520562: step 906, loss 0.0640889, acc 0.98
2016-09-07T01:27:53.185426: step 907, loss 0.0479122, acc 0.98
2016-09-07T01:27:53.886021: step 908, loss 0.0586955, acc 0.98
2016-09-07T01:27:54.598361: step 909, loss 0.134288, acc 0.94
2016-09-07T01:27:55.280656: step 910, loss 0.187534, acc 0.94
2016-09-07T01:27:55.983193: step 911, loss 0.167018, acc 0.96
2016-09-07T01:27:56.660921: step 912, loss 0.0293601, acc 1
2016-09-07T01:27:57.374361: step 913, loss 0.0654237, acc 0.96
2016-09-07T01:27:58.046363: step 914, loss 0.113895, acc 0.96
2016-09-07T01:27:58.723911: step 915, loss 0.142289, acc 0.94
2016-09-07T01:27:59.422320: step 916, loss 0.0854516, acc 0.94
2016-09-07T01:28:00.120539: step 917, loss 0.186135, acc 0.96
2016-09-07T01:28:00.867692: step 918, loss 0.376406, acc 0.84
2016-09-07T01:28:01.539126: step 919, loss 0.0764982, acc 0.98
2016-09-07T01:28:02.247157: step 920, loss 0.103415, acc 0.96
2016-09-07T01:28:02.945385: step 921, loss 0.13693, acc 0.92
2016-09-07T01:28:03.620623: step 922, loss 0.170555, acc 0.92
2016-09-07T01:28:04.310247: step 923, loss 0.277899, acc 0.88
2016-09-07T01:28:05.015331: step 924, loss 0.116868, acc 0.96
2016-09-07T01:28:05.741659: step 925, loss 0.127194, acc 0.96
2016-09-07T01:28:06.419297: step 926, loss 0.148224, acc 0.94
2016-09-07T01:28:07.116354: step 927, loss 0.210566, acc 0.88
2016-09-07T01:28:07.798866: step 928, loss 0.205208, acc 0.88
2016-09-07T01:28:08.479406: step 929, loss 0.239205, acc 0.92
2016-09-07T01:28:09.172065: step 930, loss 0.0600536, acc 0.98
2016-09-07T01:28:09.864510: step 931, loss 0.129375, acc 0.96
2016-09-07T01:28:10.577191: step 932, loss 0.0651758, acc 1
2016-09-07T01:28:11.249253: step 933, loss 0.0888446, acc 0.98
2016-09-07T01:28:11.965454: step 934, loss 0.216271, acc 0.84
2016-09-07T01:28:12.633466: step 935, loss 0.257033, acc 0.88
2016-09-07T01:28:13.324372: step 936, loss 0.126173, acc 0.92
2016-09-07T01:28:14.036387: step 937, loss 0.413376, acc 0.92
2016-09-07T01:28:14.696731: step 938, loss 0.167882, acc 0.96
2016-09-07T01:28:15.403463: step 939, loss 0.151523, acc 0.9
2016-09-07T01:28:16.075760: step 940, loss 0.20574, acc 0.92
2016-09-07T01:28:16.771171: step 941, loss 0.138445, acc 0.98
2016-09-07T01:28:17.458093: step 942, loss 0.158594, acc 0.88
2016-09-07T01:28:18.139285: step 943, loss 0.150235, acc 0.96
2016-09-07T01:28:18.830883: step 944, loss 0.144251, acc 0.94
2016-09-07T01:28:19.528255: step 945, loss 0.267096, acc 0.9
2016-09-07T01:28:20.238259: step 946, loss 0.276203, acc 0.88
2016-09-07T01:28:20.928838: step 947, loss 0.140573, acc 0.96
2016-09-07T01:28:21.620928: step 948, loss 0.497036, acc 0.92
2016-09-07T01:28:22.322560: step 949, loss 0.103357, acc 0.96
2016-09-07T01:28:23.026280: step 950, loss 0.0925546, acc 0.96
2016-09-07T01:28:23.751595: step 951, loss 0.0902267, acc 0.92
2016-09-07T01:28:24.441406: step 952, loss 0.157511, acc 0.92
2016-09-07T01:28:25.143391: step 953, loss 0.153416, acc 0.92
2016-09-07T01:28:25.837308: step 954, loss 0.121983, acc 0.96
2016-09-07T01:28:26.526925: step 955, loss 0.102392, acc 0.96
2016-09-07T01:28:27.230032: step 956, loss 0.0799213, acc 0.98
2016-09-07T01:28:27.927361: step 957, loss 0.0991769, acc 0.94
2016-09-07T01:28:28.630610: step 958, loss 0.1393, acc 0.96
2016-09-07T01:28:29.312955: step 959, loss 0.101447, acc 0.98
2016-09-07T01:28:29.945036: step 960, loss 0.116718, acc 0.931818
2016-09-07T01:28:30.634146: step 961, loss 0.0547701, acc 0.98
2016-09-07T01:28:31.331485: step 962, loss 0.111803, acc 0.94
2016-09-07T01:28:32.065904: step 963, loss 0.0959058, acc 0.94
2016-09-07T01:28:32.748025: step 964, loss 0.0828345, acc 0.94
2016-09-07T01:28:33.481780: step 965, loss 0.0972139, acc 0.98
2016-09-07T01:28:34.170504: step 966, loss 0.0818262, acc 0.98
2016-09-07T01:28:34.854508: step 967, loss 0.0646456, acc 0.98
2016-09-07T01:28:35.544505: step 968, loss 0.0980649, acc 0.96
2016-09-07T01:28:36.236907: step 969, loss 0.140536, acc 0.92
2016-09-07T01:28:36.919848: step 970, loss 0.110611, acc 0.96
2016-09-07T01:28:37.592584: step 971, loss 0.115795, acc 0.92
2016-09-07T01:28:38.311104: step 972, loss 0.0622509, acc 0.98
2016-09-07T01:28:39.017449: step 973, loss 0.0488102, acc 0.98
2016-09-07T01:28:39.711949: step 974, loss 0.0714365, acc 0.94
2016-09-07T01:28:40.395107: step 975, loss 0.0608843, acc 0.98
2016-09-07T01:28:41.072213: step 976, loss 0.0896466, acc 0.96
2016-09-07T01:28:41.764478: step 977, loss 0.0626516, acc 0.98
2016-09-07T01:28:42.435409: step 978, loss 0.0753916, acc 0.96
2016-09-07T01:28:43.146317: step 979, loss 0.215421, acc 0.94
2016-09-07T01:28:43.831617: step 980, loss 0.0419446, acc 0.98
2016-09-07T01:28:44.533719: step 981, loss 0.125023, acc 0.94
2016-09-07T01:28:45.220973: step 982, loss 0.131506, acc 0.9
2016-09-07T01:28:45.910241: step 983, loss 0.0345447, acc 0.98
2016-09-07T01:28:46.602594: step 984, loss 0.0919741, acc 0.96
2016-09-07T01:28:47.263356: step 985, loss 0.0696306, acc 0.96
2016-09-07T01:28:47.964817: step 986, loss 0.0424442, acc 0.98
2016-09-07T01:28:48.646539: step 987, loss 0.0591855, acc 0.96
2016-09-07T01:28:49.347848: step 988, loss 0.0455943, acc 1
2016-09-07T01:28:50.036788: step 989, loss 0.137088, acc 0.92
2016-09-07T01:28:50.753107: step 990, loss 0.115126, acc 0.94
2016-09-07T01:28:51.464096: step 991, loss 0.0590795, acc 0.98
2016-09-07T01:28:52.138604: step 992, loss 0.0934361, acc 0.94
2016-09-07T01:28:52.823023: step 993, loss 0.0368381, acc 0.98
2016-09-07T01:28:53.492461: step 994, loss 0.0390506, acc 0.98
2016-09-07T01:28:54.173746: step 995, loss 0.096841, acc 0.94
2016-09-07T01:28:54.875583: step 996, loss 0.0932751, acc 0.96
2016-09-07T01:28:55.561068: step 997, loss 0.0298173, acc 1
2016-09-07T01:28:56.255648: step 998, loss 0.0751941, acc 0.94
2016-09-07T01:28:56.941648: step 999, loss 0.171232, acc 0.9
2016-09-07T01:28:57.650056: step 1000, loss 0.0708877, acc 0.96

Evaluation:
2016-09-07T01:29:01.076913: step 1000, loss 0.920293, acc 0.769231

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-1000

2016-09-07T01:29:02.715462: step 1001, loss 0.0807407, acc 0.96
2016-09-07T01:29:03.439456: step 1002, loss 0.022807, acc 0.98
2016-09-07T01:29:04.099835: step 1003, loss 0.0596371, acc 0.96
2016-09-07T01:29:04.805659: step 1004, loss 0.0567466, acc 0.96
2016-09-07T01:29:05.486916: step 1005, loss 0.0475704, acc 0.98
2016-09-07T01:29:06.188053: step 1006, loss 0.145459, acc 0.9
2016-09-07T01:29:06.878638: step 1007, loss 0.095234, acc 0.96
2016-09-07T01:29:07.586524: step 1008, loss 0.26203, acc 0.9
2016-09-07T01:29:08.286547: step 1009, loss 0.255231, acc 0.94
2016-09-07T01:29:08.971893: step 1010, loss 0.101444, acc 0.96
2016-09-07T01:29:09.671426: step 1011, loss 0.0987691, acc 0.94
2016-09-07T01:29:10.379193: step 1012, loss 0.110667, acc 0.94
2016-09-07T01:29:11.075917: step 1013, loss 0.143763, acc 0.96
2016-09-07T01:29:11.781431: step 1014, loss 0.0778364, acc 0.98
2016-09-07T01:29:12.473995: step 1015, loss 0.068691, acc 0.96
2016-09-07T01:29:13.197616: step 1016, loss 0.0170716, acc 1
2016-09-07T01:29:13.919191: step 1017, loss 0.0953964, acc 0.94
2016-09-07T01:29:14.616673: step 1018, loss 0.086609, acc 0.96
2016-09-07T01:29:15.321986: step 1019, loss 0.155689, acc 0.92
2016-09-07T01:29:16.010141: step 1020, loss 0.0294714, acc 1
2016-09-07T01:29:16.726749: step 1021, loss 0.0616957, acc 0.96
2016-09-07T01:29:17.401024: step 1022, loss 0.055295, acc 0.98
2016-09-07T01:29:18.087078: step 1023, loss 0.114553, acc 0.96
2016-09-07T01:29:18.780463: step 1024, loss 0.172607, acc 0.9
2016-09-07T01:29:19.476220: step 1025, loss 0.102054, acc 0.94
2016-09-07T01:29:20.225204: step 1026, loss 0.0967592, acc 0.94
2016-09-07T01:29:20.895775: step 1027, loss 0.0990722, acc 0.94
2016-09-07T01:29:21.592810: step 1028, loss 0.0585998, acc 0.98
2016-09-07T01:29:22.295287: step 1029, loss 0.0913655, acc 0.98
2016-09-07T01:29:22.997204: step 1030, loss 0.074076, acc 0.98
2016-09-07T01:29:23.686128: step 1031, loss 0.05067, acc 0.98
2016-09-07T01:29:24.381253: step 1032, loss 0.0856755, acc 0.96
2016-09-07T01:29:25.079658: step 1033, loss 0.0498561, acc 0.98
2016-09-07T01:29:25.749049: step 1034, loss 0.0365705, acc 1
2016-09-07T01:29:26.474731: step 1035, loss 0.0311442, acc 0.98
2016-09-07T01:29:27.176742: step 1036, loss 0.141831, acc 0.94
2016-09-07T01:29:27.876421: step 1037, loss 0.0240756, acc 1
2016-09-07T01:29:28.599269: step 1038, loss 0.0653033, acc 0.96
2016-09-07T01:29:29.282897: step 1039, loss 0.0603106, acc 0.96
2016-09-07T01:29:29.987237: step 1040, loss 0.121051, acc 0.92
2016-09-07T01:29:30.687206: step 1041, loss 0.0839194, acc 0.96
2016-09-07T01:29:31.359660: step 1042, loss 0.0477587, acc 0.98
2016-09-07T01:29:32.062029: step 1043, loss 0.0768058, acc 0.96
2016-09-07T01:29:32.745926: step 1044, loss 0.117591, acc 0.92
2016-09-07T01:29:33.426657: step 1045, loss 0.0701677, acc 0.94
2016-09-07T01:29:34.098506: step 1046, loss 0.183554, acc 0.94
2016-09-07T01:29:34.800917: step 1047, loss 0.0302657, acc 0.98
2016-09-07T01:29:35.504541: step 1048, loss 0.043362, acc 0.98
2016-09-07T01:29:36.207133: step 1049, loss 0.0689372, acc 0.96
2016-09-07T01:29:36.894759: step 1050, loss 0.210027, acc 0.88
2016-09-07T01:29:37.599995: step 1051, loss 0.0873225, acc 0.96
2016-09-07T01:29:38.300692: step 1052, loss 0.0673924, acc 0.96
2016-09-07T01:29:38.968369: step 1053, loss 0.130618, acc 0.92
2016-09-07T01:29:39.689845: step 1054, loss 0.0989086, acc 0.94
2016-09-07T01:29:40.397567: step 1055, loss 0.123411, acc 0.94
2016-09-07T01:29:41.110670: step 1056, loss 0.0771888, acc 0.98
2016-09-07T01:29:41.799676: step 1057, loss 0.0566072, acc 0.96
2016-09-07T01:29:42.476614: step 1058, loss 0.109962, acc 0.92
2016-09-07T01:29:43.178943: step 1059, loss 0.137921, acc 0.96
2016-09-07T01:29:43.851079: step 1060, loss 0.0709125, acc 1
2016-09-07T01:29:44.524630: step 1061, loss 0.151051, acc 0.96
2016-09-07T01:29:45.228801: step 1062, loss 0.0923841, acc 0.94
2016-09-07T01:29:45.934357: step 1063, loss 0.178629, acc 0.98
2016-09-07T01:29:46.624994: step 1064, loss 0.0847464, acc 0.96
2016-09-07T01:29:47.287544: step 1065, loss 0.0964726, acc 0.96
2016-09-07T01:29:47.979085: step 1066, loss 0.146692, acc 0.96
2016-09-07T01:29:48.637850: step 1067, loss 0.0481191, acc 0.98
2016-09-07T01:29:49.319061: step 1068, loss 0.116391, acc 0.94
2016-09-07T01:29:50.012564: step 1069, loss 0.0836502, acc 0.96
2016-09-07T01:29:50.702367: step 1070, loss 0.245022, acc 0.9
2016-09-07T01:29:51.373724: step 1071, loss 0.060981, acc 0.98
2016-09-07T01:29:52.066482: step 1072, loss 0.102793, acc 0.94
2016-09-07T01:29:52.758077: step 1073, loss 0.0983524, acc 0.98
2016-09-07T01:29:53.409747: step 1074, loss 0.244006, acc 0.94
2016-09-07T01:29:54.103923: step 1075, loss 0.0425718, acc 0.98
2016-09-07T01:29:54.799810: step 1076, loss 0.210822, acc 0.88
2016-09-07T01:29:55.490695: step 1077, loss 0.195887, acc 0.94
2016-09-07T01:29:56.203667: step 1078, loss 0.084586, acc 0.96
2016-09-07T01:29:56.897297: step 1079, loss 0.0870333, acc 0.96
2016-09-07T01:29:57.615305: step 1080, loss 0.129272, acc 0.92
2016-09-07T01:29:58.280063: step 1081, loss 0.171014, acc 0.94
2016-09-07T01:29:58.960879: step 1082, loss 0.0719631, acc 0.96
2016-09-07T01:29:59.632348: step 1083, loss 0.090457, acc 0.96
2016-09-07T01:30:00.353120: step 1084, loss 0.0960692, acc 0.96
2016-09-07T01:30:01.042053: step 1085, loss 0.221858, acc 0.88
2016-09-07T01:30:01.740717: step 1086, loss 0.117972, acc 0.96
2016-09-07T01:30:02.457757: step 1087, loss 0.0509558, acc 0.98
2016-09-07T01:30:03.144995: step 1088, loss 0.0760832, acc 0.96
2016-09-07T01:30:03.823253: step 1089, loss 0.105843, acc 0.98
2016-09-07T01:30:04.553926: step 1090, loss 0.0694193, acc 1
2016-09-07T01:30:05.249304: step 1091, loss 0.0471648, acc 1
2016-09-07T01:30:05.936457: step 1092, loss 0.0928797, acc 0.96
2016-09-07T01:30:06.595803: step 1093, loss 0.0719282, acc 0.98
2016-09-07T01:30:07.307342: step 1094, loss 0.0737902, acc 0.96
2016-09-07T01:30:07.997715: step 1095, loss 0.0543872, acc 0.98
2016-09-07T01:30:08.667264: step 1096, loss 0.0401935, acc 0.98
2016-09-07T01:30:09.363218: step 1097, loss 0.258152, acc 0.86
2016-09-07T01:30:10.047073: step 1098, loss 0.0831318, acc 0.94
2016-09-07T01:30:10.742708: step 1099, loss 0.0382656, acc 1
2016-09-07T01:30:11.395710: step 1100, loss 0.137651, acc 0.94

Evaluation:
2016-09-07T01:30:14.801668: step 1100, loss 0.951092, acc 0.774859

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-1100

2016-09-07T01:30:16.591674: step 1101, loss 0.0734917, acc 0.96
2016-09-07T01:30:17.287503: step 1102, loss 0.119544, acc 0.98
2016-09-07T01:30:17.975250: step 1103, loss 0.0484753, acc 0.96
2016-09-07T01:30:18.636772: step 1104, loss 0.0928376, acc 0.96
2016-09-07T01:30:19.338412: step 1105, loss 0.010968, acc 1
2016-09-07T01:30:20.005921: step 1106, loss 0.130433, acc 0.9
2016-09-07T01:30:20.701353: step 1107, loss 0.165209, acc 0.9
2016-09-07T01:30:21.379712: step 1108, loss 0.0638409, acc 0.98
2016-09-07T01:30:22.062650: step 1109, loss 0.111573, acc 0.94
2016-09-07T01:30:22.748798: step 1110, loss 0.0923484, acc 0.98
2016-09-07T01:30:23.425921: step 1111, loss 0.0842165, acc 0.96
2016-09-07T01:30:24.146262: step 1112, loss 0.13624, acc 0.96
2016-09-07T01:30:24.861613: step 1113, loss 0.0541133, acc 0.98
2016-09-07T01:30:25.551278: step 1114, loss 0.10096, acc 0.94
2016-09-07T01:30:26.257312: step 1115, loss 0.0811056, acc 0.96
2016-09-07T01:30:26.950824: step 1116, loss 0.10955, acc 0.92
2016-09-07T01:30:27.654307: step 1117, loss 0.121506, acc 0.9
2016-09-07T01:30:28.344979: step 1118, loss 0.134576, acc 0.92
2016-09-07T01:30:29.066416: step 1119, loss 0.0419695, acc 1
2016-09-07T01:30:29.774674: step 1120, loss 0.146235, acc 0.96
2016-09-07T01:30:30.476758: step 1121, loss 0.0832323, acc 0.98
2016-09-07T01:30:31.177031: step 1122, loss 0.0840924, acc 0.92
2016-09-07T01:30:31.869244: step 1123, loss 0.060235, acc 0.98
2016-09-07T01:30:32.565023: step 1124, loss 0.0466264, acc 0.98
2016-09-07T01:30:33.250604: step 1125, loss 0.0326388, acc 1
2016-09-07T01:30:33.934937: step 1126, loss 0.133094, acc 0.96
2016-09-07T01:30:34.634713: step 1127, loss 0.0479338, acc 0.98
2016-09-07T01:30:35.334702: step 1128, loss 0.0647021, acc 0.96
2016-09-07T01:30:36.043155: step 1129, loss 0.309075, acc 0.88
2016-09-07T01:30:36.714668: step 1130, loss 0.0978905, acc 0.96
2016-09-07T01:30:37.408044: step 1131, loss 0.0216196, acc 1
2016-09-07T01:30:38.092732: step 1132, loss 0.0890325, acc 0.98
2016-09-07T01:30:38.775860: step 1133, loss 0.0749725, acc 0.96
2016-09-07T01:30:39.465905: step 1134, loss 0.129861, acc 0.92
2016-09-07T01:30:40.164262: step 1135, loss 0.0974837, acc 0.96
2016-09-07T01:30:40.871877: step 1136, loss 0.0257055, acc 1
2016-09-07T01:30:41.561255: step 1137, loss 0.256809, acc 0.9
2016-09-07T01:30:42.260671: step 1138, loss 0.198184, acc 0.86
2016-09-07T01:30:42.935097: step 1139, loss 0.0549307, acc 0.98
2016-09-07T01:30:43.597468: step 1140, loss 0.0521497, acc 0.98
2016-09-07T01:30:44.275139: step 1141, loss 0.0638171, acc 0.98
2016-09-07T01:30:44.966904: step 1142, loss 0.0629782, acc 0.96
2016-09-07T01:30:45.665563: step 1143, loss 0.156717, acc 0.94
2016-09-07T01:30:46.344579: step 1144, loss 0.0886804, acc 0.92
2016-09-07T01:30:47.036881: step 1145, loss 0.0393795, acc 0.98
2016-09-07T01:30:47.705257: step 1146, loss 0.0974444, acc 0.94
2016-09-07T01:30:48.390639: step 1147, loss 0.144463, acc 0.94
2016-09-07T01:30:49.088279: step 1148, loss 0.0465425, acc 1
2016-09-07T01:30:49.762609: step 1149, loss 0.181468, acc 0.92
2016-09-07T01:30:50.455313: step 1150, loss 0.24557, acc 0.94
2016-09-07T01:30:51.134436: step 1151, loss 0.0896459, acc 0.96
2016-09-07T01:30:51.822184: step 1152, loss 0.124936, acc 0.954545
2016-09-07T01:30:52.504098: step 1153, loss 0.0934757, acc 0.98
2016-09-07T01:30:53.216389: step 1154, loss 0.0340393, acc 1
2016-09-07T01:30:53.934579: step 1155, loss 0.0700355, acc 0.96
2016-09-07T01:30:54.608857: step 1156, loss 0.0355073, acc 0.98
2016-09-07T01:30:55.324492: step 1157, loss 0.0480313, acc 0.96
2016-09-07T01:30:55.990801: step 1158, loss 0.0891049, acc 0.96
2016-09-07T01:30:56.690206: step 1159, loss 0.0326241, acc 0.98
2016-09-07T01:30:57.371743: step 1160, loss 0.307002, acc 0.92
2016-09-07T01:30:58.073432: step 1161, loss 0.0360874, acc 0.98
2016-09-07T01:30:58.769180: step 1162, loss 0.102843, acc 0.96
2016-09-07T01:30:59.458652: step 1163, loss 0.0556495, acc 0.98
2016-09-07T01:31:00.174210: step 1164, loss 0.0744162, acc 0.94
2016-09-07T01:31:00.884632: step 1165, loss 0.114101, acc 0.94
2016-09-07T01:31:01.581319: step 1166, loss 0.0580528, acc 0.98
2016-09-07T01:31:02.273060: step 1167, loss 0.0763048, acc 0.98
2016-09-07T01:31:02.985850: step 1168, loss 0.0270484, acc 0.98
2016-09-07T01:31:03.678736: step 1169, loss 0.0723895, acc 0.98
2016-09-07T01:31:04.344377: step 1170, loss 0.0516056, acc 0.98
2016-09-07T01:31:05.063045: step 1171, loss 0.027908, acc 0.98
2016-09-07T01:31:05.765636: step 1172, loss 0.0664879, acc 0.96
2016-09-07T01:31:06.474746: step 1173, loss 0.0485507, acc 0.98
2016-09-07T01:31:07.172821: step 1174, loss 0.0974896, acc 0.98
2016-09-07T01:31:07.870069: step 1175, loss 0.0504067, acc 0.98
2016-09-07T01:31:08.592211: step 1176, loss 0.032445, acc 1
2016-09-07T01:31:09.288467: step 1177, loss 0.0950309, acc 0.96
2016-09-07T01:31:09.982918: step 1178, loss 0.0575626, acc 0.96
2016-09-07T01:31:10.668547: step 1179, loss 0.0215259, acc 1
2016-09-07T01:31:11.365996: step 1180, loss 0.0297109, acc 0.98
2016-09-07T01:31:12.066156: step 1181, loss 0.162277, acc 0.92
2016-09-07T01:31:12.738614: step 1182, loss 0.0810998, acc 0.96
2016-09-07T01:31:13.455542: step 1183, loss 0.220591, acc 0.92
2016-09-07T01:31:14.143359: step 1184, loss 0.094754, acc 0.94
2016-09-07T01:31:14.847405: step 1185, loss 0.0872391, acc 0.96
2016-09-07T01:31:15.554918: step 1186, loss 0.0694826, acc 0.94
2016-09-07T01:31:16.267043: step 1187, loss 0.0568608, acc 0.98
2016-09-07T01:31:16.971777: step 1188, loss 0.0356607, acc 1
2016-09-07T01:31:17.654717: step 1189, loss 0.0400723, acc 0.98
2016-09-07T01:31:18.358279: step 1190, loss 0.167756, acc 0.92
2016-09-07T01:31:19.047839: step 1191, loss 0.00615993, acc 1
2016-09-07T01:31:19.747924: step 1192, loss 0.0274215, acc 1
2016-09-07T01:31:20.412139: step 1193, loss 0.0988828, acc 0.92
2016-09-07T01:31:21.089543: step 1194, loss 0.0515494, acc 0.98
2016-09-07T01:31:21.776428: step 1195, loss 0.0884341, acc 0.96
2016-09-07T01:31:22.455751: step 1196, loss 0.0554938, acc 0.98
2016-09-07T01:31:23.180338: step 1197, loss 0.123281, acc 0.98
2016-09-07T01:31:23.891770: step 1198, loss 0.053074, acc 0.98
2016-09-07T01:31:24.578055: step 1199, loss 0.0744819, acc 0.96
2016-09-07T01:31:25.260295: step 1200, loss 0.198013, acc 0.9

Evaluation:
2016-09-07T01:31:28.685115: step 1200, loss 0.895669, acc 0.771107

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-1200

2016-09-07T01:31:30.407562: step 1201, loss 0.163442, acc 0.94
2016-09-07T01:31:31.124362: step 1202, loss 0.0644013, acc 0.96
2016-09-07T01:31:31.810851: step 1203, loss 0.206399, acc 0.9
2016-09-07T01:31:32.516696: step 1204, loss 0.0807746, acc 0.96
2016-09-07T01:31:33.208963: step 1205, loss 0.0524986, acc 0.96
2016-09-07T01:31:33.915618: step 1206, loss 0.101049, acc 0.96
2016-09-07T01:31:34.617006: step 1207, loss 0.150397, acc 0.94
2016-09-07T01:31:35.331225: step 1208, loss 0.108947, acc 0.92
2016-09-07T01:31:36.024595: step 1209, loss 0.117193, acc 0.96
2016-09-07T01:31:36.735588: step 1210, loss 0.0577021, acc 0.98
2016-09-07T01:31:37.438926: step 1211, loss 0.0376236, acc 1
2016-09-07T01:31:38.128142: step 1212, loss 0.0242482, acc 1
2016-09-07T01:31:38.810902: step 1213, loss 0.0406662, acc 1
2016-09-07T01:31:39.523147: step 1214, loss 0.0752096, acc 0.96
2016-09-07T01:31:40.199537: step 1215, loss 0.0684619, acc 0.98
2016-09-07T01:31:40.882756: step 1216, loss 0.0373118, acc 0.98
2016-09-07T01:31:41.573421: step 1217, loss 0.0901798, acc 0.94
2016-09-07T01:31:42.282975: step 1218, loss 0.0591929, acc 0.96
2016-09-07T01:31:42.965282: step 1219, loss 0.0771982, acc 0.94
2016-09-07T01:31:43.673541: step 1220, loss 0.0615698, acc 0.98
2016-09-07T01:31:44.360145: step 1221, loss 0.140112, acc 0.92
2016-09-07T01:31:45.059468: step 1222, loss 0.0616912, acc 0.98
2016-09-07T01:31:45.755894: step 1223, loss 0.073459, acc 0.96
2016-09-07T01:31:46.438971: step 1224, loss 0.0400864, acc 0.98
2016-09-07T01:31:47.145585: step 1225, loss 0.015361, acc 1
2016-09-07T01:31:47.845159: step 1226, loss 0.0604246, acc 0.98
2016-09-07T01:31:48.542497: step 1227, loss 0.0310064, acc 1
2016-09-07T01:31:49.229321: step 1228, loss 0.100708, acc 0.96
2016-09-07T01:31:49.926515: step 1229, loss 0.0583373, acc 0.96
2016-09-07T01:31:50.631692: step 1230, loss 0.0544315, acc 0.98
2016-09-07T01:31:51.301368: step 1231, loss 0.0861823, acc 0.96
2016-09-07T01:31:52.020602: step 1232, loss 0.0432888, acc 1
2016-09-07T01:31:52.728171: step 1233, loss 0.0660522, acc 0.96
2016-09-07T01:31:53.427410: step 1234, loss 0.159273, acc 0.98
2016-09-07T01:31:54.127894: step 1235, loss 0.100745, acc 0.96
2016-09-07T01:31:54.816557: step 1236, loss 0.0913461, acc 0.92
2016-09-07T01:31:55.547171: step 1237, loss 0.0501575, acc 0.98
2016-09-07T01:31:56.240916: step 1238, loss 0.215696, acc 0.88
2016-09-07T01:31:56.918215: step 1239, loss 0.0654582, acc 0.96
2016-09-07T01:31:57.598507: step 1240, loss 0.0425602, acc 0.98
2016-09-07T01:31:58.286238: step 1241, loss 0.0999123, acc 0.98
2016-09-07T01:31:58.979396: step 1242, loss 0.171616, acc 0.92
2016-09-07T01:31:59.636995: step 1243, loss 0.0652097, acc 0.98
2016-09-07T01:32:00.353742: step 1244, loss 0.0577936, acc 0.98
2016-09-07T01:32:01.045458: step 1245, loss 0.0838017, acc 0.94
2016-09-07T01:32:01.733996: step 1246, loss 0.0542044, acc 0.98
2016-09-07T01:32:02.429350: step 1247, loss 0.0844255, acc 0.96
2016-09-07T01:32:03.127542: step 1248, loss 0.0908823, acc 0.96
2016-09-07T01:32:03.835967: step 1249, loss 0.0756167, acc 0.96
2016-09-07T01:32:04.492633: step 1250, loss 0.175831, acc 0.88
2016-09-07T01:32:05.203582: step 1251, loss 0.0401112, acc 1
2016-09-07T01:32:05.897048: step 1252, loss 0.0263001, acc 1
2016-09-07T01:32:06.604532: step 1253, loss 0.0321372, acc 0.98
2016-09-07T01:32:07.283834: step 1254, loss 0.104977, acc 0.96
2016-09-07T01:32:07.990273: step 1255, loss 0.0220531, acc 1
2016-09-07T01:32:08.702424: step 1256, loss 0.0975862, acc 0.96
2016-09-07T01:32:09.386677: step 1257, loss 0.0141076, acc 1
2016-09-07T01:32:10.084924: step 1258, loss 0.0784275, acc 0.98
2016-09-07T01:32:10.791158: step 1259, loss 0.0982417, acc 0.98
2016-09-07T01:32:11.486675: step 1260, loss 0.154698, acc 0.96
2016-09-07T01:32:12.170220: step 1261, loss 0.0912085, acc 0.96
2016-09-07T01:32:12.871182: step 1262, loss 0.0714647, acc 0.96
2016-09-07T01:32:13.577101: step 1263, loss 0.209302, acc 0.9
2016-09-07T01:32:14.268344: step 1264, loss 0.0757459, acc 0.96
2016-09-07T01:32:14.962444: step 1265, loss 0.14029, acc 0.92
2016-09-07T01:32:15.650554: step 1266, loss 0.105246, acc 0.94
2016-09-07T01:32:16.322125: step 1267, loss 0.0828252, acc 0.96
2016-09-07T01:32:17.010022: step 1268, loss 0.121163, acc 0.94
2016-09-07T01:32:17.693416: step 1269, loss 0.0489134, acc 1
2016-09-07T01:32:18.394346: step 1270, loss 0.0846246, acc 0.96
2016-09-07T01:32:19.051563: step 1271, loss 0.0735167, acc 0.98
2016-09-07T01:32:19.736420: step 1272, loss 0.0968782, acc 0.98
2016-09-07T01:32:20.424643: step 1273, loss 0.199509, acc 0.9
2016-09-07T01:32:21.130507: step 1274, loss 0.0918521, acc 0.94
2016-09-07T01:32:21.841229: step 1275, loss 0.177747, acc 0.9
2016-09-07T01:32:22.543093: step 1276, loss 0.0593991, acc 0.98
2016-09-07T01:32:23.250309: step 1277, loss 0.199673, acc 0.92
2016-09-07T01:32:23.922883: step 1278, loss 0.103042, acc 0.98
2016-09-07T01:32:24.638562: step 1279, loss 0.0629105, acc 0.98
2016-09-07T01:32:25.330062: step 1280, loss 0.0889733, acc 0.96
2016-09-07T01:32:26.008905: step 1281, loss 0.0710508, acc 0.94
2016-09-07T01:32:26.707300: step 1282, loss 0.121005, acc 0.94
2016-09-07T01:32:27.389678: step 1283, loss 0.135089, acc 0.92
2016-09-07T01:32:28.111716: step 1284, loss 0.141462, acc 0.98
2016-09-07T01:32:28.811381: step 1285, loss 0.0904453, acc 0.96
2016-09-07T01:32:29.497470: step 1286, loss 0.121207, acc 0.94
2016-09-07T01:32:30.160210: step 1287, loss 0.0562602, acc 0.98
2016-09-07T01:32:30.844751: step 1288, loss 0.10935, acc 0.98
2016-09-07T01:32:31.549873: step 1289, loss 0.0509115, acc 0.98
2016-09-07T01:32:32.228360: step 1290, loss 0.064205, acc 0.98
2016-09-07T01:32:32.920065: step 1291, loss 0.0915926, acc 0.96
2016-09-07T01:32:33.594863: step 1292, loss 0.0813704, acc 0.96
2016-09-07T01:32:34.290090: step 1293, loss 0.0310233, acc 1
2016-09-07T01:32:34.987427: step 1294, loss 0.0608576, acc 0.98
2016-09-07T01:32:35.674759: step 1295, loss 0.0513632, acc 0.98
2016-09-07T01:32:36.376092: step 1296, loss 0.168212, acc 0.94
2016-09-07T01:32:37.046032: step 1297, loss 0.0831313, acc 0.96
2016-09-07T01:32:37.732062: step 1298, loss 0.0373315, acc 0.98
2016-09-07T01:32:38.440082: step 1299, loss 0.0171491, acc 1
2016-09-07T01:32:39.136677: step 1300, loss 0.139601, acc 0.96

Evaluation:
2016-09-07T01:32:42.588207: step 1300, loss 0.947007, acc 0.78424

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-1300

2016-09-07T01:32:44.291568: step 1301, loss 0.125536, acc 0.94
2016-09-07T01:32:44.989668: step 1302, loss 0.142755, acc 0.9
2016-09-07T01:32:45.669006: step 1303, loss 0.049917, acc 0.96
2016-09-07T01:32:46.377243: step 1304, loss 0.0157233, acc 1
2016-09-07T01:32:47.078794: step 1305, loss 0.024527, acc 1
2016-09-07T01:32:47.749959: step 1306, loss 0.0401032, acc 1
2016-09-07T01:32:48.474184: step 1307, loss 0.0380627, acc 0.98
2016-09-07T01:32:49.178020: step 1308, loss 0.00284855, acc 1
2016-09-07T01:32:49.864129: step 1309, loss 0.096448, acc 0.96
2016-09-07T01:32:50.563417: step 1310, loss 0.11578, acc 0.94
2016-09-07T01:32:51.244638: step 1311, loss 0.198921, acc 0.92
2016-09-07T01:32:51.945254: step 1312, loss 0.0986535, acc 0.96
2016-09-07T01:32:52.620837: step 1313, loss 0.110513, acc 0.94
2016-09-07T01:32:53.331936: step 1314, loss 0.0815141, acc 0.98
2016-09-07T01:32:54.032058: step 1315, loss 0.0630414, acc 0.98
2016-09-07T01:32:54.718811: step 1316, loss 0.150565, acc 0.96
2016-09-07T01:32:55.411041: step 1317, loss 0.085008, acc 0.96
2016-09-07T01:32:56.097859: step 1318, loss 0.0655016, acc 0.96
2016-09-07T01:32:56.824084: step 1319, loss 0.0716547, acc 0.96
2016-09-07T01:32:57.502721: step 1320, loss 0.136574, acc 0.96
2016-09-07T01:32:58.201456: step 1321, loss 0.0856625, acc 0.94
2016-09-07T01:32:58.877279: step 1322, loss 0.163261, acc 0.92
2016-09-07T01:32:59.553263: step 1323, loss 0.108956, acc 0.92
2016-09-07T01:33:00.258030: step 1324, loss 0.123086, acc 0.96
2016-09-07T01:33:00.934912: step 1325, loss 0.168646, acc 0.88
2016-09-07T01:33:01.689555: step 1326, loss 0.0954145, acc 0.98
2016-09-07T01:33:02.405844: step 1327, loss 0.0678683, acc 0.98
2016-09-07T01:33:03.089056: step 1328, loss 0.0204949, acc 1
2016-09-07T01:33:03.802272: step 1329, loss 0.0756757, acc 0.98
2016-09-07T01:33:04.479483: step 1330, loss 0.103392, acc 0.94
2016-09-07T01:33:05.200472: step 1331, loss 0.180011, acc 0.94
2016-09-07T01:33:05.878815: step 1332, loss 0.0565961, acc 1
2016-09-07T01:33:06.585089: step 1333, loss 0.0745259, acc 0.94
2016-09-07T01:33:07.283297: step 1334, loss 0.0223252, acc 1
2016-09-07T01:33:07.974269: step 1335, loss 0.0346172, acc 0.98
2016-09-07T01:33:08.677769: step 1336, loss 0.151837, acc 0.92
2016-09-07T01:33:09.387612: step 1337, loss 0.13156, acc 0.96
2016-09-07T01:33:10.099959: step 1338, loss 0.0304541, acc 0.98
2016-09-07T01:33:10.767796: step 1339, loss 0.206866, acc 0.94
2016-09-07T01:33:11.455237: step 1340, loss 0.0729914, acc 0.98
2016-09-07T01:33:12.166988: step 1341, loss 0.0533161, acc 0.96
2016-09-07T01:33:12.865863: step 1342, loss 0.22948, acc 0.94
2016-09-07T01:33:13.574585: step 1343, loss 0.0694243, acc 0.96
2016-09-07T01:33:14.195850: step 1344, loss 0.106212, acc 0.954545
2016-09-07T01:33:14.905839: step 1345, loss 0.0561828, acc 0.98
2016-09-07T01:33:15.583581: step 1346, loss 0.0799147, acc 0.98
2016-09-07T01:33:16.277273: step 1347, loss 0.097148, acc 0.98
2016-09-07T01:33:16.975142: step 1348, loss 0.0691962, acc 0.94
2016-09-07T01:33:17.674531: step 1349, loss 0.045033, acc 0.98
2016-09-07T01:33:18.367994: step 1350, loss 0.135612, acc 0.94
2016-09-07T01:33:19.040425: step 1351, loss 0.103554, acc 0.96
2016-09-07T01:33:19.751209: step 1352, loss 0.0460865, acc 0.96
2016-09-07T01:33:20.425637: step 1353, loss 0.0305328, acc 1
2016-09-07T01:33:21.125597: step 1354, loss 0.0591435, acc 0.98
2016-09-07T01:33:21.815815: step 1355, loss 0.0542583, acc 0.98
2016-09-07T01:33:22.493383: step 1356, loss 0.0307188, acc 1
2016-09-07T01:33:23.194299: step 1357, loss 0.13128, acc 0.92
2016-09-07T01:33:23.855660: step 1358, loss 0.0979444, acc 0.96
2016-09-07T01:33:24.562767: step 1359, loss 0.021883, acc 1
2016-09-07T01:33:25.252451: step 1360, loss 0.00690234, acc 1
2016-09-07T01:33:25.944709: step 1361, loss 0.223948, acc 0.92
2016-09-07T01:33:26.639966: step 1362, loss 0.0568254, acc 0.96
2016-09-07T01:33:27.350556: step 1363, loss 0.0436455, acc 0.98
2016-09-07T01:33:28.046236: step 1364, loss 0.0530896, acc 1
2016-09-07T01:33:28.721722: step 1365, loss 0.194006, acc 0.96
2016-09-07T01:33:29.432303: step 1366, loss 0.0539833, acc 0.98
2016-09-07T01:33:30.118562: step 1367, loss 0.0455215, acc 0.96
2016-09-07T01:33:30.839475: step 1368, loss 0.0557545, acc 0.96
2016-09-07T01:33:31.546702: step 1369, loss 0.0423497, acc 0.98
2016-09-07T01:33:32.252850: step 1370, loss 0.0196677, acc 1
2016-09-07T01:33:32.967809: step 1371, loss 0.0395689, acc 0.98
2016-09-07T01:33:33.638577: step 1372, loss 0.0509244, acc 0.98
2016-09-07T01:33:34.313692: step 1373, loss 0.148608, acc 0.94
2016-09-07T01:33:34.986216: step 1374, loss 0.02515, acc 1
2016-09-07T01:33:35.668464: step 1375, loss 0.0654995, acc 0.96
2016-09-07T01:33:36.363135: step 1376, loss 0.0727227, acc 0.94
2016-09-07T01:33:37.062855: step 1377, loss 0.0589739, acc 0.98
2016-09-07T01:33:37.779692: step 1378, loss 0.106, acc 0.92
2016-09-07T01:33:38.441136: step 1379, loss 0.106535, acc 0.94
2016-09-07T01:33:39.122961: step 1380, loss 0.142818, acc 0.96
2016-09-07T01:33:39.819167: step 1381, loss 0.0772014, acc 0.96
2016-09-07T01:33:40.502066: step 1382, loss 0.0306917, acc 0.98
2016-09-07T01:33:41.201088: step 1383, loss 0.0256618, acc 1
2016-09-07T01:33:41.895502: step 1384, loss 0.0102078, acc 1
2016-09-07T01:33:42.627932: step 1385, loss 0.0480728, acc 0.98
2016-09-07T01:33:43.338245: step 1386, loss 0.0775506, acc 0.96
2016-09-07T01:33:44.034135: step 1387, loss 0.028959, acc 1
2016-09-07T01:33:44.729877: step 1388, loss 0.0322151, acc 1
2016-09-07T01:33:45.417535: step 1389, loss 0.0101634, acc 1
2016-09-07T01:33:46.143756: step 1390, loss 0.0934222, acc 0.96
2016-09-07T01:33:46.827316: step 1391, loss 0.029874, acc 1
2016-09-07T01:33:47.539534: step 1392, loss 0.0158233, acc 1
2016-09-07T01:33:48.223438: step 1393, loss 0.0255142, acc 1
2016-09-07T01:33:48.927285: step 1394, loss 0.0388231, acc 0.96
2016-09-07T01:33:49.629750: step 1395, loss 0.0574998, acc 0.98
2016-09-07T01:33:50.294006: step 1396, loss 0.045719, acc 0.98
2016-09-07T01:33:51.012858: step 1397, loss 0.0521518, acc 0.98
2016-09-07T01:33:51.690154: step 1398, loss 0.0567083, acc 0.98
2016-09-07T01:33:52.381825: step 1399, loss 0.0394867, acc 1
2016-09-07T01:33:53.078784: step 1400, loss 0.0547552, acc 0.98

Evaluation:
2016-09-07T01:33:56.515176: step 1400, loss 1.12552, acc 0.782364

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-1400

2016-09-07T01:33:58.220455: step 1401, loss 0.0810357, acc 0.96
2016-09-07T01:33:58.905071: step 1402, loss 0.0718561, acc 0.96
2016-09-07T01:33:59.646102: step 1403, loss 0.062124, acc 0.96
2016-09-07T01:34:00.341007: step 1404, loss 0.056216, acc 0.98
2016-09-07T01:34:01.027996: step 1405, loss 0.055279, acc 0.94
2016-09-07T01:34:01.739558: step 1406, loss 0.128642, acc 0.94
2016-09-07T01:34:02.412277: step 1407, loss 0.0905835, acc 0.94
2016-09-07T01:34:03.123906: step 1408, loss 0.190805, acc 0.92
2016-09-07T01:34:03.827554: step 1409, loss 0.039596, acc 1
2016-09-07T01:34:04.527507: step 1410, loss 0.115629, acc 0.96
2016-09-07T01:34:05.211204: step 1411, loss 0.116046, acc 0.96
2016-09-07T01:34:05.908138: step 1412, loss 0.00453916, acc 1
2016-09-07T01:34:06.635588: step 1413, loss 0.0753019, acc 0.96
2016-09-07T01:34:07.315603: step 1414, loss 0.118785, acc 0.96
2016-09-07T01:34:08.005819: step 1415, loss 0.0141627, acc 1
2016-09-07T01:34:08.689095: step 1416, loss 0.105419, acc 0.94
2016-09-07T01:34:09.364519: step 1417, loss 0.0182957, acc 1
2016-09-07T01:34:10.071016: step 1418, loss 0.0107694, acc 1
2016-09-07T01:34:10.769654: step 1419, loss 0.0564976, acc 0.96
2016-09-07T01:34:11.477904: step 1420, loss 0.0847833, acc 0.94
2016-09-07T01:34:12.172754: step 1421, loss 0.140163, acc 0.94
2016-09-07T01:34:12.895093: step 1422, loss 0.0362995, acc 0.98
2016-09-07T01:34:13.600771: step 1423, loss 0.0350096, acc 1
2016-09-07T01:34:14.287221: step 1424, loss 0.0595594, acc 0.96
2016-09-07T01:34:14.993701: step 1425, loss 0.0257338, acc 0.98
2016-09-07T01:34:15.693997: step 1426, loss 0.0910742, acc 0.96
2016-09-07T01:34:16.378393: step 1427, loss 0.0305114, acc 0.98
2016-09-07T01:34:17.051539: step 1428, loss 0.24024, acc 0.92
2016-09-07T01:34:17.746728: step 1429, loss 0.0239072, acc 1
2016-09-07T01:34:18.439380: step 1430, loss 0.091854, acc 0.96
2016-09-07T01:34:19.141506: step 1431, loss 0.0902595, acc 0.98
2016-09-07T01:34:19.850623: step 1432, loss 0.105807, acc 0.96
2016-09-07T01:34:20.532969: step 1433, loss 0.117376, acc 0.94
2016-09-07T01:34:21.212996: step 1434, loss 0.0394226, acc 1
2016-09-07T01:34:21.918808: step 1435, loss 0.0814176, acc 0.96
2016-09-07T01:34:22.610602: step 1436, loss 0.20168, acc 0.92
2016-09-07T01:34:23.305682: step 1437, loss 0.0783893, acc 0.98
2016-09-07T01:34:23.967967: step 1438, loss 0.0772288, acc 0.96
2016-09-07T01:34:24.671244: step 1439, loss 0.0505365, acc 0.96
2016-09-07T01:34:25.333736: step 1440, loss 0.160226, acc 0.96
2016-09-07T01:34:26.028054: step 1441, loss 0.010348, acc 1
2016-09-07T01:34:26.710302: step 1442, loss 0.113214, acc 0.96
2016-09-07T01:34:27.402825: step 1443, loss 0.0321807, acc 1
2016-09-07T01:34:28.097160: step 1444, loss 0.0194335, acc 1
2016-09-07T01:34:28.781044: step 1445, loss 0.0773103, acc 0.96
2016-09-07T01:34:29.497460: step 1446, loss 0.041698, acc 1
2016-09-07T01:34:30.192202: step 1447, loss 0.0634368, acc 0.98
2016-09-07T01:34:30.887969: step 1448, loss 0.0924485, acc 0.96
2016-09-07T01:34:31.597997: step 1449, loss 0.162663, acc 0.92
2016-09-07T01:34:32.293851: step 1450, loss 0.0715495, acc 0.96
2016-09-07T01:34:33.016336: step 1451, loss 0.0260572, acc 0.98
2016-09-07T01:34:33.703666: step 1452, loss 0.0680647, acc 0.98
2016-09-07T01:34:34.405616: step 1453, loss 0.069247, acc 0.98
2016-09-07T01:34:35.118535: step 1454, loss 0.0764489, acc 0.96
2016-09-07T01:34:35.802996: step 1455, loss 0.0879087, acc 0.94
2016-09-07T01:34:36.517666: step 1456, loss 0.0735806, acc 0.94
2016-09-07T01:34:37.213267: step 1457, loss 0.133243, acc 0.96
2016-09-07T01:34:37.921738: step 1458, loss 0.0865866, acc 0.98
2016-09-07T01:34:38.610337: step 1459, loss 0.0255922, acc 1
2016-09-07T01:34:39.297565: step 1460, loss 0.083837, acc 0.94
2016-09-07T01:34:39.990746: step 1461, loss 0.0828678, acc 0.98
2016-09-07T01:34:40.689787: step 1462, loss 0.105172, acc 0.98
2016-09-07T01:34:41.399269: step 1463, loss 0.0722779, acc 0.96
2016-09-07T01:34:42.081249: step 1464, loss 0.257715, acc 0.94
2016-09-07T01:34:42.790099: step 1465, loss 0.0626391, acc 0.96
2016-09-07T01:34:43.475398: step 1466, loss 0.0446985, acc 0.98
2016-09-07T01:34:44.180950: step 1467, loss 0.193745, acc 0.96
2016-09-07T01:34:44.889398: step 1468, loss 0.0215351, acc 1
2016-09-07T01:34:45.581404: step 1469, loss 0.106019, acc 0.96
2016-09-07T01:34:46.294700: step 1470, loss 0.124843, acc 0.92
2016-09-07T01:34:46.972356: step 1471, loss 0.194565, acc 0.9
2016-09-07T01:34:47.658084: step 1472, loss 0.08209, acc 0.98
2016-09-07T01:34:48.376818: step 1473, loss 0.0628819, acc 0.96
2016-09-07T01:34:49.077250: step 1474, loss 0.02553, acc 1
2016-09-07T01:34:49.773531: step 1475, loss 0.199251, acc 0.92
2016-09-07T01:34:50.430636: step 1476, loss 0.100032, acc 0.94
2016-09-07T01:34:51.145349: step 1477, loss 0.0587658, acc 1
2016-09-07T01:34:51.852558: step 1478, loss 0.0975702, acc 0.94
2016-09-07T01:34:52.553253: step 1479, loss 0.0160326, acc 1
2016-09-07T01:34:53.248519: step 1480, loss 0.0780552, acc 0.98
2016-09-07T01:34:53.930147: step 1481, loss 0.0402759, acc 1
2016-09-07T01:34:54.645336: step 1482, loss 0.0329899, acc 0.98
2016-09-07T01:34:55.344757: step 1483, loss 0.148266, acc 0.96
2016-09-07T01:34:56.034838: step 1484, loss 0.0437031, acc 0.98
2016-09-07T01:34:56.719159: step 1485, loss 0.104701, acc 0.98
2016-09-07T01:34:57.418499: step 1486, loss 0.111129, acc 0.94
2016-09-07T01:34:58.101486: step 1487, loss 0.0720084, acc 0.98
2016-09-07T01:34:58.767612: step 1488, loss 0.046288, acc 0.98
2016-09-07T01:34:59.483636: step 1489, loss 0.0941538, acc 0.94
2016-09-07T01:35:00.166417: step 1490, loss 0.141445, acc 0.96
2016-09-07T01:35:00.955662: step 1491, loss 0.0116933, acc 1
2016-09-07T01:35:01.650056: step 1492, loss 0.102186, acc 0.98
2016-09-07T01:35:02.334368: step 1493, loss 0.0357553, acc 0.98
2016-09-07T01:35:03.042732: step 1494, loss 0.089092, acc 0.96
2016-09-07T01:35:03.713257: step 1495, loss 0.0408532, acc 1
2016-09-07T01:35:04.429186: step 1496, loss 0.0885155, acc 0.94
2016-09-07T01:35:05.120422: step 1497, loss 0.0456365, acc 0.98
2016-09-07T01:35:05.830891: step 1498, loss 0.0935064, acc 0.94
2016-09-07T01:35:06.549410: step 1499, loss 0.118371, acc 0.92
2016-09-07T01:35:07.241146: step 1500, loss 0.0903199, acc 0.98

Evaluation:
2016-09-07T01:35:10.775607: step 1500, loss 1.04705, acc 0.771107

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-1500

2016-09-07T01:35:12.667511: step 1501, loss 0.14304, acc 0.92
2016-09-07T01:35:13.332089: step 1502, loss 0.0753993, acc 0.96
2016-09-07T01:35:14.040152: step 1503, loss 0.0234632, acc 1
2016-09-07T01:35:14.729804: step 1504, loss 0.0858446, acc 0.96
2016-09-07T01:35:15.410425: step 1505, loss 0.0478739, acc 0.98
2016-09-07T01:35:16.107632: step 1506, loss 0.0474954, acc 0.98
2016-09-07T01:35:16.805368: step 1507, loss 0.0932457, acc 0.94
2016-09-07T01:35:17.526129: step 1508, loss 0.0113018, acc 1
2016-09-07T01:35:18.225031: step 1509, loss 0.0778274, acc 0.96
2016-09-07T01:35:18.888276: step 1510, loss 0.116261, acc 0.96
2016-09-07T01:35:19.586479: step 1511, loss 0.05444, acc 0.98
2016-09-07T01:35:20.279340: step 1512, loss 0.0819549, acc 0.96
2016-09-07T01:35:20.980433: step 1513, loss 0.0645576, acc 0.96
2016-09-07T01:35:21.662184: step 1514, loss 0.059065, acc 0.96
2016-09-07T01:35:22.366307: step 1515, loss 0.0975201, acc 0.96
2016-09-07T01:35:23.051954: step 1516, loss 0.0564523, acc 0.98
2016-09-07T01:35:23.756634: step 1517, loss 0.0494345, acc 0.96
2016-09-07T01:35:24.455410: step 1518, loss 0.0986307, acc 0.96
2016-09-07T01:35:25.151486: step 1519, loss 0.0285525, acc 1
2016-09-07T01:35:25.851242: step 1520, loss 0.0328097, acc 1
2016-09-07T01:35:26.529389: step 1521, loss 0.179605, acc 0.9
2016-09-07T01:35:27.231643: step 1522, loss 0.0902825, acc 0.94
2016-09-07T01:35:27.920180: step 1523, loss 0.0527718, acc 0.96
2016-09-07T01:35:28.597374: step 1524, loss 0.0122792, acc 1
2016-09-07T01:35:29.274105: step 1525, loss 0.0450048, acc 0.98
2016-09-07T01:35:29.970155: step 1526, loss 0.0346362, acc 0.98
2016-09-07T01:35:30.679866: step 1527, loss 0.092336, acc 0.98
2016-09-07T01:35:31.355529: step 1528, loss 0.0829707, acc 0.98
2016-09-07T01:35:32.069930: step 1529, loss 0.0845683, acc 0.98
2016-09-07T01:35:32.788832: step 1530, loss 0.0183311, acc 1
2016-09-07T01:35:33.499168: step 1531, loss 0.0506651, acc 0.98
2016-09-07T01:35:34.195062: step 1532, loss 0.0498037, acc 0.98
2016-09-07T01:35:34.866824: step 1533, loss 0.0996575, acc 0.94
2016-09-07T01:35:35.565183: step 1534, loss 0.0788909, acc 0.96
2016-09-07T01:35:36.243398: step 1535, loss 0.072436, acc 0.98
2016-09-07T01:35:36.886823: step 1536, loss 0.0267274, acc 0.977273
2016-09-07T01:35:37.585528: step 1537, loss 0.038424, acc 0.96
2016-09-07T01:35:38.261495: step 1538, loss 0.0858482, acc 0.98
2016-09-07T01:35:38.960131: step 1539, loss 0.0587308, acc 0.98
2016-09-07T01:35:39.659473: step 1540, loss 0.0500881, acc 1
2016-09-07T01:35:40.359468: step 1541, loss 0.0185563, acc 0.98
2016-09-07T01:35:41.050233: step 1542, loss 0.0421324, acc 0.98
2016-09-07T01:35:41.740197: step 1543, loss 0.0695866, acc 0.98
2016-09-07T01:35:42.416798: step 1544, loss 0.0283383, acc 1
2016-09-07T01:35:43.115349: step 1545, loss 0.231479, acc 0.94
2016-09-07T01:35:43.813828: step 1546, loss 0.16764, acc 0.96
2016-09-07T01:35:44.489023: step 1547, loss 0.129335, acc 0.94
2016-09-07T01:35:45.200416: step 1548, loss 0.0299573, acc 1
2016-09-07T01:35:45.880856: step 1549, loss 0.0752252, acc 0.98
2016-09-07T01:35:46.565147: step 1550, loss 0.107767, acc 0.94
2016-09-07T01:35:47.259606: step 1551, loss 0.048193, acc 0.96
2016-09-07T01:35:47.950987: step 1552, loss 0.0678085, acc 0.98
2016-09-07T01:35:48.649376: step 1553, loss 0.141915, acc 0.94
2016-09-07T01:35:49.324460: step 1554, loss 0.0311668, acc 1
2016-09-07T01:35:50.079680: step 1555, loss 0.0599158, acc 0.98
2016-09-07T01:35:50.802911: step 1556, loss 0.0551925, acc 0.96
2016-09-07T01:35:51.507455: step 1557, loss 0.058495, acc 0.94
2016-09-07T01:35:52.211954: step 1558, loss 0.0623172, acc 0.96
2016-09-07T01:35:52.903085: step 1559, loss 0.0769125, acc 0.96
2016-09-07T01:35:53.619503: step 1560, loss 0.0143415, acc 1
2016-09-07T01:35:54.284556: step 1561, loss 0.0612135, acc 0.98
2016-09-07T01:35:54.966685: step 1562, loss 0.0252857, acc 1
2016-09-07T01:35:55.670422: step 1563, loss 0.0462592, acc 0.98
2016-09-07T01:35:56.366907: step 1564, loss 0.0157796, acc 1
2016-09-07T01:35:57.072233: step 1565, loss 0.0451554, acc 1
2016-09-07T01:35:57.753768: step 1566, loss 0.0745565, acc 0.98
2016-09-07T01:35:58.469003: step 1567, loss 0.0221201, acc 1
2016-09-07T01:35:59.156089: step 1568, loss 0.0794427, acc 0.96
2016-09-07T01:35:59.855321: step 1569, loss 0.0140653, acc 1
2016-09-07T01:36:00.569571: step 1570, loss 0.0269338, acc 1
2016-09-07T01:36:01.274620: step 1571, loss 0.0204291, acc 1
2016-09-07T01:36:01.971089: step 1572, loss 0.0639648, acc 0.96
2016-09-07T01:36:02.638544: step 1573, loss 0.0685599, acc 0.96
2016-09-07T01:36:03.368444: step 1574, loss 0.0685594, acc 0.96
2016-09-07T01:36:04.074747: step 1575, loss 0.0359123, acc 0.98
2016-09-07T01:36:04.779663: step 1576, loss 0.0103948, acc 1
2016-09-07T01:36:05.500773: step 1577, loss 0.0784403, acc 0.96
2016-09-07T01:36:06.161699: step 1578, loss 0.0666616, acc 0.96
2016-09-07T01:36:06.879553: step 1579, loss 0.0190899, acc 0.98
2016-09-07T01:36:07.583456: step 1580, loss 0.0192027, acc 0.98
2016-09-07T01:36:08.266696: step 1581, loss 0.0334616, acc 0.98
2016-09-07T01:36:08.959403: step 1582, loss 0.0317529, acc 1
2016-09-07T01:36:09.648110: step 1583, loss 0.227638, acc 0.94
2016-09-07T01:36:10.348318: step 1584, loss 0.0307265, acc 1
2016-09-07T01:36:11.012924: step 1585, loss 0.255422, acc 0.88
2016-09-07T01:36:11.725995: step 1586, loss 0.061779, acc 0.96
2016-09-07T01:36:12.404962: step 1587, loss 0.0213655, acc 1
2016-09-07T01:36:13.092007: step 1588, loss 0.0132162, acc 1
2016-09-07T01:36:13.778589: step 1589, loss 0.0211558, acc 1
2016-09-07T01:36:14.474330: step 1590, loss 0.112603, acc 0.94
2016-09-07T01:36:15.162577: step 1591, loss 0.047171, acc 0.98
2016-09-07T01:36:15.838886: step 1592, loss 0.0158074, acc 1
2016-09-07T01:36:16.548226: step 1593, loss 0.131313, acc 0.92
2016-09-07T01:36:17.250698: step 1594, loss 0.0610483, acc 0.98
2016-09-07T01:36:17.942033: step 1595, loss 0.142674, acc 0.94
2016-09-07T01:36:18.647471: step 1596, loss 0.0481386, acc 0.98
2016-09-07T01:36:19.337368: step 1597, loss 0.179767, acc 0.92
2016-09-07T01:36:20.043111: step 1598, loss 0.0623239, acc 0.96
2016-09-07T01:36:20.721372: step 1599, loss 0.0859854, acc 0.96
2016-09-07T01:36:21.404601: step 1600, loss 0.0363568, acc 1

Evaluation:
2016-09-07T01:36:25.031412: step 1600, loss 1.08012, acc 0.768293

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-1600

2016-09-07T01:36:26.799610: step 1601, loss 0.0566719, acc 0.98
2016-09-07T01:36:27.505951: step 1602, loss 0.0410038, acc 0.98
2016-09-07T01:36:28.214185: step 1603, loss 0.019633, acc 1
2016-09-07T01:36:28.893853: step 1604, loss 0.0516997, acc 0.96
2016-09-07T01:36:29.591758: step 1605, loss 0.140316, acc 0.94
2016-09-07T01:36:30.267597: step 1606, loss 0.122645, acc 0.94
2016-09-07T01:36:30.996329: step 1607, loss 0.129176, acc 0.98
2016-09-07T01:36:31.690753: step 1608, loss 0.108629, acc 0.94
2016-09-07T01:36:32.377872: step 1609, loss 0.0336377, acc 1
2016-09-07T01:36:33.066274: step 1610, loss 0.120566, acc 0.96
2016-09-07T01:36:33.764478: step 1611, loss 0.0388267, acc 0.96
2016-09-07T01:36:34.470949: step 1612, loss 0.0831999, acc 0.98
2016-09-07T01:36:35.153447: step 1613, loss 0.0951458, acc 0.98
2016-09-07T01:36:35.862933: step 1614, loss 0.0531233, acc 0.98
2016-09-07T01:36:36.573588: step 1615, loss 0.0776645, acc 0.96
2016-09-07T01:36:37.276755: step 1616, loss 0.0722892, acc 0.94
2016-09-07T01:36:38.011243: step 1617, loss 0.0367863, acc 0.98
2016-09-07T01:36:38.688794: step 1618, loss 0.0342689, acc 0.98
2016-09-07T01:36:39.414166: step 1619, loss 0.0516007, acc 0.98
2016-09-07T01:36:40.100957: step 1620, loss 0.0563991, acc 0.98
2016-09-07T01:36:40.785927: step 1621, loss 0.0169694, acc 1
2016-09-07T01:36:41.484916: step 1622, loss 0.0543709, acc 1
2016-09-07T01:36:42.199810: step 1623, loss 0.063947, acc 0.98
2016-09-07T01:36:42.925199: step 1624, loss 0.120052, acc 0.92
2016-09-07T01:36:43.618664: step 1625, loss 0.0508888, acc 0.98
2016-09-07T01:36:44.296934: step 1626, loss 0.0691782, acc 0.98
2016-09-07T01:36:44.991836: step 1627, loss 0.152841, acc 0.92
2016-09-07T01:36:45.673957: step 1628, loss 0.0173954, acc 1
2016-09-07T01:36:46.375639: step 1629, loss 0.0530901, acc 0.96
2016-09-07T01:36:47.053146: step 1630, loss 0.0658583, acc 0.98
2016-09-07T01:36:47.771867: step 1631, loss 0.110445, acc 0.96
2016-09-07T01:36:48.430417: step 1632, loss 0.0388981, acc 0.98
2016-09-07T01:36:49.110597: step 1633, loss 0.0436728, acc 0.98
2016-09-07T01:36:49.812684: step 1634, loss 0.0747846, acc 0.98
2016-09-07T01:36:50.500882: step 1635, loss 0.0724382, acc 0.96
2016-09-07T01:36:51.178747: step 1636, loss 0.0513389, acc 0.98
2016-09-07T01:36:51.851752: step 1637, loss 0.0174753, acc 1
2016-09-07T01:36:52.564521: step 1638, loss 0.121864, acc 0.98
2016-09-07T01:36:53.231783: step 1639, loss 0.104407, acc 0.98
2016-09-07T01:36:53.937801: step 1640, loss 0.0453644, acc 1
2016-09-07T01:36:54.627065: step 1641, loss 0.0584878, acc 0.96
2016-09-07T01:36:55.323225: step 1642, loss 0.243099, acc 0.92
2016-09-07T01:36:56.033812: step 1643, loss 0.0513778, acc 0.98
2016-09-07T01:36:56.715320: step 1644, loss 0.0224196, acc 1
2016-09-07T01:36:57.434180: step 1645, loss 0.218751, acc 0.94
2016-09-07T01:36:58.134494: step 1646, loss 0.125616, acc 0.92
2016-09-07T01:36:58.839758: step 1647, loss 0.0511373, acc 0.96
2016-09-07T01:36:59.531527: step 1648, loss 0.0238735, acc 0.98
2016-09-07T01:37:00.261930: step 1649, loss 0.0922578, acc 0.94
2016-09-07T01:37:00.971554: step 1650, loss 0.0729643, acc 0.96
2016-09-07T01:37:01.674570: step 1651, loss 0.0362899, acc 1
2016-09-07T01:37:02.354549: step 1652, loss 0.0416178, acc 0.96
2016-09-07T01:37:03.035194: step 1653, loss 0.0960721, acc 0.92
2016-09-07T01:37:03.709545: step 1654, loss 0.0307069, acc 1
2016-09-07T01:37:04.393521: step 1655, loss 0.0239326, acc 1
2016-09-07T01:37:05.086600: step 1656, loss 0.075062, acc 0.96
2016-09-07T01:37:05.796772: step 1657, loss 0.150231, acc 0.98
2016-09-07T01:37:06.531435: step 1658, loss 0.0905489, acc 0.96
2016-09-07T01:37:07.229351: step 1659, loss 0.0979984, acc 0.96
2016-09-07T01:37:07.929839: step 1660, loss 0.0425564, acc 1
2016-09-07T01:37:08.630521: step 1661, loss 0.0432203, acc 1
2016-09-07T01:37:09.335581: step 1662, loss 0.065963, acc 0.98
2016-09-07T01:37:10.031710: step 1663, loss 0.0284534, acc 1
2016-09-07T01:37:10.728444: step 1664, loss 0.135687, acc 0.92
2016-09-07T01:37:11.421673: step 1665, loss 0.0162059, acc 1
2016-09-07T01:37:12.113829: step 1666, loss 0.0911031, acc 0.98
2016-09-07T01:37:12.830454: step 1667, loss 0.0947335, acc 0.94
2016-09-07T01:37:13.514911: step 1668, loss 0.0402968, acc 0.98
2016-09-07T01:37:14.240703: step 1669, loss 0.100541, acc 0.92
2016-09-07T01:37:14.908961: step 1670, loss 0.0773681, acc 0.98
2016-09-07T01:37:15.587502: step 1671, loss 0.0320243, acc 1
2016-09-07T01:37:16.275320: step 1672, loss 0.0701271, acc 0.96
2016-09-07T01:37:16.961268: step 1673, loss 0.0569996, acc 0.96
2016-09-07T01:37:17.660726: step 1674, loss 0.0691671, acc 0.96
2016-09-07T01:37:18.322096: step 1675, loss 0.134647, acc 0.96
2016-09-07T01:37:19.049824: step 1676, loss 0.0502471, acc 1
2016-09-07T01:37:19.742730: step 1677, loss 0.0147599, acc 1
2016-09-07T01:37:20.417255: step 1678, loss 0.0516789, acc 0.98
2016-09-07T01:37:21.112713: step 1679, loss 0.0705556, acc 0.96
2016-09-07T01:37:21.805365: step 1680, loss 0.0258138, acc 1
2016-09-07T01:37:22.513507: step 1681, loss 0.105853, acc 0.92
2016-09-07T01:37:23.174875: step 1682, loss 0.0220565, acc 1
2016-09-07T01:37:23.920447: step 1683, loss 0.208426, acc 0.96
2016-09-07T01:37:24.607417: step 1684, loss 0.0549103, acc 0.98
2016-09-07T01:37:25.274718: step 1685, loss 0.0801606, acc 0.96
2016-09-07T01:37:25.980800: step 1686, loss 0.0593442, acc 0.98
2016-09-07T01:37:26.671234: step 1687, loss 0.0887224, acc 0.96
2016-09-07T01:37:27.382063: step 1688, loss 0.0327274, acc 0.98
2016-09-07T01:37:28.050082: step 1689, loss 0.0553738, acc 0.98
2016-09-07T01:37:28.753510: step 1690, loss 0.0454263, acc 0.98
2016-09-07T01:37:29.444763: step 1691, loss 0.1409, acc 0.92
2016-09-07T01:37:30.133897: step 1692, loss 0.0373589, acc 1
2016-09-07T01:37:30.818943: step 1693, loss 0.0394817, acc 0.98
2016-09-07T01:37:31.506618: step 1694, loss 0.103135, acc 0.94
2016-09-07T01:37:32.203033: step 1695, loss 0.0267094, acc 1
2016-09-07T01:37:32.878499: step 1696, loss 0.0236171, acc 1
2016-09-07T01:37:33.582360: step 1697, loss 0.0312509, acc 1
2016-09-07T01:37:34.296058: step 1698, loss 0.183125, acc 0.92
2016-09-07T01:37:35.012673: step 1699, loss 0.015695, acc 1
2016-09-07T01:37:35.710273: step 1700, loss 0.0225128, acc 0.98

Evaluation:
2016-09-07T01:37:39.231798: step 1700, loss 1.05529, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-1700

2016-09-07T01:37:41.025335: step 1701, loss 0.0415268, acc 0.98
2016-09-07T01:37:41.727219: step 1702, loss 0.144502, acc 0.92
2016-09-07T01:37:42.408336: step 1703, loss 0.0739941, acc 0.98
2016-09-07T01:37:43.118231: step 1704, loss 0.137675, acc 0.92
2016-09-07T01:37:43.829279: step 1705, loss 0.0420346, acc 1
2016-09-07T01:37:44.524487: step 1706, loss 0.0697846, acc 0.96
2016-09-07T01:37:45.216767: step 1707, loss 0.0271907, acc 1
2016-09-07T01:37:45.895900: step 1708, loss 0.0569924, acc 0.98
2016-09-07T01:37:46.593931: step 1709, loss 0.0426549, acc 0.98
2016-09-07T01:37:47.294052: step 1710, loss 0.0551415, acc 1
2016-09-07T01:37:47.977419: step 1711, loss 0.106562, acc 0.98
2016-09-07T01:37:48.697491: step 1712, loss 0.175935, acc 0.96
2016-09-07T01:37:49.384480: step 1713, loss 0.0367114, acc 0.98
2016-09-07T01:37:50.071451: step 1714, loss 0.213129, acc 0.92
2016-09-07T01:37:50.741062: step 1715, loss 0.0912527, acc 0.96
2016-09-07T01:37:51.453080: step 1716, loss 0.121393, acc 0.96
2016-09-07T01:37:52.131170: step 1717, loss 0.0190224, acc 1
2016-09-07T01:37:52.843553: step 1718, loss 0.0427308, acc 0.98
2016-09-07T01:37:53.528393: step 1719, loss 0.0360606, acc 0.98
2016-09-07T01:37:54.230624: step 1720, loss 0.0662172, acc 0.98
2016-09-07T01:37:54.932051: step 1721, loss 0.0318451, acc 0.98
2016-09-07T01:37:55.610345: step 1722, loss 0.140143, acc 0.96
2016-09-07T01:37:56.334004: step 1723, loss 0.0153193, acc 1
2016-09-07T01:37:57.033839: step 1724, loss 0.12263, acc 0.94
2016-09-07T01:37:57.751852: step 1725, loss 0.077245, acc 0.96
2016-09-07T01:37:58.433517: step 1726, loss 0.130524, acc 0.94
2016-09-07T01:37:59.136661: step 1727, loss 0.179798, acc 0.92
2016-09-07T01:37:59.814693: step 1728, loss 0.0360496, acc 0.954545
2016-09-07T01:38:00.547026: step 1729, loss 0.0345926, acc 0.98
2016-09-07T01:38:01.251499: step 1730, loss 0.0447088, acc 0.96
2016-09-07T01:38:01.990808: step 1731, loss 0.0608048, acc 0.98
2016-09-07T01:38:02.681166: step 1732, loss 0.109791, acc 0.94
2016-09-07T01:38:03.371635: step 1733, loss 0.0164019, acc 1
2016-09-07T01:38:04.029788: step 1734, loss 0.0528372, acc 1
2016-09-07T01:38:04.736044: step 1735, loss 0.0425563, acc 1
2016-09-07T01:38:05.441164: step 1736, loss 0.0369019, acc 0.98
2016-09-07T01:38:06.140047: step 1737, loss 0.0311822, acc 1
2016-09-07T01:38:06.835336: step 1738, loss 0.00810715, acc 1
2016-09-07T01:38:07.542311: step 1739, loss 0.069961, acc 0.96
2016-09-07T01:38:08.274837: step 1740, loss 0.0161648, acc 1
2016-09-07T01:38:08.978199: step 1741, loss 0.183286, acc 0.92
2016-09-07T01:38:09.675712: step 1742, loss 0.0450199, acc 0.98
2016-09-07T01:38:10.389352: step 1743, loss 0.120557, acc 0.96
2016-09-07T01:38:11.125672: step 1744, loss 0.0366319, acc 0.98
2016-09-07T01:38:11.826987: step 1745, loss 0.211662, acc 0.94
2016-09-07T01:38:12.513021: step 1746, loss 0.0300115, acc 0.98
2016-09-07T01:38:13.227304: step 1747, loss 0.119314, acc 0.94
2016-09-07T01:38:13.913869: step 1748, loss 0.0480902, acc 0.96
2016-09-07T01:38:14.616090: step 1749, loss 0.0571781, acc 0.96
2016-09-07T01:38:15.322723: step 1750, loss 0.119272, acc 0.92
2016-09-07T01:38:16.015769: step 1751, loss 0.0424748, acc 0.98
2016-09-07T01:38:16.739720: step 1752, loss 0.0443456, acc 0.98
2016-09-07T01:38:17.442294: step 1753, loss 0.0598208, acc 0.98
2016-09-07T01:38:18.131355: step 1754, loss 0.0115577, acc 1
2016-09-07T01:38:18.825393: step 1755, loss 0.0332101, acc 0.98
2016-09-07T01:38:19.536491: step 1756, loss 0.0836785, acc 0.94
2016-09-07T01:38:20.275454: step 1757, loss 0.158795, acc 0.94
2016-09-07T01:38:20.953072: step 1758, loss 0.0643352, acc 0.98
2016-09-07T01:38:21.640885: step 1759, loss 0.0742617, acc 0.96
2016-09-07T01:38:22.330725: step 1760, loss 0.136057, acc 0.96
2016-09-07T01:38:23.027102: step 1761, loss 0.041744, acc 0.98
2016-09-07T01:38:23.732219: step 1762, loss 0.0515635, acc 0.98
2016-09-07T01:38:24.430066: step 1763, loss 0.0491912, acc 0.98
2016-09-07T01:38:25.146142: step 1764, loss 0.057073, acc 0.98
2016-09-07T01:38:25.827559: step 1765, loss 0.0279847, acc 1
2016-09-07T01:38:26.515886: step 1766, loss 0.0558843, acc 0.96
2016-09-07T01:38:27.209754: step 1767, loss 0.0927716, acc 0.94
2016-09-07T01:38:27.920088: step 1768, loss 0.0131964, acc 1
2016-09-07T01:38:28.618480: step 1769, loss 0.0389922, acc 1
2016-09-07T01:38:29.304306: step 1770, loss 0.039751, acc 0.98
2016-09-07T01:38:30.006969: step 1771, loss 0.0592869, acc 0.98
2016-09-07T01:38:30.702492: step 1772, loss 0.0363503, acc 0.98
2016-09-07T01:38:31.394771: step 1773, loss 0.126664, acc 0.96
2016-09-07T01:38:32.110717: step 1774, loss 0.0706157, acc 0.94
2016-09-07T01:38:32.799931: step 1775, loss 0.0257242, acc 0.98
2016-09-07T01:38:33.509449: step 1776, loss 0.0394859, acc 0.98
2016-09-07T01:38:34.209072: step 1777, loss 0.0152951, acc 1
2016-09-07T01:38:34.937586: step 1778, loss 0.0804419, acc 0.94
2016-09-07T01:38:35.626997: step 1779, loss 0.129281, acc 0.96
2016-09-07T01:38:36.324848: step 1780, loss 0.00771386, acc 1
2016-09-07T01:38:37.046013: step 1781, loss 0.080248, acc 0.98
2016-09-07T01:38:37.734279: step 1782, loss 0.0836347, acc 0.96
2016-09-07T01:38:38.463205: step 1783, loss 0.0461297, acc 0.98
2016-09-07T01:38:39.161494: step 1784, loss 0.184164, acc 0.98
2016-09-07T01:38:39.890992: step 1785, loss 0.0251939, acc 0.98
2016-09-07T01:38:40.593885: step 1786, loss 0.0161943, acc 1
2016-09-07T01:38:41.286914: step 1787, loss 0.0177478, acc 1
2016-09-07T01:38:42.007808: step 1788, loss 0.0378492, acc 1
2016-09-07T01:38:42.694944: step 1789, loss 0.0652104, acc 0.96
2016-09-07T01:38:43.404725: step 1790, loss 0.0963284, acc 0.94
2016-09-07T01:38:44.116033: step 1791, loss 0.0410816, acc 0.98
2016-09-07T01:38:44.820889: step 1792, loss 0.0829626, acc 0.96
2016-09-07T01:38:45.537661: step 1793, loss 0.10967, acc 0.94
2016-09-07T01:38:46.243107: step 1794, loss 0.026738, acc 0.98
2016-09-07T01:38:46.920989: step 1795, loss 0.0409358, acc 0.98
2016-09-07T01:38:47.614082: step 1796, loss 0.0223959, acc 0.98
2016-09-07T01:38:48.305241: step 1797, loss 0.00570855, acc 1
2016-09-07T01:38:49.014802: step 1798, loss 0.0526911, acc 0.98
2016-09-07T01:38:49.693875: step 1799, loss 0.0545266, acc 0.96
2016-09-07T01:38:50.387474: step 1800, loss 0.0584534, acc 0.98

Evaluation:
2016-09-07T01:38:53.966894: step 1800, loss 1.11908, acc 0.776735

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-1800

2016-09-07T01:38:55.743215: step 1801, loss 0.0199408, acc 0.98
2016-09-07T01:38:56.468583: step 1802, loss 0.0481198, acc 0.98
2016-09-07T01:38:57.166208: step 1803, loss 0.0321538, acc 0.98
2016-09-07T01:38:57.848277: step 1804, loss 0.0821029, acc 0.94
2016-09-07T01:38:58.540663: step 1805, loss 0.0379016, acc 0.98
2016-09-07T01:38:59.255621: step 1806, loss 0.0105203, acc 1
2016-09-07T01:38:59.953370: step 1807, loss 0.0276621, acc 1
2016-09-07T01:39:00.659317: step 1808, loss 0.0231512, acc 1
2016-09-07T01:39:01.360509: step 1809, loss 0.0194894, acc 1
2016-09-07T01:39:02.073262: step 1810, loss 0.0127198, acc 1
2016-09-07T01:39:02.762513: step 1811, loss 0.0391881, acc 0.96
2016-09-07T01:39:03.451003: step 1812, loss 0.0209167, acc 1
2016-09-07T01:39:04.154972: step 1813, loss 0.0125418, acc 1
2016-09-07T01:39:04.879870: step 1814, loss 0.145695, acc 0.96
2016-09-07T01:39:05.580274: step 1815, loss 0.0056783, acc 1
2016-09-07T01:39:06.272624: step 1816, loss 0.00609482, acc 1
2016-09-07T01:39:06.973656: step 1817, loss 0.139289, acc 0.96
2016-09-07T01:39:07.664533: step 1818, loss 0.0234729, acc 1
2016-09-07T01:39:08.372889: step 1819, loss 0.154356, acc 0.96
2016-09-07T01:39:09.059211: step 1820, loss 0.0199399, acc 1
2016-09-07T01:39:09.776163: step 1821, loss 0.0225887, acc 0.98
2016-09-07T01:39:10.469057: step 1822, loss 0.0781355, acc 0.96
2016-09-07T01:39:11.162213: step 1823, loss 0.102185, acc 0.96
2016-09-07T01:39:11.856097: step 1824, loss 0.018334, acc 1
2016-09-07T01:39:12.538550: step 1825, loss 0.0560116, acc 0.96
2016-09-07T01:39:13.263533: step 1826, loss 0.120648, acc 0.96
2016-09-07T01:39:13.930988: step 1827, loss 0.00622171, acc 1
2016-09-07T01:39:14.607890: step 1828, loss 0.01345, acc 1
2016-09-07T01:39:15.309028: step 1829, loss 0.0106984, acc 1
2016-09-07T01:39:16.002660: step 1830, loss 0.00984767, acc 1
2016-09-07T01:39:16.687296: step 1831, loss 0.053565, acc 0.98
2016-09-07T01:39:17.361565: step 1832, loss 0.0546796, acc 0.98
2016-09-07T01:39:18.071624: step 1833, loss 0.0374035, acc 0.96
2016-09-07T01:39:18.738670: step 1834, loss 0.0428898, acc 0.98
2016-09-07T01:39:19.423047: step 1835, loss 0.067561, acc 0.98
2016-09-07T01:39:20.118002: step 1836, loss 0.065408, acc 0.96
2016-09-07T01:39:20.821392: step 1837, loss 0.0383592, acc 0.98
2016-09-07T01:39:21.511474: step 1838, loss 0.0510194, acc 0.96
2016-09-07T01:39:22.191715: step 1839, loss 0.120222, acc 0.94
2016-09-07T01:39:22.903681: step 1840, loss 0.0277749, acc 0.98
2016-09-07T01:39:23.594057: step 1841, loss 0.0664576, acc 0.98
2016-09-07T01:39:24.302006: step 1842, loss 0.120348, acc 0.94
2016-09-07T01:39:24.998611: step 1843, loss 0.0578603, acc 0.96
2016-09-07T01:39:25.700487: step 1844, loss 0.0929478, acc 0.96
2016-09-07T01:39:26.404554: step 1845, loss 0.0568186, acc 0.98
2016-09-07T01:39:27.083267: step 1846, loss 0.0384226, acc 0.98
2016-09-07T01:39:27.803562: step 1847, loss 0.148335, acc 0.98
2016-09-07T01:39:28.508536: step 1848, loss 0.0326921, acc 1
2016-09-07T01:39:29.188697: step 1849, loss 0.0382413, acc 0.98
2016-09-07T01:39:29.898474: step 1850, loss 0.0607019, acc 0.96
2016-09-07T01:39:30.591786: step 1851, loss 0.0635141, acc 0.98
2016-09-07T01:39:31.315805: step 1852, loss 0.0264304, acc 0.98
2016-09-07T01:39:32.027033: step 1853, loss 0.0645975, acc 0.96
2016-09-07T01:39:32.718972: step 1854, loss 0.0148252, acc 1
2016-09-07T01:39:33.406044: step 1855, loss 0.0462368, acc 0.98
2016-09-07T01:39:34.101812: step 1856, loss 0.0285486, acc 0.98
2016-09-07T01:39:34.816174: step 1857, loss 0.0229984, acc 1
2016-09-07T01:39:35.494296: step 1858, loss 0.0586963, acc 0.96
2016-09-07T01:39:36.195652: step 1859, loss 0.125583, acc 0.92
2016-09-07T01:39:36.905538: step 1860, loss 0.0757132, acc 0.94
2016-09-07T01:39:37.599585: step 1861, loss 0.0437845, acc 0.98
2016-09-07T01:39:38.304295: step 1862, loss 0.0566129, acc 0.96
2016-09-07T01:39:39.007543: step 1863, loss 0.0182649, acc 1
2016-09-07T01:39:39.737646: step 1864, loss 0.0572333, acc 0.98
2016-09-07T01:39:40.420428: step 1865, loss 0.0271116, acc 0.98
2016-09-07T01:39:41.134557: step 1866, loss 0.00845135, acc 1
2016-09-07T01:39:41.846858: step 1867, loss 0.0679354, acc 0.94
2016-09-07T01:39:42.543333: step 1868, loss 0.0245534, acc 0.98
2016-09-07T01:39:43.239804: step 1869, loss 0.0658605, acc 0.96
2016-09-07T01:39:43.899933: step 1870, loss 0.104795, acc 0.96
2016-09-07T01:39:44.597284: step 1871, loss 0.018603, acc 0.98
2016-09-07T01:39:45.273024: step 1872, loss 0.106969, acc 0.98
2016-09-07T01:39:45.978853: step 1873, loss 0.0762614, acc 0.96
2016-09-07T01:39:46.678435: step 1874, loss 0.0177032, acc 1
2016-09-07T01:39:47.401029: step 1875, loss 0.0260941, acc 0.98
2016-09-07T01:39:48.124499: step 1876, loss 0.143066, acc 0.92
2016-09-07T01:39:48.815750: step 1877, loss 0.111978, acc 0.92
2016-09-07T01:39:49.530988: step 1878, loss 0.0477329, acc 0.98
2016-09-07T01:39:50.227237: step 1879, loss 0.0168665, acc 1
2016-09-07T01:39:50.902455: step 1880, loss 0.0718643, acc 0.96
2016-09-07T01:39:51.587316: step 1881, loss 0.0041965, acc 1
2016-09-07T01:39:52.276189: step 1882, loss 0.0425341, acc 0.98
2016-09-07T01:39:52.998864: step 1883, loss 0.0411688, acc 0.98
2016-09-07T01:39:53.694404: step 1884, loss 0.00867958, acc 1
2016-09-07T01:39:54.376747: step 1885, loss 0.158487, acc 0.96
2016-09-07T01:39:55.066999: step 1886, loss 0.142365, acc 0.96
2016-09-07T01:39:55.768464: step 1887, loss 0.0620958, acc 0.94
2016-09-07T01:39:56.472633: step 1888, loss 0.022738, acc 1
2016-09-07T01:39:57.154307: step 1889, loss 0.00772078, acc 1
2016-09-07T01:39:57.889434: step 1890, loss 0.0399617, acc 0.96
2016-09-07T01:39:58.584629: step 1891, loss 0.0500535, acc 0.98
2016-09-07T01:39:59.276511: step 1892, loss 0.00926166, acc 1
2016-09-07T01:39:59.980851: step 1893, loss 0.0775715, acc 0.94
2016-09-07T01:40:00.718123: step 1894, loss 0.110961, acc 0.98
2016-09-07T01:40:01.431258: step 1895, loss 0.0967974, acc 0.96
2016-09-07T01:40:02.161757: step 1896, loss 0.0730779, acc 0.98
2016-09-07T01:40:02.864546: step 1897, loss 0.0169677, acc 1
2016-09-07T01:40:03.572721: step 1898, loss 0.073111, acc 0.98
2016-09-07T01:40:04.289052: step 1899, loss 0.0265814, acc 0.98
2016-09-07T01:40:05.013611: step 1900, loss 0.0432627, acc 1

Evaluation:
2016-09-07T01:40:08.643084: step 1900, loss 1.10749, acc 0.769231

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-1900

2016-09-07T01:40:10.469531: step 1901, loss 0.229928, acc 0.92
2016-09-07T01:40:11.169953: step 1902, loss 0.0114207, acc 1
2016-09-07T01:40:11.857174: step 1903, loss 0.0157885, acc 1
2016-09-07T01:40:12.545735: step 1904, loss 0.0373858, acc 0.98
2016-09-07T01:40:13.245901: step 1905, loss 0.0629491, acc 0.98
2016-09-07T01:40:13.906568: step 1906, loss 0.0438165, acc 0.98
2016-09-07T01:40:14.617881: step 1907, loss 0.0349679, acc 0.98
2016-09-07T01:40:15.303042: step 1908, loss 0.0668384, acc 0.98
2016-09-07T01:40:16.019655: step 1909, loss 0.0180254, acc 0.98
2016-09-07T01:40:16.704713: step 1910, loss 0.113127, acc 0.94
2016-09-07T01:40:17.401652: step 1911, loss 0.0409986, acc 0.98
2016-09-07T01:40:18.092874: step 1912, loss 0.0156164, acc 1
2016-09-07T01:40:18.794690: step 1913, loss 0.0658078, acc 0.98
2016-09-07T01:40:19.506265: step 1914, loss 0.0260495, acc 1
2016-09-07T01:40:20.215582: step 1915, loss 0.082819, acc 0.98
2016-09-07T01:40:20.916030: step 1916, loss 0.0314568, acc 0.98
2016-09-07T01:40:21.614743: step 1917, loss 0.0777272, acc 0.96
2016-09-07T01:40:22.319948: step 1918, loss 0.191133, acc 0.96
2016-09-07T01:40:23.026179: step 1919, loss 0.0612821, acc 0.98
2016-09-07T01:40:23.655747: step 1920, loss 0.013036, acc 1
2016-09-07T01:40:24.352151: step 1921, loss 0.04193, acc 0.98
2016-09-07T01:40:25.044501: step 1922, loss 0.0384835, acc 0.98
2016-09-07T01:40:25.749640: step 1923, loss 0.232908, acc 0.88
2016-09-07T01:40:26.462651: step 1924, loss 0.0655529, acc 0.94
2016-09-07T01:40:27.137917: step 1925, loss 0.0207358, acc 1
2016-09-07T01:40:27.855340: step 1926, loss 0.0565431, acc 1
2016-09-07T01:40:28.549954: step 1927, loss 0.0756875, acc 0.94
2016-09-07T01:40:29.230916: step 1928, loss 0.046031, acc 0.96
2016-09-07T01:40:29.954169: step 1929, loss 0.0833727, acc 0.98
2016-09-07T01:40:30.657341: step 1930, loss 0.00392553, acc 1
2016-09-07T01:40:31.357120: step 1931, loss 0.0645138, acc 0.96
2016-09-07T01:40:32.050374: step 1932, loss 0.0628022, acc 0.98
2016-09-07T01:40:32.781838: step 1933, loss 0.0773634, acc 0.98
2016-09-07T01:40:33.482164: step 1934, loss 0.0700713, acc 0.96
2016-09-07T01:40:34.185862: step 1935, loss 0.017626, acc 1
2016-09-07T01:40:34.875787: step 1936, loss 0.0256946, acc 0.98
2016-09-07T01:40:35.560698: step 1937, loss 0.0132832, acc 1
2016-09-07T01:40:36.276226: step 1938, loss 0.0106135, acc 1
2016-09-07T01:40:36.975232: step 1939, loss 0.0605317, acc 0.98
2016-09-07T01:40:37.679283: step 1940, loss 0.0281984, acc 0.98
2016-09-07T01:40:38.372379: step 1941, loss 0.0964143, acc 0.98
2016-09-07T01:40:39.052171: step 1942, loss 0.0873163, acc 0.92
2016-09-07T01:40:39.748930: step 1943, loss 0.0128526, acc 1
2016-09-07T01:40:40.432596: step 1944, loss 0.018588, acc 1
2016-09-07T01:40:41.147887: step 1945, loss 0.0714152, acc 0.96
2016-09-07T01:40:41.850862: step 1946, loss 0.0220448, acc 0.98
2016-09-07T01:40:42.534518: step 1947, loss 0.0173703, acc 0.98
2016-09-07T01:40:43.244358: step 1948, loss 0.00707891, acc 1
2016-09-07T01:40:43.929360: step 1949, loss 0.0271099, acc 1
2016-09-07T01:40:44.631469: step 1950, loss 0.0610707, acc 0.98
2016-09-07T01:40:45.306172: step 1951, loss 0.0570459, acc 0.96
2016-09-07T01:40:46.014815: step 1952, loss 0.0941456, acc 0.94
2016-09-07T01:40:46.718149: step 1953, loss 0.0359096, acc 0.98
2016-09-07T01:40:47.427463: step 1954, loss 0.0286887, acc 0.98
2016-09-07T01:40:48.141889: step 1955, loss 0.0398523, acc 0.98
2016-09-07T01:40:48.810459: step 1956, loss 0.0352252, acc 0.98
2016-09-07T01:40:49.545603: step 1957, loss 0.0185807, acc 1
2016-09-07T01:40:50.253698: step 1958, loss 0.0119073, acc 1
2016-09-07T01:40:50.940187: step 1959, loss 0.0707889, acc 0.96
2016-09-07T01:40:51.625445: step 1960, loss 0.0173992, acc 0.98
2016-09-07T01:40:52.308229: step 1961, loss 0.0508167, acc 0.98
2016-09-07T01:40:53.025331: step 1962, loss 0.0337828, acc 0.98
2016-09-07T01:40:53.714653: step 1963, loss 0.0268041, acc 1
2016-09-07T01:40:54.418950: step 1964, loss 0.0132227, acc 1
2016-09-07T01:40:55.124428: step 1965, loss 0.0675813, acc 0.96
2016-09-07T01:40:55.814514: step 1966, loss 0.0739124, acc 0.94
2016-09-07T01:40:56.493041: step 1967, loss 0.0436859, acc 0.98
2016-09-07T01:40:57.190086: step 1968, loss 0.0773468, acc 0.96
2016-09-07T01:40:57.909264: step 1969, loss 0.0874613, acc 0.96
2016-09-07T01:40:58.619399: step 1970, loss 0.0238452, acc 1
2016-09-07T01:40:59.306974: step 1971, loss 0.0360836, acc 0.98
2016-09-07T01:41:00.005195: step 1972, loss 0.0181586, acc 1
2016-09-07T01:41:00.744045: step 1973, loss 0.0230827, acc 1
2016-09-07T01:41:01.471950: step 1974, loss 0.0250386, acc 1
2016-09-07T01:41:02.145533: step 1975, loss 0.0288722, acc 1
2016-09-07T01:41:02.839313: step 1976, loss 0.092306, acc 0.98
2016-09-07T01:41:03.540278: step 1977, loss 0.038476, acc 0.98
2016-09-07T01:41:04.242650: step 1978, loss 0.00199708, acc 1
2016-09-07T01:41:04.951214: step 1979, loss 0.0341237, acc 0.98
2016-09-07T01:41:05.652981: step 1980, loss 0.0276643, acc 0.98
2016-09-07T01:41:06.369667: step 1981, loss 0.0217832, acc 1
2016-09-07T01:41:07.058161: step 1982, loss 0.0445646, acc 0.98
2016-09-07T01:41:07.759579: step 1983, loss 0.0477104, acc 0.96
2016-09-07T01:41:08.460639: step 1984, loss 0.103409, acc 0.94
2016-09-07T01:41:09.172263: step 1985, loss 0.131603, acc 0.98
2016-09-07T01:41:09.900592: step 1986, loss 0.0149844, acc 1
2016-09-07T01:41:10.598464: step 1987, loss 0.071052, acc 0.94
2016-09-07T01:41:11.295734: step 1988, loss 0.0258237, acc 0.98
2016-09-07T01:41:12.000407: step 1989, loss 0.174179, acc 0.94
2016-09-07T01:41:12.687637: step 1990, loss 0.0999826, acc 0.96
2016-09-07T01:41:13.377496: step 1991, loss 0.0931578, acc 0.98
2016-09-07T01:41:14.043602: step 1992, loss 0.0549382, acc 0.98
2016-09-07T01:41:14.757082: step 1993, loss 0.0111323, acc 1
2016-09-07T01:41:15.484439: step 1994, loss 0.000727625, acc 1
2016-09-07T01:41:16.199313: step 1995, loss 0.0303247, acc 0.98
2016-09-07T01:41:16.901109: step 1996, loss 0.0556386, acc 0.96
2016-09-07T01:41:17.584972: step 1997, loss 0.0177563, acc 1
2016-09-07T01:41:18.291157: step 1998, loss 0.0405658, acc 0.98
2016-09-07T01:41:18.981470: step 1999, loss 0.116406, acc 0.94
2016-09-07T01:41:19.683909: step 2000, loss 0.0358608, acc 1

Evaluation:
2016-09-07T01:41:23.263981: step 2000, loss 1.21731, acc 0.772045

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-2000

2016-09-07T01:41:25.066014: step 2001, loss 0.0679068, acc 0.96
2016-09-07T01:41:25.770951: step 2002, loss 0.0406289, acc 0.98
2016-09-07T01:41:26.479792: step 2003, loss 0.0448476, acc 1
2016-09-07T01:41:27.195857: step 2004, loss 0.0314904, acc 1
2016-09-07T01:41:27.935207: step 2005, loss 0.0472059, acc 0.98
2016-09-07T01:41:28.621246: step 2006, loss 0.0619792, acc 0.96
2016-09-07T01:41:29.299567: step 2007, loss 0.0229519, acc 1
2016-09-07T01:41:29.992261: step 2008, loss 0.0173043, acc 1
2016-09-07T01:41:30.705299: step 2009, loss 0.232695, acc 0.9
2016-09-07T01:41:31.390707: step 2010, loss 0.080966, acc 0.96
2016-09-07T01:41:32.072055: step 2011, loss 0.0732208, acc 0.92
2016-09-07T01:41:32.798911: step 2012, loss 0.0800615, acc 0.96
2016-09-07T01:41:33.472780: step 2013, loss 0.0402365, acc 0.98
2016-09-07T01:41:34.154222: step 2014, loss 0.11449, acc 0.94
2016-09-07T01:41:34.862378: step 2015, loss 0.151619, acc 0.98
2016-09-07T01:41:35.566025: step 2016, loss 0.0512574, acc 0.96
2016-09-07T01:41:36.270448: step 2017, loss 0.0396879, acc 0.98
2016-09-07T01:41:36.966222: step 2018, loss 0.0806899, acc 0.98
2016-09-07T01:41:37.701939: step 2019, loss 0.0415625, acc 0.96
2016-09-07T01:41:38.412858: step 2020, loss 0.0417743, acc 0.98
2016-09-07T01:41:39.110885: step 2021, loss 0.0556222, acc 0.98
2016-09-07T01:41:39.804078: step 2022, loss 0.0922251, acc 0.96
2016-09-07T01:41:40.467444: step 2023, loss 0.120871, acc 0.92
2016-09-07T01:41:41.194477: step 2024, loss 0.0489843, acc 0.98
2016-09-07T01:41:41.894896: step 2025, loss 0.0355088, acc 1
2016-09-07T01:41:42.595363: step 2026, loss 0.0819282, acc 0.94
2016-09-07T01:41:43.294372: step 2027, loss 0.0292448, acc 0.98
2016-09-07T01:41:43.988757: step 2028, loss 0.127718, acc 0.96
2016-09-07T01:41:44.693049: step 2029, loss 0.0383578, acc 0.96
2016-09-07T01:41:45.392357: step 2030, loss 0.10536, acc 0.98
2016-09-07T01:41:46.120705: step 2031, loss 0.0759066, acc 0.94
2016-09-07T01:41:46.820856: step 2032, loss 0.00641104, acc 1
2016-09-07T01:41:47.514388: step 2033, loss 0.287854, acc 0.96
2016-09-07T01:41:48.213087: step 2034, loss 0.012822, acc 1
2016-09-07T01:41:48.925684: step 2035, loss 0.0968839, acc 0.94
2016-09-07T01:41:49.657384: step 2036, loss 0.0599543, acc 0.96
2016-09-07T01:41:50.369598: step 2037, loss 0.0622875, acc 0.98
2016-09-07T01:41:51.061333: step 2038, loss 0.0440077, acc 0.98
2016-09-07T01:41:51.758939: step 2039, loss 0.0428467, acc 1
2016-09-07T01:41:52.448223: step 2040, loss 0.0625402, acc 0.96
2016-09-07T01:41:53.140640: step 2041, loss 0.117062, acc 0.96
2016-09-07T01:41:53.838928: step 2042, loss 0.04342, acc 0.98
2016-09-07T01:41:54.551045: step 2043, loss 0.0129076, acc 1
2016-09-07T01:41:55.272943: step 2044, loss 0.0340833, acc 0.98
2016-09-07T01:41:55.954966: step 2045, loss 0.0564074, acc 0.96
2016-09-07T01:41:56.644362: step 2046, loss 0.0446801, acc 0.96
2016-09-07T01:41:57.346835: step 2047, loss 0.113106, acc 0.96
2016-09-07T01:41:58.056717: step 2048, loss 0.205459, acc 0.9
2016-09-07T01:41:58.758757: step 2049, loss 0.0602852, acc 0.96
2016-09-07T01:41:59.475070: step 2050, loss 0.0760826, acc 0.94
2016-09-07T01:42:00.180846: step 2051, loss 0.106668, acc 0.96
2016-09-07T01:42:00.901748: step 2052, loss 0.0957256, acc 0.96
2016-09-07T01:42:01.617898: step 2053, loss 0.0924856, acc 0.94
2016-09-07T01:42:02.280213: step 2054, loss 0.0320789, acc 1
2016-09-07T01:42:02.978721: step 2055, loss 0.0214679, acc 1
2016-09-07T01:42:03.674289: step 2056, loss 0.0877221, acc 0.94
2016-09-07T01:42:04.368132: step 2057, loss 0.0911317, acc 0.94
2016-09-07T01:42:05.078049: step 2058, loss 0.0622593, acc 0.96
2016-09-07T01:42:05.778910: step 2059, loss 0.069542, acc 0.98
2016-09-07T01:42:06.498444: step 2060, loss 0.0315545, acc 1
2016-09-07T01:42:07.204419: step 2061, loss 0.0446821, acc 0.98
2016-09-07T01:42:07.899088: step 2062, loss 0.0311051, acc 1
2016-09-07T01:42:08.613801: step 2063, loss 0.0775754, acc 0.98
2016-09-07T01:42:09.316899: step 2064, loss 0.0683932, acc 0.96
2016-09-07T01:42:10.012551: step 2065, loss 0.0259332, acc 1
2016-09-07T01:42:10.685978: step 2066, loss 0.0403345, acc 0.98
2016-09-07T01:42:11.370438: step 2067, loss 0.0644907, acc 0.96
2016-09-07T01:42:12.047604: step 2068, loss 0.0801388, acc 0.94
2016-09-07T01:42:12.753690: step 2069, loss 0.0082521, acc 1
2016-09-07T01:42:13.451438: step 2070, loss 0.0446189, acc 0.98
2016-09-07T01:42:14.152852: step 2071, loss 0.0403423, acc 0.98
2016-09-07T01:42:14.887296: step 2072, loss 0.0475556, acc 0.98
2016-09-07T01:42:15.570994: step 2073, loss 0.0172333, acc 1
2016-09-07T01:42:16.263138: step 2074, loss 0.0622766, acc 0.96
2016-09-07T01:42:16.971027: step 2075, loss 0.127102, acc 0.94
2016-09-07T01:42:17.665013: step 2076, loss 0.0510623, acc 0.98
2016-09-07T01:42:18.366074: step 2077, loss 0.0444478, acc 0.98
2016-09-07T01:42:19.015907: step 2078, loss 0.0450256, acc 0.98
2016-09-07T01:42:19.720500: step 2079, loss 0.0223335, acc 1
2016-09-07T01:42:20.416055: step 2080, loss 0.0884819, acc 0.94
2016-09-07T01:42:21.100068: step 2081, loss 0.0161137, acc 1
2016-09-07T01:42:21.806865: step 2082, loss 0.0328657, acc 1
2016-09-07T01:42:22.512114: step 2083, loss 0.0484598, acc 0.98
2016-09-07T01:42:23.233256: step 2084, loss 0.0233205, acc 1
2016-09-07T01:42:23.922370: step 2085, loss 0.0377248, acc 1
2016-09-07T01:42:24.614838: step 2086, loss 0.0419505, acc 0.96
2016-09-07T01:42:25.306150: step 2087, loss 0.0358538, acc 0.98
2016-09-07T01:42:26.000118: step 2088, loss 0.0447403, acc 0.98
2016-09-07T01:42:26.725548: step 2089, loss 0.020064, acc 1
2016-09-07T01:42:27.395844: step 2090, loss 0.0588273, acc 0.96
2016-09-07T01:42:28.112392: step 2091, loss 0.0177409, acc 0.98
2016-09-07T01:42:28.793467: step 2092, loss 0.0127907, acc 1
2016-09-07T01:42:29.490987: step 2093, loss 0.0775289, acc 0.96
2016-09-07T01:42:30.190115: step 2094, loss 0.0661616, acc 0.98
2016-09-07T01:42:30.880882: step 2095, loss 0.0158413, acc 1
2016-09-07T01:42:31.606682: step 2096, loss 0.0589385, acc 0.98
2016-09-07T01:42:32.294111: step 2097, loss 0.0614497, acc 0.98
2016-09-07T01:42:33.008621: step 2098, loss 0.0350665, acc 1
2016-09-07T01:42:33.708107: step 2099, loss 0.0169977, acc 0.98
2016-09-07T01:42:34.404265: step 2100, loss 0.0180149, acc 0.98

Evaluation:
2016-09-07T01:42:38.030591: step 2100, loss 1.37155, acc 0.772045

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-2100

2016-09-07T01:42:39.827910: step 2101, loss 0.0126888, acc 1
2016-09-07T01:42:40.536781: step 2102, loss 0.0796114, acc 0.96
2016-09-07T01:42:41.246152: step 2103, loss 0.0677964, acc 0.98
2016-09-07T01:42:41.934549: step 2104, loss 0.0725816, acc 0.96
2016-09-07T01:42:42.679116: step 2105, loss 0.0662856, acc 0.96
2016-09-07T01:42:43.389847: step 2106, loss 0.0123181, acc 1
2016-09-07T01:42:44.098547: step 2107, loss 0.024726, acc 1
2016-09-07T01:42:44.780598: step 2108, loss 0.0123824, acc 1
2016-09-07T01:42:45.445708: step 2109, loss 0.0632778, acc 0.96
2016-09-07T01:42:46.147680: step 2110, loss 0.160147, acc 0.9
2016-09-07T01:42:46.850898: step 2111, loss 0.105646, acc 0.96
2016-09-07T01:42:47.498769: step 2112, loss 0.124802, acc 0.977273
2016-09-07T01:42:48.201112: step 2113, loss 0.0284699, acc 1
2016-09-07T01:42:48.888912: step 2114, loss 0.0200183, acc 1
2016-09-07T01:42:49.582841: step 2115, loss 0.140464, acc 0.94
2016-09-07T01:42:50.264634: step 2116, loss 0.140812, acc 0.94
2016-09-07T01:42:50.971120: step 2117, loss 0.0312402, acc 1
2016-09-07T01:42:51.656576: step 2118, loss 0.116865, acc 0.96
2016-09-07T01:42:52.340488: step 2119, loss 0.0379576, acc 1
2016-09-07T01:42:53.018115: step 2120, loss 0.0377817, acc 0.98
2016-09-07T01:42:53.715367: step 2121, loss 0.0316929, acc 1
2016-09-07T01:42:54.417467: step 2122, loss 0.0960102, acc 0.94
2016-09-07T01:42:55.088962: step 2123, loss 0.0490225, acc 0.98
2016-09-07T01:42:55.795223: step 2124, loss 0.0368307, acc 0.96
2016-09-07T01:42:56.486529: step 2125, loss 0.150336, acc 0.94
2016-09-07T01:42:57.177057: step 2126, loss 0.140624, acc 0.94
2016-09-07T01:42:57.903539: step 2127, loss 0.0697546, acc 0.94
2016-09-07T01:42:58.584774: step 2128, loss 0.0903108, acc 0.98
2016-09-07T01:42:59.282924: step 2129, loss 0.0697152, acc 0.98
2016-09-07T01:42:59.961591: step 2130, loss 0.0450679, acc 0.98
2016-09-07T01:43:00.718580: step 2131, loss 0.0291504, acc 1
2016-09-07T01:43:01.406306: step 2132, loss 0.0720752, acc 0.98
2016-09-07T01:43:02.104065: step 2133, loss 0.0384845, acc 0.98
2016-09-07T01:43:02.815637: step 2134, loss 0.090496, acc 0.94
2016-09-07T01:43:03.496010: step 2135, loss 0.100761, acc 0.94
2016-09-07T01:43:04.208984: step 2136, loss 0.0248275, acc 0.98
2016-09-07T01:43:04.907036: step 2137, loss 0.052387, acc 0.98
2016-09-07T01:43:05.624522: step 2138, loss 0.0801218, acc 0.98
2016-09-07T01:43:06.318628: step 2139, loss 0.00526306, acc 1
2016-09-07T01:43:07.022541: step 2140, loss 0.00896637, acc 1
2016-09-07T01:43:07.721700: step 2141, loss 0.0512439, acc 0.96
2016-09-07T01:43:08.398832: step 2142, loss 0.0473131, acc 0.96
2016-09-07T01:43:09.109721: step 2143, loss 0.0618521, acc 0.96
2016-09-07T01:43:09.858271: step 2144, loss 0.0766429, acc 0.96
2016-09-07T01:43:10.555909: step 2145, loss 0.0406412, acc 0.98
2016-09-07T01:43:11.256595: step 2146, loss 0.067681, acc 0.98
2016-09-07T01:43:11.930941: step 2147, loss 0.0424443, acc 1
2016-09-07T01:43:12.629615: step 2148, loss 0.228006, acc 0.92
2016-09-07T01:43:13.320718: step 2149, loss 0.164936, acc 0.96
2016-09-07T01:43:13.993943: step 2150, loss 0.0260515, acc 0.98
2016-09-07T01:43:14.680513: step 2151, loss 0.0346904, acc 0.98
2016-09-07T01:43:15.375154: step 2152, loss 0.0498619, acc 0.98
2016-09-07T01:43:16.056169: step 2153, loss 0.0536362, acc 0.98
2016-09-07T01:43:16.733589: step 2154, loss 0.0989359, acc 0.92
2016-09-07T01:43:17.471482: step 2155, loss 0.0314002, acc 1
2016-09-07T01:43:18.168997: step 2156, loss 0.0261403, acc 1
2016-09-07T01:43:18.866121: step 2157, loss 0.0817478, acc 0.96
2016-09-07T01:43:19.573527: step 2158, loss 0.060588, acc 0.98
2016-09-07T01:43:20.294222: step 2159, loss 0.0775336, acc 0.96
2016-09-07T01:43:21.013872: step 2160, loss 0.0541011, acc 0.94
2016-09-07T01:43:21.710877: step 2161, loss 0.00498702, acc 1
2016-09-07T01:43:22.402824: step 2162, loss 0.0369199, acc 1
2016-09-07T01:43:23.089855: step 2163, loss 0.0112853, acc 1
2016-09-07T01:43:23.801530: step 2164, loss 0.0324881, acc 1
2016-09-07T01:43:24.486989: step 2165, loss 0.109967, acc 0.96
2016-09-07T01:43:25.155337: step 2166, loss 0.0164411, acc 1
2016-09-07T01:43:25.886199: step 2167, loss 0.0362457, acc 0.96
2016-09-07T01:43:26.600502: step 2168, loss 0.0431868, acc 0.98
2016-09-07T01:43:27.310857: step 2169, loss 0.0463631, acc 0.96
2016-09-07T01:43:28.014932: step 2170, loss 0.0143227, acc 1
2016-09-07T01:43:28.703592: step 2171, loss 0.0682599, acc 0.94
2016-09-07T01:43:29.422431: step 2172, loss 0.0191945, acc 1
2016-09-07T01:43:30.103753: step 2173, loss 0.108935, acc 0.96
2016-09-07T01:43:30.795533: step 2174, loss 0.0201061, acc 1
2016-09-07T01:43:31.488451: step 2175, loss 0.124772, acc 0.94
2016-09-07T01:43:32.188220: step 2176, loss 0.0203209, acc 1
2016-09-07T01:43:32.888368: step 2177, loss 0.0234271, acc 1
2016-09-07T01:43:33.551005: step 2178, loss 0.0713389, acc 0.96
2016-09-07T01:43:34.267491: step 2179, loss 0.0299322, acc 0.98
2016-09-07T01:43:34.998308: step 2180, loss 0.0130428, acc 1
2016-09-07T01:43:35.689484: step 2181, loss 0.0161325, acc 1
2016-09-07T01:43:36.387401: step 2182, loss 0.0968281, acc 0.94
2016-09-07T01:43:37.084940: step 2183, loss 0.123532, acc 0.98
2016-09-07T01:43:37.783453: step 2184, loss 0.0766696, acc 0.96
2016-09-07T01:43:38.472686: step 2185, loss 0.0161545, acc 1
2016-09-07T01:43:39.182173: step 2186, loss 0.0209952, acc 0.98
2016-09-07T01:43:39.882877: step 2187, loss 0.259918, acc 0.9
2016-09-07T01:43:40.582473: step 2188, loss 0.134884, acc 0.92
2016-09-07T01:43:41.281852: step 2189, loss 0.000807, acc 1
2016-09-07T01:43:41.952432: step 2190, loss 0.0045325, acc 1
2016-09-07T01:43:42.671632: step 2191, loss 0.0439665, acc 0.96
2016-09-07T01:43:43.357196: step 2192, loss 0.050564, acc 0.98
2016-09-07T01:43:44.092883: step 2193, loss 0.00524738, acc 1
2016-09-07T01:43:44.797980: step 2194, loss 0.0143459, acc 1
2016-09-07T01:43:45.490589: step 2195, loss 0.0282641, acc 0.98
2016-09-07T01:43:46.182316: step 2196, loss 0.030097, acc 0.98
2016-09-07T01:43:46.857766: step 2197, loss 0.0416942, acc 0.98
2016-09-07T01:43:47.562021: step 2198, loss 0.0437249, acc 1
2016-09-07T01:43:48.256198: step 2199, loss 0.0861027, acc 0.96
2016-09-07T01:43:48.960897: step 2200, loss 0.0812547, acc 0.98

Evaluation:
2016-09-07T01:43:52.557241: step 2200, loss 1.20967, acc 0.765478

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-2200

2016-09-07T01:43:54.459705: step 2201, loss 0.0134855, acc 1
2016-09-07T01:43:55.165193: step 2202, loss 0.0950097, acc 0.96
2016-09-07T01:43:55.901659: step 2203, loss 0.042198, acc 0.98
2016-09-07T01:43:56.583494: step 2204, loss 0.0709501, acc 0.98
2016-09-07T01:43:57.262633: step 2205, loss 0.0463684, acc 0.96
2016-09-07T01:43:57.949480: step 2206, loss 0.056409, acc 0.98
2016-09-07T01:43:58.643944: step 2207, loss 0.0257309, acc 1
2016-09-07T01:43:59.357178: step 2208, loss 0.0555739, acc 0.98
2016-09-07T01:44:00.026594: step 2209, loss 0.0394936, acc 0.98
2016-09-07T01:44:00.796271: step 2210, loss 0.0984655, acc 0.94
2016-09-07T01:44:01.477058: step 2211, loss 0.0524598, acc 0.96
2016-09-07T01:44:02.181686: step 2212, loss 0.0359785, acc 0.98
2016-09-07T01:44:02.865605: step 2213, loss 0.0271929, acc 1
2016-09-07T01:44:03.567948: step 2214, loss 0.032183, acc 0.98
2016-09-07T01:44:04.276097: step 2215, loss 0.0778872, acc 0.98
2016-09-07T01:44:04.972587: step 2216, loss 0.0286355, acc 1
2016-09-07T01:44:05.665094: step 2217, loss 0.0334841, acc 0.98
2016-09-07T01:44:06.354505: step 2218, loss 0.12036, acc 0.9
2016-09-07T01:44:07.045349: step 2219, loss 0.0130641, acc 1
2016-09-07T01:44:07.744952: step 2220, loss 0.117958, acc 0.94
2016-09-07T01:44:08.405637: step 2221, loss 0.0754028, acc 0.96
2016-09-07T01:44:09.122458: step 2222, loss 0.0440723, acc 0.98
2016-09-07T01:44:09.816931: step 2223, loss 0.0058253, acc 1
2016-09-07T01:44:10.502199: step 2224, loss 0.0188057, acc 1
2016-09-07T01:44:11.205135: step 2225, loss 0.0184336, acc 1
2016-09-07T01:44:11.913655: step 2226, loss 0.0179947, acc 1
2016-09-07T01:44:12.632097: step 2227, loss 0.0617276, acc 0.98
2016-09-07T01:44:13.330916: step 2228, loss 0.079482, acc 0.96
2016-09-07T01:44:14.004423: step 2229, loss 0.0389724, acc 0.98
2016-09-07T01:44:14.703944: step 2230, loss 0.0788, acc 0.98
2016-09-07T01:44:15.409248: step 2231, loss 0.0016901, acc 1
2016-09-07T01:44:16.100561: step 2232, loss 0.0747061, acc 0.96
2016-09-07T01:44:16.816991: step 2233, loss 0.0490848, acc 0.94
2016-09-07T01:44:17.542998: step 2234, loss 0.107082, acc 0.98
2016-09-07T01:44:18.220230: step 2235, loss 0.087122, acc 0.94
2016-09-07T01:44:18.934224: step 2236, loss 0.0544261, acc 0.98
2016-09-07T01:44:19.642533: step 2237, loss 0.0401689, acc 0.98
2016-09-07T01:44:20.327028: step 2238, loss 0.0232136, acc 0.98
2016-09-07T01:44:21.014356: step 2239, loss 0.0188102, acc 1
2016-09-07T01:44:21.681878: step 2240, loss 0.00233281, acc 1
2016-09-07T01:44:22.403432: step 2241, loss 0.0503001, acc 0.96
2016-09-07T01:44:23.063832: step 2242, loss 0.0783907, acc 0.96
2016-09-07T01:44:23.761572: step 2243, loss 0.147943, acc 0.9
2016-09-07T01:44:24.461156: step 2244, loss 0.0226593, acc 1
2016-09-07T01:44:25.175886: step 2245, loss 0.049333, acc 1
2016-09-07T01:44:25.890664: step 2246, loss 0.0305952, acc 1
2016-09-07T01:44:26.608465: step 2247, loss 0.0191802, acc 0.98
2016-09-07T01:44:27.302637: step 2248, loss 0.0415709, acc 0.98
2016-09-07T01:44:28.008133: step 2249, loss 0.0224119, acc 1
2016-09-07T01:44:28.700781: step 2250, loss 0.0168781, acc 1
2016-09-07T01:44:29.403484: step 2251, loss 0.014969, acc 1
2016-09-07T01:44:30.089601: step 2252, loss 0.27608, acc 0.96
2016-09-07T01:44:30.818702: step 2253, loss 0.0338441, acc 1
2016-09-07T01:44:31.523605: step 2254, loss 0.00169303, acc 1
2016-09-07T01:44:32.215553: step 2255, loss 0.022231, acc 1
2016-09-07T01:44:32.917650: step 2256, loss 0.0680807, acc 0.98
2016-09-07T01:44:33.625792: step 2257, loss 0.0324917, acc 0.98
2016-09-07T01:44:34.354418: step 2258, loss 0.100805, acc 0.96
2016-09-07T01:44:35.036868: step 2259, loss 0.0282967, acc 0.98
2016-09-07T01:44:35.726161: step 2260, loss 0.061388, acc 0.98
2016-09-07T01:44:36.417556: step 2261, loss 0.0938698, acc 0.94
2016-09-07T01:44:37.113763: step 2262, loss 0.0188014, acc 1
2016-09-07T01:44:37.818003: step 2263, loss 0.0110819, acc 1
2016-09-07T01:44:38.512516: step 2264, loss 0.0322146, acc 0.96
2016-09-07T01:44:39.240509: step 2265, loss 0.0260499, acc 1
2016-09-07T01:44:39.930435: step 2266, loss 0.0704513, acc 0.98
2016-09-07T01:44:40.609905: step 2267, loss 0.193611, acc 0.96
2016-09-07T01:44:41.293075: step 2268, loss 0.0488876, acc 0.98
2016-09-07T01:44:41.998365: step 2269, loss 0.0231856, acc 1
2016-09-07T01:44:42.708823: step 2270, loss 0.0376147, acc 1
2016-09-07T01:44:43.390427: step 2271, loss 0.0509356, acc 0.96
2016-09-07T01:44:44.091313: step 2272, loss 0.0640073, acc 0.98
2016-09-07T01:44:44.796784: step 2273, loss 0.00206615, acc 1
2016-09-07T01:44:45.480737: step 2274, loss 0.0933466, acc 0.96
2016-09-07T01:44:46.193454: step 2275, loss 0.0586828, acc 0.94
2016-09-07T01:44:46.883556: step 2276, loss 0.0464235, acc 0.96
2016-09-07T01:44:47.616932: step 2277, loss 0.0488541, acc 1
2016-09-07T01:44:48.294658: step 2278, loss 0.0358486, acc 0.96
2016-09-07T01:44:48.992332: step 2279, loss 0.0130715, acc 1
2016-09-07T01:44:49.705627: step 2280, loss 0.0970897, acc 0.94
2016-09-07T01:44:50.414440: step 2281, loss 0.0516107, acc 0.98
2016-09-07T01:44:51.138316: step 2282, loss 0.0344763, acc 1
2016-09-07T01:44:51.809061: step 2283, loss 0.0929335, acc 0.96
2016-09-07T01:44:52.492728: step 2284, loss 0.087151, acc 0.96
2016-09-07T01:44:53.195828: step 2285, loss 0.0302862, acc 0.98
2016-09-07T01:44:53.910041: step 2286, loss 0.0356338, acc 1
2016-09-07T01:44:54.601674: step 2287, loss 0.0256717, acc 0.98
2016-09-07T01:44:55.287665: step 2288, loss 0.110428, acc 0.94
2016-09-07T01:44:55.988903: step 2289, loss 0.0197254, acc 1
2016-09-07T01:44:56.678199: step 2290, loss 0.244974, acc 0.94
2016-09-07T01:44:57.364260: step 2291, loss 0.0349584, acc 0.98
2016-09-07T01:44:58.088854: step 2292, loss 0.0478856, acc 1
2016-09-07T01:44:58.774115: step 2293, loss 0.00320698, acc 1
2016-09-07T01:44:59.471774: step 2294, loss 0.0216056, acc 1
2016-09-07T01:45:00.147884: step 2295, loss 0.0309715, acc 0.98
2016-09-07T01:45:00.869445: step 2296, loss 0.0748548, acc 0.98
2016-09-07T01:45:01.554637: step 2297, loss 0.0931172, acc 0.96
2016-09-07T01:45:02.264275: step 2298, loss 0.0256792, acc 1
2016-09-07T01:45:02.973787: step 2299, loss 0.0432234, acc 0.98
2016-09-07T01:45:03.670780: step 2300, loss 0.0590305, acc 0.98

Evaluation:
2016-09-07T01:45:07.254567: step 2300, loss 1.25486, acc 0.765478

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-2300

2016-09-07T01:45:09.164814: step 2301, loss 0.0287154, acc 1
2016-09-07T01:45:09.855329: step 2302, loss 0.0174885, acc 1
2016-09-07T01:45:10.565114: step 2303, loss 0.0296029, acc 0.98
2016-09-07T01:45:11.219077: step 2304, loss 0.00225475, acc 1
2016-09-07T01:45:11.949360: step 2305, loss 0.0375935, acc 1
2016-09-07T01:45:12.664468: step 2306, loss 0.0216245, acc 0.98
2016-09-07T01:45:13.366269: step 2307, loss 0.0496296, acc 0.96
2016-09-07T01:45:14.105010: step 2308, loss 0.0085959, acc 1
2016-09-07T01:45:14.806811: step 2309, loss 0.0360912, acc 0.98
2016-09-07T01:45:15.498092: step 2310, loss 0.062282, acc 0.94
2016-09-07T01:45:16.172232: step 2311, loss 0.0270791, acc 0.98
2016-09-07T01:45:16.883125: step 2312, loss 0.0201447, acc 0.98
2016-09-07T01:45:17.630302: step 2313, loss 0.00442998, acc 1
2016-09-07T01:45:18.325561: step 2314, loss 0.0101705, acc 1
2016-09-07T01:45:19.016092: step 2315, loss 0.0282309, acc 0.98
2016-09-07T01:45:19.725641: step 2316, loss 0.0550886, acc 0.98
2016-09-07T01:45:20.415597: step 2317, loss 0.0684581, acc 0.94
2016-09-07T01:45:21.122644: step 2318, loss 0.0385597, acc 0.98
2016-09-07T01:45:21.798916: step 2319, loss 0.0168603, acc 1
2016-09-07T01:45:22.512631: step 2320, loss 0.0500206, acc 0.96
2016-09-07T01:45:23.188713: step 2321, loss 0.0338836, acc 0.96
2016-09-07T01:45:23.882781: step 2322, loss 0.0184749, acc 1
2016-09-07T01:45:24.572628: step 2323, loss 0.121421, acc 0.98
2016-09-07T01:45:25.262754: step 2324, loss 0.0124564, acc 1
2016-09-07T01:45:25.954016: step 2325, loss 0.0126846, acc 1
2016-09-07T01:45:26.641670: step 2326, loss 0.041434, acc 0.98
2016-09-07T01:45:27.346139: step 2327, loss 0.00663464, acc 1
2016-09-07T01:45:28.047738: step 2328, loss 0.0991123, acc 0.94
2016-09-07T01:45:28.736523: step 2329, loss 0.0308514, acc 0.98
2016-09-07T01:45:29.435462: step 2330, loss 0.00754882, acc 1
2016-09-07T01:45:30.114950: step 2331, loss 0.00156375, acc 1
2016-09-07T01:45:30.819224: step 2332, loss 0.0276551, acc 0.98
2016-09-07T01:45:31.488944: step 2333, loss 0.035123, acc 0.98
2016-09-07T01:45:32.209312: step 2334, loss 0.0245453, acc 0.98
2016-09-07T01:45:32.922981: step 2335, loss 0.00200571, acc 1
2016-09-07T01:45:33.610719: step 2336, loss 0.0885118, acc 0.98
2016-09-07T01:45:34.325085: step 2337, loss 0.0502904, acc 0.96
2016-09-07T01:45:35.027691: step 2338, loss 0.0991025, acc 0.96
2016-09-07T01:45:35.731011: step 2339, loss 0.0189484, acc 1
2016-09-07T01:45:36.437929: step 2340, loss 0.0529759, acc 0.98
2016-09-07T01:45:37.134134: step 2341, loss 0.0387288, acc 0.98
2016-09-07T01:45:37.827827: step 2342, loss 0.0251612, acc 1
2016-09-07T01:45:38.549086: step 2343, loss 0.0137952, acc 1
2016-09-07T01:45:39.259833: step 2344, loss 0.0197077, acc 0.98
2016-09-07T01:45:39.933781: step 2345, loss 0.0499238, acc 0.96
2016-09-07T01:45:40.635137: step 2346, loss 0.0225309, acc 1
2016-09-07T01:45:41.320342: step 2347, loss 0.0183926, acc 1
2016-09-07T01:45:42.016793: step 2348, loss 0.0166276, acc 1
2016-09-07T01:45:42.722213: step 2349, loss 0.00926607, acc 1
2016-09-07T01:45:43.406533: step 2350, loss 0.118226, acc 0.94
2016-09-07T01:45:44.120329: step 2351, loss 0.021531, acc 0.98
2016-09-07T01:45:44.815812: step 2352, loss 0.0518042, acc 0.96
2016-09-07T01:45:45.501951: step 2353, loss 0.0375977, acc 0.98
2016-09-07T01:45:46.222338: step 2354, loss 0.0157653, acc 1
2016-09-07T01:45:46.895319: step 2355, loss 0.011101, acc 1
2016-09-07T01:45:47.577071: step 2356, loss 0.0327845, acc 0.98
2016-09-07T01:45:48.264488: step 2357, loss 0.00726895, acc 1
2016-09-07T01:45:48.967782: step 2358, loss 0.0414277, acc 0.96
2016-09-07T01:45:49.676970: step 2359, loss 0.0145181, acc 1
2016-09-07T01:45:50.374645: step 2360, loss 0.107078, acc 0.98
2016-09-07T01:45:51.085437: step 2361, loss 0.106851, acc 0.96
2016-09-07T01:45:51.787305: step 2362, loss 0.0702934, acc 0.98
2016-09-07T01:45:52.482729: step 2363, loss 0.00630044, acc 1
2016-09-07T01:45:53.152716: step 2364, loss 0.0469013, acc 0.98
2016-09-07T01:45:53.862558: step 2365, loss 0.0424006, acc 0.96
2016-09-07T01:45:54.569903: step 2366, loss 0.0228555, acc 1
2016-09-07T01:45:55.265715: step 2367, loss 0.028415, acc 1
2016-09-07T01:45:55.959017: step 2368, loss 0.0978122, acc 0.96
2016-09-07T01:45:56.661790: step 2369, loss 0.053007, acc 0.96
2016-09-07T01:45:57.379155: step 2370, loss 0.068894, acc 0.96
2016-09-07T01:45:58.058270: step 2371, loss 0.00210425, acc 1
2016-09-07T01:45:58.747981: step 2372, loss 0.0456411, acc 0.98
2016-09-07T01:45:59.442971: step 2373, loss 0.0088682, acc 1
2016-09-07T01:46:00.127561: step 2374, loss 0.00333444, acc 1
2016-09-07T01:46:00.841913: step 2375, loss 0.0739752, acc 0.98
2016-09-07T01:46:01.503999: step 2376, loss 0.0213975, acc 1
2016-09-07T01:46:02.216537: step 2377, loss 0.00420492, acc 1
2016-09-07T01:46:02.924223: step 2378, loss 0.0444152, acc 0.98
2016-09-07T01:46:03.620848: step 2379, loss 0.0183961, acc 1
2016-09-07T01:46:04.321672: step 2380, loss 0.116115, acc 0.96
2016-09-07T01:46:05.002191: step 2381, loss 0.0224032, acc 0.98
2016-09-07T01:46:05.694431: step 2382, loss 0.00863735, acc 1
2016-09-07T01:46:06.368418: step 2383, loss 0.105871, acc 0.94
2016-09-07T01:46:07.093918: step 2384, loss 0.0804141, acc 0.94
2016-09-07T01:46:07.794547: step 2385, loss 0.0257799, acc 0.98
2016-09-07T01:46:08.489651: step 2386, loss 0.036357, acc 0.98
2016-09-07T01:46:09.193611: step 2387, loss 0.0418716, acc 0.98
2016-09-07T01:46:09.905645: step 2388, loss 0.0439691, acc 0.98
2016-09-07T01:46:10.621342: step 2389, loss 0.0162676, acc 1
2016-09-07T01:46:11.339502: step 2390, loss 0.0507158, acc 0.96
2016-09-07T01:46:12.028681: step 2391, loss 0.0134502, acc 1
2016-09-07T01:46:12.722607: step 2392, loss 0.0802586, acc 0.96
2016-09-07T01:46:13.419893: step 2393, loss 0.0353276, acc 0.98
2016-09-07T01:46:14.125700: step 2394, loss 0.0945668, acc 0.98
2016-09-07T01:46:14.806926: step 2395, loss 0.0744211, acc 0.96
2016-09-07T01:46:15.499351: step 2396, loss 0.0228817, acc 0.98
2016-09-07T01:46:16.191870: step 2397, loss 0.0913126, acc 0.94
2016-09-07T01:46:16.907948: step 2398, loss 0.139508, acc 0.96
2016-09-07T01:46:17.604726: step 2399, loss 0.0461104, acc 0.96
2016-09-07T01:46:18.315286: step 2400, loss 0.207167, acc 0.96

Evaluation:
2016-09-07T01:46:21.917376: step 2400, loss 1.41299, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-2400

2016-09-07T01:46:23.869836: step 2401, loss 0.0224847, acc 1
2016-09-07T01:46:24.558791: step 2402, loss 0.0524448, acc 0.98
2016-09-07T01:46:25.236871: step 2403, loss 0.0368832, acc 0.98
2016-09-07T01:46:25.938370: step 2404, loss 0.0613674, acc 0.96
2016-09-07T01:46:26.637231: step 2405, loss 0.0138243, acc 1
2016-09-07T01:46:27.341364: step 2406, loss 0.0595252, acc 0.98
2016-09-07T01:46:28.030769: step 2407, loss 0.0321033, acc 0.98
2016-09-07T01:46:28.736431: step 2408, loss 0.0303856, acc 0.98
2016-09-07T01:46:29.417010: step 2409, loss 0.0717926, acc 0.98
2016-09-07T01:46:30.120709: step 2410, loss 0.0170077, acc 1
2016-09-07T01:46:30.816806: step 2411, loss 0.0121134, acc 1
2016-09-07T01:46:31.507648: step 2412, loss 0.0485473, acc 0.98
2016-09-07T01:46:32.218292: step 2413, loss 0.0332458, acc 0.98
2016-09-07T01:46:32.914376: step 2414, loss 0.18415, acc 0.94
2016-09-07T01:46:33.604274: step 2415, loss 0.0260795, acc 1
2016-09-07T01:46:34.325686: step 2416, loss 0.0209461, acc 1
2016-09-07T01:46:35.020781: step 2417, loss 0.0507469, acc 0.98
2016-09-07T01:46:35.737340: step 2418, loss 0.0624914, acc 0.94
2016-09-07T01:46:36.415526: step 2419, loss 0.0415659, acc 1
2016-09-07T01:46:37.112257: step 2420, loss 0.0543858, acc 0.96
2016-09-07T01:46:37.814793: step 2421, loss 0.0390293, acc 0.98
2016-09-07T01:46:38.493275: step 2422, loss 0.0328232, acc 1
2016-09-07T01:46:39.194074: step 2423, loss 0.0310998, acc 1
2016-09-07T01:46:39.882883: step 2424, loss 0.0602185, acc 1
2016-09-07T01:46:40.578259: step 2425, loss 0.147742, acc 0.96
2016-09-07T01:46:41.266794: step 2426, loss 0.0801554, acc 0.94
2016-09-07T01:46:41.974965: step 2427, loss 0.0032604, acc 1
2016-09-07T01:46:42.653917: step 2428, loss 0.175728, acc 0.92
2016-09-07T01:46:43.367717: step 2429, loss 0.0248942, acc 0.98
2016-09-07T01:46:44.070405: step 2430, loss 0.0368928, acc 1
2016-09-07T01:46:44.773096: step 2431, loss 0.0394028, acc 0.98
2016-09-07T01:46:45.476174: step 2432, loss 0.0115782, acc 1
2016-09-07T01:46:46.174932: step 2433, loss 0.00155589, acc 1
2016-09-07T01:46:46.866506: step 2434, loss 0.026159, acc 1
2016-09-07T01:46:47.592743: step 2435, loss 0.206802, acc 0.92
2016-09-07T01:46:48.276931: step 2436, loss 0.06495, acc 1
2016-09-07T01:46:48.998542: step 2437, loss 0.0663186, acc 0.94
2016-09-07T01:46:49.702772: step 2438, loss 0.0184586, acc 1
2016-09-07T01:46:50.396587: step 2439, loss 0.000388468, acc 1
2016-09-07T01:46:51.091295: step 2440, loss 0.0257616, acc 1
2016-09-07T01:46:51.780489: step 2441, loss 0.0286662, acc 1
2016-09-07T01:46:52.457395: step 2442, loss 0.0251683, acc 1
2016-09-07T01:46:53.143950: step 2443, loss 0.0120254, acc 1
2016-09-07T01:46:53.861319: step 2444, loss 0.0203849, acc 0.98
2016-09-07T01:46:54.564118: step 2445, loss 0.112804, acc 0.98
2016-09-07T01:46:55.254460: step 2446, loss 0.0335819, acc 1
2016-09-07T01:46:55.935439: step 2447, loss 0.0158824, acc 1
2016-09-07T01:46:56.631788: step 2448, loss 0.0405077, acc 0.96
2016-09-07T01:46:57.337128: step 2449, loss 0.0746155, acc 0.96
2016-09-07T01:46:58.005899: step 2450, loss 0.022772, acc 0.98
2016-09-07T01:46:58.730460: step 2451, loss 0.0261713, acc 0.98
2016-09-07T01:46:59.405788: step 2452, loss 0.0689839, acc 0.96
2016-09-07T01:47:00.084952: step 2453, loss 0.0429925, acc 0.98
2016-09-07T01:47:00.816070: step 2454, loss 0.0322239, acc 1
2016-09-07T01:47:01.514275: step 2455, loss 0.0365792, acc 0.96
2016-09-07T01:47:02.209361: step 2456, loss 0.0259036, acc 0.98
2016-09-07T01:47:02.900871: step 2457, loss 0.0475363, acc 0.98
2016-09-07T01:47:03.599185: step 2458, loss 0.0719609, acc 0.98
2016-09-07T01:47:04.319044: step 2459, loss 0.0812029, acc 0.94
2016-09-07T01:47:05.026519: step 2460, loss 0.0353486, acc 0.98
2016-09-07T01:47:05.724545: step 2461, loss 0.0400353, acc 0.98
2016-09-07T01:47:06.420556: step 2462, loss 0.0525588, acc 0.96
2016-09-07T01:47:07.139128: step 2463, loss 0.0487943, acc 0.98
2016-09-07T01:47:07.848890: step 2464, loss 0.0309194, acc 1
2016-09-07T01:47:08.535155: step 2465, loss 0.121317, acc 0.98
2016-09-07T01:47:09.282484: step 2466, loss 0.0431399, acc 0.98
2016-09-07T01:47:09.987472: step 2467, loss 0.0207675, acc 1
2016-09-07T01:47:10.695493: step 2468, loss 0.0920275, acc 0.94
2016-09-07T01:47:11.370402: step 2469, loss 0.00890933, acc 1
2016-09-07T01:47:12.063416: step 2470, loss 0.0608805, acc 1
2016-09-07T01:47:12.766206: step 2471, loss 0.0370054, acc 0.98
2016-09-07T01:47:13.470570: step 2472, loss 0.0612805, acc 0.94
2016-09-07T01:47:14.179559: step 2473, loss 0.125769, acc 0.92
2016-09-07T01:47:14.873269: step 2474, loss 0.0558643, acc 0.98
2016-09-07T01:47:15.587003: step 2475, loss 0.0172905, acc 1
2016-09-07T01:47:16.295078: step 2476, loss 0.0165341, acc 1
2016-09-07T01:47:17.014071: step 2477, loss 0.0372271, acc 0.98
2016-09-07T01:47:17.717942: step 2478, loss 0.0619864, acc 0.96
2016-09-07T01:47:18.412408: step 2479, loss 0.068092, acc 0.96
2016-09-07T01:47:19.122179: step 2480, loss 0.0221833, acc 0.98
2016-09-07T01:47:19.813244: step 2481, loss 0.0521684, acc 0.98
2016-09-07T01:47:20.541071: step 2482, loss 0.0431222, acc 0.98
2016-09-07T01:47:21.232324: step 2483, loss 0.0106889, acc 1
2016-09-07T01:47:21.927318: step 2484, loss 0.00424428, acc 1
2016-09-07T01:47:22.618014: step 2485, loss 0.00432823, acc 1
2016-09-07T01:47:23.286944: step 2486, loss 0.0294816, acc 0.98
2016-09-07T01:47:24.003682: step 2487, loss 0.0172194, acc 1
2016-09-07T01:47:24.710479: step 2488, loss 0.0394411, acc 1
2016-09-07T01:47:25.420080: step 2489, loss 0.0383125, acc 0.98
2016-09-07T01:47:26.119729: step 2490, loss 0.060033, acc 0.98
2016-09-07T01:47:26.812292: step 2491, loss 0.081948, acc 0.96
2016-09-07T01:47:27.552589: step 2492, loss 0.0481479, acc 0.98
2016-09-07T01:47:28.251182: step 2493, loss 0.0536029, acc 0.98
2016-09-07T01:47:28.944968: step 2494, loss 0.0300422, acc 0.98
2016-09-07T01:47:29.633067: step 2495, loss 0.0405256, acc 0.98
2016-09-07T01:47:30.298551: step 2496, loss 0.00707325, acc 1
2016-09-07T01:47:31.002052: step 2497, loss 0.00494924, acc 1
2016-09-07T01:47:31.673101: step 2498, loss 0.0878075, acc 0.98
2016-09-07T01:47:32.391063: step 2499, loss 0.0643732, acc 0.98
2016-09-07T01:47:33.091602: step 2500, loss 0.0168665, acc 1

Evaluation:
2016-09-07T01:47:36.673764: step 2500, loss 1.45395, acc 0.763602

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-2500

2016-09-07T01:47:38.455039: step 2501, loss 0.0623491, acc 0.98
2016-09-07T01:47:39.167886: step 2502, loss 0.0637418, acc 0.94
2016-09-07T01:47:39.869417: step 2503, loss 0.0613691, acc 0.96
2016-09-07T01:47:40.577236: step 2504, loss 0.0275578, acc 0.98
2016-09-07T01:47:41.280918: step 2505, loss 0.0273555, acc 0.98
2016-09-07T01:47:42.020820: step 2506, loss 0.0134697, acc 1
2016-09-07T01:47:42.736962: step 2507, loss 0.00131283, acc 1
2016-09-07T01:47:43.433557: step 2508, loss 0.0265348, acc 0.98
2016-09-07T01:47:44.113488: step 2509, loss 0.0877325, acc 0.98
2016-09-07T01:47:44.798439: step 2510, loss 0.0417058, acc 0.98
2016-09-07T01:47:45.499296: step 2511, loss 0.00462159, acc 1
2016-09-07T01:47:46.181074: step 2512, loss 0.010218, acc 1
2016-09-07T01:47:46.883535: step 2513, loss 0.052868, acc 0.98
2016-09-07T01:47:47.583634: step 2514, loss 0.0144426, acc 0.98
2016-09-07T01:47:48.286174: step 2515, loss 0.0335612, acc 0.96
2016-09-07T01:47:48.980512: step 2516, loss 0.0105895, acc 1
2016-09-07T01:47:49.661717: step 2517, loss 0.0238406, acc 1
2016-09-07T01:47:50.395922: step 2518, loss 0.0132536, acc 1
2016-09-07T01:47:51.094955: step 2519, loss 0.0132544, acc 1
2016-09-07T01:47:51.788640: step 2520, loss 0.0249262, acc 0.98
2016-09-07T01:47:52.485267: step 2521, loss 0.0719272, acc 0.96
2016-09-07T01:47:53.182014: step 2522, loss 0.000149376, acc 1
2016-09-07T01:47:53.905852: step 2523, loss 0.0290641, acc 0.98
2016-09-07T01:47:54.584433: step 2524, loss 0.0251626, acc 0.98
2016-09-07T01:47:55.309232: step 2525, loss 0.0870829, acc 0.96
2016-09-07T01:47:56.016073: step 2526, loss 0.0140988, acc 1
2016-09-07T01:47:56.701025: step 2527, loss 0.0258856, acc 1
2016-09-07T01:47:57.398575: step 2528, loss 0.0253838, acc 1
2016-09-07T01:47:58.096141: step 2529, loss 0.0331292, acc 0.98
2016-09-07T01:47:58.811370: step 2530, loss 0.190835, acc 0.98
2016-09-07T01:47:59.490072: step 2531, loss 0.0133786, acc 1
2016-09-07T01:48:00.180274: step 2532, loss 0.0490437, acc 0.98
2016-09-07T01:48:00.907718: step 2533, loss 0.126429, acc 0.96
2016-09-07T01:48:01.600071: step 2534, loss 0.0242114, acc 1
2016-09-07T01:48:02.320896: step 2535, loss 0.00798892, acc 1
2016-09-07T01:48:03.001745: step 2536, loss 0.002691, acc 1
2016-09-07T01:48:03.709383: step 2537, loss 0.161856, acc 0.94
2016-09-07T01:48:04.412345: step 2538, loss 0.00439647, acc 1
2016-09-07T01:48:05.104034: step 2539, loss 0.0616667, acc 0.98
2016-09-07T01:48:05.806447: step 2540, loss 0.00239358, acc 1
2016-09-07T01:48:06.529130: step 2541, loss 0.0347098, acc 1
2016-09-07T01:48:07.256590: step 2542, loss 0.0329301, acc 0.98
2016-09-07T01:48:07.932596: step 2543, loss 0.0145137, acc 1
2016-09-07T01:48:08.665532: step 2544, loss 0.260258, acc 0.96
2016-09-07T01:48:09.367599: step 2545, loss 0.0238642, acc 0.98
2016-09-07T01:48:10.065536: step 2546, loss 0.0372014, acc 1
2016-09-07T01:48:10.790306: step 2547, loss 0.00648906, acc 1
2016-09-07T01:48:11.474710: step 2548, loss 0.025388, acc 1
2016-09-07T01:48:12.184395: step 2549, loss 0.00477866, acc 1
2016-09-07T01:48:12.881318: step 2550, loss 0.0100771, acc 1
2016-09-07T01:48:13.588398: step 2551, loss 0.029001, acc 1
2016-09-07T01:48:14.290538: step 2552, loss 0.0800036, acc 0.98
2016-09-07T01:48:15.002293: step 2553, loss 0.0267963, acc 1
2016-09-07T01:48:15.709406: step 2554, loss 0.000239991, acc 1
2016-09-07T01:48:16.401336: step 2555, loss 0.139745, acc 0.96
2016-09-07T01:48:17.093289: step 2556, loss 0.035362, acc 1
2016-09-07T01:48:17.777600: step 2557, loss 0.0308356, acc 0.98
2016-09-07T01:48:18.465819: step 2558, loss 0.0187385, acc 0.98
2016-09-07T01:48:19.181425: step 2559, loss 0.0344114, acc 1
2016-09-07T01:48:19.861396: step 2560, loss 0.0838482, acc 0.94
2016-09-07T01:48:20.560965: step 2561, loss 0.0247318, acc 0.98
2016-09-07T01:48:21.262566: step 2562, loss 0.0522014, acc 0.98
2016-09-07T01:48:21.961961: step 2563, loss 0.0560187, acc 0.96
2016-09-07T01:48:22.677990: step 2564, loss 0.0273969, acc 1
2016-09-07T01:48:23.370798: step 2565, loss 0.0625715, acc 0.96
2016-09-07T01:48:24.082211: step 2566, loss 0.0302015, acc 1
2016-09-07T01:48:24.754783: step 2567, loss 0.0213885, acc 1
2016-09-07T01:48:25.438934: step 2568, loss 0.0553244, acc 0.98
2016-09-07T01:48:26.134824: step 2569, loss 0.00944626, acc 1
2016-09-07T01:48:26.836367: step 2570, loss 0.0370905, acc 0.98
2016-09-07T01:48:27.553306: step 2571, loss 0.00328801, acc 1
2016-09-07T01:48:28.250859: step 2572, loss 0.0331677, acc 0.98
2016-09-07T01:48:28.978603: step 2573, loss 0.00957345, acc 1
2016-09-07T01:48:29.688783: step 2574, loss 0.176465, acc 0.96
2016-09-07T01:48:30.382952: step 2575, loss 0.0188557, acc 0.98
2016-09-07T01:48:31.085983: step 2576, loss 0.0487316, acc 0.96
2016-09-07T01:48:31.780095: step 2577, loss 0.023527, acc 1
2016-09-07T01:48:32.503169: step 2578, loss 0.069538, acc 0.96
2016-09-07T01:48:33.172935: step 2579, loss 0.115251, acc 0.96
2016-09-07T01:48:33.884645: step 2580, loss 0.00401039, acc 1
2016-09-07T01:48:34.586175: step 2581, loss 0.0308507, acc 0.98
2016-09-07T01:48:35.274940: step 2582, loss 0.0978768, acc 0.98
2016-09-07T01:48:35.966484: step 2583, loss 0.0141694, acc 1
2016-09-07T01:48:36.668400: step 2584, loss 0.0304471, acc 1
2016-09-07T01:48:37.396912: step 2585, loss 0.00286668, acc 1
2016-09-07T01:48:38.094144: step 2586, loss 0.0434219, acc 0.96
2016-09-07T01:48:38.797848: step 2587, loss 0.0482346, acc 0.96
2016-09-07T01:48:39.492778: step 2588, loss 0.0553432, acc 0.98
2016-09-07T01:48:40.192024: step 2589, loss 0.0149406, acc 1
2016-09-07T01:48:40.898089: step 2590, loss 0.0102047, acc 1
2016-09-07T01:48:41.583372: step 2591, loss 0.0315156, acc 1
2016-09-07T01:48:42.283392: step 2592, loss 0.0676935, acc 0.98
2016-09-07T01:48:42.985136: step 2593, loss 0.0109429, acc 1
2016-09-07T01:48:43.674308: step 2594, loss 0.0511881, acc 0.98
2016-09-07T01:48:44.383802: step 2595, loss 0.142367, acc 0.94
2016-09-07T01:48:45.036888: step 2596, loss 0.192034, acc 0.94
2016-09-07T01:48:45.727909: step 2597, loss 0.0611049, acc 0.96
2016-09-07T01:48:46.412364: step 2598, loss 0.038044, acc 0.98
2016-09-07T01:48:47.116541: step 2599, loss 0.109301, acc 0.98
2016-09-07T01:48:47.845422: step 2600, loss 0.0825798, acc 0.96

Evaluation:
2016-09-07T01:48:51.451683: step 2600, loss 1.44032, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-2600

2016-09-07T01:48:53.306583: step 2601, loss 0.104189, acc 0.94
2016-09-07T01:48:53.997951: step 2602, loss 0.0386479, acc 1
2016-09-07T01:48:54.677911: step 2603, loss 0.0755445, acc 0.96
2016-09-07T01:48:55.401373: step 2604, loss 0.0315244, acc 0.98
2016-09-07T01:48:56.096931: step 2605, loss 0.0558916, acc 0.98
2016-09-07T01:48:56.825031: step 2606, loss 0.038821, acc 0.98
2016-09-07T01:48:57.538106: step 2607, loss 0.0194978, acc 1
2016-09-07T01:48:58.228263: step 2608, loss 0.0121095, acc 1
2016-09-07T01:48:58.951525: step 2609, loss 0.07696, acc 0.94
2016-09-07T01:48:59.630459: step 2610, loss 0.0355199, acc 1
2016-09-07T01:49:00.366647: step 2611, loss 0.0546066, acc 0.96
2016-09-07T01:49:01.055885: step 2612, loss 0.0290302, acc 1
2016-09-07T01:49:01.744744: step 2613, loss 0.0211915, acc 0.98
2016-09-07T01:49:02.443837: step 2614, loss 0.0658422, acc 0.94
2016-09-07T01:49:03.131849: step 2615, loss 0.0155571, acc 1
2016-09-07T01:49:03.847059: step 2616, loss 0.109193, acc 0.96
2016-09-07T01:49:04.578095: step 2617, loss 0.0292482, acc 0.98
2016-09-07T01:49:05.293270: step 2618, loss 0.0283838, acc 0.98
2016-09-07T01:49:05.999663: step 2619, loss 0.0448289, acc 0.98
2016-09-07T01:49:06.696462: step 2620, loss 0.0125798, acc 1
2016-09-07T01:49:07.421762: step 2621, loss 0.0647753, acc 0.96
2016-09-07T01:49:08.145612: step 2622, loss 0.00643474, acc 1
2016-09-07T01:49:08.837492: step 2623, loss 0.0377901, acc 1
2016-09-07T01:49:09.547777: step 2624, loss 0.0142818, acc 1
2016-09-07T01:49:10.262380: step 2625, loss 0.0406894, acc 0.98
2016-09-07T01:49:10.982717: step 2626, loss 0.0661247, acc 0.96
2016-09-07T01:49:11.701372: step 2627, loss 0.0558655, acc 0.96
2016-09-07T01:49:12.382899: step 2628, loss 0.0444375, acc 0.98
2016-09-07T01:49:13.096787: step 2629, loss 0.0616052, acc 0.98
2016-09-07T01:49:13.768494: step 2630, loss 0.0335931, acc 0.98
2016-09-07T01:49:14.472231: step 2631, loss 0.00345682, acc 1
2016-09-07T01:49:15.144661: step 2632, loss 0.0253627, acc 0.98
2016-09-07T01:49:15.859327: step 2633, loss 0.200095, acc 0.96
2016-09-07T01:49:16.569903: step 2634, loss 0.0145248, acc 1
2016-09-07T01:49:17.272412: step 2635, loss 0.209683, acc 0.96
2016-09-07T01:49:17.972413: step 2636, loss 0.0776863, acc 0.96
2016-09-07T01:49:18.662377: step 2637, loss 0.0716759, acc 0.96
2016-09-07T01:49:19.399038: step 2638, loss 0.0483825, acc 0.98
2016-09-07T01:49:20.095027: step 2639, loss 0.05088, acc 0.98
2016-09-07T01:49:20.767962: step 2640, loss 0.0372231, acc 0.98
2016-09-07T01:49:21.475355: step 2641, loss 0.0247017, acc 0.98
2016-09-07T01:49:22.168179: step 2642, loss 0.00951315, acc 1
2016-09-07T01:49:22.897847: step 2643, loss 0.00579645, acc 1
2016-09-07T01:49:23.586158: step 2644, loss 0.0670793, acc 0.98
2016-09-07T01:49:24.300077: step 2645, loss 0.0287807, acc 1
2016-09-07T01:49:25.009796: step 2646, loss 0.0313971, acc 0.98
2016-09-07T01:49:25.695881: step 2647, loss 0.0362854, acc 0.98
2016-09-07T01:49:26.391489: step 2648, loss 0.00905287, acc 1
2016-09-07T01:49:27.104099: step 2649, loss 0.0159365, acc 1
2016-09-07T01:49:27.815838: step 2650, loss 0.0500063, acc 0.98
2016-09-07T01:49:28.489107: step 2651, loss 0.0332575, acc 1
2016-09-07T01:49:29.184973: step 2652, loss 0.0127767, acc 1
2016-09-07T01:49:29.850896: step 2653, loss 0.0345623, acc 0.98
2016-09-07T01:49:30.538451: step 2654, loss 0.0342335, acc 1
2016-09-07T01:49:31.245197: step 2655, loss 0.0281785, acc 0.98
2016-09-07T01:49:31.916545: step 2656, loss 0.0444652, acc 0.98
2016-09-07T01:49:32.619729: step 2657, loss 0.011154, acc 1
2016-09-07T01:49:33.305382: step 2658, loss 0.0310575, acc 1
2016-09-07T01:49:33.991315: step 2659, loss 0.0105767, acc 1
2016-09-07T01:49:34.690048: step 2660, loss 0.0141569, acc 1
2016-09-07T01:49:35.416244: step 2661, loss 0.0832324, acc 0.98
2016-09-07T01:49:36.101439: step 2662, loss 0.0223042, acc 0.98
2016-09-07T01:49:36.790278: step 2663, loss 0.0266669, acc 0.98
2016-09-07T01:49:37.495750: step 2664, loss 0.0529523, acc 0.96
2016-09-07T01:49:38.189287: step 2665, loss 0.192071, acc 0.94
2016-09-07T01:49:38.885632: step 2666, loss 0.0972607, acc 0.98
2016-09-07T01:49:39.583375: step 2667, loss 0.0262872, acc 0.98
2016-09-07T01:49:40.273003: step 2668, loss 0.0677002, acc 0.94
2016-09-07T01:49:40.988531: step 2669, loss 0.00632343, acc 1
2016-09-07T01:49:41.670405: step 2670, loss 0.0164123, acc 1
2016-09-07T01:49:42.360145: step 2671, loss 0.00498432, acc 1
2016-09-07T01:49:43.074310: step 2672, loss 0.0142686, acc 1
2016-09-07T01:49:43.764713: step 2673, loss 0.0268179, acc 1
2016-09-07T01:49:44.459792: step 2674, loss 0.0960824, acc 0.98
2016-09-07T01:49:45.158485: step 2675, loss 0.0394962, acc 0.98
2016-09-07T01:49:45.874573: step 2676, loss 0.00368768, acc 1
2016-09-07T01:49:46.550733: step 2677, loss 0.0186059, acc 1
2016-09-07T01:49:47.252316: step 2678, loss 0.0281173, acc 0.98
2016-09-07T01:49:47.937555: step 2679, loss 0.00831876, acc 1
2016-09-07T01:49:48.618251: step 2680, loss 0.27202, acc 0.92
2016-09-07T01:49:49.310254: step 2681, loss 0.0306511, acc 1
2016-09-07T01:49:50.001725: step 2682, loss 0.0178359, acc 0.98
2016-09-07T01:49:50.715994: step 2683, loss 0.111553, acc 0.96
2016-09-07T01:49:51.416751: step 2684, loss 0.00358788, acc 1
2016-09-07T01:49:52.115790: step 2685, loss 0.023236, acc 1
2016-09-07T01:49:52.790579: step 2686, loss 0.0252342, acc 1
2016-09-07T01:49:53.481683: step 2687, loss 0.0416342, acc 0.96
2016-09-07T01:49:54.133929: step 2688, loss 0.0681294, acc 0.977273
2016-09-07T01:49:54.802904: step 2689, loss 0.0246011, acc 1
2016-09-07T01:49:55.526168: step 2690, loss 0.039615, acc 0.98
2016-09-07T01:49:56.230738: step 2691, loss 0.0205619, acc 0.98
2016-09-07T01:49:56.941417: step 2692, loss 0.0026097, acc 1
2016-09-07T01:49:57.623747: step 2693, loss 0.0406273, acc 0.98
2016-09-07T01:49:58.321643: step 2694, loss 0.142071, acc 0.96
2016-09-07T01:49:59.057664: step 2695, loss 0.00415082, acc 1
2016-09-07T01:49:59.736902: step 2696, loss 0.012613, acc 1
2016-09-07T01:50:00.451378: step 2697, loss 0.0344256, acc 0.98
2016-09-07T01:50:01.153280: step 2698, loss 0.0379073, acc 0.98
2016-09-07T01:50:01.855979: step 2699, loss 0.0134709, acc 1
2016-09-07T01:50:02.564294: step 2700, loss 0.0303893, acc 1

Evaluation:
2016-09-07T01:50:06.189846: step 2700, loss 1.23356, acc 0.75985

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-2700

2016-09-07T01:50:08.038230: step 2701, loss 0.0205697, acc 1
2016-09-07T01:50:08.748398: step 2702, loss 0.0956124, acc 0.96
2016-09-07T01:50:09.444191: step 2703, loss 0.0869224, acc 0.96
2016-09-07T01:50:10.153511: step 2704, loss 0.0208737, acc 1
2016-09-07T01:50:10.850525: step 2705, loss 0.0559415, acc 0.96
2016-09-07T01:50:11.537968: step 2706, loss 0.016506, acc 1
2016-09-07T01:50:12.250993: step 2707, loss 0.0538486, acc 0.96
2016-09-07T01:50:12.917512: step 2708, loss 0.00848752, acc 1
2016-09-07T01:50:13.625312: step 2709, loss 0.0702205, acc 0.96
2016-09-07T01:50:14.325006: step 2710, loss 0.0451, acc 0.98
2016-09-07T01:50:15.019635: step 2711, loss 0.0114602, acc 1
2016-09-07T01:50:15.710624: step 2712, loss 0.0185795, acc 1
2016-09-07T01:50:16.405212: step 2713, loss 0.0188798, acc 1
2016-09-07T01:50:17.127178: step 2714, loss 0.00171621, acc 1
2016-09-07T01:50:17.815751: step 2715, loss 0.0379596, acc 0.98
2016-09-07T01:50:18.486256: step 2716, loss 0.0148285, acc 1
2016-09-07T01:50:19.191828: step 2717, loss 0.0262443, acc 1
2016-09-07T01:50:19.906689: step 2718, loss 0.0256246, acc 1
2016-09-07T01:50:20.616029: step 2719, loss 0.00906092, acc 1
2016-09-07T01:50:21.301911: step 2720, loss 0.11099, acc 0.96
2016-09-07T01:50:22.007610: step 2721, loss 0.0211153, acc 0.98
2016-09-07T01:50:22.711510: step 2722, loss 0.142902, acc 0.92
2016-09-07T01:50:23.411900: step 2723, loss 0.0160969, acc 1
2016-09-07T01:50:24.152513: step 2724, loss 0.111507, acc 0.94
2016-09-07T01:50:24.825337: step 2725, loss 0.0251915, acc 0.98
2016-09-07T01:50:25.547230: step 2726, loss 0.00570113, acc 1
2016-09-07T01:50:26.232843: step 2727, loss 0.00616062, acc 1
2016-09-07T01:50:26.902807: step 2728, loss 0.00695831, acc 1
2016-09-07T01:50:27.586075: step 2729, loss 0.0639987, acc 0.94
2016-09-07T01:50:28.271116: step 2730, loss 0.0164309, acc 1
2016-09-07T01:50:29.005624: step 2731, loss 0.0133096, acc 1
2016-09-07T01:50:29.672168: step 2732, loss 0.0175728, acc 0.98
2016-09-07T01:50:30.392124: step 2733, loss 0.0440943, acc 1
2016-09-07T01:50:31.084303: step 2734, loss 0.0161132, acc 1
2016-09-07T01:50:31.797350: step 2735, loss 0.0121959, acc 1
2016-09-07T01:50:32.506792: step 2736, loss 0.0630377, acc 0.96
2016-09-07T01:50:33.199611: step 2737, loss 0.0257049, acc 0.98
2016-09-07T01:50:33.904742: step 2738, loss 0.0280808, acc 1
2016-09-07T01:50:34.582281: step 2739, loss 0.00948654, acc 1
2016-09-07T01:50:35.269828: step 2740, loss 0.036392, acc 0.98
2016-09-07T01:50:35.973262: step 2741, loss 0.031561, acc 0.98
2016-09-07T01:50:36.666672: step 2742, loss 0.0241557, acc 0.98
2016-09-07T01:50:37.365834: step 2743, loss 0.00157135, acc 1
2016-09-07T01:50:38.053246: step 2744, loss 0.0341773, acc 0.98
2016-09-07T01:50:38.748835: step 2745, loss 0.0814955, acc 0.96
2016-09-07T01:50:39.431758: step 2746, loss 0.0584686, acc 0.98
2016-09-07T01:50:40.125938: step 2747, loss 0.0474398, acc 0.96
2016-09-07T01:50:40.831477: step 2748, loss 0.0359559, acc 0.98
2016-09-07T01:50:41.544748: step 2749, loss 0.0271152, acc 0.98
2016-09-07T01:50:42.248378: step 2750, loss 0.0329546, acc 0.98
2016-09-07T01:50:42.929465: step 2751, loss 0.0667478, acc 0.98
2016-09-07T01:50:43.654228: step 2752, loss 0.0298884, acc 0.98
2016-09-07T01:50:44.368375: step 2753, loss 0.00326073, acc 1
2016-09-07T01:50:45.083909: step 2754, loss 0.046503, acc 0.98
2016-09-07T01:50:45.801369: step 2755, loss 0.028654, acc 0.98
2016-09-07T01:50:46.470870: step 2756, loss 0.0666891, acc 0.98
2016-09-07T01:50:47.190839: step 2757, loss 0.0519342, acc 0.98
2016-09-07T01:50:47.917300: step 2758, loss 0.127713, acc 0.94
2016-09-07T01:50:48.600620: step 2759, loss 0.0122376, acc 1
2016-09-07T01:50:49.282025: step 2760, loss 0.00595985, acc 1
2016-09-07T01:50:49.969669: step 2761, loss 0.0901634, acc 0.96
2016-09-07T01:50:50.678197: step 2762, loss 0.0370856, acc 0.96
2016-09-07T01:50:51.364460: step 2763, loss 0.000816813, acc 1
2016-09-07T01:50:52.081912: step 2764, loss 0.0111505, acc 1
2016-09-07T01:50:52.778491: step 2765, loss 0.0109111, acc 1
2016-09-07T01:50:53.481612: step 2766, loss 0.206603, acc 0.98
2016-09-07T01:50:54.180811: step 2767, loss 0.0552193, acc 0.94
2016-09-07T01:50:54.857842: step 2768, loss 0.0378924, acc 0.98
2016-09-07T01:50:55.569254: step 2769, loss 0.0982931, acc 0.96
2016-09-07T01:50:56.253995: step 2770, loss 0.0639368, acc 0.98
2016-09-07T01:50:56.935639: step 2771, loss 0.0507631, acc 0.98
2016-09-07T01:50:57.629565: step 2772, loss 0.0131922, acc 1
2016-09-07T01:50:58.321664: step 2773, loss 0.0865137, acc 0.94
2016-09-07T01:50:59.025099: step 2774, loss 0.0482729, acc 0.98
2016-09-07T01:50:59.712873: step 2775, loss 0.00625438, acc 1
2016-09-07T01:51:00.441239: step 2776, loss 0.0112074, acc 1
2016-09-07T01:51:01.126499: step 2777, loss 0.0853657, acc 0.94
2016-09-07T01:51:01.825038: step 2778, loss 0.0569177, acc 0.96
2016-09-07T01:51:02.516651: step 2779, loss 0.0466766, acc 1
2016-09-07T01:51:03.209810: step 2780, loss 0.0341861, acc 0.96
2016-09-07T01:51:03.918582: step 2781, loss 0.0594829, acc 0.96
2016-09-07T01:51:04.606330: step 2782, loss 0.00690695, acc 1
2016-09-07T01:51:05.315474: step 2783, loss 0.0622301, acc 0.98
2016-09-07T01:51:06.035410: step 2784, loss 0.0388829, acc 0.96
2016-09-07T01:51:06.750980: step 2785, loss 0.0342568, acc 0.98
2016-09-07T01:51:07.428359: step 2786, loss 0.0471365, acc 0.98
2016-09-07T01:51:08.129334: step 2787, loss 0.0564111, acc 0.98
2016-09-07T01:51:08.827636: step 2788, loss 0.0517757, acc 0.96
2016-09-07T01:51:09.518514: step 2789, loss 0.0382592, acc 0.98
2016-09-07T01:51:10.215404: step 2790, loss 0.0268925, acc 1
2016-09-07T01:51:10.909414: step 2791, loss 0.041237, acc 0.96
2016-09-07T01:51:11.591486: step 2792, loss 0.0313779, acc 1
2016-09-07T01:51:12.305987: step 2793, loss 0.0939233, acc 0.96
2016-09-07T01:51:12.973800: step 2794, loss 0.0227679, acc 1
2016-09-07T01:51:13.685587: step 2795, loss 0.0117322, acc 1
2016-09-07T01:51:14.382079: step 2796, loss 0.0286645, acc 0.98
2016-09-07T01:51:15.088083: step 2797, loss 0.0807079, acc 0.98
2016-09-07T01:51:15.785808: step 2798, loss 0.0222594, acc 1
2016-09-07T01:51:16.477219: step 2799, loss 0.0168343, acc 1
2016-09-07T01:51:17.213773: step 2800, loss 0.00717717, acc 1

Evaluation:
2016-09-07T01:51:20.902806: step 2800, loss 1.32188, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-2800

2016-09-07T01:51:22.658822: step 2801, loss 0.139151, acc 0.98
2016-09-07T01:51:23.378012: step 2802, loss 0.0746265, acc 0.98
2016-09-07T01:51:24.071655: step 2803, loss 0.00795501, acc 1
2016-09-07T01:51:24.743072: step 2804, loss 0.0441942, acc 0.98
2016-09-07T01:51:25.445720: step 2805, loss 0.0342909, acc 0.98
2016-09-07T01:51:26.153503: step 2806, loss 0.044311, acc 0.96
2016-09-07T01:51:26.857661: step 2807, loss 0.0569134, acc 0.98
2016-09-07T01:51:27.561272: step 2808, loss 0.038363, acc 0.98
2016-09-07T01:51:28.237591: step 2809, loss 0.101164, acc 0.98
2016-09-07T01:51:28.936398: step 2810, loss 0.036315, acc 0.98
2016-09-07T01:51:29.628948: step 2811, loss 0.0003815, acc 1
2016-09-07T01:51:30.334948: step 2812, loss 0.00928738, acc 1
2016-09-07T01:51:31.018699: step 2813, loss 0.0162743, acc 1
2016-09-07T01:51:31.759305: step 2814, loss 0.0596409, acc 0.96
2016-09-07T01:51:32.449921: step 2815, loss 0.144208, acc 0.94
2016-09-07T01:51:33.148957: step 2816, loss 0.03781, acc 0.98
2016-09-07T01:51:33.837297: step 2817, loss 0.00132646, acc 1
2016-09-07T01:51:34.532886: step 2818, loss 0.0218333, acc 1
2016-09-07T01:51:35.276420: step 2819, loss 0.115185, acc 0.94
2016-09-07T01:51:35.965674: step 2820, loss 0.0644792, acc 0.98
2016-09-07T01:51:36.697017: step 2821, loss 0.11198, acc 0.96
2016-09-07T01:51:37.414756: step 2822, loss 0.0443418, acc 0.98
2016-09-07T01:51:38.132775: step 2823, loss 0.000866497, acc 1
2016-09-07T01:51:38.859984: step 2824, loss 0.0740082, acc 0.96
2016-09-07T01:51:39.562100: step 2825, loss 0.0613653, acc 0.98
2016-09-07T01:51:40.259853: step 2826, loss 0.0270221, acc 0.98
2016-09-07T01:51:40.967864: step 2827, loss 0.0304173, acc 1
2016-09-07T01:51:41.666111: step 2828, loss 0.0360247, acc 1
2016-09-07T01:51:42.376720: step 2829, loss 0.0213688, acc 1
2016-09-07T01:51:43.082638: step 2830, loss 0.039985, acc 0.98
2016-09-07T01:51:43.797674: step 2831, loss 0.0140861, acc 1
2016-09-07T01:51:44.510888: step 2832, loss 0.0561811, acc 0.96
2016-09-07T01:51:45.228075: step 2833, loss 0.0380875, acc 0.98
2016-09-07T01:51:45.923063: step 2834, loss 0.12871, acc 0.96
2016-09-07T01:51:46.621529: step 2835, loss 0.0341789, acc 0.98
2016-09-07T01:51:47.344439: step 2836, loss 0.0205902, acc 0.98
2016-09-07T01:51:48.029454: step 2837, loss 0.0880323, acc 0.98
2016-09-07T01:51:48.724997: step 2838, loss 0.0382916, acc 0.98
2016-09-07T01:51:49.417671: step 2839, loss 0.0132322, acc 1
2016-09-07T01:51:50.116984: step 2840, loss 0.0128601, acc 1
2016-09-07T01:51:50.813784: step 2841, loss 0.0898729, acc 0.98
2016-09-07T01:51:51.488263: step 2842, loss 0.0323392, acc 0.98
2016-09-07T01:51:52.201574: step 2843, loss 0.0448499, acc 0.96
2016-09-07T01:51:52.876426: step 2844, loss 0.00165624, acc 1
2016-09-07T01:51:53.557355: step 2845, loss 0.00844156, acc 1
2016-09-07T01:51:54.243344: step 2846, loss 0.0887799, acc 0.98
2016-09-07T01:51:54.938386: step 2847, loss 0.028497, acc 1
2016-09-07T01:51:55.649330: step 2848, loss 0.0682402, acc 0.96
2016-09-07T01:51:56.331729: step 2849, loss 0.00354011, acc 1
2016-09-07T01:51:57.063986: step 2850, loss 0.0361528, acc 0.96
2016-09-07T01:51:57.782050: step 2851, loss 0.0307008, acc 0.98
2016-09-07T01:51:58.478601: step 2852, loss 0.0427258, acc 0.98
2016-09-07T01:51:59.166541: step 2853, loss 0.102458, acc 0.98
2016-09-07T01:51:59.886948: step 2854, loss 0.0354348, acc 0.98
2016-09-07T01:52:00.638798: step 2855, loss 0.0342436, acc 0.98
2016-09-07T01:52:01.328638: step 2856, loss 0.0321856, acc 0.98
2016-09-07T01:52:02.027072: step 2857, loss 0.0338934, acc 0.98
2016-09-07T01:52:02.702827: step 2858, loss 0.0247173, acc 0.98
2016-09-07T01:52:03.410485: step 2859, loss 0.0319405, acc 1
2016-09-07T01:52:04.104669: step 2860, loss 0.0486495, acc 0.96
2016-09-07T01:52:04.793404: step 2861, loss 0.0527726, acc 0.98
2016-09-07T01:52:05.522243: step 2862, loss 0.0821119, acc 0.96
2016-09-07T01:52:06.214641: step 2863, loss 0.0219527, acc 0.98
2016-09-07T01:52:06.917413: step 2864, loss 0.01572, acc 1
2016-09-07T01:52:07.621517: step 2865, loss 0.0308711, acc 0.98
2016-09-07T01:52:08.290491: step 2866, loss 0.112634, acc 0.96
2016-09-07T01:52:09.003735: step 2867, loss 0.0501287, acc 0.96
2016-09-07T01:52:09.681247: step 2868, loss 0.0499889, acc 0.98
2016-09-07T01:52:10.387409: step 2869, loss 0.036608, acc 0.98
2016-09-07T01:52:11.098196: step 2870, loss 0.00199322, acc 1
2016-09-07T01:52:11.794699: step 2871, loss 0.0446765, acc 0.98
2016-09-07T01:52:12.510317: step 2872, loss 0.0852096, acc 0.96
2016-09-07T01:52:13.183901: step 2873, loss 0.0159604, acc 1
2016-09-07T01:52:13.904191: step 2874, loss 0.00963189, acc 1
2016-09-07T01:52:14.618600: step 2875, loss 0.0302323, acc 1
2016-09-07T01:52:15.327695: step 2876, loss 0.121106, acc 0.96
2016-09-07T01:52:16.022421: step 2877, loss 0.0301477, acc 0.98
2016-09-07T01:52:16.705823: step 2878, loss 0.0661117, acc 0.96
2016-09-07T01:52:17.433238: step 2879, loss 0.0156057, acc 1
2016-09-07T01:52:18.060909: step 2880, loss 0.0252346, acc 0.977273
2016-09-07T01:52:18.754385: step 2881, loss 0.155837, acc 0.94
2016-09-07T01:52:19.446923: step 2882, loss 0.047142, acc 0.96
2016-09-07T01:52:20.153833: step 2883, loss 0.0685883, acc 0.96
2016-09-07T01:52:20.850577: step 2884, loss 0.076371, acc 0.96
2016-09-07T01:52:21.513325: step 2885, loss 0.0171512, acc 1
2016-09-07T01:52:22.248700: step 2886, loss 0.10953, acc 0.96
2016-09-07T01:52:22.964018: step 2887, loss 0.0174518, acc 1
2016-09-07T01:52:23.711740: step 2888, loss 0.0303576, acc 0.98
2016-09-07T01:52:24.437848: step 2889, loss 0.163282, acc 0.96
2016-09-07T01:52:25.120533: step 2890, loss 0.0012973, acc 1
2016-09-07T01:52:25.837602: step 2891, loss 0.0350382, acc 0.98
2016-09-07T01:52:26.535768: step 2892, loss 0.035083, acc 1
2016-09-07T01:52:27.227011: step 2893, loss 0.0556463, acc 0.98
2016-09-07T01:52:27.918357: step 2894, loss 0.0104142, acc 1
2016-09-07T01:52:28.615693: step 2895, loss 0.0226528, acc 1
2016-09-07T01:52:29.306997: step 2896, loss 0.0382475, acc 0.98
2016-09-07T01:52:29.989827: step 2897, loss 0.0302346, acc 0.98
2016-09-07T01:52:30.699081: step 2898, loss 0.0167662, acc 1
2016-09-07T01:52:31.396813: step 2899, loss 0.052278, acc 0.96
2016-09-07T01:52:32.099713: step 2900, loss 0.0142823, acc 1

Evaluation:
2016-09-07T01:52:35.747872: step 2900, loss 1.11682, acc 0.766416

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-2900

2016-09-07T01:52:37.665356: step 2901, loss 0.0211106, acc 1
2016-09-07T01:52:38.343985: step 2902, loss 0.100488, acc 0.94
2016-09-07T01:52:39.070366: step 2903, loss 0.0194339, acc 1
2016-09-07T01:52:39.770809: step 2904, loss 0.0205799, acc 1
2016-09-07T01:52:40.446177: step 2905, loss 0.0334921, acc 0.98
2016-09-07T01:52:41.153611: step 2906, loss 0.0106565, acc 1
2016-09-07T01:52:41.864932: step 2907, loss 0.0214502, acc 1
2016-09-07T01:52:42.590782: step 2908, loss 0.0211794, acc 1
2016-09-07T01:52:43.294636: step 2909, loss 0.0600635, acc 0.96
2016-09-07T01:52:43.970280: step 2910, loss 0.0379127, acc 0.98
2016-09-07T01:52:44.677191: step 2911, loss 0.0364711, acc 0.98
2016-09-07T01:52:45.370964: step 2912, loss 0.0383388, acc 1
2016-09-07T01:52:46.078487: step 2913, loss 0.0668983, acc 0.98
2016-09-07T01:52:46.754395: step 2914, loss 0.0114873, acc 1
2016-09-07T01:52:47.472663: step 2915, loss 0.056449, acc 0.96
2016-09-07T01:52:48.184456: step 2916, loss 0.0424306, acc 1
2016-09-07T01:52:48.881629: step 2917, loss 0.0449563, acc 0.98
2016-09-07T01:52:49.603685: step 2918, loss 0.0495768, acc 0.96
2016-09-07T01:52:50.297602: step 2919, loss 0.0146499, acc 1
2016-09-07T01:52:51.010637: step 2920, loss 0.00506577, acc 1
2016-09-07T01:52:51.692049: step 2921, loss 0.0193652, acc 0.98
2016-09-07T01:52:52.378632: step 2922, loss 0.00425521, acc 1
2016-09-07T01:52:53.093638: step 2923, loss 0.00407572, acc 1
2016-09-07T01:52:53.831285: step 2924, loss 0.0640148, acc 0.98
2016-09-07T01:52:54.559729: step 2925, loss 0.0725201, acc 0.96
2016-09-07T01:52:55.264148: step 2926, loss 0.0282961, acc 0.98
2016-09-07T01:52:55.991081: step 2927, loss 0.00608479, acc 1
2016-09-07T01:52:56.683898: step 2928, loss 0.016184, acc 1
2016-09-07T01:52:57.374390: step 2929, loss 0.00588677, acc 1
2016-09-07T01:52:58.078591: step 2930, loss 0.0686884, acc 0.96
2016-09-07T01:52:58.774036: step 2931, loss 0.0136297, acc 1
2016-09-07T01:52:59.499463: step 2932, loss 0.0909114, acc 0.96
2016-09-07T01:53:00.182908: step 2933, loss 0.0109817, acc 1
2016-09-07T01:53:00.907404: step 2934, loss 0.00815651, acc 1
2016-09-07T01:53:01.617085: step 2935, loss 0.00938666, acc 1
2016-09-07T01:53:02.318210: step 2936, loss 0.0175582, acc 1
2016-09-07T01:53:03.050230: step 2937, loss 0.0113635, acc 1
2016-09-07T01:53:03.733643: step 2938, loss 0.175261, acc 0.96
2016-09-07T01:53:04.418214: step 2939, loss 0.0363136, acc 1
2016-09-07T01:53:05.109821: step 2940, loss 0.0262758, acc 0.98
2016-09-07T01:53:05.798348: step 2941, loss 0.0379676, acc 0.98
2016-09-07T01:53:06.474190: step 2942, loss 0.0277237, acc 1
2016-09-07T01:53:07.172432: step 2943, loss 0.0736567, acc 0.98
2016-09-07T01:53:07.882143: step 2944, loss 0.00720077, acc 1
2016-09-07T01:53:08.567898: step 2945, loss 0.126287, acc 0.9
2016-09-07T01:53:09.264691: step 2946, loss 0.116703, acc 0.94
2016-09-07T01:53:09.976323: step 2947, loss 0.00554667, acc 1
2016-09-07T01:53:10.704413: step 2948, loss 0.00710271, acc 1
2016-09-07T01:53:11.410501: step 2949, loss 0.0579821, acc 0.96
2016-09-07T01:53:12.099220: step 2950, loss 0.0399993, acc 0.98
2016-09-07T01:53:12.812011: step 2951, loss 0.00907792, acc 1
2016-09-07T01:53:13.511170: step 2952, loss 0.0301277, acc 0.98
2016-09-07T01:53:14.213455: step 2953, loss 0.0104701, acc 1
2016-09-07T01:53:14.920364: step 2954, loss 0.019195, acc 1
2016-09-07T01:53:15.604608: step 2955, loss 0.0131365, acc 1
2016-09-07T01:53:16.320570: step 2956, loss 0.0537617, acc 0.98
2016-09-07T01:53:17.026637: step 2957, loss 0.147439, acc 0.96
2016-09-07T01:53:17.713292: step 2958, loss 0.0382224, acc 0.98
2016-09-07T01:53:18.412797: step 2959, loss 0.137413, acc 0.94
2016-09-07T01:53:19.095583: step 2960, loss 0.111119, acc 0.96
2016-09-07T01:53:19.795247: step 2961, loss 0.0215511, acc 1
2016-09-07T01:53:20.453139: step 2962, loss 0.00310132, acc 1
2016-09-07T01:53:21.181243: step 2963, loss 0.0357983, acc 0.98
2016-09-07T01:53:21.880244: step 2964, loss 0.0442797, acc 0.98
2016-09-07T01:53:22.561843: step 2965, loss 0.0444941, acc 0.96
2016-09-07T01:53:23.261113: step 2966, loss 0.237945, acc 0.96
2016-09-07T01:53:23.965104: step 2967, loss 0.00920101, acc 1
2016-09-07T01:53:24.687085: step 2968, loss 0.0294502, acc 0.98
2016-09-07T01:53:25.388218: step 2969, loss 0.0270595, acc 0.98
2016-09-07T01:53:26.076357: step 2970, loss 0.0957663, acc 0.98
2016-09-07T01:53:26.781898: step 2971, loss 0.129129, acc 0.98
2016-09-07T01:53:27.473619: step 2972, loss 0.113775, acc 0.96
2016-09-07T01:53:28.161137: step 2973, loss 0.0214209, acc 1
2016-09-07T01:53:28.824757: step 2974, loss 0.069309, acc 0.98
2016-09-07T01:53:29.556333: step 2975, loss 0.0361902, acc 0.98
2016-09-07T01:53:30.232658: step 2976, loss 0.0396944, acc 0.98
2016-09-07T01:53:30.920135: step 2977, loss 0.0481043, acc 0.98
2016-09-07T01:53:31.615089: step 2978, loss 0.0714534, acc 0.98
2016-09-07T01:53:32.302767: step 2979, loss 0.00697802, acc 1
2016-09-07T01:53:33.031516: step 2980, loss 0.045241, acc 0.96
2016-09-07T01:53:33.708692: step 2981, loss 0.0312635, acc 0.98
2016-09-07T01:53:34.433846: step 2982, loss 0.0651628, acc 0.98
2016-09-07T01:53:35.138089: step 2983, loss 0.102352, acc 0.94
2016-09-07T01:53:35.835950: step 2984, loss 0.0177159, acc 1
2016-09-07T01:53:36.553790: step 2985, loss 0.0482423, acc 0.98
2016-09-07T01:53:37.238829: step 2986, loss 0.0219182, acc 1
2016-09-07T01:53:37.976202: step 2987, loss 0.128783, acc 0.98
2016-09-07T01:53:38.678918: step 2988, loss 0.0371177, acc 0.98
2016-09-07T01:53:39.375574: step 2989, loss 0.0350276, acc 0.98
2016-09-07T01:53:40.098873: step 2990, loss 0.0587723, acc 0.98
2016-09-07T01:53:40.789088: step 2991, loss 0.0688656, acc 0.98
2016-09-07T01:53:41.522023: step 2992, loss 0.00659136, acc 1
2016-09-07T01:53:42.223903: step 2993, loss 0.067413, acc 0.98
2016-09-07T01:53:42.902112: step 2994, loss 0.0610573, acc 0.98
2016-09-07T01:53:43.588204: step 2995, loss 0.0619763, acc 0.96
2016-09-07T01:53:44.303314: step 2996, loss 0.114604, acc 0.92
2016-09-07T01:53:44.998729: step 2997, loss 0.0318175, acc 0.98
2016-09-07T01:53:45.656607: step 2998, loss 0.0241552, acc 1
2016-09-07T01:53:46.363680: step 2999, loss 0.0757361, acc 0.96
2016-09-07T01:53:47.058678: step 3000, loss 0.0872194, acc 0.94

Evaluation:
2016-09-07T01:53:50.685679: step 3000, loss 1.20929, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-3000

2016-09-07T01:53:52.459318: step 3001, loss 0.110497, acc 0.98
2016-09-07T01:53:53.146213: step 3002, loss 0.0283763, acc 1
2016-09-07T01:53:53.845436: step 3003, loss 0.117866, acc 0.94
2016-09-07T01:53:54.517303: step 3004, loss 0.0972708, acc 0.96
2016-09-07T01:53:55.203575: step 3005, loss 0.0484851, acc 0.98
2016-09-07T01:53:55.921748: step 3006, loss 0.0192489, acc 1
2016-09-07T01:53:56.599413: step 3007, loss 0.0469304, acc 0.98
2016-09-07T01:53:57.302978: step 3008, loss 0.0151644, acc 1
2016-09-07T01:53:58.009293: step 3009, loss 0.0399989, acc 0.98
2016-09-07T01:53:58.692597: step 3010, loss 0.0363477, acc 1
2016-09-07T01:53:59.389154: step 3011, loss 0.0619801, acc 1
2016-09-07T01:54:00.106361: step 3012, loss 0.00729496, acc 1
2016-09-07T01:54:00.844571: step 3013, loss 0.109069, acc 0.96
2016-09-07T01:54:01.531225: step 3014, loss 0.0224021, acc 0.98
2016-09-07T01:54:02.241683: step 3015, loss 0.00897228, acc 1
2016-09-07T01:54:02.950342: step 3016, loss 0.0208075, acc 1
2016-09-07T01:54:03.656403: step 3017, loss 0.0680365, acc 0.96
2016-09-07T01:54:04.372004: step 3018, loss 0.0119804, acc 1
2016-09-07T01:54:05.041546: step 3019, loss 0.0427885, acc 1
2016-09-07T01:54:05.734301: step 3020, loss 0.0217139, acc 1
2016-09-07T01:54:06.431794: step 3021, loss 0.105895, acc 0.98
2016-09-07T01:54:07.141198: step 3022, loss 0.0276548, acc 1
2016-09-07T01:54:07.835931: step 3023, loss 0.0185723, acc 1
2016-09-07T01:54:08.549659: step 3024, loss 0.072963, acc 0.96
2016-09-07T01:54:09.262257: step 3025, loss 0.0308997, acc 0.98
2016-09-07T01:54:09.957401: step 3026, loss 0.0101595, acc 1
2016-09-07T01:54:10.660901: step 3027, loss 0.00883733, acc 1
2016-09-07T01:54:11.356449: step 3028, loss 0.0592491, acc 1
2016-09-07T01:54:12.062541: step 3029, loss 0.10905, acc 0.94
2016-09-07T01:54:12.787921: step 3030, loss 0.118534, acc 0.98
2016-09-07T01:54:13.469284: step 3031, loss 0.0635629, acc 0.96
2016-09-07T01:54:14.173391: step 3032, loss 0.0448484, acc 0.98
2016-09-07T01:54:14.871210: step 3033, loss 0.00754951, acc 1
2016-09-07T01:54:15.568474: step 3034, loss 0.0356422, acc 0.98
2016-09-07T01:54:16.263604: step 3035, loss 0.0546509, acc 0.98
2016-09-07T01:54:16.962799: step 3036, loss 0.0191004, acc 0.98
2016-09-07T01:54:17.684545: step 3037, loss 0.0557802, acc 0.96
2016-09-07T01:54:18.402331: step 3038, loss 0.0452322, acc 1
2016-09-07T01:54:19.107250: step 3039, loss 0.00856369, acc 1
2016-09-07T01:54:19.811779: step 3040, loss 0.137054, acc 0.92
2016-09-07T01:54:20.514058: step 3041, loss 0.00601637, acc 1
2016-09-07T01:54:21.234790: step 3042, loss 0.0446161, acc 0.96
2016-09-07T01:54:21.943072: step 3043, loss 0.0270552, acc 1
2016-09-07T01:54:22.625955: step 3044, loss 0.00394481, acc 1
2016-09-07T01:54:23.335149: step 3045, loss 0.021839, acc 1
2016-09-07T01:54:24.015047: step 3046, loss 0.00602862, acc 1
2016-09-07T01:54:24.713457: step 3047, loss 0.053462, acc 0.98
2016-09-07T01:54:25.396912: step 3048, loss 0.0868682, acc 0.96
2016-09-07T01:54:26.118354: step 3049, loss 0.0400232, acc 0.98
2016-09-07T01:54:26.789201: step 3050, loss 0.0127963, acc 1
2016-09-07T01:54:27.491152: step 3051, loss 0.0126554, acc 1
2016-09-07T01:54:28.217581: step 3052, loss 0.0119374, acc 1
2016-09-07T01:54:28.902520: step 3053, loss 0.0292935, acc 0.98
2016-09-07T01:54:29.638844: step 3054, loss 0.0122795, acc 1
2016-09-07T01:54:30.331774: step 3055, loss 0.00223987, acc 1
2016-09-07T01:54:31.014124: step 3056, loss 0.0284715, acc 1
2016-09-07T01:54:31.697867: step 3057, loss 0.0563154, acc 0.96
2016-09-07T01:54:32.387369: step 3058, loss 0.0610005, acc 0.98
2016-09-07T01:54:33.097036: step 3059, loss 0.0117489, acc 1
2016-09-07T01:54:33.788543: step 3060, loss 0.0733102, acc 0.94
2016-09-07T01:54:34.505203: step 3061, loss 0.0166446, acc 0.98
2016-09-07T01:54:35.213078: step 3062, loss 0.0272359, acc 0.98
2016-09-07T01:54:35.912345: step 3063, loss 0.0203463, acc 1
2016-09-07T01:54:36.609947: step 3064, loss 0.0130791, acc 1
2016-09-07T01:54:37.323537: step 3065, loss 0.00606869, acc 1
2016-09-07T01:54:38.022659: step 3066, loss 0.0372146, acc 0.96
2016-09-07T01:54:38.693151: step 3067, loss 0.0417482, acc 1
2016-09-07T01:54:39.402778: step 3068, loss 0.00199334, acc 1
2016-09-07T01:54:40.109163: step 3069, loss 0.0614682, acc 0.96
2016-09-07T01:54:40.830159: step 3070, loss 0.0294691, acc 1
2016-09-07T01:54:41.551877: step 3071, loss 0.0327966, acc 0.98
2016-09-07T01:54:42.188526: step 3072, loss 0.000502614, acc 1
2016-09-07T01:54:42.908807: step 3073, loss 0.0197256, acc 0.98
2016-09-07T01:54:43.600199: step 3074, loss 0.0778539, acc 0.96
2016-09-07T01:54:44.297751: step 3075, loss 0.0998375, acc 0.94
2016-09-07T01:54:44.968863: step 3076, loss 0.00411172, acc 1
2016-09-07T01:54:45.683866: step 3077, loss 0.00829208, acc 1
2016-09-07T01:54:46.381766: step 3078, loss 0.0144917, acc 1
2016-09-07T01:54:47.067583: step 3079, loss 0.0285059, acc 1
2016-09-07T01:54:47.788768: step 3080, loss 0.0168435, acc 1
2016-09-07T01:54:48.497195: step 3081, loss 0.00176578, acc 1
2016-09-07T01:54:49.192332: step 3082, loss 0.00205936, acc 1
2016-09-07T01:54:49.903919: step 3083, loss 0.0611948, acc 0.98
2016-09-07T01:54:50.600623: step 3084, loss 0.00127281, acc 1
2016-09-07T01:54:51.344058: step 3085, loss 0.0407606, acc 0.98
2016-09-07T01:54:52.031856: step 3086, loss 0.0199958, acc 1
2016-09-07T01:54:52.729574: step 3087, loss 0.00289151, acc 1
2016-09-07T01:54:53.416361: step 3088, loss 0.0410056, acc 0.98
2016-09-07T01:54:54.115976: step 3089, loss 0.0342769, acc 1
2016-09-07T01:54:54.792819: step 3090, loss 0.0221983, acc 1
2016-09-07T01:54:55.469259: step 3091, loss 0.00532859, acc 1
2016-09-07T01:54:56.194412: step 3092, loss 0.00293882, acc 1
2016-09-07T01:54:56.915437: step 3093, loss 0.00311118, acc 1
2016-09-07T01:54:57.610503: step 3094, loss 0.0145543, acc 1
2016-09-07T01:54:58.333725: step 3095, loss 0.00143693, acc 1
2016-09-07T01:54:59.042349: step 3096, loss 0.0152491, acc 1
2016-09-07T01:54:59.749402: step 3097, loss 0.0169585, acc 1
2016-09-07T01:55:00.481400: step 3098, loss 0.0947631, acc 0.98
2016-09-07T01:55:01.182013: step 3099, loss 0.0198554, acc 1
2016-09-07T01:55:01.887312: step 3100, loss 0.000533266, acc 1

Evaluation:
2016-09-07T01:55:05.521466: step 3100, loss 1.68045, acc 0.775797

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-3100

2016-09-07T01:55:07.387435: step 3101, loss 0.0461154, acc 0.98
2016-09-07T01:55:08.105066: step 3102, loss 0.0198357, acc 1
2016-09-07T01:55:08.796340: step 3103, loss 0.0576518, acc 0.98
2016-09-07T01:55:09.489627: step 3104, loss 0.0310543, acc 1
2016-09-07T01:55:10.184645: step 3105, loss 0.0284445, acc 0.98
2016-09-07T01:55:10.873324: step 3106, loss 0.00686824, acc 1
2016-09-07T01:55:11.562988: step 3107, loss 0.0201851, acc 0.98
2016-09-07T01:55:12.238711: step 3108, loss 0.0113211, acc 1
2016-09-07T01:55:12.949711: step 3109, loss 0.0380009, acc 0.96
2016-09-07T01:55:13.634664: step 3110, loss 0.0451311, acc 0.98
2016-09-07T01:55:14.314785: step 3111, loss 0.0438823, acc 0.98
2016-09-07T01:55:15.020084: step 3112, loss 0.00706153, acc 1
2016-09-07T01:55:15.728112: step 3113, loss 0.0364538, acc 0.96
2016-09-07T01:55:16.429463: step 3114, loss 0.00682289, acc 1
2016-09-07T01:55:17.129666: step 3115, loss 0.0112706, acc 1
2016-09-07T01:55:17.847286: step 3116, loss 0.017485, acc 0.98
2016-09-07T01:55:18.596187: step 3117, loss 0.140272, acc 0.98
2016-09-07T01:55:19.297384: step 3118, loss 0.0605494, acc 0.98
2016-09-07T01:55:20.006713: step 3119, loss 0.0110879, acc 1
2016-09-07T01:55:20.706900: step 3120, loss 0.0336562, acc 0.98
2016-09-07T01:55:21.423140: step 3121, loss 0.0921478, acc 0.96
2016-09-07T01:55:22.107350: step 3122, loss 0.0359746, acc 0.96
2016-09-07T01:55:22.794013: step 3123, loss 0.00526921, acc 1
2016-09-07T01:55:23.490949: step 3124, loss 0.0323622, acc 0.98
2016-09-07T01:55:24.168338: step 3125, loss 0.0435312, acc 0.98
2016-09-07T01:55:24.858322: step 3126, loss 0.0177111, acc 1
2016-09-07T01:55:25.546815: step 3127, loss 0.0144303, acc 1
2016-09-07T01:55:26.260048: step 3128, loss 0.0217042, acc 0.98
2016-09-07T01:55:26.974223: step 3129, loss 0.0661721, acc 0.96
2016-09-07T01:55:27.654125: step 3130, loss 0.0689407, acc 0.96
2016-09-07T01:55:28.346614: step 3131, loss 0.0176218, acc 1
2016-09-07T01:55:29.058246: step 3132, loss 0.00937126, acc 1
2016-09-07T01:55:29.750271: step 3133, loss 0.041142, acc 0.98
2016-09-07T01:55:30.412749: step 3134, loss 0.00850179, acc 1
2016-09-07T01:55:31.108385: step 3135, loss 0.0119379, acc 1
2016-09-07T01:55:31.799899: step 3136, loss 0.0819567, acc 0.96
2016-09-07T01:55:32.496463: step 3137, loss 0.0826762, acc 0.98
2016-09-07T01:55:33.193531: step 3138, loss 0.015342, acc 0.98
2016-09-07T01:55:33.886349: step 3139, loss 0.0512275, acc 0.98
2016-09-07T01:55:34.608072: step 3140, loss 0.00241995, acc 1
2016-09-07T01:55:35.330838: step 3141, loss 0.00255911, acc 1
2016-09-07T01:55:36.014006: step 3142, loss 0.0351252, acc 0.98
2016-09-07T01:55:36.711508: step 3143, loss 0.0407111, acc 0.98
2016-09-07T01:55:37.407090: step 3144, loss 0.0347524, acc 0.98
2016-09-07T01:55:38.103186: step 3145, loss 0.00573813, acc 1
2016-09-07T01:55:38.778409: step 3146, loss 0.0420199, acc 0.98
2016-09-07T01:55:39.506750: step 3147, loss 0.0112287, acc 1
2016-09-07T01:55:40.211590: step 3148, loss 0.0824215, acc 0.98
2016-09-07T01:55:40.905825: step 3149, loss 0.00835258, acc 1
2016-09-07T01:55:41.626862: step 3150, loss 0.0236489, acc 0.98
2016-09-07T01:55:42.333059: step 3151, loss 0.0739295, acc 0.94
2016-09-07T01:55:43.040615: step 3152, loss 0.0279875, acc 0.98
2016-09-07T01:55:43.737855: step 3153, loss 0.00148065, acc 1
2016-09-07T01:55:44.415930: step 3154, loss 0.0798131, acc 0.96
2016-09-07T01:55:45.091483: step 3155, loss 0.017587, acc 0.98
2016-09-07T01:55:45.807408: step 3156, loss 0.115249, acc 0.96
2016-09-07T01:55:46.546666: step 3157, loss 0.0172908, acc 1
2016-09-07T01:55:47.224299: step 3158, loss 0.0311908, acc 1
2016-09-07T01:55:47.942710: step 3159, loss 0.0652176, acc 0.96
2016-09-07T01:55:48.648890: step 3160, loss 0.15174, acc 0.96
2016-09-07T01:55:49.359671: step 3161, loss 0.00708619, acc 1
2016-09-07T01:55:50.083186: step 3162, loss 0.0649913, acc 0.98
2016-09-07T01:55:50.754256: step 3163, loss 0.0825378, acc 0.98
2016-09-07T01:55:51.476113: step 3164, loss 0.0436405, acc 0.98
2016-09-07T01:55:52.169808: step 3165, loss 0.248995, acc 0.94
2016-09-07T01:55:52.873839: step 3166, loss 0.0141307, acc 1
2016-09-07T01:55:53.577454: step 3167, loss 0.0369505, acc 0.98
2016-09-07T01:55:54.274450: step 3168, loss 0.0502552, acc 0.98
2016-09-07T01:55:54.970879: step 3169, loss 0.079767, acc 0.96
2016-09-07T01:55:55.663708: step 3170, loss 0.0822934, acc 0.92
2016-09-07T01:55:56.365370: step 3171, loss 0.0343725, acc 0.98
2016-09-07T01:55:57.061564: step 3172, loss 0.0365545, acc 1
2016-09-07T01:55:57.773348: step 3173, loss 0.0252245, acc 0.98
2016-09-07T01:55:58.470840: step 3174, loss 0.00438082, acc 1
2016-09-07T01:55:59.158580: step 3175, loss 0.0631022, acc 0.96
2016-09-07T01:55:59.859828: step 3176, loss 0.0299019, acc 1
2016-09-07T01:56:00.586461: step 3177, loss 0.00287917, acc 1
2016-09-07T01:56:01.278918: step 3178, loss 0.0456208, acc 1
2016-09-07T01:56:01.982184: step 3179, loss 0.0167962, acc 1
2016-09-07T01:56:02.677080: step 3180, loss 0.0727025, acc 0.96
2016-09-07T01:56:03.383734: step 3181, loss 0.0270204, acc 1
2016-09-07T01:56:04.061325: step 3182, loss 0.0229932, acc 1
2016-09-07T01:56:04.769492: step 3183, loss 0.00320185, acc 1
2016-09-07T01:56:05.476139: step 3184, loss 0.169463, acc 0.96
2016-09-07T01:56:06.179137: step 3185, loss 0.0357655, acc 0.98
2016-09-07T01:56:06.891861: step 3186, loss 0.0230042, acc 1
2016-09-07T01:56:07.599040: step 3187, loss 0.0576339, acc 0.98
2016-09-07T01:56:08.302591: step 3188, loss 0.152462, acc 0.96
2016-09-07T01:56:08.998528: step 3189, loss 0.0589295, acc 0.98
2016-09-07T01:56:09.696202: step 3190, loss 0.00855199, acc 1
2016-09-07T01:56:10.389645: step 3191, loss 0.107437, acc 0.98
2016-09-07T01:56:11.093472: step 3192, loss 0.0908426, acc 0.96
2016-09-07T01:56:11.840707: step 3193, loss 0.00628723, acc 1
2016-09-07T01:56:12.537295: step 3194, loss 0.0800012, acc 0.98
2016-09-07T01:56:13.241145: step 3195, loss 0.0131376, acc 1
2016-09-07T01:56:13.933443: step 3196, loss 0.0244227, acc 0.98
2016-09-07T01:56:14.650771: step 3197, loss 0.0352534, acc 0.98
2016-09-07T01:56:15.344060: step 3198, loss 0.00381099, acc 1
2016-09-07T01:56:16.015926: step 3199, loss 0.0229033, acc 1
2016-09-07T01:56:16.715535: step 3200, loss 0.077615, acc 0.98

Evaluation:
2016-09-07T01:56:20.357001: step 3200, loss 1.11749, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-3200

2016-09-07T01:56:22.151183: step 3201, loss 0.00352636, acc 1
2016-09-07T01:56:22.844096: step 3202, loss 0.0933228, acc 0.92
2016-09-07T01:56:23.524565: step 3203, loss 0.121867, acc 0.94
2016-09-07T01:56:24.239154: step 3204, loss 0.032244, acc 0.98
2016-09-07T01:56:24.945422: step 3205, loss 0.0980466, acc 0.94
2016-09-07T01:56:25.613029: step 3206, loss 0.0569516, acc 1
2016-09-07T01:56:26.319297: step 3207, loss 0.0184318, acc 1
2016-09-07T01:56:27.005301: step 3208, loss 0.0328565, acc 0.98
2016-09-07T01:56:27.687689: step 3209, loss 0.02359, acc 1
2016-09-07T01:56:28.379844: step 3210, loss 0.0269668, acc 0.98
2016-09-07T01:56:29.093745: step 3211, loss 0.0568876, acc 0.98
2016-09-07T01:56:29.806030: step 3212, loss 0.0165532, acc 1
2016-09-07T01:56:30.466272: step 3213, loss 0.020292, acc 1
2016-09-07T01:56:31.162634: step 3214, loss 0.0191752, acc 1
2016-09-07T01:56:31.856922: step 3215, loss 0.0158666, acc 1
2016-09-07T01:56:32.548053: step 3216, loss 0.0165308, acc 1
2016-09-07T01:56:33.251644: step 3217, loss 0.0854494, acc 0.96
2016-09-07T01:56:33.957728: step 3218, loss 0.00376332, acc 1
2016-09-07T01:56:34.679690: step 3219, loss 0.00983688, acc 1
2016-09-07T01:56:35.349163: step 3220, loss 0.0380932, acc 1
2016-09-07T01:56:36.041057: step 3221, loss 0.0394477, acc 1
2016-09-07T01:56:36.730325: step 3222, loss 0.0288542, acc 1
2016-09-07T01:56:37.420719: step 3223, loss 0.141162, acc 0.96
2016-09-07T01:56:38.116797: step 3224, loss 0.0102785, acc 1
2016-09-07T01:56:38.820895: step 3225, loss 0.0335546, acc 0.98
2016-09-07T01:56:39.529940: step 3226, loss 0.0546108, acc 0.98
2016-09-07T01:56:40.211826: step 3227, loss 0.0071278, acc 1
2016-09-07T01:56:40.910215: step 3228, loss 0.0155464, acc 1
2016-09-07T01:56:41.588168: step 3229, loss 0.016682, acc 1
2016-09-07T01:56:42.285196: step 3230, loss 0.0517529, acc 0.96
2016-09-07T01:56:42.972420: step 3231, loss 0.0110945, acc 1
2016-09-07T01:56:43.652108: step 3232, loss 0.0371858, acc 0.98
2016-09-07T01:56:44.367400: step 3233, loss 0.00316984, acc 1
2016-09-07T01:56:45.057980: step 3234, loss 0.00906414, acc 1
2016-09-07T01:56:45.756421: step 3235, loss 0.0178692, acc 1
2016-09-07T01:56:46.452945: step 3236, loss 0.0453813, acc 0.96
2016-09-07T01:56:47.156711: step 3237, loss 0.0310832, acc 0.98
2016-09-07T01:56:47.862461: step 3238, loss 0.0191075, acc 1
2016-09-07T01:56:48.548121: step 3239, loss 0.0175252, acc 1
2016-09-07T01:56:49.288797: step 3240, loss 0.0497614, acc 0.96
2016-09-07T01:56:49.982849: step 3241, loss 0.0569824, acc 0.96
2016-09-07T01:56:50.685989: step 3242, loss 0.0331989, acc 0.98
2016-09-07T01:56:51.416650: step 3243, loss 0.045269, acc 0.98
2016-09-07T01:56:52.099798: step 3244, loss 0.0679387, acc 0.98
2016-09-07T01:56:52.813326: step 3245, loss 0.0125305, acc 1
2016-09-07T01:56:53.502971: step 3246, loss 0.0627719, acc 0.96
2016-09-07T01:56:54.213216: step 3247, loss 0.00680909, acc 1
2016-09-07T01:56:54.897041: step 3248, loss 0.1246, acc 0.98
2016-09-07T01:56:55.598441: step 3249, loss 0.130748, acc 0.94
2016-09-07T01:56:56.312995: step 3250, loss 0.0460061, acc 0.98
2016-09-07T01:56:56.988861: step 3251, loss 0.0275052, acc 0.98
2016-09-07T01:56:57.691544: step 3252, loss 0.105592, acc 0.96
2016-09-07T01:56:58.383100: step 3253, loss 0.0473126, acc 0.98
2016-09-07T01:56:59.092201: step 3254, loss 0.0807041, acc 0.96
2016-09-07T01:56:59.812602: step 3255, loss 0.0199017, acc 1
2016-09-07T01:57:00.523119: step 3256, loss 0.0632395, acc 0.96
2016-09-07T01:57:01.239947: step 3257, loss 0.00131438, acc 1
2016-09-07T01:57:01.941516: step 3258, loss 0.0617553, acc 0.96
2016-09-07T01:57:02.619088: step 3259, loss 0.0437649, acc 0.96
2016-09-07T01:57:03.310396: step 3260, loss 0.00320658, acc 1
2016-09-07T01:57:03.998622: step 3261, loss 0.0577268, acc 0.96
2016-09-07T01:57:04.714004: step 3262, loss 0.0264568, acc 0.98
2016-09-07T01:57:05.389724: step 3263, loss 0.00463854, acc 1
2016-09-07T01:57:06.057312: step 3264, loss 0.00327491, acc 1
2016-09-07T01:57:06.747977: step 3265, loss 0.0291648, acc 0.98
2016-09-07T01:57:07.447232: step 3266, loss 0.0589503, acc 0.96
2016-09-07T01:57:08.157817: step 3267, loss 0.0545498, acc 0.98
2016-09-07T01:57:08.867576: step 3268, loss 0.0465888, acc 0.98
2016-09-07T01:57:09.593409: step 3269, loss 0.0134855, acc 1
2016-09-07T01:57:10.287552: step 3270, loss 0.102947, acc 0.98
2016-09-07T01:57:10.975556: step 3271, loss 0.0494104, acc 0.96
2016-09-07T01:57:11.668124: step 3272, loss 0.011136, acc 1
2016-09-07T01:57:12.371881: step 3273, loss 0.0329537, acc 0.98
2016-09-07T01:57:13.065762: step 3274, loss 0.0252559, acc 0.98
2016-09-07T01:57:13.771824: step 3275, loss 0.0036077, acc 1
2016-09-07T01:57:14.519275: step 3276, loss 0.0804872, acc 0.96
2016-09-07T01:57:15.218411: step 3277, loss 0.0564388, acc 0.98
2016-09-07T01:57:15.919892: step 3278, loss 0.0572633, acc 0.98
2016-09-07T01:57:16.615779: step 3279, loss 0.0127495, acc 1
2016-09-07T01:57:17.313475: step 3280, loss 0.0246464, acc 1
2016-09-07T01:57:18.033097: step 3281, loss 0.046869, acc 0.96
2016-09-07T01:57:18.702547: step 3282, loss 0.035937, acc 0.98
2016-09-07T01:57:19.385455: step 3283, loss 0.046761, acc 0.96
2016-09-07T01:57:20.095505: step 3284, loss 0.0579961, acc 0.98
2016-09-07T01:57:20.821328: step 3285, loss 0.0569191, acc 0.96
2016-09-07T01:57:21.573481: step 3286, loss 0.0153483, acc 1
2016-09-07T01:57:22.258292: step 3287, loss 0.0826954, acc 0.98
2016-09-07T01:57:22.947831: step 3288, loss 0.026671, acc 0.98
2016-09-07T01:57:23.655303: step 3289, loss 0.112717, acc 0.98
2016-09-07T01:57:24.350491: step 3290, loss 0.0561262, acc 0.98
2016-09-07T01:57:25.031201: step 3291, loss 0.00352475, acc 1
2016-09-07T01:57:25.743879: step 3292, loss 0.0118738, acc 1
2016-09-07T01:57:26.482986: step 3293, loss 0.0186229, acc 1
2016-09-07T01:57:27.172493: step 3294, loss 0.0115379, acc 1
2016-09-07T01:57:27.857082: step 3295, loss 0.0346922, acc 0.96
2016-09-07T01:57:28.541405: step 3296, loss 0.039599, acc 0.98
2016-09-07T01:57:29.240752: step 3297, loss 0.018321, acc 0.98
2016-09-07T01:57:29.950241: step 3298, loss 0.102034, acc 0.94
2016-09-07T01:57:30.624091: step 3299, loss 0.0598658, acc 0.94
2016-09-07T01:57:31.324816: step 3300, loss 0.0394797, acc 0.98

Evaluation:
2016-09-07T01:57:34.963083: step 3300, loss 1.28581, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-3300

2016-09-07T01:57:36.882384: step 3301, loss 0.0131763, acc 1
2016-09-07T01:57:37.570892: step 3302, loss 0.00144532, acc 1
2016-09-07T01:57:38.269390: step 3303, loss 0.00467987, acc 1
2016-09-07T01:57:38.974898: step 3304, loss 0.00835663, acc 1
2016-09-07T01:57:39.727685: step 3305, loss 0.0236891, acc 1
2016-09-07T01:57:40.403512: step 3306, loss 0.100874, acc 0.98
2016-09-07T01:57:41.097289: step 3307, loss 0.0190091, acc 1
2016-09-07T01:57:41.788731: step 3308, loss 0.018611, acc 1
2016-09-07T01:57:42.499393: step 3309, loss 0.0215721, acc 0.98
2016-09-07T01:57:43.227227: step 3310, loss 0.0471687, acc 0.98
2016-09-07T01:57:43.923561: step 3311, loss 0.0254817, acc 1
2016-09-07T01:57:44.614610: step 3312, loss 0.0806675, acc 0.98
2016-09-07T01:57:45.313997: step 3313, loss 0.016379, acc 1
2016-09-07T01:57:45.994895: step 3314, loss 0.0259322, acc 1
2016-09-07T01:57:46.706095: step 3315, loss 0.0724935, acc 0.98
2016-09-07T01:57:47.384069: step 3316, loss 0.0168661, acc 1
2016-09-07T01:57:48.088581: step 3317, loss 0.000935983, acc 1
2016-09-07T01:57:48.787884: step 3318, loss 0.0410124, acc 0.98
2016-09-07T01:57:49.463555: step 3319, loss 0.0167198, acc 1
2016-09-07T01:57:50.159591: step 3320, loss 0.0266941, acc 0.98
2016-09-07T01:57:50.851164: step 3321, loss 0.0322565, acc 0.98
2016-09-07T01:57:51.546656: step 3322, loss 0.0580506, acc 0.98
2016-09-07T01:57:52.216859: step 3323, loss 0.0133675, acc 1
2016-09-07T01:57:52.920784: step 3324, loss 0.0376862, acc 0.98
2016-09-07T01:57:53.613467: step 3325, loss 0.0705849, acc 0.96
2016-09-07T01:57:54.318961: step 3326, loss 0.102859, acc 0.96
2016-09-07T01:57:55.007754: step 3327, loss 0.0201631, acc 1
2016-09-07T01:57:55.694535: step 3328, loss 0.0870827, acc 0.96
2016-09-07T01:57:56.386751: step 3329, loss 0.0824778, acc 0.98
2016-09-07T01:57:57.059230: step 3330, loss 0.0182511, acc 1
2016-09-07T01:57:57.769677: step 3331, loss 0.00640178, acc 1
2016-09-07T01:57:58.460255: step 3332, loss 0.0441605, acc 1
2016-09-07T01:57:59.173557: step 3333, loss 0.00422893, acc 1
2016-09-07T01:57:59.871030: step 3334, loss 0.0304085, acc 0.98
2016-09-07T01:58:00.588665: step 3335, loss 0.0307097, acc 0.98
2016-09-07T01:58:01.299153: step 3336, loss 0.0767977, acc 0.98
2016-09-07T01:58:01.987766: step 3337, loss 0.0104879, acc 1
2016-09-07T01:58:02.684582: step 3338, loss 0.00312243, acc 1
2016-09-07T01:58:03.377555: step 3339, loss 0.0114341, acc 1
2016-09-07T01:58:04.075614: step 3340, loss 0.0261507, acc 1
2016-09-07T01:58:04.769362: step 3341, loss 0.0036392, acc 1
2016-09-07T01:58:05.442569: step 3342, loss 0.000987715, acc 1
2016-09-07T01:58:06.156193: step 3343, loss 0.0822754, acc 0.96
2016-09-07T01:58:06.868302: step 3344, loss 0.138554, acc 0.96
2016-09-07T01:58:07.580474: step 3345, loss 0.00495785, acc 1
2016-09-07T01:58:08.275064: step 3346, loss 0.0183561, acc 1
2016-09-07T01:58:08.988836: step 3347, loss 0.0550111, acc 0.96
2016-09-07T01:58:09.735217: step 3348, loss 0.00521927, acc 1
2016-09-07T01:58:10.470488: step 3349, loss 0.0695706, acc 0.96
2016-09-07T01:58:11.172096: step 3350, loss 0.0236244, acc 0.98
2016-09-07T01:58:11.864075: step 3351, loss 0.0237045, acc 1
2016-09-07T01:58:12.553920: step 3352, loss 0.0841236, acc 0.98
2016-09-07T01:58:13.277993: step 3353, loss 0.0155191, acc 1
2016-09-07T01:58:13.967395: step 3354, loss 0.00588275, acc 1
2016-09-07T01:58:14.675346: step 3355, loss 0.0206993, acc 1
2016-09-07T01:58:15.364239: step 3356, loss 0.00529219, acc 1
2016-09-07T01:58:16.077721: step 3357, loss 0.000340317, acc 1
2016-09-07T01:58:16.789566: step 3358, loss 0.0853003, acc 0.96
2016-09-07T01:58:17.465683: step 3359, loss 0.0173529, acc 1
2016-09-07T01:58:18.157454: step 3360, loss 0.148591, acc 0.96
2016-09-07T01:58:18.846808: step 3361, loss 0.0571946, acc 0.98
2016-09-07T01:58:19.548267: step 3362, loss 0.206787, acc 0.92
2016-09-07T01:58:20.244817: step 3363, loss 0.0230705, acc 1
2016-09-07T01:58:20.947408: step 3364, loss 0.0179724, acc 1
2016-09-07T01:58:21.670391: step 3365, loss 0.00488954, acc 1
2016-09-07T01:58:22.359101: step 3366, loss 0.0759133, acc 0.98
2016-09-07T01:58:23.072290: step 3367, loss 0.0227224, acc 0.98
2016-09-07T01:58:23.796873: step 3368, loss 0.0594329, acc 0.96
2016-09-07T01:58:24.468753: step 3369, loss 0.0350223, acc 0.98
2016-09-07T01:58:25.187162: step 3370, loss 0.0642091, acc 0.96
2016-09-07T01:58:25.862573: step 3371, loss 0.0325511, acc 0.98
2016-09-07T01:58:26.579183: step 3372, loss 0.050238, acc 0.98
2016-09-07T01:58:27.293441: step 3373, loss 0.0365624, acc 0.98
2016-09-07T01:58:27.996555: step 3374, loss 0.0181035, acc 0.98
2016-09-07T01:58:28.678148: step 3375, loss 0.0401827, acc 0.98
2016-09-07T01:58:29.386243: step 3376, loss 0.0128207, acc 1
2016-09-07T01:58:30.096285: step 3377, loss 0.0246531, acc 1
2016-09-07T01:58:30.805211: step 3378, loss 0.00250351, acc 1
2016-09-07T01:58:31.500006: step 3379, loss 0.00449284, acc 1
2016-09-07T01:58:32.191530: step 3380, loss 0.0484622, acc 0.98
2016-09-07T01:58:32.888201: step 3381, loss 0.00377613, acc 1
2016-09-07T01:58:33.588599: step 3382, loss 0.0292179, acc 0.98
2016-09-07T01:58:34.278256: step 3383, loss 0.00395952, acc 1
2016-09-07T01:58:34.999854: step 3384, loss 0.0504168, acc 0.98
2016-09-07T01:58:35.677403: step 3385, loss 0.0205922, acc 1
2016-09-07T01:58:36.369468: step 3386, loss 0.00212616, acc 1
2016-09-07T01:58:37.069993: step 3387, loss 0.0192235, acc 0.98
2016-09-07T01:58:37.772190: step 3388, loss 0.0142088, acc 1
2016-09-07T01:58:38.478085: step 3389, loss 0.126693, acc 0.94
2016-09-07T01:58:39.148451: step 3390, loss 0.0513906, acc 0.98
2016-09-07T01:58:39.871901: step 3391, loss 0.0585131, acc 0.96
2016-09-07T01:58:40.565862: step 3392, loss 0.0601759, acc 0.96
2016-09-07T01:58:41.265251: step 3393, loss 0.0103102, acc 1
2016-09-07T01:58:41.961998: step 3394, loss 0.0401515, acc 0.96
2016-09-07T01:58:42.659203: step 3395, loss 0.0593331, acc 1
2016-09-07T01:58:43.382727: step 3396, loss 0.0334668, acc 0.98
2016-09-07T01:58:44.086814: step 3397, loss 0.0597489, acc 0.96
2016-09-07T01:58:44.776600: step 3398, loss 0.0289886, acc 0.98
2016-09-07T01:58:45.486965: step 3399, loss 0.00575511, acc 1
2016-09-07T01:58:46.189469: step 3400, loss 0.0274222, acc 0.98

Evaluation:
2016-09-07T01:58:49.838303: step 3400, loss 1.47959, acc 0.782364

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-3400

2016-09-07T01:58:51.614088: step 3401, loss 0.0285564, acc 1
2016-09-07T01:58:52.317968: step 3402, loss 0.0457322, acc 0.98
2016-09-07T01:58:53.026902: step 3403, loss 0.0229817, acc 0.98
2016-09-07T01:58:53.723783: step 3404, loss 0.00164424, acc 1
2016-09-07T01:58:54.410603: step 3405, loss 0.044891, acc 0.98
2016-09-07T01:58:55.104906: step 3406, loss 0.020484, acc 0.98
2016-09-07T01:58:55.801882: step 3407, loss 0.0136398, acc 1
2016-09-07T01:58:56.510970: step 3408, loss 0.0317483, acc 1
2016-09-07T01:58:57.195470: step 3409, loss 0.00590455, acc 1
2016-09-07T01:58:57.933231: step 3410, loss 0.0135602, acc 1
2016-09-07T01:58:58.633679: step 3411, loss 0.0118404, acc 1
2016-09-07T01:58:59.335202: step 3412, loss 0.0596201, acc 0.96
2016-09-07T01:59:00.040176: step 3413, loss 0.0457726, acc 0.96
2016-09-07T01:59:00.781388: step 3414, loss 0.188031, acc 0.96
2016-09-07T01:59:01.500391: step 3415, loss 0.0323858, acc 1
2016-09-07T01:59:02.175920: step 3416, loss 0.0947718, acc 0.98
2016-09-07T01:59:02.872659: step 3417, loss 0.00427959, acc 1
2016-09-07T01:59:03.578790: step 3418, loss 0.0356032, acc 0.98
2016-09-07T01:59:04.271215: step 3419, loss 0.0363581, acc 0.98
2016-09-07T01:59:04.996108: step 3420, loss 0.000696671, acc 1
2016-09-07T01:59:05.676368: step 3421, loss 0.00523901, acc 1
2016-09-07T01:59:06.380971: step 3422, loss 0.0263049, acc 1
2016-09-07T01:59:07.086455: step 3423, loss 0.0115415, acc 1
2016-09-07T01:59:07.795884: step 3424, loss 0.0831298, acc 0.96
2016-09-07T01:59:08.509818: step 3425, loss 0.0437637, acc 0.98
2016-09-07T01:59:09.194874: step 3426, loss 0.0568372, acc 0.96
2016-09-07T01:59:09.907711: step 3427, loss 0.0856165, acc 0.96
2016-09-07T01:59:10.624716: step 3428, loss 0.0620731, acc 0.98
2016-09-07T01:59:11.319529: step 3429, loss 0.0354147, acc 0.96
2016-09-07T01:59:12.036483: step 3430, loss 0.0444611, acc 0.98
2016-09-07T01:59:12.723829: step 3431, loss 0.00114542, acc 1
2016-09-07T01:59:13.440352: step 3432, loss 0.0283786, acc 0.98
2016-09-07T01:59:14.155069: step 3433, loss 0.00223337, acc 1
2016-09-07T01:59:14.853884: step 3434, loss 0.150433, acc 0.98
2016-09-07T01:59:15.564534: step 3435, loss 0.0226752, acc 1
2016-09-07T01:59:16.254651: step 3436, loss 0.0389441, acc 0.98
2016-09-07T01:59:16.942866: step 3437, loss 0.0868489, acc 0.94
2016-09-07T01:59:17.613726: step 3438, loss 0.128061, acc 0.94
2016-09-07T01:59:18.333350: step 3439, loss 0.0143804, acc 1
2016-09-07T01:59:19.026418: step 3440, loss 0.00562278, acc 1
2016-09-07T01:59:19.738988: step 3441, loss 0.0399054, acc 0.98
2016-09-07T01:59:20.461663: step 3442, loss 0.010594, acc 1
2016-09-07T01:59:21.158250: step 3443, loss 0.0171283, acc 1
2016-09-07T01:59:21.893702: step 3444, loss 0.10733, acc 0.98
2016-09-07T01:59:22.591406: step 3445, loss 0.0149601, acc 1
2016-09-07T01:59:23.312084: step 3446, loss 0.0013851, acc 1
2016-09-07T01:59:24.024168: step 3447, loss 0.0430468, acc 0.96
2016-09-07T01:59:24.744785: step 3448, loss 0.0425276, acc 0.98
2016-09-07T01:59:25.457856: step 3449, loss 0.0144698, acc 1
2016-09-07T01:59:26.170734: step 3450, loss 0.0127217, acc 1
2016-09-07T01:59:26.886708: step 3451, loss 0.0325962, acc 0.98
2016-09-07T01:59:27.585030: step 3452, loss 0.0124131, acc 1
2016-09-07T01:59:28.309007: step 3453, loss 0.0595776, acc 0.94
2016-09-07T01:59:29.035115: step 3454, loss 0.0181029, acc 1
2016-09-07T01:59:29.718487: step 3455, loss 0.0212588, acc 1
2016-09-07T01:59:30.360712: step 3456, loss 0.0012017, acc 1
2016-09-07T01:59:31.068953: step 3457, loss 0.0153416, acc 1
2016-09-07T01:59:31.759993: step 3458, loss 0.0102646, acc 1
2016-09-07T01:59:32.467250: step 3459, loss 0.10595, acc 0.9
2016-09-07T01:59:33.163534: step 3460, loss 0.0471018, acc 0.98
2016-09-07T01:59:33.860046: step 3461, loss 0.0360058, acc 1
2016-09-07T01:59:34.619467: step 3462, loss 0.0137832, acc 1
2016-09-07T01:59:35.355388: step 3463, loss 0.0135219, acc 1
2016-09-07T01:59:36.025848: step 3464, loss 0.0160382, acc 0.98
2016-09-07T01:59:36.719238: step 3465, loss 0.0316187, acc 0.98
2016-09-07T01:59:37.421238: step 3466, loss 0.0265174, acc 1
2016-09-07T01:59:38.090515: step 3467, loss 0.0448077, acc 0.98
2016-09-07T01:59:38.803159: step 3468, loss 0.0134948, acc 1
2016-09-07T01:59:39.500743: step 3469, loss 0.0355996, acc 0.96
2016-09-07T01:59:40.177647: step 3470, loss 0.0167788, acc 1
2016-09-07T01:59:40.878502: step 3471, loss 0.0169831, acc 1
2016-09-07T01:59:41.612863: step 3472, loss 0.0124516, acc 1
2016-09-07T01:59:42.333247: step 3473, loss 0.0350621, acc 0.98
2016-09-07T01:59:43.021907: step 3474, loss 0.0281257, acc 0.98
2016-09-07T01:59:43.698633: step 3475, loss 0.103609, acc 0.98
2016-09-07T01:59:44.397464: step 3476, loss 0.0147293, acc 1
2016-09-07T01:59:45.100599: step 3477, loss 0.0225524, acc 0.98
2016-09-07T01:59:45.787098: step 3478, loss 0.0560194, acc 0.98
2016-09-07T01:59:46.464263: step 3479, loss 0.000679206, acc 1
2016-09-07T01:59:47.175637: step 3480, loss 0.00204527, acc 1
2016-09-07T01:59:47.880395: step 3481, loss 0.0163926, acc 1
2016-09-07T01:59:48.599637: step 3482, loss 0.0130776, acc 1
2016-09-07T01:59:49.303796: step 3483, loss 0.000876026, acc 1
2016-09-07T01:59:50.015724: step 3484, loss 0.0168543, acc 0.98
2016-09-07T01:59:50.730552: step 3485, loss 0.119606, acc 0.94
2016-09-07T01:59:51.423069: step 3486, loss 0.0345875, acc 0.96
2016-09-07T01:59:52.142717: step 3487, loss 0.0101544, acc 1
2016-09-07T01:59:52.852797: step 3488, loss 0.0195295, acc 0.98
2016-09-07T01:59:53.543384: step 3489, loss 0.0184942, acc 1
2016-09-07T01:59:54.278585: step 3490, loss 0.0362336, acc 0.96
2016-09-07T01:59:54.974752: step 3491, loss 0.0220582, acc 0.98
2016-09-07T01:59:55.680058: step 3492, loss 0.0284098, acc 0.98
2016-09-07T01:59:56.369918: step 3493, loss 0.0185635, acc 1
2016-09-07T01:59:57.066547: step 3494, loss 0.0122195, acc 1
2016-09-07T01:59:57.761317: step 3495, loss 0.112907, acc 0.98
2016-09-07T01:59:58.453302: step 3496, loss 0.0171001, acc 1
2016-09-07T01:59:59.168653: step 3497, loss 0.0421922, acc 0.98
2016-09-07T01:59:59.882237: step 3498, loss 0.0249903, acc 1
2016-09-07T02:00:00.596270: step 3499, loss 0.0248317, acc 0.98
2016-09-07T02:00:01.303966: step 3500, loss 0.00359347, acc 1

Evaluation:
2016-09-07T02:00:04.998975: step 3500, loss 1.64318, acc 0.76454

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-3500

2016-09-07T02:00:06.810888: step 3501, loss 0.0425067, acc 0.96
2016-09-07T02:00:07.535096: step 3502, loss 0.0266985, acc 0.98
2016-09-07T02:00:08.219139: step 3503, loss 0.0616619, acc 0.98
2016-09-07T02:00:08.921455: step 3504, loss 0.0868761, acc 0.96
2016-09-07T02:00:09.629586: step 3505, loss 0.0169932, acc 1
2016-09-07T02:00:10.307778: step 3506, loss 0.0584007, acc 0.96
2016-09-07T02:00:11.004688: step 3507, loss 0.0839958, acc 0.96
2016-09-07T02:00:11.685055: step 3508, loss 0.0146986, acc 1
2016-09-07T02:00:12.405649: step 3509, loss 0.0202093, acc 1
2016-09-07T02:00:13.113180: step 3510, loss 0.0203972, acc 0.98
2016-09-07T02:00:13.826536: step 3511, loss 0.0304856, acc 0.98
2016-09-07T02:00:14.516520: step 3512, loss 0.214112, acc 0.94
2016-09-07T02:00:15.224653: step 3513, loss 0.00856589, acc 1
2016-09-07T02:00:15.944232: step 3514, loss 0.0113033, acc 1
2016-09-07T02:00:16.628851: step 3515, loss 0.0559473, acc 0.96
2016-09-07T02:00:17.302233: step 3516, loss 0.0132839, acc 1
2016-09-07T02:00:18.006719: step 3517, loss 0.0374292, acc 0.96
2016-09-07T02:00:18.711731: step 3518, loss 0.0149366, acc 1
2016-09-07T02:00:19.423571: step 3519, loss 0.00841778, acc 1
2016-09-07T02:00:20.086902: step 3520, loss 0.0369465, acc 0.98
2016-09-07T02:00:20.805278: step 3521, loss 0.0606221, acc 0.98
2016-09-07T02:00:21.489528: step 3522, loss 0.0633069, acc 0.96
2016-09-07T02:00:22.169763: step 3523, loss 0.0164233, acc 1
2016-09-07T02:00:22.861605: step 3524, loss 0.0151183, acc 1
2016-09-07T02:00:23.549853: step 3525, loss 0.00162203, acc 1
2016-09-07T02:00:24.232741: step 3526, loss 0.0336259, acc 0.98
2016-09-07T02:00:24.923261: step 3527, loss 0.0108168, acc 1
2016-09-07T02:00:25.652146: step 3528, loss 0.0792826, acc 0.98
2016-09-07T02:00:26.323607: step 3529, loss 0.0357559, acc 0.98
2016-09-07T02:00:27.003223: step 3530, loss 0.02045, acc 1
2016-09-07T02:00:27.705331: step 3531, loss 0.0321774, acc 0.98
2016-09-07T02:00:28.395525: step 3532, loss 0.124218, acc 0.9
2016-09-07T02:00:29.118603: step 3533, loss 0.0333946, acc 0.98
2016-09-07T02:00:29.795025: step 3534, loss 0.0259206, acc 0.98
2016-09-07T02:00:30.515911: step 3535, loss 0.0597005, acc 0.98
2016-09-07T02:00:31.209907: step 3536, loss 0.0342687, acc 0.98
2016-09-07T02:00:31.916705: step 3537, loss 0.0162632, acc 1
2016-09-07T02:00:32.616901: step 3538, loss 0.016203, acc 0.98
2016-09-07T02:00:33.289230: step 3539, loss 0.0447925, acc 0.98
2016-09-07T02:00:33.994736: step 3540, loss 0.0353548, acc 0.98
2016-09-07T02:00:34.688334: step 3541, loss 0.0270465, acc 0.98
2016-09-07T02:00:35.375800: step 3542, loss 0.00368968, acc 1
2016-09-07T02:00:36.074552: step 3543, loss 0.036003, acc 0.98
2016-09-07T02:00:36.768552: step 3544, loss 0.00401313, acc 1
2016-09-07T02:00:37.456877: step 3545, loss 0.0691294, acc 0.98
2016-09-07T02:00:38.131605: step 3546, loss 0.0364586, acc 0.96
2016-09-07T02:00:38.838315: step 3547, loss 0.0266513, acc 1
2016-09-07T02:00:39.549122: step 3548, loss 0.105215, acc 0.94
2016-09-07T02:00:40.245978: step 3549, loss 0.013061, acc 1
2016-09-07T02:00:40.966557: step 3550, loss 0.0796999, acc 0.96
2016-09-07T02:00:41.686945: step 3551, loss 0.025673, acc 0.98
2016-09-07T02:00:42.395475: step 3552, loss 0.0486517, acc 0.98
2016-09-07T02:00:43.087674: step 3553, loss 0.06202, acc 0.96
2016-09-07T02:00:43.787522: step 3554, loss 0.00699315, acc 1
2016-09-07T02:00:44.492219: step 3555, loss 0.0543956, acc 0.96
2016-09-07T02:00:45.192941: step 3556, loss 0.0511873, acc 0.98
2016-09-07T02:00:45.911487: step 3557, loss 0.0122524, acc 1
2016-09-07T02:00:46.584164: step 3558, loss 0.0165049, acc 1
2016-09-07T02:00:47.301750: step 3559, loss 0.0151152, acc 0.98
2016-09-07T02:00:48.003713: step 3560, loss 0.0292245, acc 0.98
2016-09-07T02:00:48.730060: step 3561, loss 0.00867039, acc 1
2016-09-07T02:00:49.420761: step 3562, loss 0.0258206, acc 0.98
2016-09-07T02:00:50.124670: step 3563, loss 0.0365675, acc 1
2016-09-07T02:00:50.846891: step 3564, loss 0.16971, acc 0.96
2016-09-07T02:00:51.531715: step 3565, loss 0.0213627, acc 1
2016-09-07T02:00:52.223307: step 3566, loss 0.027097, acc 1
2016-09-07T02:00:52.911908: step 3567, loss 0.0571312, acc 0.98
2016-09-07T02:00:53.619097: step 3568, loss 0.00940358, acc 1
2016-09-07T02:00:54.313115: step 3569, loss 0.0056273, acc 1
2016-09-07T02:00:54.980184: step 3570, loss 0.0368556, acc 0.98
2016-09-07T02:00:55.701641: step 3571, loss 0.0168026, acc 1
2016-09-07T02:00:56.390852: step 3572, loss 0.146798, acc 0.94
2016-09-07T02:00:57.092462: step 3573, loss 0.0397704, acc 0.98
2016-09-07T02:00:57.797813: step 3574, loss 0.0382739, acc 0.96
2016-09-07T02:00:58.498882: step 3575, loss 0.0130304, acc 1
2016-09-07T02:00:59.194426: step 3576, loss 0.112483, acc 0.94
2016-09-07T02:00:59.892809: step 3577, loss 0.114794, acc 0.96
2016-09-07T02:01:00.606406: step 3578, loss 0.0290402, acc 0.98
2016-09-07T02:01:01.285759: step 3579, loss 0.0406518, acc 0.98
2016-09-07T02:01:01.979357: step 3580, loss 0.00557118, acc 1
2016-09-07T02:01:02.658580: step 3581, loss 0.0174669, acc 1
2016-09-07T02:01:03.353625: step 3582, loss 0.00577066, acc 1
2016-09-07T02:01:04.069760: step 3583, loss 0.0129748, acc 1
2016-09-07T02:01:04.765830: step 3584, loss 0.0092579, acc 1
2016-09-07T02:01:05.463804: step 3585, loss 0.0691876, acc 0.98
2016-09-07T02:01:06.164004: step 3586, loss 0.0494329, acc 0.98
2016-09-07T02:01:06.865376: step 3587, loss 0.071604, acc 0.94
2016-09-07T02:01:07.567883: step 3588, loss 0.0316301, acc 0.98
2016-09-07T02:01:08.230212: step 3589, loss 0.0453918, acc 0.94
2016-09-07T02:01:08.945943: step 3590, loss 0.000434705, acc 1
2016-09-07T02:01:09.635441: step 3591, loss 0.0192931, acc 1
2016-09-07T02:01:10.325497: step 3592, loss 0.0202143, acc 0.98
2016-09-07T02:01:11.040404: step 3593, loss 0.0193706, acc 1
2016-09-07T02:01:11.729087: step 3594, loss 0.0109605, acc 1
2016-09-07T02:01:12.430284: step 3595, loss 0.011844, acc 1
2016-09-07T02:01:13.109987: step 3596, loss 0.0547938, acc 0.96
2016-09-07T02:01:13.792214: step 3597, loss 0.0207415, acc 1
2016-09-07T02:01:14.496363: step 3598, loss 0.00757097, acc 1
2016-09-07T02:01:15.184803: step 3599, loss 0.00406358, acc 1
2016-09-07T02:01:15.879883: step 3600, loss 0.0750343, acc 0.98

Evaluation:
2016-09-07T02:01:19.071268: step 3600, loss 1.60671, acc 0.772045

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-3600

2016-09-07T02:01:20.830385: step 3601, loss 0.00969481, acc 1
2016-09-07T02:01:21.505685: step 3602, loss 0.0945229, acc 0.98
2016-09-07T02:01:22.219789: step 3603, loss 0.0148647, acc 1
2016-09-07T02:01:22.918528: step 3604, loss 0.158724, acc 0.94
2016-09-07T02:01:23.637919: step 3605, loss 0.0497554, acc 0.96
2016-09-07T02:01:24.331384: step 3606, loss 0.0337288, acc 0.98
2016-09-07T02:01:25.010562: step 3607, loss 0.0412891, acc 0.98
2016-09-07T02:01:25.691483: step 3608, loss 0.0133807, acc 1
2016-09-07T02:01:26.388085: step 3609, loss 0.0113448, acc 1
2016-09-07T02:01:27.067136: step 3610, loss 0.0773687, acc 0.96
2016-09-07T02:01:27.772899: step 3611, loss 0.0509211, acc 0.98
2016-09-07T02:01:28.468586: step 3612, loss 0.0328594, acc 1
2016-09-07T02:01:29.187458: step 3613, loss 0.0134695, acc 1
2016-09-07T02:01:29.874559: step 3614, loss 0.0431144, acc 0.96
2016-09-07T02:01:30.577775: step 3615, loss 0.0106112, acc 1
2016-09-07T02:01:31.274072: step 3616, loss 0.0833629, acc 0.98
2016-09-07T02:01:31.964908: step 3617, loss 0.00335944, acc 1
2016-09-07T02:01:32.653488: step 3618, loss 0.0150094, acc 1
2016-09-07T02:01:33.347173: step 3619, loss 0.0237222, acc 0.98
2016-09-07T02:01:34.038475: step 3620, loss 0.0191918, acc 0.98
2016-09-07T02:01:34.706750: step 3621, loss 0.158783, acc 0.96
2016-09-07T02:01:35.412500: step 3622, loss 0.0289364, acc 0.96
2016-09-07T02:01:36.138231: step 3623, loss 0.0324426, acc 0.98
2016-09-07T02:01:36.845265: step 3624, loss 0.00860035, acc 1
2016-09-07T02:01:37.533747: step 3625, loss 0.0301403, acc 1
2016-09-07T02:01:38.240470: step 3626, loss 0.0126278, acc 1
2016-09-07T02:01:38.961783: step 3627, loss 0.00201075, acc 1
2016-09-07T02:01:39.643353: step 3628, loss 0.014243, acc 1
2016-09-07T02:01:40.341218: step 3629, loss 0.0132207, acc 1
2016-09-07T02:01:41.039832: step 3630, loss 0.0178539, acc 1
2016-09-07T02:01:41.745779: step 3631, loss 0.003134, acc 1
2016-09-07T02:01:42.453866: step 3632, loss 0.00717913, acc 1
2016-09-07T02:01:43.144489: step 3633, loss 0.0297211, acc 0.98
2016-09-07T02:01:43.855181: step 3634, loss 0.0214451, acc 0.98
2016-09-07T02:01:44.549669: step 3635, loss 0.036236, acc 0.98
2016-09-07T02:01:45.234806: step 3636, loss 0.0325545, acc 0.98
2016-09-07T02:01:45.942049: step 3637, loss 0.00956994, acc 1
2016-09-07T02:01:46.637484: step 3638, loss 0.000645839, acc 1
2016-09-07T02:01:47.338310: step 3639, loss 0.144423, acc 0.96
2016-09-07T02:01:48.029695: step 3640, loss 0.00872515, acc 1
2016-09-07T02:01:48.717444: step 3641, loss 0.0447142, acc 0.96
2016-09-07T02:01:49.396022: step 3642, loss 0.00238472, acc 1
2016-09-07T02:01:50.088407: step 3643, loss 0.0232603, acc 1
2016-09-07T02:01:50.783245: step 3644, loss 0.00830147, acc 1
2016-09-07T02:01:51.461739: step 3645, loss 0.0336475, acc 0.98
2016-09-07T02:01:52.183592: step 3646, loss 0.0141285, acc 1
2016-09-07T02:01:52.869582: step 3647, loss 0.0149851, acc 1
2016-09-07T02:01:53.514517: step 3648, loss 0.0246853, acc 0.977273
2016-09-07T02:01:54.204997: step 3649, loss 0.0164045, acc 1
2016-09-07T02:01:54.887602: step 3650, loss 0.0251744, acc 0.98
2016-09-07T02:01:55.576757: step 3651, loss 0.0418641, acc 1
2016-09-07T02:01:56.255862: step 3652, loss 0.137788, acc 0.94
2016-09-07T02:01:56.964301: step 3653, loss 0.0222518, acc 1
2016-09-07T02:01:57.649961: step 3654, loss 0.0137984, acc 1
2016-09-07T02:01:58.339958: step 3655, loss 0.00642737, acc 1
2016-09-07T02:01:59.051533: step 3656, loss 0.0869928, acc 0.96
2016-09-07T02:01:59.757028: step 3657, loss 0.113615, acc 0.96
2016-09-07T02:02:00.483119: step 3658, loss 0.0256902, acc 1
2016-09-07T02:02:01.157714: step 3659, loss 0.00953013, acc 1
2016-09-07T02:02:01.859454: step 3660, loss 0.0345533, acc 0.98
2016-09-07T02:02:02.542181: step 3661, loss 0.0753532, acc 0.96
2016-09-07T02:02:03.231678: step 3662, loss 0.0767348, acc 0.98
2016-09-07T02:02:03.927269: step 3663, loss 0.00109851, acc 1
2016-09-07T02:02:04.610109: step 3664, loss 0.0114378, acc 1
2016-09-07T02:02:05.312586: step 3665, loss 0.187184, acc 0.94
2016-09-07T02:02:06.005952: step 3666, loss 0.0105013, acc 1
2016-09-07T02:02:06.700750: step 3667, loss 0.025352, acc 1
2016-09-07T02:02:07.397917: step 3668, loss 0.119567, acc 0.94
2016-09-07T02:02:08.102787: step 3669, loss 0.0731917, acc 0.98
2016-09-07T02:02:08.796494: step 3670, loss 0.0408379, acc 0.96
2016-09-07T02:02:09.493275: step 3671, loss 0.043434, acc 0.96
2016-09-07T02:02:10.191910: step 3672, loss 0.0261119, acc 1
2016-09-07T02:02:10.849104: step 3673, loss 0.0276909, acc 1
2016-09-07T02:02:11.560345: step 3674, loss 0.127392, acc 0.98
2016-09-07T02:02:12.271164: step 3675, loss 0.0122318, acc 1
2016-09-07T02:02:12.963731: step 3676, loss 0.0458829, acc 0.98
2016-09-07T02:02:13.653302: step 3677, loss 0.00890801, acc 1
2016-09-07T02:02:14.329387: step 3678, loss 0.0450368, acc 0.98
2016-09-07T02:02:15.020808: step 3679, loss 0.00322645, acc 1
2016-09-07T02:02:15.712660: step 3680, loss 0.0679529, acc 0.94
2016-09-07T02:02:16.391555: step 3681, loss 0.0298034, acc 1
2016-09-07T02:02:17.083912: step 3682, loss 0.0203888, acc 1
2016-09-07T02:02:17.763821: step 3683, loss 0.0118643, acc 1
2016-09-07T02:02:18.475923: step 3684, loss 0.0178073, acc 1
2016-09-07T02:02:19.135649: step 3685, loss 0.0343527, acc 0.98
2016-09-07T02:02:19.861019: step 3686, loss 0.00363649, acc 1
2016-09-07T02:02:20.550105: step 3687, loss 0.00218499, acc 1
2016-09-07T02:02:21.248171: step 3688, loss 0.00321891, acc 1
2016-09-07T02:02:21.991365: step 3689, loss 0.0465424, acc 0.98
2016-09-07T02:02:22.682583: step 3690, loss 0.0039406, acc 1
2016-09-07T02:02:23.410694: step 3691, loss 0.0442665, acc 0.96
2016-09-07T02:02:24.095588: step 3692, loss 0.0183211, acc 1
2016-09-07T02:02:24.789190: step 3693, loss 0.00673141, acc 1
2016-09-07T02:02:25.491554: step 3694, loss 0.00204209, acc 1
2016-09-07T02:02:26.175405: step 3695, loss 0.0154551, acc 1
2016-09-07T02:02:26.882573: step 3696, loss 0.0188819, acc 1
2016-09-07T02:02:27.551252: step 3697, loss 0.0238403, acc 0.98
2016-09-07T02:02:28.266110: step 3698, loss 0.099295, acc 0.96
2016-09-07T02:02:28.924082: step 3699, loss 0.00837031, acc 1
2016-09-07T02:02:29.608058: step 3700, loss 0.021984, acc 1

Evaluation:
2016-09-07T02:02:32.773030: step 3700, loss 1.45904, acc 0.787054

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-3700

2016-09-07T02:02:34.510382: step 3701, loss 0.0164477, acc 1
2016-09-07T02:02:35.202843: step 3702, loss 0.0374408, acc 0.98
2016-09-07T02:02:35.901814: step 3703, loss 0.0231339, acc 0.98
2016-09-07T02:02:36.631216: step 3704, loss 0.0023381, acc 1
2016-09-07T02:02:37.303360: step 3705, loss 0.0212719, acc 0.98
2016-09-07T02:02:37.998829: step 3706, loss 0.0177638, acc 0.98
2016-09-07T02:02:38.700503: step 3707, loss 0.00216292, acc 1
2016-09-07T02:02:39.389022: step 3708, loss 0.0255932, acc 0.98
2016-09-07T02:02:40.083915: step 3709, loss 0.0354386, acc 0.98
2016-09-07T02:02:40.751079: step 3710, loss 0.0220932, acc 1
2016-09-07T02:02:41.444454: step 3711, loss 0.0378856, acc 0.96
2016-09-07T02:02:42.124619: step 3712, loss 0.0256259, acc 0.98
2016-09-07T02:02:42.816467: step 3713, loss 0.0463662, acc 0.98
2016-09-07T02:02:43.520919: step 3714, loss 0.0227817, acc 1
2016-09-07T02:02:44.202438: step 3715, loss 0.105836, acc 0.96
2016-09-07T02:02:44.905493: step 3716, loss 0.124728, acc 0.98
2016-09-07T02:02:45.596904: step 3717, loss 0.0181282, acc 0.98
2016-09-07T02:02:46.318670: step 3718, loss 0.0125179, acc 1
2016-09-07T02:02:47.020765: step 3719, loss 0.00893285, acc 1
2016-09-07T02:02:47.714183: step 3720, loss 0.0244128, acc 1
2016-09-07T02:02:48.412345: step 3721, loss 0.00157098, acc 1
2016-09-07T02:02:49.115692: step 3722, loss 0.019255, acc 0.98
2016-09-07T02:02:49.817220: step 3723, loss 0.0342393, acc 0.98
2016-09-07T02:02:50.492235: step 3724, loss 0.0788234, acc 0.94
2016-09-07T02:02:51.191298: step 3725, loss 0.0168531, acc 1
2016-09-07T02:02:51.906632: step 3726, loss 0.0259877, acc 0.98
2016-09-07T02:02:52.597680: step 3727, loss 0.00442366, acc 1
2016-09-07T02:02:53.289393: step 3728, loss 0.000338157, acc 1
2016-09-07T02:02:53.962491: step 3729, loss 0.0662948, acc 0.98
2016-09-07T02:02:54.687719: step 3730, loss 0.0610516, acc 0.98
2016-09-07T02:02:55.389665: step 3731, loss 0.0365794, acc 0.98
2016-09-07T02:02:56.092183: step 3732, loss 0.000415054, acc 1
2016-09-07T02:02:56.783036: step 3733, loss 0.00964458, acc 1
2016-09-07T02:02:57.491250: step 3734, loss 0.0265092, acc 1
2016-09-07T02:02:58.168365: step 3735, loss 0.0115768, acc 1
2016-09-07T02:02:58.835914: step 3736, loss 0.0203217, acc 0.98
2016-09-07T02:02:59.547439: step 3737, loss 0.0729173, acc 0.96
2016-09-07T02:03:00.250447: step 3738, loss 0.0903203, acc 0.96
2016-09-07T02:03:00.935241: step 3739, loss 0.0597811, acc 0.96
2016-09-07T02:03:01.621476: step 3740, loss 0.0177839, acc 1
2016-09-07T02:03:02.305210: step 3741, loss 0.0113468, acc 1
2016-09-07T02:03:03.009362: step 3742, loss 0.0413967, acc 0.98
2016-09-07T02:03:03.682284: step 3743, loss 0.0038122, acc 1
2016-09-07T02:03:04.386361: step 3744, loss 0.0233383, acc 0.98
2016-09-07T02:03:05.074770: step 3745, loss 0.0148844, acc 1
2016-09-07T02:03:05.770148: step 3746, loss 0.0583794, acc 0.96
2016-09-07T02:03:06.462245: step 3747, loss 0.0329952, acc 0.98
2016-09-07T02:03:07.171661: step 3748, loss 0.0446235, acc 0.98
2016-09-07T02:03:07.881695: step 3749, loss 0.0203941, acc 0.98
2016-09-07T02:03:08.548833: step 3750, loss 0.0151675, acc 1
2016-09-07T02:03:09.260385: step 3751, loss 0.0149242, acc 1
2016-09-07T02:03:09.985782: step 3752, loss 0.0862593, acc 0.96
2016-09-07T02:03:10.672811: step 3753, loss 0.2012, acc 0.96
2016-09-07T02:03:11.346128: step 3754, loss 0.173641, acc 0.96
2016-09-07T02:03:12.018345: step 3755, loss 0.0175393, acc 0.98
2016-09-07T02:03:12.733872: step 3756, loss 0.0454537, acc 0.98
2016-09-07T02:03:13.415103: step 3757, loss 0.057657, acc 0.98
2016-09-07T02:03:14.094744: step 3758, loss 0.0129788, acc 1
2016-09-07T02:03:14.789306: step 3759, loss 0.0161396, acc 1
2016-09-07T02:03:15.492082: step 3760, loss 0.00547794, acc 1
2016-09-07T02:03:16.190049: step 3761, loss 0.0179254, acc 1
2016-09-07T02:03:16.873552: step 3762, loss 0.0348049, acc 0.98
2016-09-07T02:03:17.594958: step 3763, loss 0.0107281, acc 1
2016-09-07T02:03:18.284444: step 3764, loss 0.0754993, acc 0.96
2016-09-07T02:03:18.972604: step 3765, loss 0.0915687, acc 0.96
2016-09-07T02:03:19.670429: step 3766, loss 0.0382964, acc 0.98
2016-09-07T02:03:20.373819: step 3767, loss 0.0376636, acc 0.98
2016-09-07T02:03:21.088421: step 3768, loss 0.0449585, acc 0.98
2016-09-07T02:03:21.762305: step 3769, loss 0.0517803, acc 0.98
2016-09-07T02:03:22.462312: step 3770, loss 0.0249204, acc 0.98
2016-09-07T02:03:23.139911: step 3771, loss 0.00242823, acc 1
2016-09-07T02:03:23.820461: step 3772, loss 0.00155169, acc 1
2016-09-07T02:03:24.501671: step 3773, loss 0.0497939, acc 0.96
2016-09-07T02:03:25.183561: step 3774, loss 0.0379599, acc 1
2016-09-07T02:03:25.870853: step 3775, loss 0.0193506, acc 1
2016-09-07T02:03:26.534638: step 3776, loss 0.0137307, acc 1
2016-09-07T02:03:27.243839: step 3777, loss 0.00617564, acc 1
2016-09-07T02:03:27.940204: step 3778, loss 0.104568, acc 0.96
2016-09-07T02:03:28.637761: step 3779, loss 0.120191, acc 0.94
2016-09-07T02:03:29.342996: step 3780, loss 0.105069, acc 0.98
2016-09-07T02:03:30.037104: step 3781, loss 0.0478151, acc 0.98
2016-09-07T02:03:30.738516: step 3782, loss 0.0291602, acc 0.98
2016-09-07T02:03:31.439765: step 3783, loss 0.00874903, acc 1
2016-09-07T02:03:32.124310: step 3784, loss 0.0548796, acc 0.98
2016-09-07T02:03:32.824355: step 3785, loss 0.0803714, acc 0.96
2016-09-07T02:03:33.529690: step 3786, loss 0.0529141, acc 0.98
2016-09-07T02:03:34.229105: step 3787, loss 0.0478494, acc 0.98
2016-09-07T02:03:34.903883: step 3788, loss 0.106543, acc 0.98
2016-09-07T02:03:35.597077: step 3789, loss 0.043206, acc 0.98
2016-09-07T02:03:36.301795: step 3790, loss 0.00865604, acc 1
2016-09-07T02:03:36.994676: step 3791, loss 0.0670167, acc 0.96
2016-09-07T02:03:37.696271: step 3792, loss 0.0442722, acc 0.98
2016-09-07T02:03:38.382188: step 3793, loss 0.0191639, acc 0.98
2016-09-07T02:03:39.082773: step 3794, loss 0.0310295, acc 0.98
2016-09-07T02:03:39.770508: step 3795, loss 0.00521824, acc 1
2016-09-07T02:03:40.549189: step 3796, loss 0.0363061, acc 0.98
2016-09-07T02:03:41.241401: step 3797, loss 0.0620787, acc 0.96
2016-09-07T02:03:41.934440: step 3798, loss 0.0350266, acc 0.98
2016-09-07T02:03:42.640218: step 3799, loss 0.0434923, acc 0.98
2016-09-07T02:03:43.327452: step 3800, loss 0.091282, acc 0.98

Evaluation:
2016-09-07T02:03:46.476402: step 3800, loss 1.3193, acc 0.772983

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-3800

2016-09-07T02:03:48.113739: step 3801, loss 0.0176523, acc 1
2016-09-07T02:03:48.834467: step 3802, loss 0.0584856, acc 0.98
2016-09-07T02:03:49.522805: step 3803, loss 0.0624783, acc 0.98
2016-09-07T02:03:50.218096: step 3804, loss 0.00682834, acc 1
2016-09-07T02:03:50.912749: step 3805, loss 0.0336769, acc 0.96
2016-09-07T02:03:51.608370: step 3806, loss 0.0620367, acc 0.94
2016-09-07T02:03:52.326472: step 3807, loss 0.0459706, acc 0.96
2016-09-07T02:03:53.001948: step 3808, loss 0.0490391, acc 0.98
2016-09-07T02:03:53.703406: step 3809, loss 0.0371798, acc 0.98
2016-09-07T02:03:54.398358: step 3810, loss 0.0126124, acc 1
2016-09-07T02:03:55.095063: step 3811, loss 0.00318127, acc 1
2016-09-07T02:03:55.799789: step 3812, loss 0.0251812, acc 0.98
2016-09-07T02:03:56.476196: step 3813, loss 0.0127706, acc 1
2016-09-07T02:03:57.231125: step 3814, loss 0.0262459, acc 0.98
2016-09-07T02:03:57.918499: step 3815, loss 0.017642, acc 1
2016-09-07T02:03:58.615337: step 3816, loss 0.0134557, acc 1
2016-09-07T02:03:59.309477: step 3817, loss 0.00497833, acc 1
2016-09-07T02:03:59.991432: step 3818, loss 0.0433622, acc 1
2016-09-07T02:04:00.710875: step 3819, loss 0.00293517, acc 1
2016-09-07T02:04:01.364928: step 3820, loss 0.0527574, acc 0.96
2016-09-07T02:04:02.078994: step 3821, loss 0.0263597, acc 0.98
2016-09-07T02:04:02.769381: step 3822, loss 0.0223619, acc 0.98
2016-09-07T02:04:03.446809: step 3823, loss 0.00192781, acc 1
2016-09-07T02:04:04.126115: step 3824, loss 0.0237494, acc 0.98
2016-09-07T02:04:04.817259: step 3825, loss 0.0755408, acc 0.98
2016-09-07T02:04:05.514494: step 3826, loss 0.0266133, acc 1
2016-09-07T02:04:06.167024: step 3827, loss 0.0919673, acc 0.96
2016-09-07T02:04:06.891254: step 3828, loss 0.0170037, acc 1
2016-09-07T02:04:07.562796: step 3829, loss 0.0124751, acc 1
2016-09-07T02:04:08.258240: step 3830, loss 0.000183594, acc 1
2016-09-07T02:04:08.950845: step 3831, loss 0.00974722, acc 1
2016-09-07T02:04:09.649611: step 3832, loss 0.0210748, acc 1
2016-09-07T02:04:10.354502: step 3833, loss 0.0110515, acc 1
2016-09-07T02:04:11.036058: step 3834, loss 0.0797154, acc 0.94
2016-09-07T02:04:11.724537: step 3835, loss 0.0333494, acc 1
2016-09-07T02:04:12.429477: step 3836, loss 0.000698762, acc 1
2016-09-07T02:04:13.116238: step 3837, loss 0.0165384, acc 1
2016-09-07T02:04:13.817800: step 3838, loss 0.00593342, acc 1
2016-09-07T02:04:14.513317: step 3839, loss 0.0337433, acc 0.98
2016-09-07T02:04:15.172231: step 3840, loss 0.0318268, acc 0.977273
2016-09-07T02:04:15.834612: step 3841, loss 0.0572934, acc 0.96
2016-09-07T02:04:16.549057: step 3842, loss 0.00774761, acc 1
2016-09-07T02:04:17.232791: step 3843, loss 0.0179548, acc 0.98
2016-09-07T02:04:17.928383: step 3844, loss 0.0273283, acc 0.98
2016-09-07T02:04:18.606693: step 3845, loss 0.0530211, acc 0.96
2016-09-07T02:04:19.287286: step 3846, loss 0.149954, acc 0.92
2016-09-07T02:04:19.981569: step 3847, loss 0.0118237, acc 1
2016-09-07T02:04:20.664490: step 3848, loss 0.00502843, acc 1
2016-09-07T02:04:21.358607: step 3849, loss 0.123943, acc 0.96
2016-09-07T02:04:22.048272: step 3850, loss 0.010651, acc 1
2016-09-07T02:04:22.745075: step 3851, loss 0.0124176, acc 1
2016-09-07T02:04:23.429635: step 3852, loss 0.0326803, acc 0.98
2016-09-07T02:04:24.126661: step 3853, loss 0.0917544, acc 0.94
2016-09-07T02:04:24.839984: step 3854, loss 0.053795, acc 1
2016-09-07T02:04:25.507535: step 3855, loss 0.0283472, acc 0.98
2016-09-07T02:04:26.218540: step 3856, loss 0.00638688, acc 1
2016-09-07T02:04:26.917232: step 3857, loss 0.0578499, acc 0.96
2016-09-07T02:04:27.619987: step 3858, loss 0.0676293, acc 0.98
2016-09-07T02:04:28.330006: step 3859, loss 0.0794528, acc 0.98
2016-09-07T02:04:29.020154: step 3860, loss 0.00590632, acc 1
2016-09-07T02:04:29.731820: step 3861, loss 0.0127519, acc 1
2016-09-07T02:04:30.422539: step 3862, loss 0.0451559, acc 0.98
2016-09-07T02:04:31.135098: step 3863, loss 0.0408323, acc 0.96
2016-09-07T02:04:31.811618: step 3864, loss 0.0241884, acc 1
2016-09-07T02:04:32.502960: step 3865, loss 0.0207153, acc 1
2016-09-07T02:04:33.205432: step 3866, loss 0.026201, acc 0.98
2016-09-07T02:04:33.881886: step 3867, loss 0.0227672, acc 0.98
2016-09-07T02:04:34.593364: step 3868, loss 0.0240313, acc 0.98
2016-09-07T02:04:35.306441: step 3869, loss 0.0222067, acc 1
2016-09-07T02:04:36.013701: step 3870, loss 0.0288985, acc 0.98
2016-09-07T02:04:36.729268: step 3871, loss 0.0161719, acc 1
2016-09-07T02:04:37.416816: step 3872, loss 0.0202146, acc 1
2016-09-07T02:04:38.138089: step 3873, loss 0.0494075, acc 0.96
2016-09-07T02:04:38.833847: step 3874, loss 0.0252085, acc 1
2016-09-07T02:04:39.535048: step 3875, loss 0.0269079, acc 0.98
2016-09-07T02:04:40.217949: step 3876, loss 0.0228658, acc 1
2016-09-07T02:04:40.895052: step 3877, loss 0.0889432, acc 0.94
2016-09-07T02:04:41.591602: step 3878, loss 0.0489149, acc 0.96
2016-09-07T02:04:42.291977: step 3879, loss 0.052152, acc 0.98
2016-09-07T02:04:43.001910: step 3880, loss 0.0129369, acc 1
2016-09-07T02:04:43.681433: step 3881, loss 0.0170294, acc 1
2016-09-07T02:04:44.358747: step 3882, loss 0.0314651, acc 0.98
2016-09-07T02:04:45.056381: step 3883, loss 0.0581847, acc 0.98
2016-09-07T02:04:45.763117: step 3884, loss 0.0256998, acc 0.98
2016-09-07T02:04:46.480631: step 3885, loss 0.0122288, acc 1
2016-09-07T02:04:47.165204: step 3886, loss 0.085099, acc 0.94
2016-09-07T02:04:47.867563: step 3887, loss 0.0155909, acc 1
2016-09-07T02:04:48.551332: step 3888, loss 0.00964662, acc 1
2016-09-07T02:04:49.242579: step 3889, loss 0.000816924, acc 1
2016-09-07T02:04:49.928913: step 3890, loss 0.0127421, acc 1
2016-09-07T02:04:50.645726: step 3891, loss 0.0137349, acc 1
2016-09-07T02:04:51.349757: step 3892, loss 0.0149822, acc 1
2016-09-07T02:04:52.040439: step 3893, loss 0.068797, acc 0.98
2016-09-07T02:04:52.729482: step 3894, loss 0.0132957, acc 1
2016-09-07T02:04:53.418467: step 3895, loss 0.010586, acc 1
2016-09-07T02:04:54.129230: step 3896, loss 0.0201141, acc 1
2016-09-07T02:04:54.838890: step 3897, loss 0.0145824, acc 1
2016-09-07T02:04:55.519969: step 3898, loss 0.0654645, acc 0.98
2016-09-07T02:04:56.231679: step 3899, loss 0.00973279, acc 1
2016-09-07T02:04:56.916392: step 3900, loss 0.0141447, acc 1

Evaluation:
2016-09-07T02:05:00.072921: step 3900, loss 1.62712, acc 0.766416

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-3900

2016-09-07T02:05:01.747099: step 3901, loss 0.0509655, acc 0.96
2016-09-07T02:05:02.439206: step 3902, loss 0.0310127, acc 0.98
2016-09-07T02:05:03.133052: step 3903, loss 0.0584303, acc 0.96
2016-09-07T02:05:03.827533: step 3904, loss 0.121749, acc 0.92
2016-09-07T02:05:04.541815: step 3905, loss 0.0141057, acc 1
2016-09-07T02:05:05.234085: step 3906, loss 0.0361245, acc 0.98
2016-09-07T02:05:05.919416: step 3907, loss 0.000173933, acc 1
2016-09-07T02:05:06.607666: step 3908, loss 0.0523374, acc 0.98
2016-09-07T02:05:07.295290: step 3909, loss 0.000597082, acc 1
2016-09-07T02:05:07.983608: step 3910, loss 0.354801, acc 0.9
2016-09-07T02:05:08.672811: step 3911, loss 0.00150249, acc 1
2016-09-07T02:05:09.379054: step 3912, loss 0.108588, acc 0.96
2016-09-07T02:05:10.067920: step 3913, loss 0.00715696, acc 1
2016-09-07T02:05:10.746485: step 3914, loss 0.030002, acc 0.98
2016-09-07T02:05:11.438728: step 3915, loss 0.0447248, acc 0.98
2016-09-07T02:05:12.126337: step 3916, loss 0.04748, acc 0.96
2016-09-07T02:05:12.833474: step 3917, loss 0.00873757, acc 1
2016-09-07T02:05:13.520079: step 3918, loss 0.0390236, acc 0.98
2016-09-07T02:05:14.239785: step 3919, loss 0.0158587, acc 0.98
2016-09-07T02:05:14.935609: step 3920, loss 0.0156044, acc 1
2016-09-07T02:05:15.628981: step 3921, loss 0.077705, acc 0.96
2016-09-07T02:05:16.310671: step 3922, loss 0.0137601, acc 1
2016-09-07T02:05:16.981790: step 3923, loss 0.0193049, acc 1
2016-09-07T02:05:17.673187: step 3924, loss 0.00735279, acc 1
2016-09-07T02:05:18.334953: step 3925, loss 0.0736946, acc 0.98
2016-09-07T02:05:19.041141: step 3926, loss 0.00823338, acc 1
2016-09-07T02:05:19.730447: step 3927, loss 0.00632464, acc 1
2016-09-07T02:05:20.429988: step 3928, loss 0.10262, acc 0.96
2016-09-07T02:05:21.126902: step 3929, loss 0.0542914, acc 0.98
2016-09-07T02:05:21.839781: step 3930, loss 0.0272936, acc 1
2016-09-07T02:05:22.549727: step 3931, loss 0.0278579, acc 0.98
2016-09-07T02:05:23.223020: step 3932, loss 0.0210632, acc 0.98
2016-09-07T02:05:23.925756: step 3933, loss 0.0716835, acc 0.96
2016-09-07T02:05:24.613021: step 3934, loss 0.0296471, acc 0.98
2016-09-07T02:05:25.288426: step 3935, loss 0.0254674, acc 0.98
2016-09-07T02:05:25.975020: step 3936, loss 0.115574, acc 0.96
2016-09-07T02:05:26.672354: step 3937, loss 0.0190343, acc 0.98
2016-09-07T02:05:27.368630: step 3938, loss 0.0631715, acc 0.96
2016-09-07T02:05:28.061988: step 3939, loss 0.0202534, acc 1
2016-09-07T02:05:28.762174: step 3940, loss 0.0354336, acc 1
2016-09-07T02:05:29.453427: step 3941, loss 0.0126479, acc 1
2016-09-07T02:05:30.166560: step 3942, loss 0.0180463, acc 1
2016-09-07T02:05:30.863543: step 3943, loss 0.0744133, acc 0.96
2016-09-07T02:05:31.541783: step 3944, loss 0.0983726, acc 0.94
2016-09-07T02:05:32.241685: step 3945, loss 0.00568498, acc 1
2016-09-07T02:05:32.915574: step 3946, loss 0.0071742, acc 1
2016-09-07T02:05:33.611883: step 3947, loss 0.0339063, acc 1
2016-09-07T02:05:34.330230: step 3948, loss 0.0233495, acc 0.98
2016-09-07T02:05:35.006066: step 3949, loss 0.355964, acc 0.98
2016-09-07T02:05:35.698365: step 3950, loss 0.00415692, acc 1
2016-09-07T02:05:36.384132: step 3951, loss 0.0554947, acc 0.96
2016-09-07T02:05:37.097606: step 3952, loss 0.0188059, acc 1
2016-09-07T02:05:37.777632: step 3953, loss 0.0382969, acc 0.98
2016-09-07T02:05:38.458508: step 3954, loss 0.0187124, acc 0.98
2016-09-07T02:05:39.163983: step 3955, loss 0.0146751, acc 1
2016-09-07T02:05:39.856258: step 3956, loss 0.0397486, acc 0.96
2016-09-07T02:05:40.580488: step 3957, loss 0.0844258, acc 0.96
2016-09-07T02:05:41.278077: step 3958, loss 0.0189487, acc 1
2016-09-07T02:05:41.988246: step 3959, loss 0.0400545, acc 0.98
2016-09-07T02:05:42.703025: step 3960, loss 0.0849209, acc 0.96
2016-09-07T02:05:43.398967: step 3961, loss 0.0185677, acc 1
2016-09-07T02:05:44.102427: step 3962, loss 0.188332, acc 0.92
2016-09-07T02:05:44.798643: step 3963, loss 0.0178584, acc 0.98
2016-09-07T02:05:45.503294: step 3964, loss 0.00396417, acc 1
2016-09-07T02:05:46.200037: step 3965, loss 0.0308852, acc 1
2016-09-07T02:05:46.882083: step 3966, loss 0.0485082, acc 0.96
2016-09-07T02:05:47.580221: step 3967, loss 0.0258806, acc 1
2016-09-07T02:05:48.279886: step 3968, loss 0.0336777, acc 0.98
2016-09-07T02:05:48.986978: step 3969, loss 0.0413695, acc 0.98
2016-09-07T02:05:49.647722: step 3970, loss 0.0193781, acc 1
2016-09-07T02:05:50.365775: step 3971, loss 0.147218, acc 0.92
2016-09-07T02:05:51.071534: step 3972, loss 0.0422071, acc 0.98
2016-09-07T02:05:51.774006: step 3973, loss 0.0430473, acc 0.98
2016-09-07T02:05:52.489302: step 3974, loss 0.0116986, acc 1
2016-09-07T02:05:53.162862: step 3975, loss 0.00745575, acc 1
2016-09-07T02:05:53.876240: step 3976, loss 0.0118645, acc 1
2016-09-07T02:05:54.552151: step 3977, loss 0.0272337, acc 1
2016-09-07T02:05:55.245376: step 3978, loss 0.00262477, acc 1
2016-09-07T02:05:55.932379: step 3979, loss 0.0260556, acc 0.98
2016-09-07T02:05:56.629337: step 3980, loss 0.0145924, acc 0.98
2016-09-07T02:05:57.322478: step 3981, loss 0.0221093, acc 1
2016-09-07T02:05:57.988874: step 3982, loss 0.0640228, acc 0.96
2016-09-07T02:05:58.689205: step 3983, loss 0.00230007, acc 1
2016-09-07T02:05:59.371661: step 3984, loss 0.047608, acc 0.98
2016-09-07T02:06:00.049599: step 3985, loss 0.0123033, acc 1
2016-09-07T02:06:00.769751: step 3986, loss 0.00594293, acc 1
2016-09-07T02:06:01.490191: step 3987, loss 0.000463777, acc 1
2016-09-07T02:06:02.202184: step 3988, loss 0.0391715, acc 0.98
2016-09-07T02:06:02.873774: step 3989, loss 0.0233167, acc 0.98
2016-09-07T02:06:03.568353: step 3990, loss 0.013499, acc 1
2016-09-07T02:06:04.257766: step 3991, loss 0.0115811, acc 1
2016-09-07T02:06:04.961678: step 3992, loss 0.0551975, acc 0.98
2016-09-07T02:06:05.680002: step 3993, loss 0.0199429, acc 0.98
2016-09-07T02:06:06.373928: step 3994, loss 0.0351759, acc 0.98
2016-09-07T02:06:07.083330: step 3995, loss 0.00407176, acc 1
2016-09-07T02:06:07.770639: step 3996, loss 0.0883982, acc 0.96
2016-09-07T02:06:08.454739: step 3997, loss 0.0694439, acc 0.98
2016-09-07T02:06:09.142983: step 3998, loss 0.023873, acc 1
2016-09-07T02:06:09.853059: step 3999, loss 0.032055, acc 0.98
2016-09-07T02:06:10.565870: step 4000, loss 0.00575477, acc 1

Evaluation:
2016-09-07T02:06:13.693783: step 4000, loss 1.62025, acc 0.76454

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-4000

2016-09-07T02:06:15.471495: step 4001, loss 0.00601126, acc 1
2016-09-07T02:06:16.147356: step 4002, loss 0.0303541, acc 0.96
2016-09-07T02:06:16.816850: step 4003, loss 0.00683951, acc 1
2016-09-07T02:06:17.548396: step 4004, loss 0.00132456, acc 1
2016-09-07T02:06:18.239555: step 4005, loss 0.0195829, acc 0.98
2016-09-07T02:06:18.937251: step 4006, loss 0.0572704, acc 0.98
2016-09-07T02:06:19.610356: step 4007, loss 0.0138997, acc 1
2016-09-07T02:06:20.315341: step 4008, loss 0.00735559, acc 1
2016-09-07T02:06:20.986761: step 4009, loss 0.0297629, acc 0.98
2016-09-07T02:06:21.669516: step 4010, loss 0.0148436, acc 1
2016-09-07T02:06:22.351632: step 4011, loss 0.0267492, acc 0.98
2016-09-07T02:06:23.035311: step 4012, loss 0.0411266, acc 0.98
2016-09-07T02:06:23.719148: step 4013, loss 0.0218138, acc 1
2016-09-07T02:06:24.410143: step 4014, loss 0.0246708, acc 1
2016-09-07T02:06:25.103860: step 4015, loss 0.0230911, acc 0.98
2016-09-07T02:06:25.782090: step 4016, loss 0.0256467, acc 1
2016-09-07T02:06:26.474550: step 4017, loss 0.0273981, acc 0.98
2016-09-07T02:06:27.163549: step 4018, loss 0.040266, acc 0.98
2016-09-07T02:06:27.861467: step 4019, loss 0.0105071, acc 1
2016-09-07T02:06:28.552975: step 4020, loss 0.001996, acc 1
2016-09-07T02:06:29.258848: step 4021, loss 0.0681714, acc 0.98
2016-09-07T02:06:29.981030: step 4022, loss 0.00192506, acc 1
2016-09-07T02:06:30.670554: step 4023, loss 0.00142819, acc 1
2016-09-07T02:06:31.341669: step 4024, loss 0.0730183, acc 0.96
2016-09-07T02:06:32.061352: step 4025, loss 0.0749822, acc 0.98
2016-09-07T02:06:32.763255: step 4026, loss 0.0718073, acc 0.98
2016-09-07T02:06:33.474243: step 4027, loss 0.00781796, acc 1
2016-09-07T02:06:34.150397: step 4028, loss 0.0673119, acc 0.98
2016-09-07T02:06:34.851682: step 4029, loss 0.0145568, acc 0.98
2016-09-07T02:06:35.531792: step 4030, loss 0.0485685, acc 0.96
2016-09-07T02:06:36.229311: step 4031, loss 0.0230029, acc 0.98
2016-09-07T02:06:36.871536: step 4032, loss 0.055445, acc 0.954545
2016-09-07T02:06:37.573002: step 4033, loss 0.0367753, acc 0.96
2016-09-07T02:06:38.296962: step 4034, loss 0.033737, acc 0.98
2016-09-07T02:06:38.973373: step 4035, loss 0.0113931, acc 1
2016-09-07T02:06:39.678702: step 4036, loss 0.0863328, acc 0.96
2016-09-07T02:06:40.386196: step 4037, loss 0.0183983, acc 1
2016-09-07T02:06:41.064456: step 4038, loss 0.0249539, acc 1
2016-09-07T02:06:41.758250: step 4039, loss 0.00483285, acc 1
2016-09-07T02:06:42.449734: step 4040, loss 0.0689781, acc 0.98
2016-09-07T02:06:43.168959: step 4041, loss 0.0232009, acc 0.98
2016-09-07T02:06:43.865578: step 4042, loss 0.0223561, acc 1
2016-09-07T02:06:44.552263: step 4043, loss 0.0797009, acc 0.94
2016-09-07T02:06:45.237827: step 4044, loss 0.0112968, acc 1
2016-09-07T02:06:45.921135: step 4045, loss 0.0483311, acc 0.98
2016-09-07T02:06:46.624383: step 4046, loss 0.0919761, acc 0.94
2016-09-07T02:06:47.312596: step 4047, loss 0.0116559, acc 1
2016-09-07T02:06:48.051006: step 4048, loss 0.00218162, acc 1
2016-09-07T02:06:48.740245: step 4049, loss 0.013303, acc 1
2016-09-07T02:06:49.410882: step 4050, loss 0.00957159, acc 1
2016-09-07T02:06:50.086178: step 4051, loss 0.0406582, acc 0.98
2016-09-07T02:06:50.765559: step 4052, loss 0.0542018, acc 0.96
2016-09-07T02:06:51.471795: step 4053, loss 0.0189188, acc 0.98
2016-09-07T02:06:52.142778: step 4054, loss 0.0351261, acc 0.98
2016-09-07T02:06:52.847057: step 4055, loss 0.0493611, acc 0.96
2016-09-07T02:06:53.555949: step 4056, loss 0.00202268, acc 1
2016-09-07T02:06:54.262221: step 4057, loss 0.000759371, acc 1
2016-09-07T02:06:54.960286: step 4058, loss 0.00114275, acc 1
2016-09-07T02:06:55.649968: step 4059, loss 0.0456902, acc 0.98
2016-09-07T02:06:56.351738: step 4060, loss 0.0104355, acc 1
2016-09-07T02:06:57.031814: step 4061, loss 0.0717803, acc 0.96
2016-09-07T02:06:57.710212: step 4062, loss 0.0181361, acc 1
2016-09-07T02:06:58.402157: step 4063, loss 0.0166852, acc 1
2016-09-07T02:06:59.088138: step 4064, loss 0.0821386, acc 0.98
2016-09-07T02:06:59.796342: step 4065, loss 0.0604127, acc 0.98
2016-09-07T02:07:00.532946: step 4066, loss 0.0494312, acc 0.98
2016-09-07T02:07:01.246526: step 4067, loss 0.022228, acc 0.98
2016-09-07T02:07:01.946229: step 4068, loss 0.0124116, acc 1
2016-09-07T02:07:02.654011: step 4069, loss 0.0217883, acc 0.98
2016-09-07T02:07:03.383313: step 4070, loss 0.0186465, acc 0.98
2016-09-07T02:07:04.080050: step 4071, loss 0.00380585, acc 1
2016-09-07T02:07:04.780510: step 4072, loss 0.00891585, acc 1
2016-09-07T02:07:05.481307: step 4073, loss 0.0407501, acc 0.98
2016-09-07T02:07:06.172287: step 4074, loss 0.00610628, acc 1
2016-09-07T02:07:06.865019: step 4075, loss 0.000332037, acc 1
2016-09-07T02:07:07.559945: step 4076, loss 0.0586625, acc 0.98
2016-09-07T02:07:08.280853: step 4077, loss 0.023124, acc 0.98
2016-09-07T02:07:08.965755: step 4078, loss 0.0116451, acc 1
2016-09-07T02:07:09.671234: step 4079, loss 0.0120889, acc 1
2016-09-07T02:07:10.360148: step 4080, loss 0.0369874, acc 0.98
2016-09-07T02:07:11.052451: step 4081, loss 0.00243403, acc 1
2016-09-07T02:07:11.733532: step 4082, loss 0.0425218, acc 0.96
2016-09-07T02:07:12.436717: step 4083, loss 0.0194578, acc 0.98
2016-09-07T02:07:13.163866: step 4084, loss 0.0842044, acc 0.96
2016-09-07T02:07:13.817637: step 4085, loss 0.00790628, acc 1
2016-09-07T02:07:14.524484: step 4086, loss 0.0121405, acc 1
2016-09-07T02:07:15.238822: step 4087, loss 0.00091423, acc 1
2016-09-07T02:07:15.954461: step 4088, loss 0.0132624, acc 1
2016-09-07T02:07:16.654880: step 4089, loss 0.0111959, acc 1
2016-09-07T02:07:17.337321: step 4090, loss 0.00742051, acc 1
2016-09-07T02:07:18.058573: step 4091, loss 0.00784204, acc 1
2016-09-07T02:07:18.725470: step 4092, loss 0.0397998, acc 0.96
2016-09-07T02:07:19.426998: step 4093, loss 0.0263005, acc 1
2016-09-07T02:07:20.135506: step 4094, loss 0.0175485, acc 0.98
2016-09-07T02:07:20.817777: step 4095, loss 0.0336761, acc 0.98
2016-09-07T02:07:21.518314: step 4096, loss 0.0777016, acc 0.96
2016-09-07T02:07:22.179703: step 4097, loss 0.0627164, acc 0.98
2016-09-07T02:07:22.905551: step 4098, loss 0.0190682, acc 1
2016-09-07T02:07:23.589045: step 4099, loss 0.0075119, acc 1
2016-09-07T02:07:24.276098: step 4100, loss 0.019246, acc 0.98

Evaluation:
2016-09-07T02:07:27.420579: step 4100, loss 1.88322, acc 0.768293

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-4100

2016-09-07T02:07:29.057042: step 4101, loss 0.0510878, acc 0.98
2016-09-07T02:07:29.767000: step 4102, loss 0.021505, acc 1
2016-09-07T02:07:30.450947: step 4103, loss 0.0195451, acc 0.98
2016-09-07T02:07:31.141089: step 4104, loss 0.0821437, acc 0.96
2016-09-07T02:07:31.805764: step 4105, loss 0.0268962, acc 0.98
2016-09-07T02:07:32.522744: step 4106, loss 0.0294278, acc 0.98
2016-09-07T02:07:33.210294: step 4107, loss 0.0180705, acc 0.98
2016-09-07T02:07:33.900287: step 4108, loss 0.163355, acc 0.98
2016-09-07T02:07:34.596886: step 4109, loss 0.01042, acc 1
2016-09-07T02:07:35.295891: step 4110, loss 0.00357738, acc 1
2016-09-07T02:07:35.988090: step 4111, loss 0.041235, acc 0.98
2016-09-07T02:07:36.662842: step 4112, loss 5.55645e-05, acc 1
2016-09-07T02:07:37.374029: step 4113, loss 0.0195476, acc 1
2016-09-07T02:07:38.060745: step 4114, loss 0.00850995, acc 1
2016-09-07T02:07:38.763222: step 4115, loss 0.0568268, acc 0.96
2016-09-07T02:07:39.447927: step 4116, loss 0.0288394, acc 0.98
2016-09-07T02:07:40.157137: step 4117, loss 0.00172916, acc 1
2016-09-07T02:07:40.845731: step 4118, loss 0.0603109, acc 0.98
2016-09-07T02:07:41.519116: step 4119, loss 0.0187335, acc 0.98
2016-09-07T02:07:42.235724: step 4120, loss 0.000759106, acc 1
2016-09-07T02:07:42.916766: step 4121, loss 0.0202434, acc 1
2016-09-07T02:07:43.605991: step 4122, loss 0.0223388, acc 0.98
2016-09-07T02:07:44.293667: step 4123, loss 0.0472157, acc 0.96
2016-09-07T02:07:44.997877: step 4124, loss 0.0427936, acc 0.96
2016-09-07T02:07:45.707084: step 4125, loss 0.00844678, acc 1
2016-09-07T02:07:46.379482: step 4126, loss 0.0367666, acc 0.98
2016-09-07T02:07:47.059950: step 4127, loss 0.017554, acc 1
2016-09-07T02:07:47.750534: step 4128, loss 0.0193745, acc 1
2016-09-07T02:07:48.443279: step 4129, loss 0.00801897, acc 1
2016-09-07T02:07:49.129481: step 4130, loss 0.00905789, acc 1
2016-09-07T02:07:49.814485: step 4131, loss 0.0142534, acc 1
2016-09-07T02:07:50.556123: step 4132, loss 0.0233074, acc 1
2016-09-07T02:07:51.261679: step 4133, loss 0.00984754, acc 1
2016-09-07T02:07:51.947618: step 4134, loss 0.03707, acc 1
2016-09-07T02:07:52.642299: step 4135, loss 0.0213727, acc 1
2016-09-07T02:07:53.341364: step 4136, loss 0.0462441, acc 0.98
2016-09-07T02:07:54.039561: step 4137, loss 0.0298718, acc 1
2016-09-07T02:07:54.707493: step 4138, loss 0.0447076, acc 0.98
2016-09-07T02:07:55.411986: step 4139, loss 0.0339373, acc 0.96
2016-09-07T02:07:56.113944: step 4140, loss 0.0530316, acc 0.98
2016-09-07T02:07:56.807084: step 4141, loss 0.159229, acc 0.94
2016-09-07T02:07:57.486582: step 4142, loss 0.0215814, acc 0.98
2016-09-07T02:07:58.188337: step 4143, loss 0.0304651, acc 0.98
2016-09-07T02:07:58.911893: step 4144, loss 0.0473856, acc 0.98
2016-09-07T02:07:59.593434: step 4145, loss 0.0599246, acc 0.98
2016-09-07T02:08:00.269439: step 4146, loss 0.0328526, acc 0.98
2016-09-07T02:08:00.944900: step 4147, loss 0.00283054, acc 1
2016-09-07T02:08:01.633721: step 4148, loss 0.0225617, acc 0.98
2016-09-07T02:08:02.312818: step 4149, loss 0.0499149, acc 0.96
2016-09-07T02:08:03.012159: step 4150, loss 0.0219221, acc 1
2016-09-07T02:08:03.703145: step 4151, loss 0.00356051, acc 1
2016-09-07T02:08:04.377410: step 4152, loss 0.0485618, acc 0.98
2016-09-07T02:08:05.068302: step 4153, loss 0.00712202, acc 1
2016-09-07T02:08:05.763336: step 4154, loss 0.0311048, acc 0.98
2016-09-07T02:08:06.437521: step 4155, loss 0.188146, acc 0.98
2016-09-07T02:08:07.125601: step 4156, loss 0.0985628, acc 0.98
2016-09-07T02:08:07.802275: step 4157, loss 0.0376023, acc 1
2016-09-07T02:08:08.509801: step 4158, loss 0.0109691, acc 1
2016-09-07T02:08:09.173936: step 4159, loss 0.00886781, acc 1
2016-09-07T02:08:09.876812: step 4160, loss 0.00434672, acc 1
2016-09-07T02:08:10.561947: step 4161, loss 0.0178288, acc 1
2016-09-07T02:08:11.245909: step 4162, loss 0.00756097, acc 1
2016-09-07T02:08:11.915952: step 4163, loss 0.0525197, acc 0.94
2016-09-07T02:08:12.618042: step 4164, loss 0.030593, acc 0.98
2016-09-07T02:08:13.339533: step 4165, loss 0.181991, acc 0.96
2016-09-07T02:08:14.017066: step 4166, loss 0.0812395, acc 0.98
2016-09-07T02:08:14.712559: step 4167, loss 0.00285857, acc 1
2016-09-07T02:08:15.385414: step 4168, loss 0.0140966, acc 1
2016-09-07T02:08:16.074259: step 4169, loss 0.0621512, acc 0.96
2016-09-07T02:08:16.770812: step 4170, loss 0.0403025, acc 0.96
2016-09-07T02:08:17.466680: step 4171, loss 0.000729453, acc 1
2016-09-07T02:08:18.178183: step 4172, loss 0.0213537, acc 0.98
2016-09-07T02:08:18.855751: step 4173, loss 0.0247271, acc 1
2016-09-07T02:08:19.552043: step 4174, loss 0.0288806, acc 1
2016-09-07T02:08:20.241311: step 4175, loss 0.0268594, acc 0.98
2016-09-07T02:08:20.936559: step 4176, loss 0.0160889, acc 1
2016-09-07T02:08:21.643746: step 4177, loss 0.0177435, acc 1
2016-09-07T02:08:22.317465: step 4178, loss 0.0237234, acc 0.98
2016-09-07T02:08:23.014275: step 4179, loss 0.0282857, acc 1
2016-09-07T02:08:23.683026: step 4180, loss 0.025212, acc 0.98
2016-09-07T02:08:24.387321: step 4181, loss 0.0717202, acc 0.96
2016-09-07T02:08:25.104974: step 4182, loss 0.00947062, acc 1
2016-09-07T02:08:25.774749: step 4183, loss 0.00155832, acc 1
2016-09-07T02:08:26.472419: step 4184, loss 0.0385126, acc 0.98
2016-09-07T02:08:27.147831: step 4185, loss 0.014018, acc 1
2016-09-07T02:08:27.854241: step 4186, loss 0.0292475, acc 1
2016-09-07T02:08:28.558761: step 4187, loss 0.0305375, acc 0.98
2016-09-07T02:08:29.249732: step 4188, loss 0.00104371, acc 1
2016-09-07T02:08:29.943614: step 4189, loss 0.0178131, acc 0.98
2016-09-07T02:08:30.642815: step 4190, loss 0.0441061, acc 1
2016-09-07T02:08:31.340750: step 4191, loss 0.0147855, acc 1
2016-09-07T02:08:31.989729: step 4192, loss 0.0238228, acc 0.98
2016-09-07T02:08:32.708545: step 4193, loss 0.000398743, acc 1
2016-09-07T02:08:33.398652: step 4194, loss 0.0763542, acc 0.98
2016-09-07T02:08:34.076930: step 4195, loss 0.0265472, acc 0.98
2016-09-07T02:08:34.765555: step 4196, loss 0.0336499, acc 0.98
2016-09-07T02:08:35.478222: step 4197, loss 0.0528519, acc 0.96
2016-09-07T02:08:36.193662: step 4198, loss 0.0400632, acc 0.98
2016-09-07T02:08:36.864039: step 4199, loss 0.0427035, acc 0.98
2016-09-07T02:08:37.551005: step 4200, loss 0.0115254, acc 1

Evaluation:
2016-09-07T02:08:40.705570: step 4200, loss 1.61236, acc 0.766416

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-4200

2016-09-07T02:08:42.395593: step 4201, loss 0.0382487, acc 0.98
2016-09-07T02:08:43.103763: step 4202, loss 0.0732256, acc 0.98
2016-09-07T02:08:43.830724: step 4203, loss 0.010569, acc 1
2016-09-07T02:08:44.519937: step 4204, loss 0.070154, acc 0.96
2016-09-07T02:08:45.193189: step 4205, loss 0.0102146, acc 1
2016-09-07T02:08:45.921679: step 4206, loss 0.021746, acc 1
2016-09-07T02:08:46.621232: step 4207, loss 0.00165412, acc 1
2016-09-07T02:08:47.330711: step 4208, loss 0.0314408, acc 0.96
2016-09-07T02:08:48.019117: step 4209, loss 0.0639707, acc 0.98
2016-09-07T02:08:48.726807: step 4210, loss 0.0138956, acc 1
2016-09-07T02:08:49.460047: step 4211, loss 0.0385217, acc 1
2016-09-07T02:08:50.124218: step 4212, loss 0.000429757, acc 1
2016-09-07T02:08:50.824149: step 4213, loss 0.00803817, acc 1
2016-09-07T02:08:51.513503: step 4214, loss 0.0220488, acc 0.98
2016-09-07T02:08:52.202014: step 4215, loss 0.00353427, acc 1
2016-09-07T02:08:52.903763: step 4216, loss 0.0359227, acc 0.98
2016-09-07T02:08:53.599570: step 4217, loss 0.040386, acc 0.98
2016-09-07T02:08:54.308943: step 4218, loss 0.0585876, acc 0.98
2016-09-07T02:08:55.004414: step 4219, loss 0.00528333, acc 1
2016-09-07T02:08:55.685604: step 4220, loss 0.0574606, acc 0.98
2016-09-07T02:08:56.382753: step 4221, loss 0.2337, acc 0.96
2016-09-07T02:08:57.066627: step 4222, loss 0.0893515, acc 0.96
2016-09-07T02:08:57.741583: step 4223, loss 0.0421949, acc 0.96
2016-09-07T02:08:58.363494: step 4224, loss 0.000368097, acc 1
2016-09-07T02:08:59.083507: step 4225, loss 0.0176847, acc 1
2016-09-07T02:08:59.765569: step 4226, loss 0.0217725, acc 1
2016-09-07T02:09:00.489540: step 4227, loss 0.0318495, acc 0.98
2016-09-07T02:09:01.198528: step 4228, loss 0.0028687, acc 1
2016-09-07T02:09:01.896123: step 4229, loss 0.0662612, acc 0.98
2016-09-07T02:09:02.590228: step 4230, loss 0.0154307, acc 1
2016-09-07T02:09:03.259430: step 4231, loss 0.0109181, acc 1
2016-09-07T02:09:03.981289: step 4232, loss 0.039366, acc 0.98
2016-09-07T02:09:04.666323: step 4233, loss 0.0340789, acc 0.98
2016-09-07T02:09:05.381655: step 4234, loss 0.0152967, acc 1
2016-09-07T02:09:06.076111: step 4235, loss 0.0171503, acc 1
2016-09-07T02:09:06.761749: step 4236, loss 0.0171566, acc 1
2016-09-07T02:09:07.480772: step 4237, loss 0.0513341, acc 0.98
2016-09-07T02:09:08.171450: step 4238, loss 0.125199, acc 0.94
2016-09-07T02:09:08.861013: step 4239, loss 0.00435205, acc 1
2016-09-07T02:09:09.537763: step 4240, loss 0.0147847, acc 1
2016-09-07T02:09:10.215670: step 4241, loss 0.0974786, acc 0.94
2016-09-07T02:09:10.930251: step 4242, loss 0.00279121, acc 1
2016-09-07T02:09:11.666022: step 4243, loss 0.0189035, acc 1
2016-09-07T02:09:12.378931: step 4244, loss 0.0223799, acc 1
2016-09-07T02:09:13.085142: step 4245, loss 0.035433, acc 0.98
2016-09-07T02:09:13.783660: step 4246, loss 0.0515377, acc 1
2016-09-07T02:09:14.506001: step 4247, loss 0.0120861, acc 1
2016-09-07T02:09:15.198002: step 4248, loss 0.00998635, acc 1
2016-09-07T02:09:15.926240: step 4249, loss 0.0234706, acc 0.98
2016-09-07T02:09:16.611702: step 4250, loss 0.000902825, acc 1
2016-09-07T02:09:17.297139: step 4251, loss 0.00105464, acc 1
2016-09-07T02:09:18.004957: step 4252, loss 0.0360502, acc 0.98
2016-09-07T02:09:18.695832: step 4253, loss 0.0810179, acc 0.96
2016-09-07T02:09:19.384965: step 4254, loss 0.0850835, acc 0.96
2016-09-07T02:09:20.056903: step 4255, loss 0.160437, acc 0.96
2016-09-07T02:09:20.750986: step 4256, loss 0.060901, acc 0.98
2016-09-07T02:09:21.445674: step 4257, loss 0.0201986, acc 0.98
2016-09-07T02:09:22.143726: step 4258, loss 0.0525464, acc 0.98
2016-09-07T02:09:22.836713: step 4259, loss 0.0636484, acc 0.98
2016-09-07T02:09:23.566198: step 4260, loss 0.0503549, acc 0.98
2016-09-07T02:09:24.270235: step 4261, loss 0.00283198, acc 1
2016-09-07T02:09:24.960841: step 4262, loss 0.0114401, acc 1
2016-09-07T02:09:25.667013: step 4263, loss 0.000422474, acc 1
2016-09-07T02:09:26.353977: step 4264, loss 0.00131907, acc 1
2016-09-07T02:09:27.059645: step 4265, loss 0.0199493, acc 0.98
2016-09-07T02:09:27.758036: step 4266, loss 0.0162969, acc 1
2016-09-07T02:09:28.443279: step 4267, loss 0.0239802, acc 0.98
2016-09-07T02:09:29.161458: step 4268, loss 0.012688, acc 1
2016-09-07T02:09:29.829911: step 4269, loss 0.0312695, acc 0.98
2016-09-07T02:09:30.544039: step 4270, loss 0.00374355, acc 1
2016-09-07T02:09:31.223882: step 4271, loss 0.00773489, acc 1
2016-09-07T02:09:31.910207: step 4272, loss 0.101695, acc 0.94
2016-09-07T02:09:32.601197: step 4273, loss 0.00484991, acc 1
2016-09-07T02:09:33.271666: step 4274, loss 0.0377707, acc 1
2016-09-07T02:09:33.981829: step 4275, loss 0.0816163, acc 0.98
2016-09-07T02:09:34.664016: step 4276, loss 0.00936621, acc 1
2016-09-07T02:09:35.364357: step 4277, loss 0.00182202, acc 1
2016-09-07T02:09:36.038206: step 4278, loss 0.0447391, acc 0.98
2016-09-07T02:09:36.730476: step 4279, loss 0.0468392, acc 0.98
2016-09-07T02:09:37.422526: step 4280, loss 0.0151159, acc 1
2016-09-07T02:09:38.095313: step 4281, loss 0.0215234, acc 1
2016-09-07T02:09:38.809159: step 4282, loss 0.000610214, acc 1
2016-09-07T02:09:39.525072: step 4283, loss 0.0216984, acc 1
2016-09-07T02:09:40.232880: step 4284, loss 0.0151285, acc 1
2016-09-07T02:09:40.914916: step 4285, loss 0.0227783, acc 0.98
2016-09-07T02:09:41.607322: step 4286, loss 0.0156789, acc 1
2016-09-07T02:09:42.312690: step 4287, loss 0.0519166, acc 0.96
2016-09-07T02:09:42.978086: step 4288, loss 0.0219629, acc 1
2016-09-07T02:09:43.686056: step 4289, loss 0.0545402, acc 0.96
2016-09-07T02:09:44.370051: step 4290, loss 0.0166268, acc 1
2016-09-07T02:09:45.062765: step 4291, loss 0.0143701, acc 1
2016-09-07T02:09:45.762617: step 4292, loss 0.132979, acc 0.98
2016-09-07T02:09:46.474674: step 4293, loss 0.0907646, acc 0.98
2016-09-07T02:09:47.199498: step 4294, loss 0.0505674, acc 0.98
2016-09-07T02:09:47.899040: step 4295, loss 0.201466, acc 0.98
2016-09-07T02:09:48.587685: step 4296, loss 0.014767, acc 1
2016-09-07T02:09:49.281569: step 4297, loss 0.00381219, acc 1
2016-09-07T02:09:49.974039: step 4298, loss 0.0336524, acc 0.98
2016-09-07T02:09:50.667887: step 4299, loss 0.0249644, acc 1
2016-09-07T02:09:51.343148: step 4300, loss 0.0744675, acc 0.96

Evaluation:
2016-09-07T02:09:54.491987: step 4300, loss 1.27428, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-4300

2016-09-07T02:09:56.273124: step 4301, loss 0.0543386, acc 0.96
2016-09-07T02:09:56.965836: step 4302, loss 0.0324685, acc 0.98
2016-09-07T02:09:57.674251: step 4303, loss 0.00958665, acc 1
2016-09-07T02:09:58.361393: step 4304, loss 0.0282558, acc 0.98
2016-09-07T02:09:59.061613: step 4305, loss 0.0464127, acc 0.98
2016-09-07T02:09:59.732263: step 4306, loss 0.0230256, acc 0.98
2016-09-07T02:10:00.440973: step 4307, loss 0.0623583, acc 0.96
2016-09-07T02:10:01.111354: step 4308, loss 0.037011, acc 0.96
2016-09-07T02:10:01.800471: step 4309, loss 0.0381632, acc 0.98
2016-09-07T02:10:02.499776: step 4310, loss 0.0386682, acc 1
2016-09-07T02:10:03.206634: step 4311, loss 0.00499287, acc 1
2016-09-07T02:10:03.913628: step 4312, loss 0.0312893, acc 0.98
2016-09-07T02:10:04.608105: step 4313, loss 0.0143274, acc 1
2016-09-07T02:10:05.305969: step 4314, loss 0.0436245, acc 0.96
2016-09-07T02:10:05.991809: step 4315, loss 0.0227968, acc 1
2016-09-07T02:10:06.683086: step 4316, loss 0.073071, acc 0.96
2016-09-07T02:10:07.373850: step 4317, loss 0.0617398, acc 0.96
2016-09-07T02:10:08.065114: step 4318, loss 0.0481677, acc 0.98
2016-09-07T02:10:08.770495: step 4319, loss 0.0612447, acc 0.96
2016-09-07T02:10:09.437392: step 4320, loss 0.0815597, acc 0.98
2016-09-07T02:10:10.156504: step 4321, loss 0.0397383, acc 0.98
2016-09-07T02:10:10.880285: step 4322, loss 0.0051603, acc 1
2016-09-07T02:10:11.590824: step 4323, loss 0.0558067, acc 0.98
2016-09-07T02:10:12.287477: step 4324, loss 0.012802, acc 1
2016-09-07T02:10:12.982149: step 4325, loss 0.0151792, acc 0.98
2016-09-07T02:10:13.693331: step 4326, loss 0.00187387, acc 1
2016-09-07T02:10:14.384401: step 4327, loss 0.0237913, acc 1
2016-09-07T02:10:15.064523: step 4328, loss 0.0271596, acc 0.98
2016-09-07T02:10:15.747221: step 4329, loss 0.0228847, acc 0.98
2016-09-07T02:10:16.417622: step 4330, loss 0.00556278, acc 1
2016-09-07T02:10:17.115506: step 4331, loss 0.0453303, acc 0.96
2016-09-07T02:10:17.774209: step 4332, loss 0.0278208, acc 0.98
2016-09-07T02:10:18.474543: step 4333, loss 0.0079311, acc 1
2016-09-07T02:10:19.161330: step 4334, loss 0.0123582, acc 1
2016-09-07T02:10:19.852766: step 4335, loss 0.0720783, acc 0.98
2016-09-07T02:10:20.533943: step 4336, loss 0.0126205, acc 1
2016-09-07T02:10:21.203834: step 4337, loss 0.0674137, acc 0.96
2016-09-07T02:10:21.902743: step 4338, loss 0.00245575, acc 1
2016-09-07T02:10:22.588746: step 4339, loss 0.0157501, acc 0.98
2016-09-07T02:10:23.287735: step 4340, loss 0.0405388, acc 0.98
2016-09-07T02:10:23.962996: step 4341, loss 0.0101065, acc 1
2016-09-07T02:10:24.639734: step 4342, loss 0.021425, acc 0.98
2016-09-07T02:10:25.324168: step 4343, loss 0.0193783, acc 1
2016-09-07T02:10:26.020600: step 4344, loss 0.195307, acc 0.98
2016-09-07T02:10:26.718112: step 4345, loss 0.022512, acc 1
2016-09-07T02:10:27.388409: step 4346, loss 0.0225066, acc 1
2016-09-07T02:10:28.114403: step 4347, loss 0.00100509, acc 1
2016-09-07T02:10:28.780002: step 4348, loss 0.0136249, acc 1
2016-09-07T02:10:29.472131: step 4349, loss 0.0368474, acc 0.98
2016-09-07T02:10:30.165115: step 4350, loss 0.0376449, acc 0.98
2016-09-07T02:10:30.857863: step 4351, loss 0.00911309, acc 1
2016-09-07T02:10:31.567254: step 4352, loss 0.0585627, acc 0.98
2016-09-07T02:10:32.230385: step 4353, loss 0.01576, acc 1
2016-09-07T02:10:32.933962: step 4354, loss 0.0301522, acc 1
2016-09-07T02:10:33.606630: step 4355, loss 0.0524833, acc 0.98
2016-09-07T02:10:34.306190: step 4356, loss 0.04268, acc 0.98
2016-09-07T02:10:34.997696: step 4357, loss 0.18276, acc 0.94
2016-09-07T02:10:35.697381: step 4358, loss 0.00185859, acc 1
2016-09-07T02:10:36.387976: step 4359, loss 0.00320962, acc 1
2016-09-07T02:10:37.046550: step 4360, loss 0.00805053, acc 1
2016-09-07T02:10:37.763170: step 4361, loss 0.00591986, acc 1
2016-09-07T02:10:38.469745: step 4362, loss 0.012881, acc 1
2016-09-07T02:10:39.149030: step 4363, loss 0.0174398, acc 1
2016-09-07T02:10:39.828662: step 4364, loss 0.0304847, acc 0.98
2016-09-07T02:10:40.535953: step 4365, loss 0.0266954, acc 1
2016-09-07T02:10:41.249387: step 4366, loss 0.0397391, acc 0.96
2016-09-07T02:10:41.943256: step 4367, loss 0.0144988, acc 1
2016-09-07T02:10:42.635488: step 4368, loss 0.0402218, acc 0.96
2016-09-07T02:10:43.331257: step 4369, loss 0.0124966, acc 1
2016-09-07T02:10:44.014783: step 4370, loss 0.0575165, acc 0.98
2016-09-07T02:10:44.706033: step 4371, loss 0.0100687, acc 1
2016-09-07T02:10:45.414530: step 4372, loss 0.0704919, acc 0.96
2016-09-07T02:10:46.128703: step 4373, loss 0.0613583, acc 0.98
2016-09-07T02:10:46.828655: step 4374, loss 0.0161358, acc 1
2016-09-07T02:10:47.537306: step 4375, loss 0.00538695, acc 1
2016-09-07T02:10:48.238923: step 4376, loss 0.0143671, acc 1
2016-09-07T02:10:48.932312: step 4377, loss 0.0128228, acc 1
2016-09-07T02:10:49.631042: step 4378, loss 0.0232973, acc 1
2016-09-07T02:10:50.290025: step 4379, loss 0.0392461, acc 0.98
2016-09-07T02:10:51.012872: step 4380, loss 0.0132894, acc 1
2016-09-07T02:10:51.704686: step 4381, loss 0.0077916, acc 1
2016-09-07T02:10:52.385860: step 4382, loss 0.0694085, acc 0.96
2016-09-07T02:10:53.085793: step 4383, loss 0.000697652, acc 1
2016-09-07T02:10:53.770148: step 4384, loss 0.0743642, acc 0.94
2016-09-07T02:10:54.492547: step 4385, loss 0.0463038, acc 0.96
2016-09-07T02:10:55.151174: step 4386, loss 0.00449891, acc 1
2016-09-07T02:10:55.862263: step 4387, loss 0.0155839, acc 1
2016-09-07T02:10:56.587264: step 4388, loss 0.000632518, acc 1
2016-09-07T02:10:57.281765: step 4389, loss 0.0358954, acc 0.98
2016-09-07T02:10:57.984118: step 4390, loss 0.0197438, acc 0.98
2016-09-07T02:10:58.661217: step 4391, loss 0.00800692, acc 1
2016-09-07T02:10:59.366435: step 4392, loss 0.0469924, acc 0.98
2016-09-07T02:11:00.071367: step 4393, loss 0.0144879, acc 1
2016-09-07T02:11:00.787467: step 4394, loss 0.0179159, acc 0.98
2016-09-07T02:11:01.480835: step 4395, loss 0.0450615, acc 0.96
2016-09-07T02:11:02.188818: step 4396, loss 0.00508759, acc 1
2016-09-07T02:11:02.876215: step 4397, loss 0.00937276, acc 1
2016-09-07T02:11:03.553921: step 4398, loss 0.021717, acc 0.98
2016-09-07T02:11:04.273111: step 4399, loss 0.0252103, acc 1
2016-09-07T02:11:04.972742: step 4400, loss 0.0711325, acc 0.98

Evaluation:
2016-09-07T02:11:08.106620: step 4400, loss 1.53704, acc 0.766416

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-4400

2016-09-07T02:11:09.820358: step 4401, loss 0.0358799, acc 0.98
2016-09-07T02:11:10.503522: step 4402, loss 0.0410039, acc 1
2016-09-07T02:11:11.208413: step 4403, loss 0.00374384, acc 1
2016-09-07T02:11:11.898533: step 4404, loss 0.00688581, acc 1
2016-09-07T02:11:12.595512: step 4405, loss 0.00119211, acc 1
2016-09-07T02:11:13.272300: step 4406, loss 0.00697789, acc 1
2016-09-07T02:11:13.951174: step 4407, loss 0.0144447, acc 1
2016-09-07T02:11:14.628758: step 4408, loss 0.0621895, acc 0.96
2016-09-07T02:11:15.314784: step 4409, loss 0.0021651, acc 1
2016-09-07T02:11:15.993066: step 4410, loss 0.0226904, acc 1
2016-09-07T02:11:16.666417: step 4411, loss 0.022257, acc 0.98
2016-09-07T02:11:17.370117: step 4412, loss 0.000287117, acc 1
2016-09-07T02:11:18.049444: step 4413, loss 0.0217482, acc 1
2016-09-07T02:11:18.730020: step 4414, loss 0.0233351, acc 1
2016-09-07T02:11:19.417817: step 4415, loss 0.0168815, acc 1
2016-09-07T02:11:20.063222: step 4416, loss 0.00160193, acc 1
2016-09-07T02:11:20.758323: step 4417, loss 0.0784392, acc 0.96
2016-09-07T02:11:21.450020: step 4418, loss 0.180294, acc 0.98
2016-09-07T02:11:22.164480: step 4419, loss 0.00737902, acc 1
2016-09-07T02:11:22.857085: step 4420, loss 0.0124625, acc 1
2016-09-07T02:11:23.559931: step 4421, loss 0.0284117, acc 0.98
2016-09-07T02:11:24.263523: step 4422, loss 0.0317251, acc 1
2016-09-07T02:11:24.958522: step 4423, loss 0.00375003, acc 1
2016-09-07T02:11:25.652794: step 4424, loss 0.00952072, acc 1
2016-09-07T02:11:26.295057: step 4425, loss 0.0175851, acc 0.98
2016-09-07T02:11:27.011422: step 4426, loss 0.018338, acc 1
2016-09-07T02:11:27.717886: step 4427, loss 0.00683735, acc 1
2016-09-07T02:11:28.418687: step 4428, loss 0.0492052, acc 0.96
2016-09-07T02:11:29.112517: step 4429, loss 0.0594691, acc 0.96
2016-09-07T02:11:29.802727: step 4430, loss 0.00642455, acc 1
2016-09-07T02:11:30.509607: step 4431, loss 0.00489855, acc 1
2016-09-07T02:11:31.182283: step 4432, loss 0.0219736, acc 0.98
2016-09-07T02:11:31.888342: step 4433, loss 0.0382935, acc 0.98
2016-09-07T02:11:32.577572: step 4434, loss 0.0177314, acc 1
2016-09-07T02:11:33.271081: step 4435, loss 0.0544154, acc 0.98
2016-09-07T02:11:33.981165: step 4436, loss 0.018402, acc 1
2016-09-07T02:11:34.665549: step 4437, loss 0.0227906, acc 0.98
2016-09-07T02:11:35.375937: step 4438, loss 0.024686, acc 1
2016-09-07T02:11:36.049832: step 4439, loss 0.0924715, acc 0.96
2016-09-07T02:11:36.761528: step 4440, loss 0.00550141, acc 1
2016-09-07T02:11:37.438315: step 4441, loss 0.00820972, acc 1
2016-09-07T02:11:38.127153: step 4442, loss 0.0162943, acc 0.98
2016-09-07T02:11:38.824137: step 4443, loss 0.0190254, acc 0.98
2016-09-07T02:11:39.515480: step 4444, loss 0.0386783, acc 0.98
2016-09-07T02:11:40.236541: step 4445, loss 0.0230983, acc 0.98
2016-09-07T02:11:40.907596: step 4446, loss 0.0477401, acc 0.98
2016-09-07T02:11:41.599318: step 4447, loss 0.065797, acc 0.98
2016-09-07T02:11:42.307919: step 4448, loss 0.14051, acc 0.9
2016-09-07T02:11:42.999046: step 4449, loss 0.0022221, acc 1
2016-09-07T02:11:43.697570: step 4450, loss 0.0748829, acc 0.94
2016-09-07T02:11:44.400403: step 4451, loss 0.000966161, acc 1
2016-09-07T02:11:45.111341: step 4452, loss 0.0268502, acc 0.98
2016-09-07T02:11:45.793492: step 4453, loss 0.0207647, acc 0.98
2016-09-07T02:11:46.461224: step 4454, loss 0.0160711, acc 1
2016-09-07T02:11:47.167564: step 4455, loss 0.0029945, acc 1
2016-09-07T02:11:47.865271: step 4456, loss 0.00999664, acc 1
2016-09-07T02:11:48.590443: step 4457, loss 0.0188667, acc 1
2016-09-07T02:11:49.264886: step 4458, loss 0.0538265, acc 0.98
2016-09-07T02:11:49.978715: step 4459, loss 0.0579922, acc 0.98
2016-09-07T02:11:50.669957: step 4460, loss 0.0626707, acc 0.94
2016-09-07T02:11:51.375416: step 4461, loss 0.0239867, acc 1
2016-09-07T02:11:52.066490: step 4462, loss 0.0472021, acc 0.98
2016-09-07T02:11:52.742152: step 4463, loss 0.000565057, acc 1
2016-09-07T02:11:53.441919: step 4464, loss 0.0410867, acc 0.98
2016-09-07T02:11:54.118757: step 4465, loss 0.0706351, acc 0.96
2016-09-07T02:11:54.823075: step 4466, loss 0.00505326, acc 1
2016-09-07T02:11:55.518125: step 4467, loss 0.026544, acc 0.98
2016-09-07T02:11:56.222340: step 4468, loss 0.00576048, acc 1
2016-09-07T02:11:56.929664: step 4469, loss 0.0331443, acc 0.98
2016-09-07T02:11:57.598660: step 4470, loss 0.0294033, acc 0.98
2016-09-07T02:11:58.304480: step 4471, loss 0.0172619, acc 0.98
2016-09-07T02:11:58.987993: step 4472, loss 0.00954066, acc 1
2016-09-07T02:11:59.689668: step 4473, loss 0.000537026, acc 1
2016-09-07T02:12:00.417986: step 4474, loss 0.0461441, acc 0.98
2016-09-07T02:12:01.115172: step 4475, loss 0.00415271, acc 1
2016-09-07T02:12:01.802859: step 4476, loss 0.0282089, acc 0.98
2016-09-07T02:12:02.489878: step 4477, loss 0.0754925, acc 0.98
2016-09-07T02:12:03.203145: step 4478, loss 0.0179142, acc 0.98
2016-09-07T02:12:03.879017: step 4479, loss 0.0754718, acc 0.96
2016-09-07T02:12:04.566485: step 4480, loss 0.0866329, acc 0.94
2016-09-07T02:12:05.258690: step 4481, loss 0.0179169, acc 1
2016-09-07T02:12:05.944483: step 4482, loss 0.014126, acc 1
2016-09-07T02:12:06.667491: step 4483, loss 0.0125956, acc 1
2016-09-07T02:12:07.358327: step 4484, loss 0.0471912, acc 0.98
2016-09-07T02:12:08.092801: step 4485, loss 0.00300102, acc 1
2016-09-07T02:12:08.777122: step 4486, loss 0.0231239, acc 0.98
2016-09-07T02:12:09.474561: step 4487, loss 0.00434455, acc 1
2016-09-07T02:12:10.156091: step 4488, loss 0.0222769, acc 1
2016-09-07T02:12:10.827156: step 4489, loss 0.00232595, acc 1
2016-09-07T02:12:11.552267: step 4490, loss 0.0259734, acc 0.98
2016-09-07T02:12:12.258431: step 4491, loss 0.00145069, acc 1
2016-09-07T02:12:12.955025: step 4492, loss 0.0232033, acc 1
2016-09-07T02:12:13.658041: step 4493, loss 0.0168075, acc 1
2016-09-07T02:12:14.370066: step 4494, loss 0.0234048, acc 0.98
2016-09-07T02:12:15.061984: step 4495, loss 0.0506406, acc 0.98
2016-09-07T02:12:15.747222: step 4496, loss 0.0565214, acc 0.98
2016-09-07T02:12:16.447467: step 4497, loss 0.0059776, acc 1
2016-09-07T02:12:17.151299: step 4498, loss 0.0167434, acc 1
2016-09-07T02:12:17.872627: step 4499, loss 0.00322266, acc 1
2016-09-07T02:12:18.575655: step 4500, loss 0.0669634, acc 0.98

Evaluation:
2016-09-07T02:12:21.747601: step 4500, loss 1.65036, acc 0.763602

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-4500

2016-09-07T02:12:23.394057: step 4501, loss 0.0156642, acc 1
2016-09-07T02:12:24.069915: step 4502, loss 0.00115076, acc 1
2016-09-07T02:12:24.774806: step 4503, loss 0.0982633, acc 0.98
2016-09-07T02:12:25.465716: step 4504, loss 0.0143434, acc 1
2016-09-07T02:12:26.163329: step 4505, loss 0.0187401, acc 1
2016-09-07T02:12:26.862525: step 4506, loss 0.0859995, acc 0.96
2016-09-07T02:12:27.568463: step 4507, loss 0.0433243, acc 0.96
2016-09-07T02:12:28.277937: step 4508, loss 0.0633092, acc 0.96
2016-09-07T02:12:28.963773: step 4509, loss 0.0107639, acc 1
2016-09-07T02:12:29.650776: step 4510, loss 0.0059229, acc 1
2016-09-07T02:12:30.329607: step 4511, loss 0.0286693, acc 0.98
2016-09-07T02:12:31.011849: step 4512, loss 0.0129973, acc 1
2016-09-07T02:12:31.757972: step 4513, loss 0.0140286, acc 1
2016-09-07T02:12:32.448917: step 4514, loss 0.0758596, acc 0.96
2016-09-07T02:12:33.171640: step 4515, loss 0.0874329, acc 0.96
2016-09-07T02:12:33.861505: step 4516, loss 0.0334027, acc 0.98
2016-09-07T02:12:34.553611: step 4517, loss 0.000243297, acc 1
2016-09-07T02:12:35.244896: step 4518, loss 0.0288955, acc 1
2016-09-07T02:12:35.938201: step 4519, loss 0.0922631, acc 0.92
2016-09-07T02:12:36.636532: step 4520, loss 0.00373462, acc 1
2016-09-07T02:12:37.327213: step 4521, loss 0.00180406, acc 1
2016-09-07T02:12:38.050458: step 4522, loss 0.00824816, acc 1
2016-09-07T02:12:38.733528: step 4523, loss 0.00819975, acc 1
2016-09-07T02:12:39.420289: step 4524, loss 0.0448433, acc 0.96
2016-09-07T02:12:40.101093: step 4525, loss 0.050547, acc 0.96
2016-09-07T02:12:40.788368: step 4526, loss 0.187015, acc 0.96
2016-09-07T02:12:41.500776: step 4527, loss 0.0630346, acc 0.98
2016-09-07T02:12:42.177406: step 4528, loss 0.00406055, acc 1
2016-09-07T02:12:42.882271: step 4529, loss 0.0117271, acc 1
2016-09-07T02:12:43.570973: step 4530, loss 0.0149485, acc 1
2016-09-07T02:12:44.268131: step 4531, loss 0.0162569, acc 1
2016-09-07T02:12:44.959582: step 4532, loss 0.0469403, acc 0.98
2016-09-07T02:12:45.667476: step 4533, loss 0.00817858, acc 1
2016-09-07T02:12:46.370147: step 4534, loss 0.0210129, acc 1
2016-09-07T02:12:47.047862: step 4535, loss 0.00357529, acc 1
2016-09-07T02:12:47.734926: step 4536, loss 0.0642246, acc 0.98
2016-09-07T02:12:48.428909: step 4537, loss 0.016818, acc 1
2016-09-07T02:12:49.128380: step 4538, loss 0.168207, acc 0.94
2016-09-07T02:12:49.807652: step 4539, loss 0.00151092, acc 1
2016-09-07T02:12:50.480702: step 4540, loss 0.0186541, acc 0.98
2016-09-07T02:12:51.203196: step 4541, loss 0.00172808, acc 1
2016-09-07T02:12:51.879222: step 4542, loss 0.0101811, acc 1
2016-09-07T02:12:52.583804: step 4543, loss 0.0197676, acc 0.98
2016-09-07T02:12:53.278497: step 4544, loss 0.016857, acc 0.98
2016-09-07T02:12:53.961195: step 4545, loss 0.00332189, acc 1
2016-09-07T02:12:54.655741: step 4546, loss 0.0264254, acc 0.98
2016-09-07T02:12:55.327757: step 4547, loss 0.0435483, acc 0.96
2016-09-07T02:12:56.048641: step 4548, loss 0.00400655, acc 1
2016-09-07T02:12:56.735790: step 4549, loss 0.065089, acc 0.98
2016-09-07T02:12:57.439279: step 4550, loss 0.0257299, acc 1
2016-09-07T02:12:58.123233: step 4551, loss 0.00320015, acc 1
2016-09-07T02:12:58.803435: step 4552, loss 0.0289585, acc 0.98
2016-09-07T02:12:59.505892: step 4553, loss 0.0267993, acc 1
2016-09-07T02:13:00.179356: step 4554, loss 0.00027814, acc 1
2016-09-07T02:13:00.921627: step 4555, loss 0.0107878, acc 1
2016-09-07T02:13:01.616329: step 4556, loss 0.0450977, acc 0.96
2016-09-07T02:13:02.309830: step 4557, loss 0.00204398, acc 1
2016-09-07T02:13:02.984789: step 4558, loss 0.0269905, acc 0.98
2016-09-07T02:13:03.669509: step 4559, loss 0.000277393, acc 1
2016-09-07T02:13:04.368870: step 4560, loss 0.0298591, acc 1
2016-09-07T02:13:05.018466: step 4561, loss 0.0100909, acc 1
2016-09-07T02:13:05.725256: step 4562, loss 0.00700486, acc 1
2016-09-07T02:13:06.441473: step 4563, loss 0.0138263, acc 1
2016-09-07T02:13:07.134957: step 4564, loss 0.035586, acc 0.98
2016-09-07T02:13:07.839519: step 4565, loss 0.0137858, acc 1
2016-09-07T02:13:08.541615: step 4566, loss 0.100107, acc 0.96
2016-09-07T02:13:09.248854: step 4567, loss 0.0164961, acc 1
2016-09-07T02:13:09.942736: step 4568, loss 0.0072064, acc 1
2016-09-07T02:13:10.638177: step 4569, loss 0.0142385, acc 1
2016-09-07T02:13:11.340346: step 4570, loss 0.0464454, acc 0.98
2016-09-07T02:13:12.038760: step 4571, loss 0.0157181, acc 0.98
2016-09-07T02:13:12.731338: step 4572, loss 0.006304, acc 1
2016-09-07T02:13:13.396014: step 4573, loss 0.0324694, acc 0.98
2016-09-07T02:13:14.131370: step 4574, loss 0.023471, acc 1
2016-09-07T02:13:14.816192: step 4575, loss 0.0294395, acc 0.98
2016-09-07T02:13:15.503850: step 4576, loss 0.0138032, acc 1
2016-09-07T02:13:16.198766: step 4577, loss 0.00509242, acc 1
2016-09-07T02:13:16.887608: step 4578, loss 0.00452341, acc 1
2016-09-07T02:13:17.627922: step 4579, loss 0.00201894, acc 1
2016-09-07T02:13:18.299539: step 4580, loss 0.0047218, acc 1
2016-09-07T02:13:18.999793: step 4581, loss 0.108961, acc 0.94
2016-09-07T02:13:19.698117: step 4582, loss 0.0482346, acc 0.98
2016-09-07T02:13:20.391512: step 4583, loss 0.0972606, acc 0.94
2016-09-07T02:13:21.095059: step 4584, loss 0.0316548, acc 0.98
2016-09-07T02:13:21.790501: step 4585, loss 0.0127929, acc 1
2016-09-07T02:13:22.485944: step 4586, loss 0.00813497, acc 1
2016-09-07T02:13:23.153897: step 4587, loss 0.0698141, acc 0.98
2016-09-07T02:13:23.848246: step 4588, loss 0.000314827, acc 1
2016-09-07T02:13:24.530610: step 4589, loss 0.0210888, acc 0.98
2016-09-07T02:13:25.228051: step 4590, loss 0.0247248, acc 1
2016-09-07T02:13:25.931414: step 4591, loss 0.0104363, acc 1
2016-09-07T02:13:26.611871: step 4592, loss 0.0104902, acc 1
2016-09-07T02:13:27.315958: step 4593, loss 0.0343847, acc 0.98
2016-09-07T02:13:28.028977: step 4594, loss 0.0180587, acc 0.98
2016-09-07T02:13:28.726610: step 4595, loss 0.00162384, acc 1
2016-09-07T02:13:29.423318: step 4596, loss 0.0273655, acc 0.98
2016-09-07T02:13:30.121822: step 4597, loss 0.010351, acc 1
2016-09-07T02:13:30.845210: step 4598, loss 0.0205316, acc 0.98
2016-09-07T02:13:31.503203: step 4599, loss 0.0524914, acc 0.98
2016-09-07T02:13:32.218390: step 4600, loss 0.0283461, acc 1

Evaluation:
2016-09-07T02:13:35.388695: step 4600, loss 1.63493, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-4600

2016-09-07T02:13:37.042050: step 4601, loss 0.022477, acc 1
2016-09-07T02:13:37.738805: step 4602, loss 0.0309258, acc 0.98
2016-09-07T02:13:38.426862: step 4603, loss 0.0171293, acc 1
2016-09-07T02:13:39.124918: step 4604, loss 0.000379299, acc 1
2016-09-07T02:13:39.814662: step 4605, loss 0.0110443, acc 1
2016-09-07T02:13:40.514736: step 4606, loss 0.0483767, acc 1
2016-09-07T02:13:41.198937: step 4607, loss 0.0312596, acc 0.98
2016-09-07T02:13:41.836785: step 4608, loss 0.0171959, acc 1
2016-09-07T02:13:42.546406: step 4609, loss 0.0428329, acc 0.96
2016-09-07T02:13:43.244579: step 4610, loss 0.0649214, acc 0.96
2016-09-07T02:13:43.923749: step 4611, loss 0.0276324, acc 1
2016-09-07T02:13:44.634317: step 4612, loss 0.0330578, acc 0.96
2016-09-07T02:13:45.331771: step 4613, loss 0.0259916, acc 0.98
2016-09-07T02:13:45.988868: step 4614, loss 0.026033, acc 0.98
2016-09-07T02:13:46.674836: step 4615, loss 0.103728, acc 0.98
2016-09-07T02:13:47.368360: step 4616, loss 0.000887883, acc 1
2016-09-07T02:13:48.074235: step 4617, loss 0.00146195, acc 1
2016-09-07T02:13:48.758895: step 4618, loss 0.000637174, acc 1
2016-09-07T02:13:49.445107: step 4619, loss 0.0193654, acc 0.98
2016-09-07T02:13:50.162898: step 4620, loss 0.0155542, acc 1
2016-09-07T02:13:50.845220: step 4621, loss 0.00912009, acc 1
2016-09-07T02:13:51.509307: step 4622, loss 0.0462667, acc 0.98
2016-09-07T02:13:52.189569: step 4623, loss 0.0322872, acc 0.98
2016-09-07T02:13:52.902972: step 4624, loss 0.00743043, acc 1
2016-09-07T02:13:53.607284: step 4625, loss 0.00758395, acc 1
2016-09-07T02:13:54.273195: step 4626, loss 0.00472565, acc 1
2016-09-07T02:13:54.960471: step 4627, loss 0.0207682, acc 0.98
2016-09-07T02:13:55.655365: step 4628, loss 0.0677828, acc 0.94
2016-09-07T02:13:56.344265: step 4629, loss 0.056951, acc 0.98
2016-09-07T02:13:57.059328: step 4630, loss 0.0577465, acc 0.96
2016-09-07T02:13:57.746140: step 4631, loss 0.0373967, acc 0.98
2016-09-07T02:13:58.463293: step 4632, loss 0.0143586, acc 1
2016-09-07T02:13:59.143078: step 4633, loss 0.0680695, acc 0.96
2016-09-07T02:13:59.853879: step 4634, loss 0.0439191, acc 0.98
2016-09-07T02:14:00.587678: step 4635, loss 0.000336865, acc 1
2016-09-07T02:14:01.298010: step 4636, loss 0.0730537, acc 0.98
2016-09-07T02:14:02.009486: step 4637, loss 0.160235, acc 0.96
2016-09-07T02:14:02.717566: step 4638, loss 0.00424025, acc 1
2016-09-07T02:14:03.432937: step 4639, loss 0.0157894, acc 1
2016-09-07T02:14:04.122806: step 4640, loss 0.117411, acc 0.94
2016-09-07T02:14:04.845334: step 4641, loss 0.0597133, acc 0.98
2016-09-07T02:14:05.535750: step 4642, loss 0.0587477, acc 0.98
2016-09-07T02:14:06.229699: step 4643, loss 0.0285828, acc 1
2016-09-07T02:14:06.941414: step 4644, loss 0.0381587, acc 0.98
2016-09-07T02:14:07.630805: step 4645, loss 0.0142452, acc 1
2016-09-07T02:14:08.312247: step 4646, loss 0.0139567, acc 1
2016-09-07T02:14:08.989590: step 4647, loss 0.0100132, acc 1
2016-09-07T02:14:09.674606: step 4648, loss 0.000526952, acc 1
2016-09-07T02:14:10.353253: step 4649, loss 0.1282, acc 0.98
2016-09-07T02:14:11.050977: step 4650, loss 0.00842042, acc 1
2016-09-07T02:14:11.742534: step 4651, loss 0.0111887, acc 1
2016-09-07T02:14:12.431537: step 4652, loss 0.0170257, acc 1
2016-09-07T02:14:13.157077: step 4653, loss 0.107762, acc 0.94
2016-09-07T02:14:13.859937: step 4654, loss 0.0511124, acc 0.98
2016-09-07T02:14:14.540830: step 4655, loss 0.0179763, acc 0.98
2016-09-07T02:14:15.235561: step 4656, loss 0.0220002, acc 1
2016-09-07T02:14:15.912345: step 4657, loss 0.00339936, acc 1
2016-09-07T02:14:16.616940: step 4658, loss 0.044558, acc 0.96
2016-09-07T02:14:17.298268: step 4659, loss 0.013965, acc 1
2016-09-07T02:14:17.977882: step 4660, loss 0.0376778, acc 0.98
2016-09-07T02:14:18.681501: step 4661, loss 0.00321763, acc 1
2016-09-07T02:14:19.373462: step 4662, loss 0.0102797, acc 1
2016-09-07T02:14:20.080333: step 4663, loss 0.0170049, acc 1
2016-09-07T02:14:20.745231: step 4664, loss 0.0902599, acc 0.98
2016-09-07T02:14:21.455593: step 4665, loss 0.0259154, acc 0.98
2016-09-07T02:14:22.140630: step 4666, loss 0.0011375, acc 1
2016-09-07T02:14:22.830778: step 4667, loss 0.0315027, acc 1
2016-09-07T02:14:23.530550: step 4668, loss 0.0143187, acc 1
2016-09-07T02:14:24.223865: step 4669, loss 0.0017803, acc 1
2016-09-07T02:14:24.919544: step 4670, loss 0.0280549, acc 0.98
2016-09-07T02:14:25.571968: step 4671, loss 0.111583, acc 0.94
2016-09-07T02:14:26.285369: step 4672, loss 0.0110782, acc 1
2016-09-07T02:14:26.986789: step 4673, loss 0.0526554, acc 0.98
2016-09-07T02:14:27.685038: step 4674, loss 0.137842, acc 0.96
2016-09-07T02:14:28.376577: step 4675, loss 0.080727, acc 0.98
2016-09-07T02:14:29.053780: step 4676, loss 0.0127666, acc 1
2016-09-07T02:14:29.759109: step 4677, loss 0.00644549, acc 1
2016-09-07T02:14:30.442173: step 4678, loss 0.0293657, acc 0.98
2016-09-07T02:14:31.126754: step 4679, loss 0.00187129, acc 1
2016-09-07T02:14:31.831181: step 4680, loss 0.0284613, acc 0.98
2016-09-07T02:14:32.534135: step 4681, loss 0.0426687, acc 0.98
2016-09-07T02:14:33.224317: step 4682, loss 0.015506, acc 1
2016-09-07T02:14:33.892479: step 4683, loss 0.00337625, acc 1
2016-09-07T02:14:34.599110: step 4684, loss 0.0402177, acc 0.98
2016-09-07T02:14:35.282099: step 4685, loss 0.0252744, acc 0.98
2016-09-07T02:14:35.958470: step 4686, loss 0.00113949, acc 1
2016-09-07T02:14:36.653195: step 4687, loss 0.0494154, acc 0.98
2016-09-07T02:14:37.361526: step 4688, loss 0.0417739, acc 0.96
2016-09-07T02:14:38.077984: step 4689, loss 0.000384093, acc 1
2016-09-07T02:14:38.773263: step 4690, loss 0.00138316, acc 1
2016-09-07T02:14:39.469029: step 4691, loss 0.0719398, acc 0.94
2016-09-07T02:14:40.162119: step 4692, loss 0.00558285, acc 1
2016-09-07T02:14:40.875147: step 4693, loss 0.0579909, acc 0.98
2016-09-07T02:14:41.580379: step 4694, loss 0.0074659, acc 1
2016-09-07T02:14:42.267412: step 4695, loss 0.0233909, acc 1
2016-09-07T02:14:42.979291: step 4696, loss 0.0168684, acc 0.98
2016-09-07T02:14:43.673447: step 4697, loss 0.0271271, acc 0.98
2016-09-07T02:14:44.372990: step 4698, loss 0.0309898, acc 0.98
2016-09-07T02:14:45.074054: step 4699, loss 0.0332186, acc 1
2016-09-07T02:14:45.775546: step 4700, loss 0.0458674, acc 0.98

Evaluation:
2016-09-07T02:14:48.934503: step 4700, loss 1.49963, acc 0.771107

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-4700

2016-09-07T02:14:50.663195: step 4701, loss 0.0977737, acc 0.98
2016-09-07T02:14:51.380507: step 4702, loss 0.0579621, acc 0.96
2016-09-07T02:14:52.066814: step 4703, loss 0.0170626, acc 0.98
2016-09-07T02:14:52.788864: step 4704, loss 0.0598352, acc 0.96
2016-09-07T02:14:53.466473: step 4705, loss 0.0133787, acc 1
2016-09-07T02:14:54.146705: step 4706, loss 0.0238367, acc 1
2016-09-07T02:14:54.848469: step 4707, loss 0.0233425, acc 1
2016-09-07T02:14:55.542221: step 4708, loss 0.0139244, acc 1
2016-09-07T02:14:56.267083: step 4709, loss 0.0201807, acc 0.98
2016-09-07T02:14:56.927162: step 4710, loss 0.0561034, acc 0.98
2016-09-07T02:14:57.610851: step 4711, loss 0.0261787, acc 0.98
2016-09-07T02:14:58.318497: step 4712, loss 0.0156555, acc 1
2016-09-07T02:14:59.027601: step 4713, loss 0.0154036, acc 1
2016-09-07T02:14:59.730109: step 4714, loss 0.0321005, acc 1
2016-09-07T02:15:00.439536: step 4715, loss 0.067463, acc 0.98
2016-09-07T02:15:01.151709: step 4716, loss 0.0563592, acc 0.98
2016-09-07T02:15:01.851424: step 4717, loss 0.0343241, acc 1
2016-09-07T02:15:02.553861: step 4718, loss 0.0374483, acc 0.98
2016-09-07T02:15:03.251633: step 4719, loss 0.0403854, acc 0.98
2016-09-07T02:15:03.935892: step 4720, loss 0.000314606, acc 1
2016-09-07T02:15:04.626580: step 4721, loss 0.0237585, acc 1
2016-09-07T02:15:05.332543: step 4722, loss 0.0799796, acc 0.96
2016-09-07T02:15:06.008283: step 4723, loss 0.00144015, acc 1
2016-09-07T02:15:06.701410: step 4724, loss 0.0817194, acc 0.96
2016-09-07T02:15:07.387697: step 4725, loss 0.0541808, acc 0.96
2016-09-07T02:15:08.070869: step 4726, loss 0.0307432, acc 0.98
2016-09-07T02:15:08.773152: step 4727, loss 0.0307454, acc 1
2016-09-07T02:15:09.484122: step 4728, loss 0.10898, acc 0.96
2016-09-07T02:15:10.160281: step 4729, loss 0.00847592, acc 1
2016-09-07T02:15:10.835007: step 4730, loss 0.0655546, acc 0.98
2016-09-07T02:15:11.551099: step 4731, loss 0.0122961, acc 1
2016-09-07T02:15:12.229436: step 4732, loss 0.00146907, acc 1
2016-09-07T02:15:12.921417: step 4733, loss 0.037128, acc 1
2016-09-07T02:15:13.598504: step 4734, loss 0.113543, acc 0.96
2016-09-07T02:15:14.303762: step 4735, loss 0.00420537, acc 1
2016-09-07T02:15:14.973096: step 4736, loss 0.00908765, acc 1
2016-09-07T02:15:15.668221: step 4737, loss 0.00304646, acc 1
2016-09-07T02:15:16.366939: step 4738, loss 0.0295798, acc 0.98
2016-09-07T02:15:17.066533: step 4739, loss 0.0741329, acc 0.98
2016-09-07T02:15:17.765206: step 4740, loss 0.137524, acc 0.96
2016-09-07T02:15:18.429475: step 4741, loss 0.0332191, acc 1
2016-09-07T02:15:19.140627: step 4742, loss 0.0150031, acc 1
2016-09-07T02:15:19.797803: step 4743, loss 0.00225655, acc 1
2016-09-07T02:15:20.476467: step 4744, loss 0.0211758, acc 0.98
2016-09-07T02:15:21.166302: step 4745, loss 0.0374195, acc 0.98
2016-09-07T02:15:21.855350: step 4746, loss 0.00883874, acc 1
2016-09-07T02:15:22.535408: step 4747, loss 0.00472171, acc 1
2016-09-07T02:15:23.231809: step 4748, loss 0.00610085, acc 1
2016-09-07T02:15:23.955280: step 4749, loss 0.0780322, acc 0.98
2016-09-07T02:15:24.644640: step 4750, loss 0.0726979, acc 0.98
2016-09-07T02:15:25.332641: step 4751, loss 0.000731719, acc 1
2016-09-07T02:15:26.042271: step 4752, loss 0.00421504, acc 1
2016-09-07T02:15:26.739526: step 4753, loss 0.0274265, acc 0.98
2016-09-07T02:15:27.442095: step 4754, loss 0.0242936, acc 1
2016-09-07T02:15:28.132801: step 4755, loss 0.000645855, acc 1
2016-09-07T02:15:28.842889: step 4756, loss 0.0539173, acc 0.98
2016-09-07T02:15:29.563316: step 4757, loss 0.0685936, acc 0.96
2016-09-07T02:15:30.257460: step 4758, loss 0.00199565, acc 1
2016-09-07T02:15:30.974351: step 4759, loss 0.0108023, acc 1
2016-09-07T02:15:31.662958: step 4760, loss 0.0108075, acc 1
2016-09-07T02:15:32.368811: step 4761, loss 0.0551207, acc 0.96
2016-09-07T02:15:33.051209: step 4762, loss 0.0888583, acc 0.94
2016-09-07T02:15:33.719660: step 4763, loss 0.0288608, acc 0.98
2016-09-07T02:15:34.417890: step 4764, loss 0.00422301, acc 1
2016-09-07T02:15:35.109271: step 4765, loss 0.0968778, acc 0.96
2016-09-07T02:15:35.807152: step 4766, loss 0.130302, acc 0.96
2016-09-07T02:15:36.484630: step 4767, loss 0.0146808, acc 1
2016-09-07T02:15:37.197433: step 4768, loss 0.0944993, acc 0.94
2016-09-07T02:15:37.924012: step 4769, loss 0.000130852, acc 1
2016-09-07T02:15:38.624290: step 4770, loss 0.0170132, acc 0.98
2016-09-07T02:15:39.313649: step 4771, loss 0.010796, acc 1
2016-09-07T02:15:39.995995: step 4772, loss 0.0236976, acc 0.98
2016-09-07T02:15:40.700862: step 4773, loss 0.04, acc 0.96
2016-09-07T02:15:41.390796: step 4774, loss 0.0875607, acc 0.96
2016-09-07T02:15:42.073984: step 4775, loss 0.00728149, acc 1
2016-09-07T02:15:42.758970: step 4776, loss 0.0221702, acc 0.98
2016-09-07T02:15:43.450080: step 4777, loss 0.00257346, acc 1
2016-09-07T02:15:44.142093: step 4778, loss 0.00451086, acc 1
2016-09-07T02:15:44.831296: step 4779, loss 0.00311149, acc 1
2016-09-07T02:15:45.541668: step 4780, loss 0.0188738, acc 1
2016-09-07T02:15:46.232450: step 4781, loss 0.00138, acc 1
2016-09-07T02:15:46.935437: step 4782, loss 0.0226541, acc 0.98
2016-09-07T02:15:47.641342: step 4783, loss 0.0771472, acc 0.98
2016-09-07T02:15:48.348607: step 4784, loss 0.0158819, acc 1
2016-09-07T02:15:49.041380: step 4785, loss 0.00480589, acc 1
2016-09-07T02:15:49.712396: step 4786, loss 0.0277701, acc 0.98
2016-09-07T02:15:50.417588: step 4787, loss 0.00222208, acc 1
2016-09-07T02:15:51.097027: step 4788, loss 0.0334563, acc 0.98
2016-09-07T02:15:51.794175: step 4789, loss 0.0150503, acc 0.98
2016-09-07T02:15:52.470599: step 4790, loss 0.0618875, acc 0.96
2016-09-07T02:15:53.165049: step 4791, loss 0.0114459, acc 1
2016-09-07T02:15:53.892617: step 4792, loss 0.139223, acc 0.96
2016-09-07T02:15:54.564244: step 4793, loss 0.11892, acc 0.96
2016-09-07T02:15:55.280070: step 4794, loss 0.0108112, acc 1
2016-09-07T02:15:55.997368: step 4795, loss 0.0202227, acc 1
2016-09-07T02:15:56.695748: step 4796, loss 0.0141508, acc 1
2016-09-07T02:15:57.372366: step 4797, loss 0.00475272, acc 1
2016-09-07T02:15:58.065533: step 4798, loss 0.0121072, acc 1
2016-09-07T02:15:58.784008: step 4799, loss 0.0579665, acc 0.96
2016-09-07T02:15:59.422028: step 4800, loss 0.00301152, acc 1

Evaluation:
2016-09-07T02:16:02.639589: step 4800, loss 1.53575, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-4800

2016-09-07T02:16:04.336024: step 4801, loss 0.0209423, acc 1
2016-09-07T02:16:05.035254: step 4802, loss 0.0694353, acc 0.98
2016-09-07T02:16:05.733378: step 4803, loss 0.105818, acc 0.96
2016-09-07T02:16:06.426798: step 4804, loss 0.0230889, acc 1
2016-09-07T02:16:07.127709: step 4805, loss 0.0224618, acc 0.98
2016-09-07T02:16:07.823409: step 4806, loss 0.0200782, acc 1
2016-09-07T02:16:08.536740: step 4807, loss 0.0199726, acc 1
2016-09-07T02:16:09.239099: step 4808, loss 0.0248292, acc 1
2016-09-07T02:16:09.927457: step 4809, loss 0.020871, acc 0.98
2016-09-07T02:16:10.665424: step 4810, loss 0.0248585, acc 0.98
2016-09-07T02:16:11.355673: step 4811, loss 0.048857, acc 0.96
2016-09-07T02:16:12.070465: step 4812, loss 0.0411545, acc 0.98
2016-09-07T02:16:12.759508: step 4813, loss 0.0543752, acc 0.96
2016-09-07T02:16:13.448426: step 4814, loss 0.0276413, acc 1
2016-09-07T02:16:14.166846: step 4815, loss 0.0631884, acc 0.98
2016-09-07T02:16:14.867497: step 4816, loss 0.00372716, acc 1
2016-09-07T02:16:15.582435: step 4817, loss 0.0324073, acc 0.98
2016-09-07T02:16:16.271466: step 4818, loss 0.00325134, acc 1
2016-09-07T02:16:16.997084: step 4819, loss 0.0282026, acc 1
2016-09-07T02:16:17.742362: step 4820, loss 0.0206676, acc 1
2016-09-07T02:16:18.472091: step 4821, loss 0.00981413, acc 1
2016-09-07T02:16:19.174244: step 4822, loss 0.00953213, acc 1
2016-09-07T02:16:19.858880: step 4823, loss 0.0101304, acc 1
2016-09-07T02:16:20.575254: step 4824, loss 0.0400355, acc 0.98
2016-09-07T02:16:21.263530: step 4825, loss 0.0304465, acc 1
2016-09-07T02:16:21.970024: step 4826, loss 0.0535908, acc 0.98
2016-09-07T02:16:22.671299: step 4827, loss 0.0160894, acc 1
2016-09-07T02:16:23.365269: step 4828, loss 0.00151265, acc 1
2016-09-07T02:16:24.070531: step 4829, loss 0.00278052, acc 1
2016-09-07T02:16:24.751808: step 4830, loss 0.0273277, acc 1
2016-09-07T02:16:25.433636: step 4831, loss 0.00577752, acc 1
2016-09-07T02:16:26.158626: step 4832, loss 0.0476213, acc 0.96
2016-09-07T02:16:26.842939: step 4833, loss 0.00760977, acc 1
2016-09-07T02:16:27.557959: step 4834, loss 0.0218143, acc 0.98
2016-09-07T02:16:28.240908: step 4835, loss 0.0240083, acc 1
2016-09-07T02:16:28.960766: step 4836, loss 0.00420826, acc 1
2016-09-07T02:16:29.648639: step 4837, loss 0.000494626, acc 1
2016-09-07T02:16:30.339239: step 4838, loss 0.120882, acc 0.98
2016-09-07T02:16:31.032986: step 4839, loss 0.0104328, acc 1
2016-09-07T02:16:31.716246: step 4840, loss 0.25521, acc 0.96
2016-09-07T02:16:32.430910: step 4841, loss 0.0545101, acc 0.96
2016-09-07T02:16:33.108709: step 4842, loss 0.0364186, acc 0.98
2016-09-07T02:16:33.820423: step 4843, loss 0.00486101, acc 1
2016-09-07T02:16:34.542207: step 4844, loss 0.00951007, acc 1
2016-09-07T02:16:35.239072: step 4845, loss 0.00180381, acc 1
2016-09-07T02:16:35.925364: step 4846, loss 0.0543014, acc 0.96
2016-09-07T02:16:36.612585: step 4847, loss 0.00763679, acc 1
2016-09-07T02:16:37.332413: step 4848, loss 0.0178894, acc 1
2016-09-07T02:16:38.028213: step 4849, loss 0.0589984, acc 0.98
2016-09-07T02:16:38.725162: step 4850, loss 0.0558543, acc 0.96
2016-09-07T02:16:39.413757: step 4851, loss 0.0489519, acc 0.96
2016-09-07T02:16:40.110399: step 4852, loss 0.0018219, acc 1
2016-09-07T02:16:40.795380: step 4853, loss 0.00554004, acc 1
2016-09-07T02:16:41.476065: step 4854, loss 0.00208367, acc 1
2016-09-07T02:16:42.181895: step 4855, loss 0.0331202, acc 0.98
2016-09-07T02:16:42.905572: step 4856, loss 0.00272783, acc 1
2016-09-07T02:16:43.593250: step 4857, loss 0.0120227, acc 1
2016-09-07T02:16:44.303355: step 4858, loss 0.0674577, acc 0.98
2016-09-07T02:16:44.985729: step 4859, loss 0.0682222, acc 0.96
2016-09-07T02:16:45.707451: step 4860, loss 0.00241666, acc 1
2016-09-07T02:16:46.382101: step 4861, loss 0.0314494, acc 1
2016-09-07T02:16:47.066443: step 4862, loss 0.0286151, acc 0.98
2016-09-07T02:16:47.756140: step 4863, loss 0.0479336, acc 0.98
2016-09-07T02:16:48.455442: step 4864, loss 0.0431631, acc 0.98
2016-09-07T02:16:49.148589: step 4865, loss 0.0495904, acc 0.98
2016-09-07T02:16:49.841220: step 4866, loss 0.0636496, acc 0.98
2016-09-07T02:16:50.571047: step 4867, loss 0.0167474, acc 1
2016-09-07T02:16:51.251870: step 4868, loss 0.00257985, acc 1
2016-09-07T02:16:51.951272: step 4869, loss 0.0624852, acc 0.96
2016-09-07T02:16:52.639633: step 4870, loss 0.0284408, acc 1
2016-09-07T02:16:53.313456: step 4871, loss 0.0204443, acc 1
2016-09-07T02:16:54.001417: step 4872, loss 0.0211362, acc 1
2016-09-07T02:16:54.681694: step 4873, loss 0.00054455, acc 1
2016-09-07T02:16:55.403586: step 4874, loss 0.0131013, acc 1
2016-09-07T02:16:56.092822: step 4875, loss 0.0279233, acc 0.98
2016-09-07T02:16:56.785047: step 4876, loss 0.0313472, acc 0.96
2016-09-07T02:16:57.462634: step 4877, loss 0.00940794, acc 1
2016-09-07T02:16:58.143501: step 4878, loss 0.00124142, acc 1
2016-09-07T02:16:58.833462: step 4879, loss 0.0011989, acc 1
2016-09-07T02:16:59.514782: step 4880, loss 0.0291209, acc 1
2016-09-07T02:17:00.257219: step 4881, loss 0.00410282, acc 1
2016-09-07T02:17:00.937636: step 4882, loss 0.0105283, acc 1
2016-09-07T02:17:01.627067: step 4883, loss 0.0705204, acc 0.98
2016-09-07T02:17:02.321957: step 4884, loss 0.0169011, acc 0.98
2016-09-07T02:17:03.029537: step 4885, loss 0.0467531, acc 0.96
2016-09-07T02:17:03.746479: step 4886, loss 0.0159928, acc 1
2016-09-07T02:17:04.435030: step 4887, loss 0.00406408, acc 1
2016-09-07T02:17:05.121097: step 4888, loss 0.0339667, acc 0.98
2016-09-07T02:17:05.817736: step 4889, loss 0.00168635, acc 1
2016-09-07T02:17:06.512907: step 4890, loss 0.0246215, acc 1
2016-09-07T02:17:07.205118: step 4891, loss 0.0398879, acc 0.98
2016-09-07T02:17:07.902415: step 4892, loss 0.00336754, acc 1
2016-09-07T02:17:08.625692: step 4893, loss 0.0302673, acc 0.98
2016-09-07T02:17:09.331101: step 4894, loss 0.00246732, acc 1
2016-09-07T02:17:10.033889: step 4895, loss 0.0265825, acc 0.98
2016-09-07T02:17:10.720131: step 4896, loss 0.0136755, acc 1
2016-09-07T02:17:11.426188: step 4897, loss 0.00175694, acc 1
2016-09-07T02:17:12.180376: step 4898, loss 0.10936, acc 0.96
2016-09-07T02:17:12.867613: step 4899, loss 0.0166922, acc 0.98
2016-09-07T02:17:13.550409: step 4900, loss 0.0753049, acc 0.96

Evaluation:
2016-09-07T02:17:16.720540: step 4900, loss 1.71709, acc 0.760788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-4900

2016-09-07T02:17:18.390893: step 4901, loss 0.00621731, acc 1
2016-09-07T02:17:19.086691: step 4902, loss 0.00105802, acc 1
2016-09-07T02:17:19.780193: step 4903, loss 0.0590824, acc 0.96
2016-09-07T02:17:20.479420: step 4904, loss 0.0713291, acc 0.96
2016-09-07T02:17:21.188431: step 4905, loss 0.00199179, acc 1
2016-09-07T02:17:21.910623: step 4906, loss 0.00690901, acc 1
2016-09-07T02:17:22.607736: step 4907, loss 0.0910096, acc 0.96
2016-09-07T02:17:23.288968: step 4908, loss 0.0202621, acc 0.98
2016-09-07T02:17:23.982029: step 4909, loss 0.0643849, acc 0.96
2016-09-07T02:17:24.690240: step 4910, loss 0.0147729, acc 1
2016-09-07T02:17:25.405440: step 4911, loss 0.0351129, acc 0.98
2016-09-07T02:17:26.073487: step 4912, loss 0.000352628, acc 1
2016-09-07T02:17:26.763376: step 4913, loss 0.000976283, acc 1
2016-09-07T02:17:27.474912: step 4914, loss 0.0238967, acc 0.98
2016-09-07T02:17:28.188153: step 4915, loss 0.00899135, acc 1
2016-09-07T02:17:28.875612: step 4916, loss 0.023784, acc 1
2016-09-07T02:17:29.548122: step 4917, loss 0.148314, acc 0.96
2016-09-07T02:17:30.262633: step 4918, loss 0.00684592, acc 1
2016-09-07T02:17:30.937874: step 4919, loss 0.0221033, acc 0.98
2016-09-07T02:17:31.630649: step 4920, loss 0.08657, acc 0.96
2016-09-07T02:17:32.326153: step 4921, loss 0.0208058, acc 0.98
2016-09-07T02:17:33.028426: step 4922, loss 0.0209359, acc 1
2016-09-07T02:17:33.718358: step 4923, loss 0.000654254, acc 1
2016-09-07T02:17:34.384157: step 4924, loss 0.0089368, acc 1
2016-09-07T02:17:35.100612: step 4925, loss 0.0411151, acc 0.98
2016-09-07T02:17:35.818256: step 4926, loss 0.0545497, acc 0.98
2016-09-07T02:17:36.549616: step 4927, loss 0.0248554, acc 1
2016-09-07T02:17:37.263188: step 4928, loss 0.00019073, acc 1
2016-09-07T02:17:37.951940: step 4929, loss 0.00925186, acc 1
2016-09-07T02:17:38.651958: step 4930, loss 0.00712961, acc 1
2016-09-07T02:17:39.331722: step 4931, loss 0.0525308, acc 0.96
2016-09-07T02:17:40.027188: step 4932, loss 0.000715696, acc 1
2016-09-07T02:17:40.735464: step 4933, loss 0.1227, acc 0.94
2016-09-07T02:17:41.425989: step 4934, loss 0.0207236, acc 0.98
2016-09-07T02:17:42.104972: step 4935, loss 0.00195351, acc 1
2016-09-07T02:17:42.779197: step 4936, loss 0.0118016, acc 1
2016-09-07T02:17:43.498950: step 4937, loss 9.36266e-05, acc 1
2016-09-07T02:17:44.157818: step 4938, loss 0.00876506, acc 1
2016-09-07T02:17:44.846470: step 4939, loss 0.0759994, acc 0.96
2016-09-07T02:17:45.551795: step 4940, loss 0.00153181, acc 1
2016-09-07T02:17:46.263727: step 4941, loss 0.0152631, acc 1
2016-09-07T02:17:46.954560: step 4942, loss 0.0580574, acc 0.98
2016-09-07T02:17:47.614511: step 4943, loss 0.00123784, acc 1
2016-09-07T02:17:48.322097: step 4944, loss 0.00729178, acc 1
2016-09-07T02:17:49.017944: step 4945, loss 0.101265, acc 0.98
2016-09-07T02:17:49.696307: step 4946, loss 0.021452, acc 0.98
2016-09-07T02:17:50.379163: step 4947, loss 0.00629504, acc 1
2016-09-07T02:17:51.065864: step 4948, loss 0.0609216, acc 0.98
2016-09-07T02:17:51.762967: step 4949, loss 0.0256485, acc 1
2016-09-07T02:17:52.438759: step 4950, loss 0.0755465, acc 0.98
2016-09-07T02:17:53.162611: step 4951, loss 0.0410032, acc 0.98
2016-09-07T02:17:53.854279: step 4952, loss 0.0398985, acc 0.98
2016-09-07T02:17:54.549740: step 4953, loss 0.126019, acc 0.98
2016-09-07T02:17:55.268670: step 4954, loss 0.0286481, acc 0.98
2016-09-07T02:17:55.964972: step 4955, loss 0.00945761, acc 1
2016-09-07T02:17:56.667939: step 4956, loss 0.000950108, acc 1
2016-09-07T02:17:57.359615: step 4957, loss 0.0162669, acc 0.98
2016-09-07T02:17:58.042114: step 4958, loss 0.0326985, acc 1
2016-09-07T02:17:58.733473: step 4959, loss 0.0520064, acc 0.96
2016-09-07T02:17:59.487182: step 4960, loss 0.00634125, acc 1
2016-09-07T02:18:00.195887: step 4961, loss 0.000618903, acc 1
2016-09-07T02:18:00.898980: step 4962, loss 0.0267835, acc 1
2016-09-07T02:18:01.613805: step 4963, loss 0.0309127, acc 0.96
2016-09-07T02:18:02.326435: step 4964, loss 0.00726754, acc 1
2016-09-07T02:18:03.028342: step 4965, loss 0.138272, acc 0.96
2016-09-07T02:18:03.736719: step 4966, loss 0.0165505, acc 1
2016-09-07T02:18:04.405674: step 4967, loss 0.000370264, acc 1
2016-09-07T02:18:05.098250: step 4968, loss 0.03821, acc 0.98
2016-09-07T02:18:05.769809: step 4969, loss 0.0310662, acc 1
2016-09-07T02:18:06.439265: step 4970, loss 0.0449164, acc 0.96
2016-09-07T02:18:07.122249: step 4971, loss 0.0873483, acc 0.96
2016-09-07T02:18:07.822705: step 4972, loss 0.00464771, acc 1
2016-09-07T02:18:08.515283: step 4973, loss 0.0572377, acc 0.96
2016-09-07T02:18:09.200931: step 4974, loss 0.00749381, acc 1
2016-09-07T02:18:09.912520: step 4975, loss 0.00293891, acc 1
2016-09-07T02:18:10.603890: step 4976, loss 0.0158497, acc 1
2016-09-07T02:18:11.283614: step 4977, loss 0.0168549, acc 1
2016-09-07T02:18:11.969510: step 4978, loss 0.0527344, acc 0.96
2016-09-07T02:18:12.649896: step 4979, loss 0.0407752, acc 0.96
2016-09-07T02:18:13.352251: step 4980, loss 0.00164646, acc 1
2016-09-07T02:18:14.039606: step 4981, loss 0.129088, acc 0.96
2016-09-07T02:18:14.726888: step 4982, loss 0.0159159, acc 1
2016-09-07T02:18:15.393125: step 4983, loss 0.00567526, acc 1
2016-09-07T02:18:16.063014: step 4984, loss 0.0628479, acc 0.98
2016-09-07T02:18:16.755160: step 4985, loss 0.0368824, acc 1
2016-09-07T02:18:17.424702: step 4986, loss 0.00145365, acc 1
2016-09-07T02:18:18.109549: step 4987, loss 0.0463033, acc 0.98
2016-09-07T02:18:18.779503: step 4988, loss 0.00594658, acc 1
2016-09-07T02:18:19.501643: step 4989, loss 0.114183, acc 0.96
2016-09-07T02:18:20.179417: step 4990, loss 0.0563141, acc 0.98
2016-09-07T02:18:20.854332: step 4991, loss 0.00738341, acc 1
2016-09-07T02:18:21.515633: step 4992, loss 0.000241592, acc 1
2016-09-07T02:18:22.220065: step 4993, loss 0.15333, acc 0.94
2016-09-07T02:18:22.939974: step 4994, loss 0.0351946, acc 0.98
2016-09-07T02:18:23.636795: step 4995, loss 0.0173589, acc 1
2016-09-07T02:18:24.369720: step 4996, loss 0.0657058, acc 0.96
2016-09-07T02:18:25.078098: step 4997, loss 0.00847515, acc 1
2016-09-07T02:18:25.775033: step 4998, loss 0.0493319, acc 0.98
2016-09-07T02:18:26.466578: step 4999, loss 0.0120749, acc 1
2016-09-07T02:18:27.147174: step 5000, loss 0.0128037, acc 1

Evaluation:
2016-09-07T02:18:30.336881: step 5000, loss 1.40314, acc 0.777674

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-5000

2016-09-07T02:18:32.054104: step 5001, loss 0.0223996, acc 0.98
2016-09-07T02:18:32.768532: step 5002, loss 0.0121787, acc 1
2016-09-07T02:18:33.448031: step 5003, loss 0.00844658, acc 1
2016-09-07T02:18:34.128541: step 5004, loss 0.00558383, acc 1
2016-09-07T02:18:34.826522: step 5005, loss 0.0650448, acc 0.96
2016-09-07T02:18:35.518836: step 5006, loss 0.0347021, acc 0.96
2016-09-07T02:18:36.215573: step 5007, loss 0.000504509, acc 1
2016-09-07T02:18:36.885888: step 5008, loss 0.00588949, acc 1
2016-09-07T02:18:37.596713: step 5009, loss 0.0544323, acc 0.94
2016-09-07T02:18:38.280033: step 5010, loss 0.0107237, acc 1
2016-09-07T02:18:38.959152: step 5011, loss 0.0291847, acc 1
2016-09-07T02:18:39.641448: step 5012, loss 0.026768, acc 0.98
2016-09-07T02:18:40.337483: step 5013, loss 0.00807969, acc 1
2016-09-07T02:18:41.046273: step 5014, loss 0.0313391, acc 0.98
2016-09-07T02:18:41.706254: step 5015, loss 0.0803292, acc 0.98
2016-09-07T02:18:42.407370: step 5016, loss 0.00762428, acc 1
2016-09-07T02:18:43.060864: step 5017, loss 0.0255551, acc 1
2016-09-07T02:18:43.758776: step 5018, loss 0.0130734, acc 1
2016-09-07T02:18:44.441012: step 5019, loss 0.00014111, acc 1
2016-09-07T02:18:45.136651: step 5020, loss 0.00341691, acc 1
2016-09-07T02:18:45.861407: step 5021, loss 0.0220043, acc 1
2016-09-07T02:18:46.523470: step 5022, loss 0.0407911, acc 0.98
2016-09-07T02:18:47.279231: step 5023, loss 0.0327399, acc 0.98
2016-09-07T02:18:47.977181: step 5024, loss 0.0726278, acc 0.98
2016-09-07T02:18:48.665251: step 5025, loss 0.034824, acc 1
2016-09-07T02:18:49.351999: step 5026, loss 0.00688292, acc 1
2016-09-07T02:18:50.025940: step 5027, loss 0.0614088, acc 0.98
2016-09-07T02:18:50.717694: step 5028, loss 0.0607906, acc 0.96
2016-09-07T02:18:51.381360: step 5029, loss 0.00717822, acc 1
2016-09-07T02:18:52.081998: step 5030, loss 0.0190131, acc 0.98
2016-09-07T02:18:52.765483: step 5031, loss 0.00032542, acc 1
2016-09-07T02:18:53.444958: step 5032, loss 0.0140374, acc 0.98
2016-09-07T02:18:54.159204: step 5033, loss 0.0180068, acc 1
2016-09-07T02:18:54.854961: step 5034, loss 0.00501924, acc 1
2016-09-07T02:18:55.579991: step 5035, loss 0.0224002, acc 0.98
2016-09-07T02:18:56.265823: step 5036, loss 0.010755, acc 1
2016-09-07T02:18:56.976014: step 5037, loss 0.0259378, acc 0.98
2016-09-07T02:18:57.662660: step 5038, loss 0.0291695, acc 1
2016-09-07T02:18:58.356867: step 5039, loss 0.011206, acc 1
2016-09-07T02:18:59.050799: step 5040, loss 0.0890731, acc 0.98
2016-09-07T02:18:59.746296: step 5041, loss 6.5362e-05, acc 1
2016-09-07T02:19:00.493141: step 5042, loss 0.0239545, acc 0.98
2016-09-07T02:19:01.195884: step 5043, loss 0.0159385, acc 0.98
2016-09-07T02:19:01.898893: step 5044, loss 0.0306718, acc 0.98
2016-09-07T02:19:02.608850: step 5045, loss 0.00103361, acc 1
2016-09-07T02:19:03.319081: step 5046, loss 0.0264812, acc 1
2016-09-07T02:19:04.030407: step 5047, loss 0.00851328, acc 1
2016-09-07T02:19:04.736437: step 5048, loss 0.0110976, acc 1
2016-09-07T02:19:05.424726: step 5049, loss 0.0335762, acc 0.98
2016-09-07T02:19:06.103920: step 5050, loss 0.00730992, acc 1
2016-09-07T02:19:06.783108: step 5051, loss 0.0118549, acc 1
2016-09-07T02:19:07.467531: step 5052, loss 0.0106023, acc 1
2016-09-07T02:19:08.183320: step 5053, loss 0.0303178, acc 0.98
2016-09-07T02:19:08.890691: step 5054, loss 0.0214591, acc 1
2016-09-07T02:19:09.542778: step 5055, loss 0.00526504, acc 1
2016-09-07T02:19:10.221646: step 5056, loss 0.0870687, acc 0.94
2016-09-07T02:19:10.930289: step 5057, loss 0.0197479, acc 0.98
2016-09-07T02:19:11.630821: step 5058, loss 0.157496, acc 0.98
2016-09-07T02:19:12.327286: step 5059, loss 0.115055, acc 0.98
2016-09-07T02:19:12.996979: step 5060, loss 0.00152913, acc 1
2016-09-07T02:19:13.696656: step 5061, loss 0.000730054, acc 1
2016-09-07T02:19:14.359651: step 5062, loss 0.160305, acc 0.98
2016-09-07T02:19:15.057524: step 5063, loss 0.0469023, acc 0.98
2016-09-07T02:19:15.747692: step 5064, loss 0.029747, acc 0.98
2016-09-07T02:19:16.458388: step 5065, loss 0.00222379, acc 1
2016-09-07T02:19:17.165056: step 5066, loss 0.00384865, acc 1
2016-09-07T02:19:17.845599: step 5067, loss 0.114474, acc 0.98
2016-09-07T02:19:18.570776: step 5068, loss 0.0498039, acc 0.96
2016-09-07T02:19:19.247108: step 5069, loss 0.0221066, acc 1
2016-09-07T02:19:19.940560: step 5070, loss 0.0014278, acc 1
2016-09-07T02:19:20.641227: step 5071, loss 0.0295605, acc 0.98
2016-09-07T02:19:21.327112: step 5072, loss 0.0516791, acc 0.98
2016-09-07T02:19:22.027595: step 5073, loss 0.00672004, acc 1
2016-09-07T02:19:22.688572: step 5074, loss 0.0024773, acc 1
2016-09-07T02:19:23.395971: step 5075, loss 0.0129834, acc 1
2016-09-07T02:19:24.085729: step 5076, loss 0.0602139, acc 0.98
2016-09-07T02:19:24.771793: step 5077, loss 0.0530038, acc 0.98
2016-09-07T02:19:25.476394: step 5078, loss 0.0367713, acc 0.98
2016-09-07T02:19:26.168327: step 5079, loss 0.00100528, acc 1
2016-09-07T02:19:26.862626: step 5080, loss 0.0378028, acc 0.98
2016-09-07T02:19:27.542501: step 5081, loss 0.0220784, acc 1
2016-09-07T02:19:28.232621: step 5082, loss 0.0183379, acc 1
2016-09-07T02:19:28.933262: step 5083, loss 0.0701163, acc 0.98
2016-09-07T02:19:29.646359: step 5084, loss 0.0155505, acc 1
2016-09-07T02:19:30.364863: step 5085, loss 0.00593473, acc 1
2016-09-07T02:19:31.071768: step 5086, loss 0.0189636, acc 1
2016-09-07T02:19:31.772705: step 5087, loss 0.0398697, acc 0.98
2016-09-07T02:19:32.506459: step 5088, loss 0.0205569, acc 0.98
2016-09-07T02:19:33.216231: step 5089, loss 0.00369559, acc 1
2016-09-07T02:19:33.916837: step 5090, loss 0.026067, acc 1
2016-09-07T02:19:34.611320: step 5091, loss 0.0242798, acc 0.98
2016-09-07T02:19:35.317287: step 5092, loss 0.050919, acc 0.98
2016-09-07T02:19:35.980479: step 5093, loss 0.0076371, acc 1
2016-09-07T02:19:36.654212: step 5094, loss 0.00954974, acc 1
2016-09-07T02:19:37.343421: step 5095, loss 0.0614969, acc 0.96
2016-09-07T02:19:38.033582: step 5096, loss 0.0069951, acc 1
2016-09-07T02:19:38.742712: step 5097, loss 0.0198278, acc 1
2016-09-07T02:19:39.429989: step 5098, loss 0.0173523, acc 1
2016-09-07T02:19:40.145120: step 5099, loss 0.00867878, acc 1
2016-09-07T02:19:40.825289: step 5100, loss 0.155273, acc 0.96

Evaluation:
2016-09-07T02:19:43.997730: step 5100, loss 1.54088, acc 0.76454

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-5100

2016-09-07T02:19:45.638258: step 5101, loss 0.0187962, acc 1
2016-09-07T02:19:46.342144: step 5102, loss 0.0462139, acc 0.98
2016-09-07T02:19:47.034270: step 5103, loss 0.0198523, acc 0.98
2016-09-07T02:19:47.733384: step 5104, loss 0.00450785, acc 1
2016-09-07T02:19:48.424048: step 5105, loss 0.00662462, acc 1
2016-09-07T02:19:49.077707: step 5106, loss 0.0671267, acc 0.96
2016-09-07T02:19:49.783662: step 5107, loss 0.0427061, acc 0.96
2016-09-07T02:19:50.500270: step 5108, loss 0.0706266, acc 0.98
2016-09-07T02:19:51.195566: step 5109, loss 0.0129036, acc 1
2016-09-07T02:19:51.887092: step 5110, loss 0.0170407, acc 0.98
2016-09-07T02:19:52.571733: step 5111, loss 0.000440243, acc 1
2016-09-07T02:19:53.288189: step 5112, loss 0.039583, acc 0.98
2016-09-07T02:19:53.949308: step 5113, loss 0.0210378, acc 1
2016-09-07T02:19:54.667395: step 5114, loss 0.0329807, acc 0.98
2016-09-07T02:19:55.379152: step 5115, loss 0.00183267, acc 1
2016-09-07T02:19:56.075007: step 5116, loss 0.00126262, acc 1
2016-09-07T02:19:56.754832: step 5117, loss 0.0145139, acc 1
2016-09-07T02:19:57.452259: step 5118, loss 0.0849432, acc 0.94
2016-09-07T02:19:58.190932: step 5119, loss 0.000190119, acc 1
2016-09-07T02:19:58.865239: step 5120, loss 0.0209771, acc 0.98
2016-09-07T02:19:59.559486: step 5121, loss 0.0133155, acc 1
2016-09-07T02:20:00.265050: step 5122, loss 0.0383475, acc 0.98
2016-09-07T02:20:00.942686: step 5123, loss 0.067677, acc 0.96
2016-09-07T02:20:01.633220: step 5124, loss 0.0248329, acc 1
2016-09-07T02:20:02.315129: step 5125, loss 0.0123135, acc 1
2016-09-07T02:20:03.020189: step 5126, loss 0.0162249, acc 1
2016-09-07T02:20:03.729106: step 5127, loss 0.000569376, acc 1
2016-09-07T02:20:04.409564: step 5128, loss 0.0141842, acc 1
2016-09-07T02:20:05.086966: step 5129, loss 0.000266141, acc 1
2016-09-07T02:20:05.767562: step 5130, loss 2.57729e-06, acc 1
2016-09-07T02:20:06.455948: step 5131, loss 0.00304599, acc 1
2016-09-07T02:20:07.115101: step 5132, loss 0.000953283, acc 1
2016-09-07T02:20:07.815468: step 5133, loss 0.0351886, acc 0.98
2016-09-07T02:20:08.495429: step 5134, loss 0.0217496, acc 0.98
2016-09-07T02:20:09.195991: step 5135, loss 0.00257226, acc 1
2016-09-07T02:20:09.898179: step 5136, loss 0.0340408, acc 0.98
2016-09-07T02:20:10.600330: step 5137, loss 0.00319663, acc 1
2016-09-07T02:20:11.293113: step 5138, loss 0.00111158, acc 1
2016-09-07T02:20:11.967963: step 5139, loss 0.0106059, acc 1
2016-09-07T02:20:12.672695: step 5140, loss 0.0616275, acc 0.98
2016-09-07T02:20:13.372679: step 5141, loss 0.0549217, acc 0.96
2016-09-07T02:20:14.062903: step 5142, loss 0.0446953, acc 0.98
2016-09-07T02:20:14.769659: step 5143, loss 0.0320236, acc 0.98
2016-09-07T02:20:15.451933: step 5144, loss 0.00138226, acc 1
2016-09-07T02:20:16.161044: step 5145, loss 0.00233628, acc 1
2016-09-07T02:20:16.857932: step 5146, loss 0.0403596, acc 0.98
2016-09-07T02:20:17.559407: step 5147, loss 0.0713854, acc 0.98
2016-09-07T02:20:18.260608: step 5148, loss 0.021486, acc 0.98
2016-09-07T02:20:18.951198: step 5149, loss 0.021698, acc 0.98
2016-09-07T02:20:19.648999: step 5150, loss 0.0143723, acc 1
2016-09-07T02:20:20.360160: step 5151, loss 0.0351599, acc 0.96
2016-09-07T02:20:21.065319: step 5152, loss 0.07961, acc 0.96
2016-09-07T02:20:21.748113: step 5153, loss 0.0491524, acc 0.98
2016-09-07T02:20:22.434645: step 5154, loss 0.0496144, acc 0.98
2016-09-07T02:20:23.139561: step 5155, loss 0.00382442, acc 1
2016-09-07T02:20:23.828598: step 5156, loss 0.00107695, acc 1
2016-09-07T02:20:24.506045: step 5157, loss 0.0192809, acc 1
2016-09-07T02:20:25.173759: step 5158, loss 0.0454015, acc 0.98
2016-09-07T02:20:25.892875: step 5159, loss 0.00146594, acc 1
2016-09-07T02:20:26.571016: step 5160, loss 0.0313287, acc 0.98
2016-09-07T02:20:27.274525: step 5161, loss 0.0378732, acc 1
2016-09-07T02:20:27.976160: step 5162, loss 0.034189, acc 1
2016-09-07T02:20:28.669783: step 5163, loss 0.0488579, acc 0.98
2016-09-07T02:20:29.384058: step 5164, loss 0.00852729, acc 1
2016-09-07T02:20:30.055825: step 5165, loss 0.00103455, acc 1
2016-09-07T02:20:30.764222: step 5166, loss 0.0471473, acc 0.96
2016-09-07T02:20:31.464121: step 5167, loss 0.0235855, acc 1
2016-09-07T02:20:32.144085: step 5168, loss 0.0246091, acc 1
2016-09-07T02:20:32.840195: step 5169, loss 0.0301308, acc 0.98
2016-09-07T02:20:33.528939: step 5170, loss 0.0663457, acc 0.98
2016-09-07T02:20:34.225779: step 5171, loss 0.0261731, acc 0.98
2016-09-07T02:20:34.905203: step 5172, loss 0.0556776, acc 0.98
2016-09-07T02:20:35.627837: step 5173, loss 0.000313282, acc 1
2016-09-07T02:20:36.319269: step 5174, loss 0.130429, acc 0.94
2016-09-07T02:20:37.009702: step 5175, loss 0.000125857, acc 1
2016-09-07T02:20:37.687826: step 5176, loss 0.0191906, acc 0.98
2016-09-07T02:20:38.373905: step 5177, loss 0.0138498, acc 1
2016-09-07T02:20:39.093535: step 5178, loss 0.00719364, acc 1
2016-09-07T02:20:39.795150: step 5179, loss 0.0308833, acc 1
2016-09-07T02:20:40.483352: step 5180, loss 0.000991169, acc 1
2016-09-07T02:20:41.173569: step 5181, loss 0.0189841, acc 0.98
2016-09-07T02:20:41.884857: step 5182, loss 0.0379495, acc 0.98
2016-09-07T02:20:42.612410: step 5183, loss 0.0251989, acc 0.98
2016-09-07T02:20:43.233303: step 5184, loss 0.0208343, acc 0.977273
2016-09-07T02:20:43.957620: step 5185, loss 0.0187356, acc 0.98
2016-09-07T02:20:44.657912: step 5186, loss 0.0460712, acc 1
2016-09-07T02:20:45.359472: step 5187, loss 0.0265319, acc 0.98
2016-09-07T02:20:46.050618: step 5188, loss 0.0177654, acc 0.98
2016-09-07T02:20:46.730437: step 5189, loss 0.000447543, acc 1
2016-09-07T02:20:47.424448: step 5190, loss 0.024152, acc 0.98
2016-09-07T02:20:48.086164: step 5191, loss 0.0495902, acc 0.98
2016-09-07T02:20:48.792217: step 5192, loss 0.0131561, acc 1
2016-09-07T02:20:49.506932: step 5193, loss 0.0105265, acc 1
2016-09-07T02:20:50.200643: step 5194, loss 0.0223983, acc 0.98
2016-09-07T02:20:50.877940: step 5195, loss 5.53643e-05, acc 1
2016-09-07T02:20:51.556850: step 5196, loss 0.0152319, acc 1
2016-09-07T02:20:52.265058: step 5197, loss 0.0324302, acc 1
2016-09-07T02:20:52.933751: step 5198, loss 0.0171451, acc 1
2016-09-07T02:20:53.609850: step 5199, loss 0.00817166, acc 1
2016-09-07T02:20:54.304179: step 5200, loss 0.0372573, acc 0.96

Evaluation:
2016-09-07T02:20:57.466358: step 5200, loss 1.7911, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-5200

2016-09-07T02:20:59.162764: step 5201, loss 0.0574547, acc 0.96
2016-09-07T02:20:59.847045: step 5202, loss 0.106073, acc 0.96
2016-09-07T02:21:00.563486: step 5203, loss 0.0241498, acc 1
2016-09-07T02:21:01.224430: step 5204, loss 0.0378441, acc 0.98
2016-09-07T02:21:01.921829: step 5205, loss 0.0440823, acc 0.98
2016-09-07T02:21:02.631212: step 5206, loss 0.0239924, acc 0.98
2016-09-07T02:21:03.323166: step 5207, loss 0.0398325, acc 0.96
2016-09-07T02:21:04.022645: step 5208, loss 0.00264491, acc 1
2016-09-07T02:21:04.716817: step 5209, loss 0.0113138, acc 1
2016-09-07T02:21:05.433711: step 5210, loss 0.0980328, acc 0.98
2016-09-07T02:21:06.103111: step 5211, loss 0.0717247, acc 0.96
2016-09-07T02:21:06.803340: step 5212, loss 0.0407095, acc 0.98
2016-09-07T02:21:07.528429: step 5213, loss 0.0328328, acc 1
2016-09-07T02:21:08.212561: step 5214, loss 0.0432494, acc 0.96
2016-09-07T02:21:08.903106: step 5215, loss 0.039776, acc 0.96
2016-09-07T02:21:09.585384: step 5216, loss 0.0288133, acc 0.98
2016-09-07T02:21:10.308385: step 5217, loss 0.0198211, acc 1
2016-09-07T02:21:10.991147: step 5218, loss 0.0143343, acc 1
2016-09-07T02:21:11.671756: step 5219, loss 0.00405025, acc 1
2016-09-07T02:21:12.367974: step 5220, loss 0.0192743, acc 1
2016-09-07T02:21:13.054025: step 5221, loss 0.0201567, acc 1
2016-09-07T02:21:13.739357: step 5222, loss 0.00610098, acc 1
2016-09-07T02:21:14.436853: step 5223, loss 0.00411893, acc 1
2016-09-07T02:21:15.150541: step 5224, loss 0.000991615, acc 1
2016-09-07T02:21:15.841245: step 5225, loss 0.0190502, acc 1
2016-09-07T02:21:16.518671: step 5226, loss 0.0781896, acc 0.98
2016-09-07T02:21:17.215723: step 5227, loss 0.0828875, acc 0.98
2016-09-07T02:21:17.918539: step 5228, loss 0.0193463, acc 0.98
2016-09-07T02:21:18.605311: step 5229, loss 0.0465253, acc 0.98
2016-09-07T02:21:19.312763: step 5230, loss 0.0297647, acc 0.98
2016-09-07T02:21:20.031279: step 5231, loss 0.0322912, acc 0.98
2016-09-07T02:21:20.726182: step 5232, loss 0.0637428, acc 0.96
2016-09-07T02:21:21.405230: step 5233, loss 0.0151591, acc 1
2016-09-07T02:21:22.096158: step 5234, loss 0.0474316, acc 0.98
2016-09-07T02:21:22.776047: step 5235, loss 0.0247146, acc 0.98
2016-09-07T02:21:23.490288: step 5236, loss 0.00629493, acc 1
2016-09-07T02:21:24.171868: step 5237, loss 0.0201055, acc 0.98
2016-09-07T02:21:24.887293: step 5238, loss 0.00606475, acc 1
2016-09-07T02:21:25.587806: step 5239, loss 0.0110779, acc 1
2016-09-07T02:21:26.282994: step 5240, loss 0.032565, acc 0.98
2016-09-07T02:21:26.966855: step 5241, loss 0.0314658, acc 0.98
2016-09-07T02:21:27.668979: step 5242, loss 0.00428239, acc 1
2016-09-07T02:21:28.376422: step 5243, loss 0.000893246, acc 1
2016-09-07T02:21:29.081338: step 5244, loss 0.0420859, acc 0.98
2016-09-07T02:21:29.759502: step 5245, loss 0.00955559, acc 1
2016-09-07T02:21:30.442664: step 5246, loss 0.0218571, acc 0.98
2016-09-07T02:21:31.132800: step 5247, loss 0.0244688, acc 0.98
2016-09-07T02:21:31.822845: step 5248, loss 0.214329, acc 0.96
2016-09-07T02:21:32.516502: step 5249, loss 0.0129695, acc 1
2016-09-07T02:21:33.218429: step 5250, loss 0.0294096, acc 0.98
2016-09-07T02:21:33.894403: step 5251, loss 0.0101698, acc 1
2016-09-07T02:21:34.588760: step 5252, loss 0.0174902, acc 1
2016-09-07T02:21:35.281834: step 5253, loss 0.000871764, acc 1
2016-09-07T02:21:36.027624: step 5254, loss 0.049937, acc 0.96
2016-09-07T02:21:36.748256: step 5255, loss 0.000264851, acc 1
2016-09-07T02:21:37.408076: step 5256, loss 0.0400901, acc 0.96
2016-09-07T02:21:38.096374: step 5257, loss 0.000560127, acc 1
2016-09-07T02:21:38.781602: step 5258, loss 0.0200314, acc 0.98
2016-09-07T02:21:39.482001: step 5259, loss 0.00162272, acc 1
2016-09-07T02:21:40.181797: step 5260, loss 0.032483, acc 0.96
2016-09-07T02:21:40.878913: step 5261, loss 0.0221314, acc 0.98
2016-09-07T02:21:41.586085: step 5262, loss 0.0197737, acc 0.98
2016-09-07T02:21:42.260513: step 5263, loss 0.0691787, acc 0.96
2016-09-07T02:21:42.941775: step 5264, loss 0.021589, acc 1
2016-09-07T02:21:43.628023: step 5265, loss 0.0298091, acc 1
2016-09-07T02:21:44.335041: step 5266, loss 0.032906, acc 0.98
2016-09-07T02:21:45.048263: step 5267, loss 0.0942011, acc 0.98
2016-09-07T02:21:45.743644: step 5268, loss 0.0117147, acc 1
2016-09-07T02:21:46.497965: step 5269, loss 0.00782534, acc 1
2016-09-07T02:21:47.189667: step 5270, loss 0.0353189, acc 0.98
2016-09-07T02:21:47.883784: step 5271, loss 0.149144, acc 0.96
2016-09-07T02:21:48.576080: step 5272, loss 0.0230792, acc 1
2016-09-07T02:21:49.279296: step 5273, loss 0.00922986, acc 1
2016-09-07T02:21:49.978250: step 5274, loss 0.012056, acc 1
2016-09-07T02:21:50.657272: step 5275, loss 0.00946651, acc 1
2016-09-07T02:21:51.353074: step 5276, loss 0.0562677, acc 0.96
2016-09-07T02:21:52.146729: step 5277, loss 0.0130146, acc 1
2016-09-07T02:21:52.877013: step 5278, loss 0.0281374, acc 0.98
2016-09-07T02:21:53.564276: step 5279, loss 0.0283684, acc 0.98
2016-09-07T02:21:54.228887: step 5280, loss 0.0479996, acc 0.98
2016-09-07T02:21:54.943664: step 5281, loss 0.00564886, acc 1
2016-09-07T02:21:55.643863: step 5282, loss 0.0144917, acc 1
2016-09-07T02:21:56.335633: step 5283, loss 0.0899863, acc 0.98
2016-09-07T02:21:57.020304: step 5284, loss 0.00184736, acc 1
2016-09-07T02:21:57.706259: step 5285, loss 0.0154866, acc 1
2016-09-07T02:21:58.423736: step 5286, loss 0.024307, acc 0.98
2016-09-07T02:21:59.125029: step 5287, loss 0.0260537, acc 1
2016-09-07T02:21:59.813779: step 5288, loss 0.0671812, acc 0.96
2016-09-07T02:22:00.554908: step 5289, loss 0.00822193, acc 1
2016-09-07T02:22:01.247051: step 5290, loss 0.0238171, acc 0.98
2016-09-07T02:22:01.939250: step 5291, loss 0.0218357, acc 1
2016-09-07T02:22:02.596473: step 5292, loss 0.0300158, acc 0.98
2016-09-07T02:22:03.306286: step 5293, loss 0.0175615, acc 1
2016-09-07T02:22:03.973768: step 5294, loss 0.0101748, acc 1
2016-09-07T02:22:04.645488: step 5295, loss 0.0512196, acc 0.98
2016-09-07T02:22:05.333572: step 5296, loss 8.77322e-05, acc 1
2016-09-07T02:22:06.023852: step 5297, loss 0.0166207, acc 1
2016-09-07T02:22:06.721371: step 5298, loss 0.00775335, acc 1
2016-09-07T02:22:07.389292: step 5299, loss 0.0138801, acc 1
2016-09-07T02:22:08.109526: step 5300, loss 0.0154393, acc 1

Evaluation:
2016-09-07T02:22:11.300941: step 5300, loss 1.75018, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-5300

2016-09-07T02:22:13.053847: step 5301, loss 0.00263725, acc 1
2016-09-07T02:22:13.750606: step 5302, loss 0.0270298, acc 1
2016-09-07T02:22:14.450629: step 5303, loss 0.00118669, acc 1
2016-09-07T02:22:15.138251: step 5304, loss 0.0409194, acc 0.98
2016-09-07T02:22:15.822434: step 5305, loss 0.0649112, acc 0.94
2016-09-07T02:22:16.523191: step 5306, loss 0.0422475, acc 0.98
2016-09-07T02:22:17.217409: step 5307, loss 0.000221117, acc 1
2016-09-07T02:22:17.910235: step 5308, loss 0.0289999, acc 0.98
2016-09-07T02:22:18.600702: step 5309, loss 0.00337611, acc 1
2016-09-07T02:22:19.309986: step 5310, loss 0.0541748, acc 0.98
2016-09-07T02:22:20.010485: step 5311, loss 0.00569829, acc 1
2016-09-07T02:22:20.698992: step 5312, loss 0.016384, acc 0.98
2016-09-07T02:22:21.414493: step 5313, loss 0.00209521, acc 1
2016-09-07T02:22:22.119632: step 5314, loss 0.051842, acc 0.98
2016-09-07T02:22:22.793244: step 5315, loss 0.077958, acc 0.92
2016-09-07T02:22:23.493760: step 5316, loss 0.0164787, acc 1
2016-09-07T02:22:24.194793: step 5317, loss 0.00953221, acc 1
2016-09-07T02:22:24.929773: step 5318, loss 0.00296687, acc 1
2016-09-07T02:22:25.612639: step 5319, loss 0.013672, acc 1
2016-09-07T02:22:26.331828: step 5320, loss 0.000556362, acc 1
2016-09-07T02:22:27.036520: step 5321, loss 0.000161602, acc 1
2016-09-07T02:22:27.728279: step 5322, loss 0.0985633, acc 0.98
2016-09-07T02:22:28.463711: step 5323, loss 0.0495285, acc 0.98
2016-09-07T02:22:29.142746: step 5324, loss 0.0503082, acc 0.98
2016-09-07T02:22:29.838333: step 5325, loss 0.0183779, acc 0.98
2016-09-07T02:22:30.524692: step 5326, loss 0.111928, acc 0.96
2016-09-07T02:22:31.204252: step 5327, loss 0.00294707, acc 1
2016-09-07T02:22:31.887910: step 5328, loss 0.0283926, acc 0.98
2016-09-07T02:22:32.593382: step 5329, loss 0.0124269, acc 1
2016-09-07T02:22:33.297404: step 5330, loss 0.00303674, acc 1
2016-09-07T02:22:33.982177: step 5331, loss 0.000579494, acc 1
2016-09-07T02:22:34.672360: step 5332, loss 0.0566206, acc 0.94
2016-09-07T02:22:35.378763: step 5333, loss 0.0727844, acc 0.94
2016-09-07T02:22:36.166144: step 5334, loss 0.0173983, acc 1
2016-09-07T02:22:36.870943: step 5335, loss 0.0375651, acc 0.98
2016-09-07T02:22:37.531706: step 5336, loss 0.0262431, acc 0.98
2016-09-07T02:22:38.244482: step 5337, loss 0.00419794, acc 1
2016-09-07T02:22:38.944562: step 5338, loss 0.0675677, acc 0.98
2016-09-07T02:22:39.621753: step 5339, loss 0.000535215, acc 1
2016-09-07T02:22:40.328115: step 5340, loss 0.00523102, acc 1
2016-09-07T02:22:41.038662: step 5341, loss 0.000834854, acc 1
2016-09-07T02:22:41.745973: step 5342, loss 0.0675854, acc 0.94
2016-09-07T02:22:42.436798: step 5343, loss 0.00158391, acc 1
2016-09-07T02:22:43.111694: step 5344, loss 0.0168997, acc 1
2016-09-07T02:22:43.815226: step 5345, loss 0.0182008, acc 0.98
2016-09-07T02:22:44.515639: step 5346, loss 0.00222895, acc 1
2016-09-07T02:22:45.197391: step 5347, loss 0.019975, acc 0.98
2016-09-07T02:22:45.875272: step 5348, loss 0.00765742, acc 1
2016-09-07T02:22:46.600910: step 5349, loss 0.0379572, acc 0.98
2016-09-07T02:22:47.281345: step 5350, loss 0.0297058, acc 1
2016-09-07T02:22:47.959949: step 5351, loss 0.0264601, acc 0.98
2016-09-07T02:22:48.658033: step 5352, loss 0.0305852, acc 1
2016-09-07T02:22:49.353625: step 5353, loss 0.0161833, acc 0.98
2016-09-07T02:22:50.046896: step 5354, loss 0.0797979, acc 0.96
2016-09-07T02:22:50.705272: step 5355, loss 0.00942388, acc 1
2016-09-07T02:22:51.403084: step 5356, loss 0.0110702, acc 1
2016-09-07T02:22:52.074390: step 5357, loss 0.00799531, acc 1
2016-09-07T02:22:52.758877: step 5358, loss 0.0653629, acc 0.96
2016-09-07T02:22:53.443782: step 5359, loss 0.0349059, acc 0.98
2016-09-07T02:22:54.127454: step 5360, loss 0.0436387, acc 0.96
2016-09-07T02:22:54.845360: step 5361, loss 0.0138626, acc 1
2016-09-07T02:22:55.527155: step 5362, loss 0.0373795, acc 0.98
2016-09-07T02:22:56.249442: step 5363, loss 0.000483354, acc 1
2016-09-07T02:22:56.946903: step 5364, loss 0.361972, acc 0.96
2016-09-07T02:22:57.636698: step 5365, loss 0.0173823, acc 0.98
2016-09-07T02:22:58.344261: step 5366, loss 0.010532, acc 1
2016-09-07T02:22:59.037062: step 5367, loss 0.00243136, acc 1
2016-09-07T02:22:59.733128: step 5368, loss 0.0432218, acc 0.98
2016-09-07T02:23:00.454154: step 5369, loss 0.00123393, acc 1
2016-09-07T02:23:01.147229: step 5370, loss 0.014209, acc 1
2016-09-07T02:23:01.845689: step 5371, loss 0.0516112, acc 0.96
2016-09-07T02:23:02.550055: step 5372, loss 0.00207503, acc 1
2016-09-07T02:23:03.242659: step 5373, loss 0.0195552, acc 1
2016-09-07T02:23:03.916680: step 5374, loss 0.103378, acc 0.96
2016-09-07T02:23:04.613078: step 5375, loss 0.00389075, acc 1
2016-09-07T02:23:05.243513: step 5376, loss 0.0293959, acc 0.977273
2016-09-07T02:23:05.954496: step 5377, loss 0.0302154, acc 0.98
2016-09-07T02:23:06.651288: step 5378, loss 0.0140999, acc 1
2016-09-07T02:23:07.350011: step 5379, loss 0.0613445, acc 0.96
2016-09-07T02:23:08.029132: step 5380, loss 0.0157445, acc 1
2016-09-07T02:23:08.721132: step 5381, loss 0.0264603, acc 0.98
2016-09-07T02:23:09.415523: step 5382, loss 0.0417064, acc 0.96
2016-09-07T02:23:10.087988: step 5383, loss 0.00216356, acc 1
2016-09-07T02:23:10.774010: step 5384, loss 0.0343165, acc 0.98
2016-09-07T02:23:11.457044: step 5385, loss 0.0334557, acc 0.98
2016-09-07T02:23:12.163097: step 5386, loss 0.00456779, acc 1
2016-09-07T02:23:12.839864: step 5387, loss 0.0170891, acc 1
2016-09-07T02:23:13.528260: step 5388, loss 0.0348935, acc 0.96
2016-09-07T02:23:14.252279: step 5389, loss 0.0641251, acc 0.96
2016-09-07T02:23:14.951279: step 5390, loss 0.0362758, acc 0.98
2016-09-07T02:23:15.638210: step 5391, loss 0.0144975, acc 1
2016-09-07T02:23:16.333239: step 5392, loss 0.00057796, acc 1
2016-09-07T02:23:17.028893: step 5393, loss 0.0137458, acc 1
2016-09-07T02:23:17.713530: step 5394, loss 0.00565662, acc 1
2016-09-07T02:23:18.399927: step 5395, loss 0.00561412, acc 1
2016-09-07T02:23:19.111160: step 5396, loss 0.0237473, acc 1
2016-09-07T02:23:19.802453: step 5397, loss 0.0271621, acc 1
2016-09-07T02:23:20.498776: step 5398, loss 0.0270001, acc 0.98
2016-09-07T02:23:21.205566: step 5399, loss 0.00324525, acc 1
2016-09-07T02:23:21.901214: step 5400, loss 0.00605886, acc 1

Evaluation:
2016-09-07T02:23:25.103385: step 5400, loss 1.77117, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-5400

2016-09-07T02:23:26.741912: step 5401, loss 0.00824903, acc 1
2016-09-07T02:23:27.451079: step 5402, loss 0.0178336, acc 0.98
2016-09-07T02:23:28.158943: step 5403, loss 0.00107847, acc 1
2016-09-07T02:23:28.862747: step 5404, loss 0.00641542, acc 1
2016-09-07T02:23:29.556515: step 5405, loss 0.114487, acc 0.96
2016-09-07T02:23:30.232389: step 5406, loss 0.00114266, acc 1
2016-09-07T02:23:30.948238: step 5407, loss 0.0221319, acc 0.98
2016-09-07T02:23:31.603650: step 5408, loss 0.149172, acc 0.96
2016-09-07T02:23:32.320680: step 5409, loss 0.0228549, acc 0.98
2016-09-07T02:23:33.002149: step 5410, loss 0.0175149, acc 0.98
2016-09-07T02:23:33.697578: step 5411, loss 0.000514493, acc 1
2016-09-07T02:23:34.427989: step 5412, loss 0.00757494, acc 1
2016-09-07T02:23:35.131526: step 5413, loss 0.00944676, acc 1
2016-09-07T02:23:35.845664: step 5414, loss 0.0177813, acc 1
2016-09-07T02:23:36.534505: step 5415, loss 0.00491745, acc 1
2016-09-07T02:23:37.211623: step 5416, loss 0.00114159, acc 1
2016-09-07T02:23:37.918595: step 5417, loss 0.0485712, acc 0.98
2016-09-07T02:23:38.606883: step 5418, loss 0.000499529, acc 1
2016-09-07T02:23:39.320651: step 5419, loss 0.0209047, acc 0.98
2016-09-07T02:23:39.994497: step 5420, loss 0.00934777, acc 1
2016-09-07T02:23:40.706223: step 5421, loss 0.017401, acc 1
2016-09-07T02:23:41.375831: step 5422, loss 0.0349079, acc 0.96
2016-09-07T02:23:42.064331: step 5423, loss 0.0313726, acc 0.98
2016-09-07T02:23:42.743761: step 5424, loss 0.0302137, acc 0.98
2016-09-07T02:23:43.431287: step 5425, loss 0.000659493, acc 1
2016-09-07T02:23:44.113308: step 5426, loss 0.01651, acc 1
2016-09-07T02:23:44.805978: step 5427, loss 0.0721419, acc 0.96
2016-09-07T02:23:45.516515: step 5428, loss 7.46468e-06, acc 1
2016-09-07T02:23:46.192744: step 5429, loss 0.0189189, acc 0.98
2016-09-07T02:23:46.880991: step 5430, loss 0.00258774, acc 1
2016-09-07T02:23:47.566203: step 5431, loss 0.0185326, acc 0.98
2016-09-07T02:23:48.271046: step 5432, loss 0.0193368, acc 0.98
2016-09-07T02:23:48.976000: step 5433, loss 0.0192814, acc 0.98
2016-09-07T02:23:49.648890: step 5434, loss 0.0490485, acc 0.96
2016-09-07T02:23:50.356839: step 5435, loss 0.0231441, acc 0.98
2016-09-07T02:23:51.067629: step 5436, loss 0.00316193, acc 1
2016-09-07T02:23:51.753216: step 5437, loss 0.0902972, acc 0.96
2016-09-07T02:23:52.432890: step 5438, loss 0.0139127, acc 1
2016-09-07T02:23:53.112220: step 5439, loss 0.0902618, acc 0.94
2016-09-07T02:23:53.813986: step 5440, loss 0.100544, acc 0.96
2016-09-07T02:23:54.486040: step 5441, loss 0.0480662, acc 0.98
2016-09-07T02:23:55.190907: step 5442, loss 0.0660771, acc 0.98
2016-09-07T02:23:55.888798: step 5443, loss 0.0772169, acc 0.96
2016-09-07T02:23:56.581191: step 5444, loss 0.0201438, acc 0.98
2016-09-07T02:23:57.272311: step 5445, loss 0.00585388, acc 1
2016-09-07T02:23:57.965264: step 5446, loss 0.0469363, acc 0.94
2016-09-07T02:23:58.687403: step 5447, loss 0.0144521, acc 1
2016-09-07T02:23:59.388460: step 5448, loss 0.0441162, acc 0.96
2016-09-07T02:24:00.068186: step 5449, loss 0.00149289, acc 1
2016-09-07T02:24:00.795540: step 5450, loss 0.0814578, acc 0.98
2016-09-07T02:24:01.493374: step 5451, loss 0.0465284, acc 0.98
2016-09-07T02:24:02.199491: step 5452, loss 0.0437267, acc 0.96
2016-09-07T02:24:02.896326: step 5453, loss 0.0184029, acc 1
2016-09-07T02:24:03.608464: step 5454, loss 0.0045569, acc 1
2016-09-07T02:24:04.297683: step 5455, loss 0.00636477, acc 1
2016-09-07T02:24:04.987383: step 5456, loss 0.0486552, acc 0.98
2016-09-07T02:24:05.682824: step 5457, loss 0.102519, acc 0.94
2016-09-07T02:24:06.380154: step 5458, loss 0.000601082, acc 1
2016-09-07T02:24:07.079849: step 5459, loss 0.0450882, acc 0.98
2016-09-07T02:24:07.777796: step 5460, loss 0.0104846, acc 1
2016-09-07T02:24:08.465787: step 5461, loss 0.00325844, acc 1
2016-09-07T02:24:09.154403: step 5462, loss 0.00410614, acc 1
2016-09-07T02:24:09.856271: step 5463, loss 0.00964286, acc 1
2016-09-07T02:24:10.580243: step 5464, loss 0.0794196, acc 0.98
2016-09-07T02:24:11.259506: step 5465, loss 0.00392607, acc 1
2016-09-07T02:24:11.970016: step 5466, loss 0.0846741, acc 0.94
2016-09-07T02:24:12.674190: step 5467, loss 0.00508789, acc 1
2016-09-07T02:24:13.400037: step 5468, loss 0.143184, acc 0.94
2016-09-07T02:24:14.101992: step 5469, loss 0.0241203, acc 1
2016-09-07T02:24:14.839882: step 5470, loss 0.0979955, acc 0.96
2016-09-07T02:24:15.543847: step 5471, loss 0.0288223, acc 0.98
2016-09-07T02:24:16.246254: step 5472, loss 0.0236293, acc 0.98
2016-09-07T02:24:16.925855: step 5473, loss 0.0277647, acc 1
2016-09-07T02:24:17.609861: step 5474, loss 0.031576, acc 0.98
2016-09-07T02:24:18.299544: step 5475, loss 0.0314026, acc 0.98
2016-09-07T02:24:19.003285: step 5476, loss 0.0352796, acc 0.98
2016-09-07T02:24:19.674639: step 5477, loss 0.02899, acc 0.98
2016-09-07T02:24:20.390829: step 5478, loss 0.0924705, acc 0.98
2016-09-07T02:24:21.097849: step 5479, loss 0.00345639, acc 1
2016-09-07T02:24:21.787211: step 5480, loss 0.01424, acc 1
2016-09-07T02:24:22.484582: step 5481, loss 0.0270188, acc 1
2016-09-07T02:24:23.163370: step 5482, loss 0.018131, acc 1
2016-09-07T02:24:23.856255: step 5483, loss 0.00265011, acc 1
2016-09-07T02:24:24.516410: step 5484, loss 0.0613301, acc 0.96
2016-09-07T02:24:25.218191: step 5485, loss 0.0650698, acc 0.96
2016-09-07T02:24:25.914880: step 5486, loss 0.0479521, acc 0.96
2016-09-07T02:24:26.606522: step 5487, loss 0.0532173, acc 0.96
2016-09-07T02:24:27.292038: step 5488, loss 0.00524971, acc 1
2016-09-07T02:24:28.006103: step 5489, loss 0.0334928, acc 1
2016-09-07T02:24:28.730387: step 5490, loss 0.0340463, acc 0.98
2016-09-07T02:24:29.400790: step 5491, loss 0.034265, acc 0.98
2016-09-07T02:24:30.088454: step 5492, loss 0.0782188, acc 0.94
2016-09-07T02:24:30.774312: step 5493, loss 0.035906, acc 0.98
2016-09-07T02:24:31.472075: step 5494, loss 0.0175034, acc 1
2016-09-07T02:24:32.156625: step 5495, loss 0.00155605, acc 1
2016-09-07T02:24:32.856034: step 5496, loss 0.0164841, acc 1
2016-09-07T02:24:33.548782: step 5497, loss 0.0104869, acc 1
2016-09-07T02:24:34.225975: step 5498, loss 0.0280972, acc 0.98
2016-09-07T02:24:34.921595: step 5499, loss 0.00913483, acc 1
2016-09-07T02:24:35.615152: step 5500, loss 0.0300852, acc 0.98

Evaluation:
2016-09-07T02:24:38.806315: step 5500, loss 1.61152, acc 0.768293

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-5500

2016-09-07T02:24:40.466762: step 5501, loss 0.0431277, acc 1
2016-09-07T02:24:41.177909: step 5502, loss 0.0220758, acc 0.98
2016-09-07T02:24:41.906851: step 5503, loss 0.0149111, acc 1
2016-09-07T02:24:42.586420: step 5504, loss 0.0178374, acc 0.98
2016-09-07T02:24:43.287966: step 5505, loss 0.00734, acc 1
2016-09-07T02:24:43.984792: step 5506, loss 0.0319494, acc 0.98
2016-09-07T02:24:44.682636: step 5507, loss 0.109537, acc 0.98
2016-09-07T02:24:45.370694: step 5508, loss 0.0113525, acc 1
2016-09-07T02:24:46.087540: step 5509, loss 0.0297348, acc 0.98
2016-09-07T02:24:46.808687: step 5510, loss 0.0174847, acc 0.98
2016-09-07T02:24:47.492680: step 5511, loss 0.0115118, acc 1
2016-09-07T02:24:48.172343: step 5512, loss 0.00750988, acc 1
2016-09-07T02:24:48.864870: step 5513, loss 0.0794856, acc 0.96
2016-09-07T02:24:49.563548: step 5514, loss 0.0563067, acc 0.96
2016-09-07T02:24:50.285076: step 5515, loss 0.0027512, acc 1
2016-09-07T02:24:50.972862: step 5516, loss 0.0301795, acc 0.98
2016-09-07T02:24:51.690566: step 5517, loss 0.0199815, acc 0.98
2016-09-07T02:24:52.378200: step 5518, loss 0.00458196, acc 1
2016-09-07T02:24:53.055015: step 5519, loss 0.0328343, acc 0.96
2016-09-07T02:24:53.772231: step 5520, loss 0.0161346, acc 1
2016-09-07T02:24:54.467537: step 5521, loss 0.04926, acc 0.96
2016-09-07T02:24:55.186397: step 5522, loss 0.0346471, acc 0.98
2016-09-07T02:24:55.887916: step 5523, loss 0.0178273, acc 0.98
2016-09-07T02:24:56.568598: step 5524, loss 0.0992648, acc 0.96
2016-09-07T02:24:57.274905: step 5525, loss 0.0458485, acc 0.96
2016-09-07T02:24:57.965882: step 5526, loss 0.000197413, acc 1
2016-09-07T02:24:58.676481: step 5527, loss 0.048432, acc 0.96
2016-09-07T02:24:59.351964: step 5528, loss 0.0152556, acc 1
2016-09-07T02:25:00.069777: step 5529, loss 0.000348095, acc 1
2016-09-07T02:25:00.796627: step 5530, loss 0.0197588, acc 1
2016-09-07T02:25:01.520262: step 5531, loss 0.0592046, acc 0.98
2016-09-07T02:25:02.199397: step 5532, loss 0.0363543, acc 0.98
2016-09-07T02:25:02.889904: step 5533, loss 0.00306959, acc 1
2016-09-07T02:25:03.597971: step 5534, loss 0.196822, acc 0.98
2016-09-07T02:25:04.272803: step 5535, loss 0.00231723, acc 1
2016-09-07T02:25:04.971613: step 5536, loss 0.0652328, acc 0.96
2016-09-07T02:25:05.671571: step 5537, loss 0.000935819, acc 1
2016-09-07T02:25:06.354378: step 5538, loss 0.00390075, acc 1
2016-09-07T02:25:07.037339: step 5539, loss 0.018147, acc 0.98
2016-09-07T02:25:07.715044: step 5540, loss 0.00868358, acc 1
2016-09-07T02:25:08.418942: step 5541, loss 0.016866, acc 1
2016-09-07T02:25:09.101410: step 5542, loss 0.0704585, acc 0.96
2016-09-07T02:25:09.786482: step 5543, loss 0.031481, acc 1
2016-09-07T02:25:10.488437: step 5544, loss 0.021241, acc 1
2016-09-07T02:25:11.202961: step 5545, loss 0.00351491, acc 1
2016-09-07T02:25:11.868994: step 5546, loss 0.0236258, acc 1
2016-09-07T02:25:12.539412: step 5547, loss 0.0218448, acc 1
2016-09-07T02:25:13.252286: step 5548, loss 0.0535863, acc 0.96
2016-09-07T02:25:13.931742: step 5549, loss 0.00673718, acc 1
2016-09-07T02:25:14.621507: step 5550, loss 0.0537064, acc 0.98
2016-09-07T02:25:15.314273: step 5551, loss 0.00214447, acc 1
2016-09-07T02:25:15.996227: step 5552, loss 0.0413234, acc 0.98
2016-09-07T02:25:16.685254: step 5553, loss 0.0270831, acc 0.98
2016-09-07T02:25:17.387322: step 5554, loss 0.0264777, acc 1
2016-09-07T02:25:18.101703: step 5555, loss 0.0145043, acc 1
2016-09-07T02:25:18.789482: step 5556, loss 0.017193, acc 0.98
2016-09-07T02:25:19.488148: step 5557, loss 0.0511053, acc 0.98
2016-09-07T02:25:20.178569: step 5558, loss 0.00144919, acc 1
2016-09-07T02:25:20.866101: step 5559, loss 0.022454, acc 0.98
2016-09-07T02:25:21.565580: step 5560, loss 0.00506366, acc 1
2016-09-07T02:25:22.234296: step 5561, loss 0.0113643, acc 1
2016-09-07T02:25:22.946619: step 5562, loss 0.031339, acc 0.98
2016-09-07T02:25:23.625192: step 5563, loss 0.0296258, acc 0.98
2016-09-07T02:25:24.332413: step 5564, loss 0.00321221, acc 1
2016-09-07T02:25:25.024986: step 5565, loss 0.011377, acc 1
2016-09-07T02:25:25.719922: step 5566, loss 0.0351495, acc 0.98
2016-09-07T02:25:26.424296: step 5567, loss 0.00172009, acc 1
2016-09-07T02:25:27.048778: step 5568, loss 0.000765956, acc 1
2016-09-07T02:25:27.749924: step 5569, loss 0.0377811, acc 0.98
2016-09-07T02:25:28.431023: step 5570, loss 0.00866102, acc 1
2016-09-07T02:25:29.112868: step 5571, loss 0.0299351, acc 0.98
2016-09-07T02:25:29.794244: step 5572, loss 0.030766, acc 0.98
2016-09-07T02:25:30.490751: step 5573, loss 0.00404033, acc 1
2016-09-07T02:25:31.193801: step 5574, loss 0.0284795, acc 0.98
2016-09-07T02:25:31.860931: step 5575, loss 0.0118757, acc 1
2016-09-07T02:25:32.575526: step 5576, loss 0.00902366, acc 1
2016-09-07T02:25:33.282451: step 5577, loss 0.00216386, acc 1
2016-09-07T02:25:34.007244: step 5578, loss 0.0279289, acc 1
2016-09-07T02:25:34.715695: step 5579, loss 0.0186694, acc 1
2016-09-07T02:25:35.396606: step 5580, loss 0.012335, acc 1
2016-09-07T02:25:36.128343: step 5581, loss 0.0471875, acc 0.96
2016-09-07T02:25:36.818379: step 5582, loss 0.00691843, acc 1
2016-09-07T02:25:37.511944: step 5583, loss 0.0565689, acc 0.98
2016-09-07T02:25:38.198633: step 5584, loss 0.0225099, acc 1
2016-09-07T02:25:38.902600: step 5585, loss 0.026799, acc 0.98
2016-09-07T02:25:39.605296: step 5586, loss 0.0882819, acc 0.96
2016-09-07T02:25:40.279801: step 5587, loss 0.0314091, acc 0.98
2016-09-07T02:25:40.992019: step 5588, loss 0.0759466, acc 0.98
2016-09-07T02:25:41.689100: step 5589, loss 0.00224352, acc 1
2016-09-07T02:25:42.387207: step 5590, loss 0.0241009, acc 0.98
2016-09-07T02:25:43.075886: step 5591, loss 0.00259838, acc 1
2016-09-07T02:25:43.759066: step 5592, loss 0.0011976, acc 1
2016-09-07T02:25:44.464539: step 5593, loss 0.0815734, acc 0.98
2016-09-07T02:25:45.146303: step 5594, loss 0.00163068, acc 1
2016-09-07T02:25:45.858792: step 5595, loss 0.000489643, acc 1
2016-09-07T02:25:46.562726: step 5596, loss 0.000201204, acc 1
2016-09-07T02:25:47.290560: step 5597, loss 0.000622637, acc 1
2016-09-07T02:25:47.985050: step 5598, loss 0.0240918, acc 0.98
2016-09-07T02:25:48.664097: step 5599, loss 0.0254145, acc 0.98
2016-09-07T02:25:49.397740: step 5600, loss 0.0689738, acc 0.96

Evaluation:
2016-09-07T02:25:52.600053: step 5600, loss 1.77041, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-5600

2016-09-07T02:25:54.379019: step 5601, loss 0.0313026, acc 0.98
2016-09-07T02:25:55.079865: step 5602, loss 0.00155212, acc 1
2016-09-07T02:25:55.759301: step 5603, loss 0.013063, acc 1
2016-09-07T02:25:56.449970: step 5604, loss 0.0289672, acc 0.98
2016-09-07T02:25:57.110694: step 5605, loss 0.0233161, acc 0.98
2016-09-07T02:25:57.825094: step 5606, loss 0.00893785, acc 1
2016-09-07T02:25:58.522475: step 5607, loss 0.00243727, acc 1
2016-09-07T02:25:59.227499: step 5608, loss 2.41439e-05, acc 1
2016-09-07T02:25:59.928714: step 5609, loss 0.0430213, acc 0.98
2016-09-07T02:26:00.653071: step 5610, loss 0.00745355, acc 1
2016-09-07T02:26:01.372538: step 5611, loss 0.0056579, acc 1
2016-09-07T02:26:02.094459: step 5612, loss 0.0298344, acc 0.98
2016-09-07T02:26:02.781811: step 5613, loss 0.0126998, acc 1
2016-09-07T02:26:03.476831: step 5614, loss 0.00869442, acc 1
2016-09-07T02:26:04.185111: step 5615, loss 0.0172728, acc 0.98
2016-09-07T02:26:04.889323: step 5616, loss 0.0233825, acc 0.98
2016-09-07T02:26:05.568169: step 5617, loss 0.00138853, acc 1
2016-09-07T02:26:06.278682: step 5618, loss 0.0425281, acc 0.98
2016-09-07T02:26:06.981764: step 5619, loss 0.0101613, acc 1
2016-09-07T02:26:07.657787: step 5620, loss 0.0164242, acc 0.98
2016-09-07T02:26:08.343880: step 5621, loss 0.000372203, acc 1
2016-09-07T02:26:09.034635: step 5622, loss 0.0281358, acc 0.98
2016-09-07T02:26:09.753543: step 5623, loss 0.0265673, acc 0.98
2016-09-07T02:26:10.453452: step 5624, loss 0.0254848, acc 0.98
2016-09-07T02:26:11.163712: step 5625, loss 0.026205, acc 0.98
2016-09-07T02:26:11.844331: step 5626, loss 0.00207593, acc 1
2016-09-07T02:26:12.548807: step 5627, loss 0.0350501, acc 0.98
2016-09-07T02:26:13.245054: step 5628, loss 0.0123349, acc 1
2016-09-07T02:26:13.924953: step 5629, loss 0.0448418, acc 0.98
2016-09-07T02:26:14.632998: step 5630, loss 0.0609019, acc 0.96
2016-09-07T02:26:15.315418: step 5631, loss 0.054274, acc 0.94
2016-09-07T02:26:16.008319: step 5632, loss 0.00427966, acc 1
2016-09-07T02:26:16.718976: step 5633, loss 0.0248465, acc 1
2016-09-07T02:26:17.432517: step 5634, loss 0.00923645, acc 1
2016-09-07T02:26:18.153205: step 5635, loss 0.0384396, acc 0.98
2016-09-07T02:26:18.842347: step 5636, loss 0.0117192, acc 1
2016-09-07T02:26:19.533885: step 5637, loss 0.0491275, acc 0.98
2016-09-07T02:26:20.231957: step 5638, loss 0.104413, acc 0.94
2016-09-07T02:26:20.931549: step 5639, loss 0.0224744, acc 0.98
2016-09-07T02:26:21.643825: step 5640, loss 0.0250196, acc 0.98
2016-09-07T02:26:22.299326: step 5641, loss 0.00130325, acc 1
2016-09-07T02:26:23.014840: step 5642, loss 0.00937941, acc 1
2016-09-07T02:26:23.689946: step 5643, loss 0.00686359, acc 1
2016-09-07T02:26:24.356606: step 5644, loss 0.0392628, acc 0.98
2016-09-07T02:26:25.058465: step 5645, loss 0.00624455, acc 1
2016-09-07T02:26:25.758268: step 5646, loss 0.0247348, acc 0.98
2016-09-07T02:26:26.451469: step 5647, loss 0.00291451, acc 1
2016-09-07T02:26:27.132679: step 5648, loss 0.026374, acc 0.98
2016-09-07T02:26:27.852152: step 5649, loss 0.00467126, acc 1
2016-09-07T02:26:28.535099: step 5650, loss 0.014784, acc 1
2016-09-07T02:26:29.242692: step 5651, loss 0.0286703, acc 0.98
2016-09-07T02:26:29.932903: step 5652, loss 0.00171403, acc 1
2016-09-07T02:26:30.605185: step 5653, loss 0.0187622, acc 1
2016-09-07T02:26:31.297300: step 5654, loss 0.057256, acc 0.96
2016-09-07T02:26:31.975887: step 5655, loss 0.0045418, acc 1
2016-09-07T02:26:32.672842: step 5656, loss 0.0034698, acc 1
2016-09-07T02:26:33.343038: step 5657, loss 0.0158185, acc 0.98
2016-09-07T02:26:34.016803: step 5658, loss 0.00103407, acc 1
2016-09-07T02:26:34.731080: step 5659, loss 0.0211696, acc 0.98
2016-09-07T02:26:35.428539: step 5660, loss 0.0141065, acc 1
2016-09-07T02:26:36.124432: step 5661, loss 0.00299701, acc 1
2016-09-07T02:26:36.794091: step 5662, loss 0.00978748, acc 1
2016-09-07T02:26:37.497580: step 5663, loss 0.0143908, acc 1
2016-09-07T02:26:38.202931: step 5664, loss 0.00909739, acc 1
2016-09-07T02:26:38.874303: step 5665, loss 0.0162411, acc 1
2016-09-07T02:26:39.563469: step 5666, loss 0.101657, acc 0.92
2016-09-07T02:26:40.268488: step 5667, loss 0.0377547, acc 0.98
2016-09-07T02:26:40.983514: step 5668, loss 0.0829535, acc 0.98
2016-09-07T02:26:41.656366: step 5669, loss 0.0440572, acc 0.96
2016-09-07T02:26:42.358592: step 5670, loss 0.0175388, acc 1
2016-09-07T02:26:43.041715: step 5671, loss 0.0539303, acc 0.96
2016-09-07T02:26:43.742045: step 5672, loss 0.0218676, acc 1
2016-09-07T02:26:44.437389: step 5673, loss 0.000988088, acc 1
2016-09-07T02:26:45.141151: step 5674, loss 0.00480906, acc 1
2016-09-07T02:26:45.841885: step 5675, loss 0.02719, acc 0.98
2016-09-07T02:26:46.520973: step 5676, loss 0.0349715, acc 0.98
2016-09-07T02:26:47.219424: step 5677, loss 0.0243999, acc 0.98
2016-09-07T02:26:47.908159: step 5678, loss 0.0308416, acc 0.98
2016-09-07T02:26:48.604157: step 5679, loss 0.0419827, acc 0.98
2016-09-07T02:26:49.280949: step 5680, loss 0.00806461, acc 1
2016-09-07T02:26:49.980159: step 5681, loss 0.00645976, acc 1
2016-09-07T02:26:50.678475: step 5682, loss 0.0133353, acc 1
2016-09-07T02:26:51.370617: step 5683, loss 0.0226622, acc 0.98
2016-09-07T02:26:52.067238: step 5684, loss 0.0152495, acc 1
2016-09-07T02:26:52.773457: step 5685, loss 0.0117815, acc 1
2016-09-07T02:26:53.462127: step 5686, loss 0.0368547, acc 0.98
2016-09-07T02:26:54.166393: step 5687, loss 0.002846, acc 1
2016-09-07T02:26:54.840045: step 5688, loss 0.0829431, acc 0.96
2016-09-07T02:26:55.555520: step 5689, loss 0.024504, acc 0.98
2016-09-07T02:26:56.248654: step 5690, loss 0.0338026, acc 0.98
2016-09-07T02:26:56.956853: step 5691, loss 0.00962259, acc 1
2016-09-07T02:26:57.655501: step 5692, loss 0.00101059, acc 1
2016-09-07T02:26:58.334428: step 5693, loss 0.00413967, acc 1
2016-09-07T02:26:59.059240: step 5694, loss 0.00742933, acc 1
2016-09-07T02:26:59.731009: step 5695, loss 0.000512193, acc 1
2016-09-07T02:27:00.462031: step 5696, loss 0.00114656, acc 1
2016-09-07T02:27:01.153218: step 5697, loss 9.073e-05, acc 1
2016-09-07T02:27:01.859101: step 5698, loss 0.0994285, acc 0.98
2016-09-07T02:27:02.557380: step 5699, loss 0.078892, acc 0.94
2016-09-07T02:27:03.234611: step 5700, loss 0.0131629, acc 1

Evaluation:
2016-09-07T02:27:06.444629: step 5700, loss 2.02496, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473182186/checkpoints/model-5700

2016-09-07T02:27:08.113019: step 5701, loss 0.0044807, acc 1
2016-09-07T02:27:08.822140: step 5702, loss 0.0346786, acc 0.96
2016-09-07T02:27:09.509903: step 5703, loss 0.0246256, acc 0.98
2016-09-07T02:27:10.184252: step 5704, loss 0.0266026, acc 0.98
2016-09-07T02:27:10.881741: step 5705, loss 0.135866, acc 0.98
2016-09-07T02:27:11.588470: step 5706, loss 0.0186095, acc 0.98
2016-09-07T02:27:12.305517: step 5707, loss 0.00245882, acc 1
2016-09-07T02:27:12.980850: step 5708, loss 0.0367858, acc 0.98
2016-09-07T02:27:13.663935: step 5709, loss 0.00867662, acc 1
2016-09-07T02:27:14.357343: step 5710, loss 0.0154765, acc 1
2016-09-07T02:27:15.027234: step 5711, loss 0.0203582, acc 1
2016-09-07T02:27:15.703106: step 5712, loss 0.0544916, acc 0.98
2016-09-07T02:27:16.390431: step 5713, loss 0.0135745, acc 1
2016-09-07T02:27:17.096962: step 5714, loss 0.00634436, acc 1
2016-09-07T02:27:17.781647: step 5715, loss 0.0182448, acc 0.98
2016-09-07T02:27:18.463462: step 5716, loss 0.00917697, acc 1
2016-09-07T02:27:19.159276: step 5717, loss 0.00715882, acc 1
2016-09-07T02:27:19.834253: step 5718, loss 0.00516124, acc 1
2016-09-07T02:27:20.531611: step 5719, loss 0.0207826, acc 1
2016-09-07T02:27:21.218425: step 5720, loss 0.000879474, acc 1
2016-09-07T02:27:21.924974: step 5721, loss 0.0105436, acc 1
2016-09-07T02:27:22.621300: step 5722, loss 0.0790444, acc 0.98
2016-09-07T02:27:23.308780: step 5723, loss 0.0164015, acc 1
2016-09-07T02:27:24.012293: step 5724, loss 0.030078, acc 0.98
2016-09-07T02:27:24.717057: step 5725, loss 0.0293715, acc 0.98
2016-09-07T02:27:25.395480: step 5726, loss 0.0169647, acc 0.98
2016-09-07T02:27:26.055679: step 5727, loss 0.00600614, acc 1
2016-09-07T02:27:26.765118: step 5728, loss 0.0381189, acc 0.98
2016-09-07T02:27:27.440816: step 5729, loss 0.000347402, acc 1
2016-09-07T02:27:28.117779: step 5730, loss 0.0272148, acc 0.98
2016-09-07T02:27:28.815954: step 5731, loss 0.00884327, acc 1
2016-09-07T02:27:29.495569: step 5732, loss 0.0195828, acc 0.98
2016-09-07T02:27:30.181358: step 5733, loss 0.0490879, acc 0.98
2016-09-07T02:27:30.885818: step 5734, loss 0.0404868, acc 0.98
2016-09-07T02:27:31.592774: step 5735, loss 0.00659841, acc 1
2016-09-07T02:27:32.264096: step 5736, loss 0.016051, acc 1
2016-09-07T02:27:32.979798: step 5737, loss 0.0278516, acc 0.98
2016-09-07T02:27:33.675535: step 5738, loss 0.0423693, acc 0.96
2016-09-07T02:27:34.363505: step 5739, loss 0.0221793, acc 0.98
2016-09-07T02:27:35.060247: step 5740, loss 0.000471298, acc 1
2016-09-07T02:27:35.732549: step 5741, loss 0.0905914, acc 0.96
2016-09-07T02:27:36.451305: step 5742, loss 0.0150447, acc 1
2016-09-07T02:27:37.160901: step 5743, loss 0.0141629, acc 0.98
2016-09-07T02:27:37.862792: step 5744, loss 0.0168981, acc 1
2016-09-07T02:27:38.549787: step 5745, loss 0.01712, acc 0.98
2016-09-07T02:27:39.253725: step 5746, loss 0.0156477, acc 1
2016-09-07T02:27:39.969584: step 5747, loss 0.0346137, acc 0.96
2016-09-07T02:27:40.658662: step 5748, loss 0.0485102, acc 0.96
2016-09-07T02:27:41.358753: step 5749, loss 0.00412448, acc 1
2016-09-07T02:27:42.059837: step 5750, loss 0.000949208, acc 1
2016-09-07T02:27:42.738399: step 5751, loss 0.00502857, acc 1
2016-09-07T02:27:43.428954: step 5752, loss 0.046718, acc 0.96
2016-09-07T02:27:44.107772: step 5753, loss 0.0178497, acc 0.98
2016-09-07T02:27:44.812318: step 5754, loss 0.0165843, acc 0.98
2016-09-07T02:27:45.491093: step 5755, loss 0.0284101, acc 0.98
2016-09-07T02:27:46.173403: step 5756, loss 0.000955096, acc 1
2016-09-07T02:27:46.871782: step 5757, loss 0.0251832, acc 0.98
2016-09-07T02:27:47.555070: step 5758, loss 0.0168806, acc 0.98
2016-09-07T02:27:48.240364: step 5759, loss 0.0177891, acc 1
2016-09-07T02:27:48.858540: step 5760, loss 0.000100267, acc 1

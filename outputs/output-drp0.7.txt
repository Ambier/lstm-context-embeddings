WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f4355c9ee90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f4355c9eed0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
DROPOUT_KEEP_PROB=0.7
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=30
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9662/1000
Writing to /home/cil/lstm-context-embeddings/runs/1473241413

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-07T17:43:52.336259: step 1, loss 0.693147, acc 0.44
2016-09-07T17:43:53.028048: step 2, loss 0.715437, acc 0.44
2016-09-07T17:43:53.705037: step 3, loss 0.703805, acc 0.4
2016-09-07T17:43:54.375954: step 4, loss 0.691336, acc 0.54
2016-09-07T17:43:55.055144: step 5, loss 0.666894, acc 0.64
2016-09-07T17:43:55.745119: step 6, loss 0.692645, acc 0.56
2016-09-07T17:43:56.400895: step 7, loss 0.749738, acc 0.46
2016-09-07T17:43:57.090016: step 8, loss 0.756596, acc 0.42
2016-09-07T17:43:57.780076: step 9, loss 0.680736, acc 0.6
2016-09-07T17:43:58.460418: step 10, loss 0.697948, acc 0.5
2016-09-07T17:43:59.127687: step 11, loss 0.706635, acc 0.4
2016-09-07T17:43:59.800881: step 12, loss 0.685683, acc 0.68
2016-09-07T17:44:00.504488: step 13, loss 0.691531, acc 0.54
2016-09-07T17:44:01.190829: step 14, loss 0.684072, acc 0.56
2016-09-07T17:44:01.869819: step 15, loss 0.687977, acc 0.54
2016-09-07T17:44:02.590558: step 16, loss 0.697226, acc 0.52
2016-09-07T17:44:03.266926: step 17, loss 0.689115, acc 0.5
2016-09-07T17:44:03.926492: step 18, loss 0.738567, acc 0.38
2016-09-07T17:44:04.620381: step 19, loss 0.707325, acc 0.44
2016-09-07T17:44:05.296299: step 20, loss 0.683478, acc 0.62
2016-09-07T17:44:05.981480: step 21, loss 0.683051, acc 0.6
2016-09-07T17:44:06.645636: step 22, loss 0.678913, acc 0.58
2016-09-07T17:44:07.360196: step 23, loss 0.692216, acc 0.52
2016-09-07T17:44:08.036280: step 24, loss 0.701214, acc 0.48
2016-09-07T17:44:08.721206: step 25, loss 0.690518, acc 0.5
2016-09-07T17:44:09.380699: step 26, loss 0.684137, acc 0.48
2016-09-07T17:44:10.059632: step 27, loss 0.659625, acc 0.6
2016-09-07T17:44:10.742207: step 28, loss 0.686788, acc 0.54
2016-09-07T17:44:11.410562: step 29, loss 0.664222, acc 0.6
2016-09-07T17:44:12.088951: step 30, loss 0.680408, acc 0.54
2016-09-07T17:44:12.759115: step 31, loss 0.620566, acc 0.62
2016-09-07T17:44:13.468279: step 32, loss 0.67722, acc 0.62
2016-09-07T17:44:14.149387: step 33, loss 0.6614, acc 0.56
2016-09-07T17:44:14.841788: step 34, loss 0.625556, acc 0.64
2016-09-07T17:44:15.544378: step 35, loss 0.593177, acc 0.7
2016-09-07T17:44:16.231916: step 36, loss 0.63775, acc 0.66
2016-09-07T17:44:16.918881: step 37, loss 0.609195, acc 0.7
2016-09-07T17:44:17.578067: step 38, loss 0.651503, acc 0.64
2016-09-07T17:44:18.278765: step 39, loss 0.565253, acc 0.76
2016-09-07T17:44:18.960840: step 40, loss 0.614992, acc 0.64
2016-09-07T17:44:19.646892: step 41, loss 0.565421, acc 0.66
2016-09-07T17:44:20.330441: step 42, loss 0.574333, acc 0.7
2016-09-07T17:44:20.996072: step 43, loss 0.575443, acc 0.72
2016-09-07T17:44:21.670983: step 44, loss 0.586345, acc 0.74
2016-09-07T17:44:22.336400: step 45, loss 0.614402, acc 0.72
2016-09-07T17:44:23.022742: step 46, loss 0.465184, acc 0.78
2016-09-07T17:44:23.690708: step 47, loss 0.459336, acc 0.82
2016-09-07T17:44:24.377127: step 48, loss 0.669906, acc 0.64
2016-09-07T17:44:25.060264: step 49, loss 0.685411, acc 0.76
2016-09-07T17:44:25.737099: step 50, loss 0.78113, acc 0.56
2016-09-07T17:44:26.420155: step 51, loss 0.706661, acc 0.7
2016-09-07T17:44:27.114931: step 52, loss 0.526462, acc 0.74
2016-09-07T17:44:27.821867: step 53, loss 0.495663, acc 0.8
2016-09-07T17:44:28.504319: step 54, loss 0.635301, acc 0.72
2016-09-07T17:44:29.166430: step 55, loss 0.588292, acc 0.7
2016-09-07T17:44:29.843916: step 56, loss 0.530034, acc 0.72
2016-09-07T17:44:30.534053: step 57, loss 0.561975, acc 0.78
2016-09-07T17:44:31.220048: step 58, loss 0.567433, acc 0.62
2016-09-07T17:44:31.894829: step 59, loss 0.572424, acc 0.78
2016-09-07T17:44:32.600168: step 60, loss 0.600519, acc 0.74
2016-09-07T17:44:33.294905: step 61, loss 0.622755, acc 0.64
2016-09-07T17:44:33.974790: step 62, loss 0.55558, acc 0.72
2016-09-07T17:44:34.633127: step 63, loss 0.536083, acc 0.78
2016-09-07T17:44:35.319314: step 64, loss 0.562004, acc 0.66
2016-09-07T17:44:36.016520: step 65, loss 0.460206, acc 0.82
2016-09-07T17:44:36.683416: step 66, loss 0.581738, acc 0.62
2016-09-07T17:44:37.385335: step 67, loss 0.490308, acc 0.74
2016-09-07T17:44:38.047784: step 68, loss 0.621285, acc 0.68
2016-09-07T17:44:38.742556: step 69, loss 0.415246, acc 0.82
2016-09-07T17:44:39.417925: step 70, loss 0.45787, acc 0.78
2016-09-07T17:44:40.109559: step 71, loss 0.559706, acc 0.7
2016-09-07T17:44:40.780139: step 72, loss 0.497027, acc 0.74
2016-09-07T17:44:41.460047: step 73, loss 0.419925, acc 0.8
2016-09-07T17:44:42.145487: step 74, loss 0.406433, acc 0.82
2016-09-07T17:44:42.817149: step 75, loss 0.506814, acc 0.72
2016-09-07T17:44:43.552379: step 76, loss 0.63351, acc 0.68
2016-09-07T17:44:44.229960: step 77, loss 0.466922, acc 0.78
2016-09-07T17:44:44.892071: step 78, loss 0.636422, acc 0.68
2016-09-07T17:44:45.576137: step 79, loss 0.477425, acc 0.76
2016-09-07T17:44:46.277349: step 80, loss 0.648883, acc 0.68
2016-09-07T17:44:46.950966: step 81, loss 0.523912, acc 0.7
2016-09-07T17:44:47.623685: step 82, loss 0.450814, acc 0.74
2016-09-07T17:44:48.321512: step 83, loss 0.529102, acc 0.78
2016-09-07T17:44:49.012141: step 84, loss 0.479783, acc 0.74
2016-09-07T17:44:49.693122: step 85, loss 0.485276, acc 0.8
2016-09-07T17:44:50.387364: step 86, loss 0.402363, acc 0.8
2016-09-07T17:44:51.073700: step 87, loss 0.567455, acc 0.7
2016-09-07T17:44:51.761433: step 88, loss 0.382548, acc 0.84
2016-09-07T17:44:52.417711: step 89, loss 0.414295, acc 0.82
2016-09-07T17:44:53.119704: step 90, loss 0.604582, acc 0.64
2016-09-07T17:44:53.780432: step 91, loss 0.456163, acc 0.78
2016-09-07T17:44:54.465398: step 92, loss 0.514571, acc 0.7
2016-09-07T17:44:55.161323: step 93, loss 0.507578, acc 0.78
2016-09-07T17:44:55.834161: step 94, loss 0.474534, acc 0.7
2016-09-07T17:44:56.512391: step 95, loss 0.386088, acc 0.84
2016-09-07T17:44:57.179684: step 96, loss 0.435067, acc 0.8
2016-09-07T17:44:57.871352: step 97, loss 0.508623, acc 0.72
2016-09-07T17:44:58.542170: step 98, loss 0.470461, acc 0.78
2016-09-07T17:44:59.236071: step 99, loss 0.575508, acc 0.7
2016-09-07T17:44:59.924660: step 100, loss 0.499484, acc 0.74

Evaluation:
2016-09-07T17:45:02.933225: step 100, loss 0.486041, acc 0.775

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-100

2016-09-07T17:45:04.648675: step 101, loss 0.544152, acc 0.78
2016-09-07T17:45:05.324566: step 102, loss 0.689134, acc 0.66
2016-09-07T17:45:06.027818: step 103, loss 0.449129, acc 0.8
2016-09-07T17:45:06.713221: step 104, loss 0.421297, acc 0.74
2016-09-07T17:45:07.396115: step 105, loss 0.46784, acc 0.74
2016-09-07T17:45:08.050304: step 106, loss 0.434434, acc 0.76
2016-09-07T17:45:08.741523: step 107, loss 0.471685, acc 0.8
2016-09-07T17:45:09.405200: step 108, loss 0.409739, acc 0.8
2016-09-07T17:45:10.102920: step 109, loss 0.482748, acc 0.8
2016-09-07T17:45:10.802720: step 110, loss 0.472233, acc 0.8
2016-09-07T17:45:11.467414: step 111, loss 0.504779, acc 0.8
2016-09-07T17:45:12.152117: step 112, loss 0.478302, acc 0.78
2016-09-07T17:45:12.850621: step 113, loss 0.471858, acc 0.74
2016-09-07T17:45:13.548253: step 114, loss 0.437094, acc 0.8
2016-09-07T17:45:14.212916: step 115, loss 0.397348, acc 0.84
2016-09-07T17:45:14.902502: step 116, loss 0.373149, acc 0.78
2016-09-07T17:45:15.579337: step 117, loss 0.736825, acc 0.58
2016-09-07T17:45:16.272395: step 118, loss 0.469213, acc 0.72
2016-09-07T17:45:16.960519: step 119, loss 0.565624, acc 0.72
2016-09-07T17:45:17.625200: step 120, loss 0.372979, acc 0.82
2016-09-07T17:45:18.302322: step 121, loss 0.595236, acc 0.76
2016-09-07T17:45:18.971997: step 122, loss 0.42907, acc 0.78
2016-09-07T17:45:19.666904: step 123, loss 0.565813, acc 0.72
2016-09-07T17:45:20.363418: step 124, loss 0.50068, acc 0.8
2016-09-07T17:45:21.037881: step 125, loss 0.672753, acc 0.64
2016-09-07T17:45:21.755099: step 126, loss 0.636302, acc 0.72
2016-09-07T17:45:22.418444: step 127, loss 0.587304, acc 0.68
2016-09-07T17:45:23.086913: step 128, loss 0.39197, acc 0.82
2016-09-07T17:45:23.736728: step 129, loss 0.396312, acc 0.86
2016-09-07T17:45:24.421443: step 130, loss 0.455179, acc 0.76
2016-09-07T17:45:25.113401: step 131, loss 0.481396, acc 0.78
2016-09-07T17:45:25.776350: step 132, loss 0.435106, acc 0.84
2016-09-07T17:45:26.448502: step 133, loss 0.520724, acc 0.68
2016-09-07T17:45:27.153907: step 134, loss 0.418264, acc 0.84
2016-09-07T17:45:27.829033: step 135, loss 0.541496, acc 0.72
2016-09-07T17:45:28.481962: step 136, loss 0.480351, acc 0.76
2016-09-07T17:45:29.183655: step 137, loss 0.496752, acc 0.76
2016-09-07T17:45:29.857982: step 138, loss 0.540663, acc 0.74
2016-09-07T17:45:30.531367: step 139, loss 0.428641, acc 0.8
2016-09-07T17:45:31.204536: step 140, loss 0.401832, acc 0.8
2016-09-07T17:45:31.891311: step 141, loss 0.54207, acc 0.72
2016-09-07T17:45:32.584669: step 142, loss 0.525165, acc 0.72
2016-09-07T17:45:33.264846: step 143, loss 0.545679, acc 0.66
2016-09-07T17:45:33.963224: step 144, loss 0.48358, acc 0.82
2016-09-07T17:45:34.631977: step 145, loss 0.494134, acc 0.76
2016-09-07T17:45:35.313093: step 146, loss 0.594171, acc 0.7
2016-09-07T17:45:36.004374: step 147, loss 0.522996, acc 0.7
2016-09-07T17:45:36.682698: step 148, loss 0.397144, acc 0.86
2016-09-07T17:45:37.366659: step 149, loss 0.527254, acc 0.72
2016-09-07T17:45:38.035931: step 150, loss 0.499472, acc 0.76
2016-09-07T17:45:38.711693: step 151, loss 0.523576, acc 0.68
2016-09-07T17:45:39.379533: step 152, loss 0.491241, acc 0.66
2016-09-07T17:45:40.071587: step 153, loss 0.580588, acc 0.7
2016-09-07T17:45:40.776196: step 154, loss 0.609938, acc 0.66
2016-09-07T17:45:41.484177: step 155, loss 0.459, acc 0.74
2016-09-07T17:45:42.179610: step 156, loss 0.410498, acc 0.84
2016-09-07T17:45:42.864516: step 157, loss 0.442999, acc 0.72
2016-09-07T17:45:43.557124: step 158, loss 0.587177, acc 0.68
2016-09-07T17:45:44.239474: step 159, loss 0.49452, acc 0.76
2016-09-07T17:45:44.925606: step 160, loss 0.555539, acc 0.72
2016-09-07T17:45:45.610033: step 161, loss 0.544199, acc 0.72
2016-09-07T17:45:46.283127: step 162, loss 0.529677, acc 0.7
2016-09-07T17:45:46.971086: step 163, loss 0.431466, acc 0.76
2016-09-07T17:45:47.638504: step 164, loss 0.396483, acc 0.82
2016-09-07T17:45:48.338514: step 165, loss 0.48392, acc 0.82
2016-09-07T17:45:49.003145: step 166, loss 0.433963, acc 0.78
2016-09-07T17:45:49.698485: step 167, loss 0.526775, acc 0.74
2016-09-07T17:45:50.391943: step 168, loss 0.560789, acc 0.72
2016-09-07T17:45:51.066331: step 169, loss 0.543546, acc 0.74
2016-09-07T17:45:51.760065: step 170, loss 0.535911, acc 0.72
2016-09-07T17:45:52.459164: step 171, loss 0.585314, acc 0.66
2016-09-07T17:45:53.154501: step 172, loss 0.503863, acc 0.82
2016-09-07T17:45:53.808425: step 173, loss 0.506353, acc 0.76
2016-09-07T17:45:54.510776: step 174, loss 0.443384, acc 0.7
2016-09-07T17:45:55.194511: step 175, loss 0.433826, acc 0.82
2016-09-07T17:45:55.878770: step 176, loss 0.397192, acc 0.9
2016-09-07T17:45:56.566747: step 177, loss 0.342006, acc 0.88
2016-09-07T17:45:57.260787: step 178, loss 0.505331, acc 0.76
2016-09-07T17:45:57.950997: step 179, loss 0.419315, acc 0.78
2016-09-07T17:45:58.629628: step 180, loss 0.323304, acc 0.8
2016-09-07T17:45:59.327569: step 181, loss 0.515957, acc 0.7
2016-09-07T17:45:59.999875: step 182, loss 0.517315, acc 0.74
2016-09-07T17:46:00.715679: step 183, loss 0.436201, acc 0.78
2016-09-07T17:46:01.385240: step 184, loss 0.598229, acc 0.68
2016-09-07T17:46:02.063898: step 185, loss 0.582017, acc 0.74
2016-09-07T17:46:02.749729: step 186, loss 0.428956, acc 0.8
2016-09-07T17:46:03.388576: step 187, loss 0.416991, acc 0.7
2016-09-07T17:46:04.070312: step 188, loss 0.443109, acc 0.76
2016-09-07T17:46:04.743718: step 189, loss 0.442726, acc 0.84
2016-09-07T17:46:05.449551: step 190, loss 0.464954, acc 0.78
2016-09-07T17:46:06.117582: step 191, loss 0.57415, acc 0.68
2016-09-07T17:46:06.822227: step 192, loss 0.53093, acc 0.72
2016-09-07T17:46:07.516434: step 193, loss 0.392868, acc 0.84
2016-09-07T17:46:07.880420: step 194, loss 0.241782, acc 1
2016-09-07T17:46:08.540140: step 195, loss 0.411219, acc 0.82
2016-09-07T17:46:09.218074: step 196, loss 0.377647, acc 0.78
2016-09-07T17:46:09.904238: step 197, loss 0.285481, acc 0.88
2016-09-07T17:46:10.565995: step 198, loss 0.346957, acc 0.84
2016-09-07T17:46:11.280128: step 199, loss 0.345712, acc 0.82
2016-09-07T17:46:11.951577: step 200, loss 0.490011, acc 0.74

Evaluation:
2016-09-07T17:46:14.903034: step 200, loss 0.456169, acc 0.8

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-200

2016-09-07T17:46:16.555152: step 201, loss 0.409452, acc 0.74
2016-09-07T17:46:17.267876: step 202, loss 0.341894, acc 0.8
2016-09-07T17:46:17.934904: step 203, loss 0.259657, acc 0.84
2016-09-07T17:46:18.631888: step 204, loss 0.538189, acc 0.8
2016-09-07T17:46:19.323298: step 205, loss 0.332882, acc 0.84
2016-09-07T17:46:20.008002: step 206, loss 0.256436, acc 0.96
2016-09-07T17:46:20.683749: step 207, loss 0.636891, acc 0.72
2016-09-07T17:46:21.355287: step 208, loss 0.339727, acc 0.88
2016-09-07T17:46:22.042768: step 209, loss 0.278909, acc 0.9
2016-09-07T17:46:22.692856: step 210, loss 0.325266, acc 0.88
2016-09-07T17:46:23.396055: step 211, loss 0.229563, acc 0.92
2016-09-07T17:46:24.061252: step 212, loss 0.555673, acc 0.8
2016-09-07T17:46:24.746978: step 213, loss 0.294104, acc 0.86
2016-09-07T17:46:25.429437: step 214, loss 0.511076, acc 0.76
2016-09-07T17:46:26.106505: step 215, loss 0.248527, acc 0.88
2016-09-07T17:46:26.784884: step 216, loss 0.488178, acc 0.76
2016-09-07T17:46:27.438663: step 217, loss 0.31803, acc 0.88
2016-09-07T17:46:28.151231: step 218, loss 0.43494, acc 0.78
2016-09-07T17:46:28.837537: step 219, loss 0.296228, acc 0.88
2016-09-07T17:46:29.521522: step 220, loss 0.268835, acc 0.92
2016-09-07T17:46:30.189905: step 221, loss 0.300093, acc 0.88
2016-09-07T17:46:30.886973: step 222, loss 0.330899, acc 0.78
2016-09-07T17:46:31.560714: step 223, loss 0.434295, acc 0.78
2016-09-07T17:46:32.220693: step 224, loss 0.39403, acc 0.82
2016-09-07T17:46:32.918093: step 225, loss 0.46189, acc 0.8
2016-09-07T17:46:33.608468: step 226, loss 0.419006, acc 0.84
2016-09-07T17:46:34.288453: step 227, loss 0.234724, acc 0.96
2016-09-07T17:46:34.961127: step 228, loss 0.368513, acc 0.84
2016-09-07T17:46:35.644554: step 229, loss 0.268188, acc 0.86
2016-09-07T17:46:36.322938: step 230, loss 0.407316, acc 0.76
2016-09-07T17:46:36.993368: step 231, loss 0.322857, acc 0.82
2016-09-07T17:46:37.683741: step 232, loss 0.282523, acc 0.92
2016-09-07T17:46:38.352523: step 233, loss 0.430776, acc 0.78
2016-09-07T17:46:39.030380: step 234, loss 0.47378, acc 0.74
2016-09-07T17:46:39.735015: step 235, loss 0.267876, acc 0.92
2016-09-07T17:46:40.423115: step 236, loss 0.229804, acc 0.9
2016-09-07T17:46:41.108543: step 237, loss 0.460362, acc 0.82
2016-09-07T17:46:41.794723: step 238, loss 0.454531, acc 0.78
2016-09-07T17:46:42.491937: step 239, loss 0.467456, acc 0.74
2016-09-07T17:46:43.150714: step 240, loss 0.304866, acc 0.84
2016-09-07T17:46:43.854948: step 241, loss 0.309125, acc 0.88
2016-09-07T17:46:44.526121: step 242, loss 0.371884, acc 0.86
2016-09-07T17:46:45.196652: step 243, loss 0.453904, acc 0.8
2016-09-07T17:46:45.883930: step 244, loss 0.265409, acc 0.88
2016-09-07T17:46:46.550659: step 245, loss 0.251597, acc 0.88
2016-09-07T17:46:47.237260: step 246, loss 0.317545, acc 0.84
2016-09-07T17:46:47.912018: step 247, loss 0.268125, acc 0.9
2016-09-07T17:46:48.619148: step 248, loss 0.265517, acc 0.84
2016-09-07T17:46:49.309701: step 249, loss 0.393578, acc 0.8
2016-09-07T17:46:50.015009: step 250, loss 0.36148, acc 0.8
2016-09-07T17:46:50.686258: step 251, loss 0.484282, acc 0.82
2016-09-07T17:46:51.375967: step 252, loss 0.310205, acc 0.86
2016-09-07T17:46:52.059539: step 253, loss 0.353739, acc 0.78
2016-09-07T17:46:52.707731: step 254, loss 0.628566, acc 0.78
2016-09-07T17:46:53.410633: step 255, loss 0.429546, acc 0.76
2016-09-07T17:46:54.118729: step 256, loss 0.290333, acc 0.88
2016-09-07T17:46:54.799716: step 257, loss 0.347833, acc 0.88
2016-09-07T17:46:55.492610: step 258, loss 0.504854, acc 0.7
2016-09-07T17:46:56.163731: step 259, loss 0.392856, acc 0.78
2016-09-07T17:46:56.838997: step 260, loss 0.317565, acc 0.88
2016-09-07T17:46:57.519053: step 261, loss 0.257059, acc 0.9
2016-09-07T17:46:58.214189: step 262, loss 0.333336, acc 0.84
2016-09-07T17:46:58.892444: step 263, loss 0.417801, acc 0.78
2016-09-07T17:46:59.581485: step 264, loss 0.289868, acc 0.9
2016-09-07T17:47:00.286210: step 265, loss 0.346309, acc 0.88
2016-09-07T17:47:00.971782: step 266, loss 0.300268, acc 0.86
2016-09-07T17:47:01.648834: step 267, loss 0.205182, acc 0.9
2016-09-07T17:47:02.307493: step 268, loss 0.332151, acc 0.82
2016-09-07T17:47:02.999589: step 269, loss 0.267221, acc 0.94
2016-09-07T17:47:03.663575: step 270, loss 0.248016, acc 0.9
2016-09-07T17:47:04.361705: step 271, loss 0.369317, acc 0.84
2016-09-07T17:47:05.047005: step 272, loss 0.360069, acc 0.86
2016-09-07T17:47:05.737571: step 273, loss 0.302828, acc 0.84
2016-09-07T17:47:06.407467: step 274, loss 0.427605, acc 0.82
2016-09-07T17:47:07.073913: step 275, loss 0.356851, acc 0.8
2016-09-07T17:47:07.766022: step 276, loss 0.422814, acc 0.8
2016-09-07T17:47:08.436959: step 277, loss 0.377983, acc 0.78
2016-09-07T17:47:09.141361: step 278, loss 0.324448, acc 0.82
2016-09-07T17:47:09.817846: step 279, loss 0.42832, acc 0.8
2016-09-07T17:47:10.490891: step 280, loss 0.52684, acc 0.72
2016-09-07T17:47:11.174791: step 281, loss 0.380395, acc 0.84
2016-09-07T17:47:11.845702: step 282, loss 0.291702, acc 0.94
2016-09-07T17:47:12.550907: step 283, loss 0.44231, acc 0.84
2016-09-07T17:47:13.242046: step 284, loss 0.429428, acc 0.82
2016-09-07T17:47:13.952111: step 285, loss 0.368428, acc 0.82
2016-09-07T17:47:14.638504: step 286, loss 0.39464, acc 0.82
2016-09-07T17:47:15.326982: step 287, loss 0.34011, acc 0.88
2016-09-07T17:47:16.017567: step 288, loss 0.351993, acc 0.82
2016-09-07T17:47:16.702763: step 289, loss 0.364653, acc 0.8
2016-09-07T17:47:17.389899: step 290, loss 0.290939, acc 0.84
2016-09-07T17:47:18.057374: step 291, loss 0.257523, acc 0.92
2016-09-07T17:47:18.754547: step 292, loss 0.274112, acc 0.92
2016-09-07T17:47:19.430270: step 293, loss 0.44642, acc 0.76
2016-09-07T17:47:20.107873: step 294, loss 0.295136, acc 0.84
2016-09-07T17:47:20.796234: step 295, loss 0.324195, acc 0.84
2016-09-07T17:47:21.476384: step 296, loss 0.246237, acc 0.88
2016-09-07T17:47:22.150796: step 297, loss 0.269372, acc 0.84
2016-09-07T17:47:22.810670: step 298, loss 0.277006, acc 0.86
2016-09-07T17:47:23.507642: step 299, loss 0.577074, acc 0.7
2016-09-07T17:47:24.196413: step 300, loss 0.251113, acc 0.9

Evaluation:
2016-09-07T17:47:27.181152: step 300, loss 0.480735, acc 0.801

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-300

2016-09-07T17:47:28.795796: step 301, loss 0.32274, acc 0.82
2016-09-07T17:47:29.503483: step 302, loss 0.229717, acc 0.9
2016-09-07T17:47:30.165118: step 303, loss 0.473945, acc 0.88
2016-09-07T17:47:30.846622: step 304, loss 0.310686, acc 0.84
2016-09-07T17:47:31.515827: step 305, loss 0.49867, acc 0.82
2016-09-07T17:47:32.229032: step 306, loss 0.327314, acc 0.9
2016-09-07T17:47:32.911270: step 307, loss 0.364217, acc 0.86
2016-09-07T17:47:33.561206: step 308, loss 0.421406, acc 0.8
2016-09-07T17:47:34.255368: step 309, loss 0.416056, acc 0.84
2016-09-07T17:47:34.926456: step 310, loss 0.304384, acc 0.84
2016-09-07T17:47:35.589219: step 311, loss 0.277124, acc 0.84
2016-09-07T17:47:36.263169: step 312, loss 0.297964, acc 0.92
2016-09-07T17:47:36.949542: step 313, loss 0.350171, acc 0.88
2016-09-07T17:47:37.624808: step 314, loss 0.50614, acc 0.78
2016-09-07T17:47:38.297489: step 315, loss 0.485507, acc 0.72
2016-09-07T17:47:38.994213: step 316, loss 0.368362, acc 0.84
2016-09-07T17:47:39.662076: step 317, loss 0.374353, acc 0.82
2016-09-07T17:47:40.372553: step 318, loss 0.239205, acc 0.96
2016-09-07T17:47:41.056466: step 319, loss 0.279962, acc 0.9
2016-09-07T17:47:41.725259: step 320, loss 0.378877, acc 0.78
2016-09-07T17:47:42.413346: step 321, loss 0.280389, acc 0.88
2016-09-07T17:47:43.087060: step 322, loss 0.372529, acc 0.84
2016-09-07T17:47:43.768127: step 323, loss 0.417231, acc 0.78
2016-09-07T17:47:44.436679: step 324, loss 0.319828, acc 0.86
2016-09-07T17:47:45.134370: step 325, loss 0.528865, acc 0.7
2016-09-07T17:47:45.834302: step 326, loss 0.452199, acc 0.78
2016-09-07T17:47:46.521530: step 327, loss 0.377432, acc 0.82
2016-09-07T17:47:47.228916: step 328, loss 0.400006, acc 0.82
2016-09-07T17:47:47.918319: step 329, loss 0.346731, acc 0.8
2016-09-07T17:47:48.624158: step 330, loss 0.432485, acc 0.82
2016-09-07T17:47:49.273285: step 331, loss 0.193067, acc 0.94
2016-09-07T17:47:49.959632: step 332, loss 0.26561, acc 0.88
2016-09-07T17:47:50.658117: step 333, loss 0.283444, acc 0.84
2016-09-07T17:47:51.315856: step 334, loss 0.31063, acc 0.88
2016-09-07T17:47:51.993774: step 335, loss 0.275271, acc 0.88
2016-09-07T17:47:52.666997: step 336, loss 0.398896, acc 0.82
2016-09-07T17:47:53.336346: step 337, loss 0.404005, acc 0.88
2016-09-07T17:47:54.001815: step 338, loss 0.449775, acc 0.7
2016-09-07T17:47:54.697004: step 339, loss 0.326954, acc 0.84
2016-09-07T17:47:55.351161: step 340, loss 0.315902, acc 0.84
2016-09-07T17:47:56.046425: step 341, loss 0.25391, acc 0.86
2016-09-07T17:47:56.710849: step 342, loss 0.300431, acc 0.88
2016-09-07T17:47:57.390803: step 343, loss 0.281949, acc 0.92
2016-09-07T17:47:58.073347: step 344, loss 0.396329, acc 0.8
2016-09-07T17:47:58.755322: step 345, loss 0.406096, acc 0.9
2016-09-07T17:47:59.435410: step 346, loss 0.464184, acc 0.78
2016-09-07T17:48:00.100820: step 347, loss 0.278528, acc 0.88
2016-09-07T17:48:00.836981: step 348, loss 0.256657, acc 0.9
2016-09-07T17:48:01.521638: step 349, loss 0.402165, acc 0.82
2016-09-07T17:48:02.202551: step 350, loss 0.294152, acc 0.9
2016-09-07T17:48:02.896306: step 351, loss 0.492228, acc 0.74
2016-09-07T17:48:03.583530: step 352, loss 0.387635, acc 0.86
2016-09-07T17:48:04.310844: step 353, loss 0.353595, acc 0.84
2016-09-07T17:48:04.977318: step 354, loss 0.414822, acc 0.74
2016-09-07T17:48:05.637211: step 355, loss 0.331041, acc 0.84
2016-09-07T17:48:06.335523: step 356, loss 0.345324, acc 0.82
2016-09-07T17:48:07.008584: step 357, loss 0.425143, acc 0.8
2016-09-07T17:48:07.674408: step 358, loss 0.336883, acc 0.86
2016-09-07T17:48:08.373455: step 359, loss 0.430495, acc 0.78
2016-09-07T17:48:09.065089: step 360, loss 0.463397, acc 0.8
2016-09-07T17:48:09.750129: step 361, loss 0.364192, acc 0.82
2016-09-07T17:48:10.454882: step 362, loss 0.418395, acc 0.82
2016-09-07T17:48:11.139125: step 363, loss 0.260743, acc 0.92
2016-09-07T17:48:11.821139: step 364, loss 0.374883, acc 0.86
2016-09-07T17:48:12.504073: step 365, loss 0.442331, acc 0.76
2016-09-07T17:48:13.198081: step 366, loss 0.346724, acc 0.88
2016-09-07T17:48:13.894365: step 367, loss 0.351651, acc 0.82
2016-09-07T17:48:14.560918: step 368, loss 0.358491, acc 0.9
2016-09-07T17:48:15.248439: step 369, loss 0.450502, acc 0.8
2016-09-07T17:48:15.920628: step 370, loss 0.408184, acc 0.74
2016-09-07T17:48:16.621083: step 371, loss 0.416344, acc 0.8
2016-09-07T17:48:17.281375: step 372, loss 0.345036, acc 0.86
2016-09-07T17:48:17.960212: step 373, loss 0.465188, acc 0.78
2016-09-07T17:48:18.669064: step 374, loss 0.461721, acc 0.8
2016-09-07T17:48:19.325456: step 375, loss 0.352409, acc 0.86
2016-09-07T17:48:20.026712: step 376, loss 0.299369, acc 0.88
2016-09-07T17:48:20.705845: step 377, loss 0.494026, acc 0.76
2016-09-07T17:48:21.377868: step 378, loss 0.293664, acc 0.86
2016-09-07T17:48:22.049834: step 379, loss 0.334886, acc 0.86
2016-09-07T17:48:22.730426: step 380, loss 0.4754, acc 0.84
2016-09-07T17:48:23.411478: step 381, loss 0.38936, acc 0.82
2016-09-07T17:48:24.063808: step 382, loss 0.361739, acc 0.72
2016-09-07T17:48:24.749198: step 383, loss 0.399928, acc 0.78
2016-09-07T17:48:25.419563: step 384, loss 0.403479, acc 0.78
2016-09-07T17:48:26.085353: step 385, loss 0.391569, acc 0.9
2016-09-07T17:48:26.781725: step 386, loss 0.375572, acc 0.82
2016-09-07T17:48:27.473951: step 387, loss 0.381985, acc 0.84
2016-09-07T17:48:27.851643: step 388, loss 0.450993, acc 0.833333
2016-09-07T17:48:28.537345: step 389, loss 0.328062, acc 0.86
2016-09-07T17:48:29.229869: step 390, loss 0.263005, acc 0.82
2016-09-07T17:48:29.913770: step 391, loss 0.252938, acc 0.92
2016-09-07T17:48:30.623481: step 392, loss 0.198219, acc 0.88
2016-09-07T17:48:31.288299: step 393, loss 0.192752, acc 0.9
2016-09-07T17:48:32.003980: step 394, loss 0.242217, acc 0.92
2016-09-07T17:48:32.694416: step 395, loss 0.206316, acc 0.92
2016-09-07T17:48:33.362872: step 396, loss 0.196432, acc 0.9
2016-09-07T17:48:34.041159: step 397, loss 0.205722, acc 0.94
2016-09-07T17:48:34.715721: step 398, loss 0.223761, acc 0.94
2016-09-07T17:48:35.397974: step 399, loss 0.228625, acc 0.88
2016-09-07T17:48:36.071290: step 400, loss 0.128145, acc 0.96

Evaluation:
2016-09-07T17:48:39.095968: step 400, loss 0.584145, acc 0.801

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-400

2016-09-07T17:48:40.714571: step 401, loss 0.253057, acc 0.86
2016-09-07T17:48:41.406799: step 402, loss 0.241642, acc 0.92
2016-09-07T17:48:42.092279: step 403, loss 0.298789, acc 0.9
2016-09-07T17:48:42.778613: step 404, loss 0.239418, acc 0.9
2016-09-07T17:48:43.455459: step 405, loss 0.212338, acc 0.92
2016-09-07T17:48:44.140244: step 406, loss 0.212493, acc 0.94
2016-09-07T17:48:44.822367: step 407, loss 0.0834996, acc 0.96
2016-09-07T17:48:45.497857: step 408, loss 0.259102, acc 0.86
2016-09-07T17:48:46.176037: step 409, loss 0.367038, acc 0.88
2016-09-07T17:48:46.857826: step 410, loss 0.189135, acc 0.9
2016-09-07T17:48:47.536641: step 411, loss 0.204218, acc 0.92
2016-09-07T17:48:48.193303: step 412, loss 0.109182, acc 0.94
2016-09-07T17:48:48.886677: step 413, loss 0.260299, acc 0.92
2016-09-07T17:48:49.568187: step 414, loss 0.309087, acc 0.92
2016-09-07T17:48:50.254613: step 415, loss 0.313571, acc 0.9
2016-09-07T17:48:50.938237: step 416, loss 0.220207, acc 0.9
2016-09-07T17:48:51.608312: step 417, loss 0.189041, acc 0.9
2016-09-07T17:48:52.274466: step 418, loss 0.228131, acc 0.92
2016-09-07T17:48:52.918462: step 419, loss 0.108323, acc 0.96
2016-09-07T17:48:53.610098: step 420, loss 0.238983, acc 0.88
2016-09-07T17:48:54.284871: step 421, loss 0.495761, acc 0.8
2016-09-07T17:48:54.970076: step 422, loss 0.233885, acc 0.9
2016-09-07T17:48:55.643256: step 423, loss 0.236727, acc 0.88
2016-09-07T17:48:56.318163: step 424, loss 0.315701, acc 0.86
2016-09-07T17:48:56.987045: step 425, loss 0.159802, acc 0.96
2016-09-07T17:48:57.660191: step 426, loss 0.203559, acc 0.94
2016-09-07T17:48:58.349500: step 427, loss 0.362901, acc 0.84
2016-09-07T17:48:59.000328: step 428, loss 0.380538, acc 0.82
2016-09-07T17:48:59.675079: step 429, loss 0.229035, acc 0.9
2016-09-07T17:49:00.375117: step 430, loss 0.18617, acc 0.94
2016-09-07T17:49:01.031177: step 431, loss 0.265056, acc 0.86
2016-09-07T17:49:01.701339: step 432, loss 0.214857, acc 0.92
2016-09-07T17:49:02.365152: step 433, loss 0.177574, acc 0.96
2016-09-07T17:49:03.039199: step 434, loss 0.315093, acc 0.9
2016-09-07T17:49:03.702626: step 435, loss 0.261177, acc 0.86
2016-09-07T17:49:04.397737: step 436, loss 0.426634, acc 0.78
2016-09-07T17:49:05.048875: step 437, loss 0.221677, acc 0.9
2016-09-07T17:49:05.743538: step 438, loss 0.180053, acc 0.94
2016-09-07T17:49:06.418102: step 439, loss 0.108236, acc 0.94
2016-09-07T17:49:07.086187: step 440, loss 0.22944, acc 0.9
2016-09-07T17:49:07.761153: step 441, loss 0.17987, acc 0.92
2016-09-07T17:49:08.450247: step 442, loss 0.13718, acc 0.94
2016-09-07T17:49:09.145041: step 443, loss 0.23508, acc 0.9
2016-09-07T17:49:09.826887: step 444, loss 0.396699, acc 0.76
2016-09-07T17:49:10.519525: step 445, loss 0.235356, acc 0.9
2016-09-07T17:49:11.191856: step 446, loss 0.220196, acc 0.9
2016-09-07T17:49:11.858782: step 447, loss 0.299663, acc 0.82
2016-09-07T17:49:12.531033: step 448, loss 0.0608042, acc 1
2016-09-07T17:49:13.224467: step 449, loss 0.102091, acc 0.96
2016-09-07T17:49:13.896796: step 450, loss 0.306279, acc 0.86
2016-09-07T17:49:14.580765: step 451, loss 0.207515, acc 0.92
2016-09-07T17:49:15.279018: step 452, loss 0.314822, acc 0.88
2016-09-07T17:49:15.948462: step 453, loss 0.254416, acc 0.86
2016-09-07T17:49:16.616849: step 454, loss 0.230909, acc 0.86
2016-09-07T17:49:17.301841: step 455, loss 0.302548, acc 0.84
2016-09-07T17:49:17.980972: step 456, loss 0.144566, acc 0.96
2016-09-07T17:49:18.656338: step 457, loss 0.225248, acc 0.86
2016-09-07T17:49:19.362323: step 458, loss 0.282408, acc 0.92
2016-09-07T17:49:20.064987: step 459, loss 0.27764, acc 0.86
2016-09-07T17:49:20.720651: step 460, loss 0.219654, acc 0.94
2016-09-07T17:49:21.415380: step 461, loss 0.22221, acc 0.92
2016-09-07T17:49:22.096306: step 462, loss 0.294138, acc 0.86
2016-09-07T17:49:22.778553: step 463, loss 0.2912, acc 0.92
2016-09-07T17:49:23.452082: step 464, loss 0.341775, acc 0.78
2016-09-07T17:49:24.124010: step 465, loss 0.286652, acc 0.88
2016-09-07T17:49:24.806987: step 466, loss 0.20047, acc 0.94
2016-09-07T17:49:25.467002: step 467, loss 0.182768, acc 0.94
2016-09-07T17:49:26.180399: step 468, loss 0.201775, acc 0.9
2016-09-07T17:49:26.883650: step 469, loss 0.274401, acc 0.88
2016-09-07T17:49:27.564633: step 470, loss 0.287753, acc 0.86
2016-09-07T17:49:28.246722: step 471, loss 0.341304, acc 0.82
2016-09-07T17:49:28.918812: step 472, loss 0.334501, acc 0.86
2016-09-07T17:49:29.613188: step 473, loss 0.215489, acc 0.88
2016-09-07T17:49:30.278853: step 474, loss 0.356291, acc 0.84
2016-09-07T17:49:30.974282: step 475, loss 0.178143, acc 0.94
2016-09-07T17:49:31.637801: step 476, loss 0.18717, acc 0.9
2016-09-07T17:49:32.304405: step 477, loss 0.298902, acc 0.86
2016-09-07T17:49:32.992787: step 478, loss 0.288192, acc 0.94
2016-09-07T17:49:33.682612: step 479, loss 0.321968, acc 0.88
2016-09-07T17:49:34.361341: step 480, loss 0.234539, acc 0.88
2016-09-07T17:49:35.021790: step 481, loss 0.151388, acc 0.92
2016-09-07T17:49:35.729898: step 482, loss 0.202723, acc 0.9
2016-09-07T17:49:36.384406: step 483, loss 0.204771, acc 0.9
2016-09-07T17:49:37.075773: step 484, loss 0.124205, acc 0.98
2016-09-07T17:49:37.753474: step 485, loss 0.288169, acc 0.84
2016-09-07T17:49:38.440922: step 486, loss 0.248304, acc 0.9
2016-09-07T17:49:39.117937: step 487, loss 0.182524, acc 0.94
2016-09-07T17:49:39.798940: step 488, loss 0.327522, acc 0.82
2016-09-07T17:49:40.509659: step 489, loss 0.375354, acc 0.88
2016-09-07T17:49:41.187943: step 490, loss 0.221085, acc 0.9
2016-09-07T17:49:41.895843: step 491, loss 0.15396, acc 0.9
2016-09-07T17:49:42.577354: step 492, loss 0.187845, acc 0.94
2016-09-07T17:49:43.250464: step 493, loss 0.119177, acc 0.94
2016-09-07T17:49:43.923343: step 494, loss 0.219497, acc 0.9
2016-09-07T17:49:44.602952: step 495, loss 0.345008, acc 0.86
2016-09-07T17:49:45.309710: step 496, loss 0.253333, acc 0.88
2016-09-07T17:49:45.977755: step 497, loss 0.181502, acc 0.92
2016-09-07T17:49:46.656179: step 498, loss 0.3753, acc 0.86
2016-09-07T17:49:47.352253: step 499, loss 0.312691, acc 0.86
2016-09-07T17:49:48.029073: step 500, loss 0.244801, acc 0.9

Evaluation:
2016-09-07T17:49:51.044235: step 500, loss 0.543526, acc 0.79

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-500

2016-09-07T17:49:52.750042: step 501, loss 0.289426, acc 0.9
2016-09-07T17:49:53.429582: step 502, loss 0.2424, acc 0.9
2016-09-07T17:49:54.105650: step 503, loss 0.186593, acc 0.88
2016-09-07T17:49:54.768568: step 504, loss 0.233088, acc 0.86
2016-09-07T17:49:55.463273: step 505, loss 0.221489, acc 0.9
2016-09-07T17:49:56.171645: step 506, loss 0.296453, acc 0.84
2016-09-07T17:49:56.843848: step 507, loss 0.20797, acc 0.92
2016-09-07T17:49:57.523193: step 508, loss 0.134855, acc 0.96
2016-09-07T17:49:58.220661: step 509, loss 0.196948, acc 0.94
2016-09-07T17:49:58.916370: step 510, loss 0.373699, acc 0.8
2016-09-07T17:49:59.616518: step 511, loss 0.151729, acc 0.94
2016-09-07T17:50:00.304050: step 512, loss 0.218834, acc 0.98
2016-09-07T17:50:01.013910: step 513, loss 0.181809, acc 0.96
2016-09-07T17:50:01.703140: step 514, loss 0.345731, acc 0.8
2016-09-07T17:50:02.383978: step 515, loss 0.210492, acc 0.9
2016-09-07T17:50:03.085072: step 516, loss 0.292729, acc 0.84
2016-09-07T17:50:03.764254: step 517, loss 0.178039, acc 0.94
2016-09-07T17:50:04.451408: step 518, loss 0.347034, acc 0.9
2016-09-07T17:50:05.117369: step 519, loss 0.322788, acc 0.82
2016-09-07T17:50:05.814016: step 520, loss 0.344396, acc 0.9
2016-09-07T17:50:06.489388: step 521, loss 0.254029, acc 0.9
2016-09-07T17:50:07.182166: step 522, loss 0.248403, acc 0.9
2016-09-07T17:50:07.876786: step 523, loss 0.302312, acc 0.84
2016-09-07T17:50:08.552548: step 524, loss 0.342937, acc 0.84
2016-09-07T17:50:09.230609: step 525, loss 0.238994, acc 0.92
2016-09-07T17:50:09.905194: step 526, loss 0.310535, acc 0.88
2016-09-07T17:50:10.598578: step 527, loss 0.265319, acc 0.92
2016-09-07T17:50:11.258263: step 528, loss 0.224729, acc 0.94
2016-09-07T17:50:11.930965: step 529, loss 0.301439, acc 0.86
2016-09-07T17:50:12.608241: step 530, loss 0.176001, acc 0.92
2016-09-07T17:50:13.279843: step 531, loss 0.236894, acc 0.94
2016-09-07T17:50:13.960604: step 532, loss 0.241724, acc 0.86
2016-09-07T17:50:14.643730: step 533, loss 0.315104, acc 0.84
2016-09-07T17:50:15.360247: step 534, loss 0.341868, acc 0.82
2016-09-07T17:50:16.025929: step 535, loss 0.190428, acc 0.9
2016-09-07T17:50:16.715573: step 536, loss 0.188112, acc 0.92
2016-09-07T17:50:17.395601: step 537, loss 0.170561, acc 0.92
2016-09-07T17:50:18.072818: step 538, loss 0.316295, acc 0.86
2016-09-07T17:50:18.771985: step 539, loss 0.261723, acc 0.9
2016-09-07T17:50:19.461166: step 540, loss 0.21597, acc 0.9
2016-09-07T17:50:20.184122: step 541, loss 0.394194, acc 0.86
2016-09-07T17:50:20.856980: step 542, loss 0.300127, acc 0.82
2016-09-07T17:50:21.533644: step 543, loss 0.255873, acc 0.86
2016-09-07T17:50:22.199400: step 544, loss 0.222114, acc 0.92
2016-09-07T17:50:22.859213: step 545, loss 0.173434, acc 0.94
2016-09-07T17:50:23.527025: step 546, loss 0.162524, acc 0.94
2016-09-07T17:50:24.198401: step 547, loss 0.264732, acc 0.88
2016-09-07T17:50:24.895235: step 548, loss 0.308906, acc 0.84
2016-09-07T17:50:25.549870: step 549, loss 0.251138, acc 0.82
2016-09-07T17:50:26.241892: step 550, loss 0.185581, acc 0.92
2016-09-07T17:50:26.931771: step 551, loss 0.245006, acc 0.9
2016-09-07T17:50:27.621709: step 552, loss 0.300277, acc 0.9
2016-09-07T17:50:28.303646: step 553, loss 0.271497, acc 0.9
2016-09-07T17:50:28.990232: step 554, loss 0.153502, acc 0.94
2016-09-07T17:50:29.661492: step 555, loss 0.244298, acc 0.9
2016-09-07T17:50:30.322059: step 556, loss 0.219026, acc 0.9
2016-09-07T17:50:31.009382: step 557, loss 0.28636, acc 0.86
2016-09-07T17:50:31.666469: step 558, loss 0.382777, acc 0.86
2016-09-07T17:50:32.347083: step 559, loss 0.21965, acc 0.9
2016-09-07T17:50:33.034838: step 560, loss 0.269017, acc 0.92
2016-09-07T17:50:33.708544: step 561, loss 0.234226, acc 0.88
2016-09-07T17:50:34.398102: step 562, loss 0.157983, acc 0.96
2016-09-07T17:50:35.079029: step 563, loss 0.332413, acc 0.88
2016-09-07T17:50:35.751563: step 564, loss 0.363184, acc 0.82
2016-09-07T17:50:36.417320: step 565, loss 0.129941, acc 0.94
2016-09-07T17:50:37.137565: step 566, loss 0.467407, acc 0.76
2016-09-07T17:50:37.811296: step 567, loss 0.347854, acc 0.86
2016-09-07T17:50:38.497847: step 568, loss 0.180708, acc 0.88
2016-09-07T17:50:39.174329: step 569, loss 0.307371, acc 0.86
2016-09-07T17:50:39.847667: step 570, loss 0.224978, acc 0.9
2016-09-07T17:50:40.531632: step 571, loss 0.166416, acc 0.96
2016-09-07T17:50:41.193315: step 572, loss 0.301403, acc 0.86
2016-09-07T17:50:41.900672: step 573, loss 0.278692, acc 0.9
2016-09-07T17:50:42.563482: step 574, loss 0.166369, acc 0.9
2016-09-07T17:50:43.242481: step 575, loss 0.220969, acc 0.86
2016-09-07T17:50:43.928200: step 576, loss 0.200361, acc 0.92
2016-09-07T17:50:44.612621: step 577, loss 0.235022, acc 0.88
2016-09-07T17:50:45.296337: step 578, loss 0.329675, acc 0.88
2016-09-07T17:50:45.981844: step 579, loss 0.195795, acc 0.92
2016-09-07T17:50:46.680932: step 580, loss 0.22431, acc 0.88
2016-09-07T17:50:47.360138: step 581, loss 0.205154, acc 0.94
2016-09-07T17:50:47.718034: step 582, loss 0.274526, acc 0.833333
2016-09-07T17:50:48.400087: step 583, loss 0.0728355, acc 1
2016-09-07T17:50:49.102320: step 584, loss 0.163461, acc 0.9
2016-09-07T17:50:49.798932: step 585, loss 0.324267, acc 0.84
2016-09-07T17:50:50.482581: step 586, loss 0.145907, acc 0.92
2016-09-07T17:50:51.172847: step 587, loss 0.152246, acc 0.94
2016-09-07T17:50:51.874098: step 588, loss 0.298145, acc 0.92
2016-09-07T17:50:52.569592: step 589, loss 0.0790773, acc 0.96
2016-09-07T17:50:53.237954: step 590, loss 0.129404, acc 0.98
2016-09-07T17:50:53.934608: step 591, loss 0.0750289, acc 0.98
2016-09-07T17:50:54.592854: step 592, loss 0.296404, acc 0.9
2016-09-07T17:50:55.261531: step 593, loss 0.0848699, acc 0.96
2016-09-07T17:50:55.925565: step 594, loss 0.0574872, acc 0.98
2016-09-07T17:50:56.602246: step 595, loss 0.243641, acc 0.86
2016-09-07T17:50:57.273951: step 596, loss 0.0885007, acc 0.96
2016-09-07T17:50:57.965388: step 597, loss 0.169643, acc 0.9
2016-09-07T17:50:58.660162: step 598, loss 0.258702, acc 0.9
2016-09-07T17:50:59.336294: step 599, loss 0.179151, acc 0.88
2016-09-07T17:50:59.996648: step 600, loss 0.0427568, acc 1

Evaluation:
2016-09-07T17:51:03.030775: step 600, loss 0.611256, acc 0.792

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-600

2016-09-07T17:51:04.674689: step 601, loss 0.0988442, acc 0.96
2016-09-07T17:51:05.329642: step 602, loss 0.149584, acc 0.92
2016-09-07T17:51:06.019665: step 603, loss 0.0855046, acc 0.96
2016-09-07T17:51:06.716383: step 604, loss 0.188106, acc 0.94
2016-09-07T17:51:07.393740: step 605, loss 0.247574, acc 0.94
2016-09-07T17:51:08.075022: step 606, loss 0.117123, acc 0.96
2016-09-07T17:51:08.760694: step 607, loss 0.121419, acc 0.92
2016-09-07T17:51:09.433268: step 608, loss 0.0814955, acc 0.98
2016-09-07T17:51:10.106748: step 609, loss 0.162873, acc 0.92
2016-09-07T17:51:10.812470: step 610, loss 0.0510481, acc 0.98
2016-09-07T17:51:11.498059: step 611, loss 0.140873, acc 0.96
2016-09-07T17:51:12.200408: step 612, loss 0.063214, acc 0.98
2016-09-07T17:51:12.904375: step 613, loss 0.173731, acc 0.92
2016-09-07T17:51:13.615699: step 614, loss 0.149581, acc 0.92
2016-09-07T17:51:14.317085: step 615, loss 0.113969, acc 0.94
2016-09-07T17:51:14.984067: step 616, loss 0.140364, acc 0.94
2016-09-07T17:51:15.663632: step 617, loss 0.0567278, acc 0.98
2016-09-07T17:51:16.340604: step 618, loss 0.228098, acc 0.88
2016-09-07T17:51:17.025904: step 619, loss 0.118484, acc 0.94
2016-09-07T17:51:17.701126: step 620, loss 0.090996, acc 0.96
2016-09-07T17:51:18.371349: step 621, loss 0.115815, acc 0.96
2016-09-07T17:51:19.056903: step 622, loss 0.0389573, acc 0.98
2016-09-07T17:51:19.734310: step 623, loss 0.180339, acc 0.94
2016-09-07T17:51:20.422381: step 624, loss 0.210547, acc 0.88
2016-09-07T17:51:21.099125: step 625, loss 0.0419951, acc 0.98
2016-09-07T17:51:21.782229: step 626, loss 0.340289, acc 0.88
2016-09-07T17:51:22.446892: step 627, loss 0.321448, acc 0.88
2016-09-07T17:51:23.104141: step 628, loss 0.113508, acc 0.94
2016-09-07T17:51:23.804912: step 629, loss 0.22314, acc 0.88
2016-09-07T17:51:24.464227: step 630, loss 0.12117, acc 0.96
2016-09-07T17:51:25.170488: step 631, loss 0.140604, acc 0.94
2016-09-07T17:51:25.845555: step 632, loss 0.226178, acc 0.9
2016-09-07T17:51:26.506516: step 633, loss 0.127012, acc 0.96
2016-09-07T17:51:27.202462: step 634, loss 0.139461, acc 0.9
2016-09-07T17:51:27.892685: step 635, loss 0.13623, acc 0.96
2016-09-07T17:51:28.595791: step 636, loss 0.305577, acc 0.86
2016-09-07T17:51:29.294853: step 637, loss 0.162677, acc 0.92
2016-09-07T17:51:30.000936: step 638, loss 0.155117, acc 0.9
2016-09-07T17:51:30.687808: step 639, loss 0.173482, acc 0.92
2016-09-07T17:51:31.377277: step 640, loss 0.0989546, acc 1
2016-09-07T17:51:32.061992: step 641, loss 0.174152, acc 0.92
2016-09-07T17:51:32.739516: step 642, loss 0.209386, acc 0.92
2016-09-07T17:51:33.441920: step 643, loss 0.210255, acc 0.92
2016-09-07T17:51:34.095551: step 644, loss 0.134931, acc 0.96
2016-09-07T17:51:34.809313: step 645, loss 0.142044, acc 0.98
2016-09-07T17:51:35.483698: step 646, loss 0.272616, acc 0.94
2016-09-07T17:51:36.170002: step 647, loss 0.119134, acc 0.92
2016-09-07T17:51:36.847017: step 648, loss 0.180231, acc 0.94
2016-09-07T17:51:37.539191: step 649, loss 0.143567, acc 0.98
2016-09-07T17:51:38.223903: step 650, loss 0.137279, acc 0.92
2016-09-07T17:51:38.877823: step 651, loss 0.199054, acc 0.92
2016-09-07T17:51:39.578608: step 652, loss 0.0769915, acc 0.98
2016-09-07T17:51:40.274631: step 653, loss 0.132266, acc 0.94
2016-09-07T17:51:40.951455: step 654, loss 0.253921, acc 0.88
2016-09-07T17:51:41.647465: step 655, loss 0.161914, acc 0.94
2016-09-07T17:51:42.328536: step 656, loss 0.154146, acc 0.96
2016-09-07T17:51:43.028439: step 657, loss 0.193111, acc 0.92
2016-09-07T17:51:43.731468: step 658, loss 0.253276, acc 0.86
2016-09-07T17:51:44.432409: step 659, loss 0.192883, acc 0.92
2016-09-07T17:51:45.130723: step 660, loss 0.086888, acc 0.98
2016-09-07T17:51:45.811163: step 661, loss 0.265627, acc 0.88
2016-09-07T17:51:46.489947: step 662, loss 0.234474, acc 0.88
2016-09-07T17:51:47.161928: step 663, loss 0.17883, acc 0.94
2016-09-07T17:51:47.838451: step 664, loss 0.170911, acc 0.88
2016-09-07T17:51:48.520369: step 665, loss 0.189735, acc 0.9
2016-09-07T17:51:49.226809: step 666, loss 0.166109, acc 0.92
2016-09-07T17:51:49.912703: step 667, loss 0.135198, acc 0.94
2016-09-07T17:51:50.585688: step 668, loss 0.186163, acc 0.86
2016-09-07T17:51:51.252077: step 669, loss 0.202757, acc 0.9
2016-09-07T17:51:51.936255: step 670, loss 0.06895, acc 1
2016-09-07T17:51:52.619968: step 671, loss 0.0911686, acc 0.98
2016-09-07T17:51:53.288427: step 672, loss 0.201707, acc 0.9
2016-09-07T17:51:53.992889: step 673, loss 0.129728, acc 0.92
2016-09-07T17:51:54.658848: step 674, loss 0.158924, acc 0.96
2016-09-07T17:51:55.338364: step 675, loss 0.224434, acc 0.9
2016-09-07T17:51:56.017158: step 676, loss 0.134476, acc 0.94
2016-09-07T17:51:56.689638: step 677, loss 0.165268, acc 0.9
2016-09-07T17:51:57.377217: step 678, loss 0.212989, acc 0.88
2016-09-07T17:51:58.089215: step 679, loss 0.0543741, acc 0.98
2016-09-07T17:51:58.798807: step 680, loss 0.158618, acc 0.88
2016-09-07T17:51:59.470551: step 681, loss 0.107731, acc 0.96
2016-09-07T17:52:00.146494: step 682, loss 0.150016, acc 0.92
2016-09-07T17:52:00.871882: step 683, loss 0.175696, acc 0.9
2016-09-07T17:52:01.557787: step 684, loss 0.219143, acc 0.9
2016-09-07T17:52:02.249483: step 685, loss 0.236819, acc 0.94
2016-09-07T17:52:02.924142: step 686, loss 0.210987, acc 0.92
2016-09-07T17:52:03.616683: step 687, loss 0.133078, acc 0.94
2016-09-07T17:52:04.298746: step 688, loss 0.146511, acc 0.94
2016-09-07T17:52:04.980100: step 689, loss 0.18288, acc 0.92
2016-09-07T17:52:05.661527: step 690, loss 0.241907, acc 0.92
2016-09-07T17:52:06.336190: step 691, loss 0.27653, acc 0.84
2016-09-07T17:52:07.025530: step 692, loss 0.129331, acc 0.94
2016-09-07T17:52:07.695029: step 693, loss 0.1772, acc 0.92
2016-09-07T17:52:08.401321: step 694, loss 0.0610606, acc 0.98
2016-09-07T17:52:09.080416: step 695, loss 0.12305, acc 0.96
2016-09-07T17:52:09.759217: step 696, loss 0.249968, acc 0.9
2016-09-07T17:52:10.443925: step 697, loss 0.158455, acc 0.94
2016-09-07T17:52:11.116082: step 698, loss 0.284038, acc 0.86
2016-09-07T17:52:11.802598: step 699, loss 0.220899, acc 0.9
2016-09-07T17:52:12.489683: step 700, loss 0.220745, acc 0.9

Evaluation:
2016-09-07T17:52:15.479889: step 700, loss 0.59143, acc 0.798

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-700

2016-09-07T17:52:17.195894: step 701, loss 0.227512, acc 0.9
2016-09-07T17:52:17.897330: step 702, loss 0.173039, acc 0.92
2016-09-07T17:52:18.554698: step 703, loss 0.0944302, acc 0.94
2016-09-07T17:52:19.245122: step 704, loss 0.139232, acc 0.92
2016-09-07T17:52:19.936333: step 705, loss 0.149394, acc 0.92
2016-09-07T17:52:20.660702: step 706, loss 0.109769, acc 0.94
2016-09-07T17:52:21.353325: step 707, loss 0.105348, acc 0.94
2016-09-07T17:52:22.058320: step 708, loss 0.142909, acc 0.92
2016-09-07T17:52:22.732555: step 709, loss 0.205553, acc 0.92
2016-09-07T17:52:23.396408: step 710, loss 0.0875108, acc 0.96
2016-09-07T17:52:24.086674: step 711, loss 0.123023, acc 0.94
2016-09-07T17:52:24.749806: step 712, loss 0.11662, acc 0.94
2016-09-07T17:52:25.422603: step 713, loss 0.106294, acc 0.98
2016-09-07T17:52:26.108820: step 714, loss 0.167797, acc 0.94
2016-09-07T17:52:26.782198: step 715, loss 0.105523, acc 0.96
2016-09-07T17:52:27.463980: step 716, loss 0.227304, acc 0.86
2016-09-07T17:52:28.151809: step 717, loss 0.241083, acc 0.86
2016-09-07T17:52:28.859732: step 718, loss 0.203299, acc 0.92
2016-09-07T17:52:29.529978: step 719, loss 0.194253, acc 0.88
2016-09-07T17:52:30.221536: step 720, loss 0.0860678, acc 0.94
2016-09-07T17:52:30.923004: step 721, loss 0.18349, acc 0.92
2016-09-07T17:52:31.615509: step 722, loss 0.243299, acc 0.9
2016-09-07T17:52:32.318768: step 723, loss 0.164007, acc 0.94
2016-09-07T17:52:33.006786: step 724, loss 0.111566, acc 0.96
2016-09-07T17:52:33.673341: step 725, loss 0.176046, acc 0.94
2016-09-07T17:52:34.347973: step 726, loss 0.0909006, acc 0.94
2016-09-07T17:52:35.038938: step 727, loss 0.266184, acc 0.92
2016-09-07T17:52:35.739704: step 728, loss 0.334536, acc 0.84
2016-09-07T17:52:36.406708: step 729, loss 0.172683, acc 0.92
2016-09-07T17:52:37.093512: step 730, loss 0.106515, acc 0.94
2016-09-07T17:52:37.796739: step 731, loss 0.0783969, acc 0.98
2016-09-07T17:52:38.513108: step 732, loss 0.215631, acc 0.92
2016-09-07T17:52:39.182393: step 733, loss 0.258217, acc 0.92
2016-09-07T17:52:39.860195: step 734, loss 0.169097, acc 0.92
2016-09-07T17:52:40.532754: step 735, loss 0.151504, acc 0.94
2016-09-07T17:52:41.224177: step 736, loss 0.390477, acc 0.86
2016-09-07T17:52:41.916877: step 737, loss 0.140943, acc 0.96
2016-09-07T17:52:42.589280: step 738, loss 0.147092, acc 0.96
2016-09-07T17:52:43.313196: step 739, loss 0.180822, acc 0.92
2016-09-07T17:52:44.000980: step 740, loss 0.282236, acc 0.88
2016-09-07T17:52:44.669160: step 741, loss 0.154268, acc 0.92
2016-09-07T17:52:45.338644: step 742, loss 0.223722, acc 0.9
2016-09-07T17:52:46.010333: step 743, loss 0.190699, acc 0.92
2016-09-07T17:52:46.699346: step 744, loss 0.229062, acc 0.92
2016-09-07T17:52:47.378852: step 745, loss 0.198738, acc 0.92
2016-09-07T17:52:48.085337: step 746, loss 0.150854, acc 0.92
2016-09-07T17:52:48.744129: step 747, loss 0.196115, acc 0.9
2016-09-07T17:52:49.425329: step 748, loss 0.14854, acc 0.9
2016-09-07T17:52:50.132302: step 749, loss 0.102007, acc 0.96
2016-09-07T17:52:50.822296: step 750, loss 0.0549664, acc 0.98
2016-09-07T17:52:51.492107: step 751, loss 0.2366, acc 0.84
2016-09-07T17:52:52.161998: step 752, loss 0.231884, acc 0.9
2016-09-07T17:52:52.860007: step 753, loss 0.140745, acc 0.96
2016-09-07T17:52:53.525484: step 754, loss 0.355193, acc 0.88
2016-09-07T17:52:54.237749: step 755, loss 0.128118, acc 0.94
2016-09-07T17:52:54.924414: step 756, loss 0.127145, acc 0.96
2016-09-07T17:52:55.603421: step 757, loss 0.292708, acc 0.82
2016-09-07T17:52:56.313560: step 758, loss 0.104302, acc 0.94
2016-09-07T17:52:56.983859: step 759, loss 0.244656, acc 0.86
2016-09-07T17:52:57.687673: step 760, loss 0.139508, acc 0.96
2016-09-07T17:52:58.353417: step 761, loss 0.236131, acc 0.88
2016-09-07T17:52:59.049657: step 762, loss 0.331328, acc 0.9
2016-09-07T17:52:59.736918: step 763, loss 0.0980679, acc 0.94
2016-09-07T17:53:00.462520: step 764, loss 0.0520014, acc 1
2016-09-07T17:53:01.142749: step 765, loss 0.147625, acc 0.98
2016-09-07T17:53:01.836197: step 766, loss 0.199176, acc 0.86
2016-09-07T17:53:02.536150: step 767, loss 0.378886, acc 0.88
2016-09-07T17:53:03.205723: step 768, loss 0.161357, acc 0.9
2016-09-07T17:53:03.892587: step 769, loss 0.248345, acc 0.88
2016-09-07T17:53:04.611441: step 770, loss 0.141807, acc 0.92
2016-09-07T17:53:05.300210: step 771, loss 0.26123, acc 0.88
2016-09-07T17:53:05.982273: step 772, loss 0.110714, acc 0.94
2016-09-07T17:53:06.652493: step 773, loss 0.193817, acc 0.9
2016-09-07T17:53:07.347868: step 774, loss 0.261355, acc 0.88
2016-09-07T17:53:08.000893: step 775, loss 0.188776, acc 0.94
2016-09-07T17:53:08.348513: step 776, loss 0.105177, acc 0.916667
2016-09-07T17:53:09.034141: step 777, loss 0.0599296, acc 1
2016-09-07T17:53:09.733228: step 778, loss 0.105577, acc 0.96
2016-09-07T17:53:10.416031: step 779, loss 0.174869, acc 0.92
2016-09-07T17:53:11.084796: step 780, loss 0.060608, acc 1
2016-09-07T17:53:11.757024: step 781, loss 0.088589, acc 0.98
2016-09-07T17:53:12.422077: step 782, loss 0.12712, acc 0.96
2016-09-07T17:53:13.115073: step 783, loss 0.112795, acc 0.98
2016-09-07T17:53:13.801498: step 784, loss 0.274019, acc 0.88
2016-09-07T17:53:14.504321: step 785, loss 0.201547, acc 0.88
2016-09-07T17:53:15.153593: step 786, loss 0.0795655, acc 0.98
2016-09-07T17:53:15.862174: step 787, loss 0.0475254, acc 1
2016-09-07T17:53:16.555382: step 788, loss 0.163745, acc 0.94
2016-09-07T17:53:17.275077: step 789, loss 0.0367965, acc 1
2016-09-07T17:53:17.955449: step 790, loss 0.0535292, acc 1
2016-09-07T17:53:18.631818: step 791, loss 0.122461, acc 0.96
2016-09-07T17:53:19.320438: step 792, loss 0.10838, acc 0.96
2016-09-07T17:53:19.984924: step 793, loss 0.0857068, acc 0.98
2016-09-07T17:53:20.698776: step 794, loss 0.143726, acc 0.94
2016-09-07T17:53:21.391229: step 795, loss 0.138596, acc 0.96
2016-09-07T17:53:22.088060: step 796, loss 0.185763, acc 0.94
2016-09-07T17:53:22.788687: step 797, loss 0.20979, acc 0.9
2016-09-07T17:53:23.469881: step 798, loss 0.0749181, acc 0.96
2016-09-07T17:53:24.178806: step 799, loss 0.145772, acc 0.94
2016-09-07T17:53:24.836571: step 800, loss 0.114443, acc 0.94

Evaluation:
2016-09-07T17:53:27.800043: step 800, loss 0.687049, acc 0.784

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-800

2016-09-07T17:53:29.507821: step 801, loss 0.089962, acc 0.96
2016-09-07T17:53:30.207692: step 802, loss 0.0777601, acc 0.96
2016-09-07T17:53:30.877434: step 803, loss 0.0722762, acc 0.96
2016-09-07T17:53:31.560776: step 804, loss 0.0561284, acc 0.98
2016-09-07T17:53:32.238131: step 805, loss 0.092404, acc 1
2016-09-07T17:53:32.921403: step 806, loss 0.102188, acc 0.96
2016-09-07T17:53:33.602384: step 807, loss 0.169651, acc 0.96
2016-09-07T17:53:34.290505: step 808, loss 0.129868, acc 0.88
2016-09-07T17:53:34.989276: step 809, loss 0.0614925, acc 0.98
2016-09-07T17:53:35.639253: step 810, loss 0.0708041, acc 0.98
2016-09-07T17:53:36.299278: step 811, loss 0.111994, acc 0.94
2016-09-07T17:53:36.969394: step 812, loss 0.148723, acc 0.94
2016-09-07T17:53:37.634256: step 813, loss 0.0866591, acc 0.96
2016-09-07T17:53:38.322874: step 814, loss 0.134949, acc 0.94
2016-09-07T17:53:39.008901: step 815, loss 0.299635, acc 0.9
2016-09-07T17:53:39.697595: step 816, loss 0.0234986, acc 1
2016-09-07T17:53:40.348271: step 817, loss 0.239091, acc 0.88
2016-09-07T17:53:41.039368: step 818, loss 0.108225, acc 0.94
2016-09-07T17:53:41.736279: step 819, loss 0.089748, acc 0.96
2016-09-07T17:53:42.422432: step 820, loss 0.065298, acc 0.98
2016-09-07T17:53:43.110117: step 821, loss 0.100075, acc 0.96
2016-09-07T17:53:43.794323: step 822, loss 0.183608, acc 0.9
2016-09-07T17:53:44.461229: step 823, loss 0.148936, acc 0.94
2016-09-07T17:53:45.119475: step 824, loss 0.0679648, acc 0.98
2016-09-07T17:53:45.823890: step 825, loss 0.164345, acc 0.96
2016-09-07T17:53:46.489884: step 826, loss 0.102689, acc 0.92
2016-09-07T17:53:47.154309: step 827, loss 0.160167, acc 0.96
2016-09-07T17:53:47.829721: step 828, loss 0.120005, acc 0.94
2016-09-07T17:53:48.547078: step 829, loss 0.0920861, acc 0.94
2016-09-07T17:53:49.227736: step 830, loss 0.0828713, acc 0.96
2016-09-07T17:53:49.922456: step 831, loss 0.126168, acc 0.92
2016-09-07T17:53:50.640179: step 832, loss 0.149406, acc 0.92
2016-09-07T17:53:51.331092: step 833, loss 0.165957, acc 0.92
2016-09-07T17:53:52.005161: step 834, loss 0.0412319, acc 1
2016-09-07T17:53:52.686008: step 835, loss 0.0640371, acc 0.98
2016-09-07T17:53:53.408559: step 836, loss 0.0178739, acc 1
2016-09-07T17:53:54.081935: step 837, loss 0.139855, acc 0.96
2016-09-07T17:53:54.743535: step 838, loss 0.0605889, acc 1
2016-09-07T17:53:55.446491: step 839, loss 0.130948, acc 0.96
2016-09-07T17:53:56.099157: step 840, loss 0.23428, acc 0.9
2016-09-07T17:53:56.762034: step 841, loss 0.163993, acc 0.9
2016-09-07T17:53:57.430789: step 842, loss 0.188636, acc 0.94
2016-09-07T17:53:58.103067: step 843, loss 0.0680411, acc 0.98
2016-09-07T17:53:58.791334: step 844, loss 0.174473, acc 0.94
2016-09-07T17:53:59.468918: step 845, loss 0.0814641, acc 0.98
2016-09-07T17:54:00.160694: step 846, loss 0.128502, acc 0.94
2016-09-07T17:54:00.866787: step 847, loss 0.097777, acc 0.96
2016-09-07T17:54:01.566640: step 848, loss 0.180009, acc 0.94
2016-09-07T17:54:02.220020: step 849, loss 0.0827547, acc 0.96
2016-09-07T17:54:02.918656: step 850, loss 0.108688, acc 0.96
2016-09-07T17:54:03.592004: step 851, loss 0.116589, acc 0.94
2016-09-07T17:54:04.286255: step 852, loss 0.186584, acc 0.94
2016-09-07T17:54:04.978653: step 853, loss 0.170223, acc 0.92
2016-09-07T17:54:05.641628: step 854, loss 0.207199, acc 0.92
2016-09-07T17:54:06.334937: step 855, loss 0.204325, acc 0.88
2016-09-07T17:54:07.009372: step 856, loss 0.102328, acc 0.98
2016-09-07T17:54:07.678662: step 857, loss 0.0716448, acc 0.98
2016-09-07T17:54:08.366602: step 858, loss 0.0650109, acc 0.98
2016-09-07T17:54:09.023501: step 859, loss 0.215952, acc 0.96
2016-09-07T17:54:09.689305: step 860, loss 0.143972, acc 0.92
2016-09-07T17:54:10.346392: step 861, loss 0.0975183, acc 0.96
2016-09-07T17:54:11.043714: step 862, loss 0.167121, acc 0.94
2016-09-07T17:54:11.705037: step 863, loss 0.043229, acc 0.98
2016-09-07T17:54:12.398509: step 864, loss 0.0676254, acc 1
2016-09-07T17:54:13.092488: step 865, loss 0.05341, acc 0.98
2016-09-07T17:54:13.784564: step 866, loss 0.144122, acc 0.94
2016-09-07T17:54:14.479553: step 867, loss 0.197351, acc 0.9
2016-09-07T17:54:15.166828: step 868, loss 0.0980839, acc 0.98
2016-09-07T17:54:15.845902: step 869, loss 0.119499, acc 0.94
2016-09-07T17:54:16.532107: step 870, loss 0.179896, acc 0.92
2016-09-07T17:54:17.227458: step 871, loss 0.11036, acc 0.94
2016-09-07T17:54:17.907340: step 872, loss 0.101807, acc 0.98
2016-09-07T17:54:18.610592: step 873, loss 0.114367, acc 0.96
2016-09-07T17:54:19.290944: step 874, loss 0.163244, acc 0.94
2016-09-07T17:54:19.982528: step 875, loss 0.121328, acc 0.98
2016-09-07T17:54:20.678536: step 876, loss 0.133561, acc 0.94
2016-09-07T17:54:21.344623: step 877, loss 0.122421, acc 0.96
2016-09-07T17:54:22.010453: step 878, loss 0.0628539, acc 0.94
2016-09-07T17:54:22.694693: step 879, loss 0.0752284, acc 0.98
2016-09-07T17:54:23.379443: step 880, loss 0.163396, acc 0.92
2016-09-07T17:54:24.077505: step 881, loss 0.258424, acc 0.94
2016-09-07T17:54:24.733535: step 882, loss 0.124338, acc 0.96
2016-09-07T17:54:25.420762: step 883, loss 0.115752, acc 0.94
2016-09-07T17:54:26.098955: step 884, loss 0.114105, acc 0.96
2016-09-07T17:54:26.802758: step 885, loss 0.178726, acc 0.92
2016-09-07T17:54:27.507467: step 886, loss 0.0703222, acc 0.94
2016-09-07T17:54:28.202940: step 887, loss 0.178025, acc 0.92
2016-09-07T17:54:28.913491: step 888, loss 0.109998, acc 0.98
2016-09-07T17:54:29.613939: step 889, loss 0.182617, acc 0.92
2016-09-07T17:54:30.312359: step 890, loss 0.0678943, acc 0.96
2016-09-07T17:54:30.991272: step 891, loss 0.0898936, acc 0.94
2016-09-07T17:54:31.680013: step 892, loss 0.132731, acc 0.92
2016-09-07T17:54:32.362847: step 893, loss 0.117704, acc 0.94
2016-09-07T17:54:33.064077: step 894, loss 0.16665, acc 0.9
2016-09-07T17:54:33.754061: step 895, loss 0.161574, acc 0.94
2016-09-07T17:54:34.404495: step 896, loss 0.162021, acc 0.94
2016-09-07T17:54:35.117929: step 897, loss 0.0838111, acc 0.96
2016-09-07T17:54:35.817391: step 898, loss 0.135327, acc 0.92
2016-09-07T17:54:36.501613: step 899, loss 0.170146, acc 0.94
2016-09-07T17:54:37.180851: step 900, loss 0.0845201, acc 0.98

Evaluation:
2016-09-07T17:54:40.192559: step 900, loss 0.791808, acc 0.775

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-900

2016-09-07T17:54:41.845029: step 901, loss 0.173165, acc 0.9
2016-09-07T17:54:42.524113: step 902, loss 0.2022, acc 0.94
2016-09-07T17:54:43.191472: step 903, loss 0.127615, acc 0.98
2016-09-07T17:54:43.867329: step 904, loss 0.11101, acc 0.96
2016-09-07T17:54:44.558075: step 905, loss 0.0737221, acc 0.96
2016-09-07T17:54:45.239758: step 906, loss 0.165047, acc 0.94
2016-09-07T17:54:45.939009: step 907, loss 0.24316, acc 0.94
2016-09-07T17:54:46.615327: step 908, loss 0.0598217, acc 0.96
2016-09-07T17:54:47.294233: step 909, loss 0.153662, acc 0.96
2016-09-07T17:54:47.979876: step 910, loss 0.114538, acc 0.96
2016-09-07T17:54:48.668788: step 911, loss 0.174468, acc 0.92
2016-09-07T17:54:49.371769: step 912, loss 0.0629369, acc 0.98
2016-09-07T17:54:50.065852: step 913, loss 0.123618, acc 0.92
2016-09-07T17:54:50.773102: step 914, loss 0.0741563, acc 0.96
2016-09-07T17:54:51.438508: step 915, loss 0.209901, acc 0.92
2016-09-07T17:54:52.120186: step 916, loss 0.0902296, acc 0.96
2016-09-07T17:54:52.800733: step 917, loss 0.0473855, acc 1
2016-09-07T17:54:53.498036: step 918, loss 0.190692, acc 0.9
2016-09-07T17:54:54.165153: step 919, loss 0.13184, acc 0.92
2016-09-07T17:54:54.847780: step 920, loss 0.15759, acc 0.92
2016-09-07T17:54:55.538851: step 921, loss 0.0753918, acc 0.98
2016-09-07T17:54:56.183414: step 922, loss 0.245668, acc 0.9
2016-09-07T17:54:56.884957: step 923, loss 0.0984265, acc 0.96
2016-09-07T17:54:57.562403: step 924, loss 0.102527, acc 0.96
2016-09-07T17:54:58.260818: step 925, loss 0.0801562, acc 0.96
2016-09-07T17:54:58.948110: step 926, loss 0.165127, acc 0.9
2016-09-07T17:54:59.630670: step 927, loss 0.130395, acc 0.9
2016-09-07T17:55:00.328467: step 928, loss 0.136901, acc 0.9
2016-09-07T17:55:00.991054: step 929, loss 0.138936, acc 0.92
2016-09-07T17:55:01.689632: step 930, loss 0.0867787, acc 0.96
2016-09-07T17:55:02.375918: step 931, loss 0.131202, acc 0.94
2016-09-07T17:55:03.065255: step 932, loss 0.109228, acc 0.96
2016-09-07T17:55:03.754577: step 933, loss 0.0297465, acc 1
2016-09-07T17:55:04.439685: step 934, loss 0.0575626, acc 0.98
2016-09-07T17:55:05.112007: step 935, loss 0.230269, acc 0.92
2016-09-07T17:55:05.772435: step 936, loss 0.0727849, acc 0.98
2016-09-07T17:55:06.472827: step 937, loss 0.109503, acc 0.96
2016-09-07T17:55:07.181020: step 938, loss 0.162708, acc 0.94
2016-09-07T17:55:07.855954: step 939, loss 0.0988001, acc 0.96
2016-09-07T17:55:08.559084: step 940, loss 0.0639258, acc 0.98
2016-09-07T17:55:09.222206: step 941, loss 0.0806584, acc 0.96
2016-09-07T17:55:09.890273: step 942, loss 0.0585308, acc 0.98
2016-09-07T17:55:10.551194: step 943, loss 0.183804, acc 0.94
2016-09-07T17:55:11.246116: step 944, loss 0.0860179, acc 0.96
2016-09-07T17:55:11.901147: step 945, loss 0.264982, acc 0.88
2016-09-07T17:55:12.582451: step 946, loss 0.254545, acc 0.84
2016-09-07T17:55:13.270500: step 947, loss 0.0899894, acc 0.98
2016-09-07T17:55:13.952505: step 948, loss 0.242152, acc 0.86
2016-09-07T17:55:14.630485: step 949, loss 0.133557, acc 0.96
2016-09-07T17:55:15.314720: step 950, loss 0.118302, acc 0.96
2016-09-07T17:55:16.015152: step 951, loss 0.0987783, acc 0.96
2016-09-07T17:55:16.690541: step 952, loss 0.105561, acc 0.96
2016-09-07T17:55:17.370198: step 953, loss 0.107597, acc 0.96
2016-09-07T17:55:18.058241: step 954, loss 0.116978, acc 0.98
2016-09-07T17:55:18.745680: step 955, loss 0.162967, acc 0.92
2016-09-07T17:55:19.430447: step 956, loss 0.0596012, acc 0.98
2016-09-07T17:55:20.129516: step 957, loss 0.15448, acc 0.94
2016-09-07T17:55:20.843443: step 958, loss 0.121772, acc 0.96
2016-09-07T17:55:21.506787: step 959, loss 0.0938418, acc 0.92
2016-09-07T17:55:22.183097: step 960, loss 0.0994476, acc 0.94
2016-09-07T17:55:22.891881: step 961, loss 0.179694, acc 0.96
2016-09-07T17:55:23.559635: step 962, loss 0.155636, acc 0.92
2016-09-07T17:55:24.248398: step 963, loss 0.161804, acc 0.92
2016-09-07T17:55:24.931267: step 964, loss 0.144627, acc 0.96
2016-09-07T17:55:25.654016: step 965, loss 0.0977781, acc 0.94
2016-09-07T17:55:26.328141: step 966, loss 0.106026, acc 0.94
2016-09-07T17:55:26.995752: step 967, loss 0.174816, acc 0.92
2016-09-07T17:55:27.671765: step 968, loss 0.0514872, acc 1
2016-09-07T17:55:28.360953: step 969, loss 0.170784, acc 0.9
2016-09-07T17:55:28.721609: step 970, loss 0.122018, acc 1
2016-09-07T17:55:29.395418: step 971, loss 0.0874422, acc 0.98
2016-09-07T17:55:30.074287: step 972, loss 0.0910047, acc 0.96
2016-09-07T17:55:30.782751: step 973, loss 0.146111, acc 0.94
2016-09-07T17:55:31.474091: step 974, loss 0.113545, acc 0.94
2016-09-07T17:55:32.174535: step 975, loss 0.0826582, acc 0.96
2016-09-07T17:55:32.875642: step 976, loss 0.135305, acc 0.94
2016-09-07T17:55:33.554550: step 977, loss 0.103978, acc 0.96
2016-09-07T17:55:34.252242: step 978, loss 0.146298, acc 0.94
2016-09-07T17:55:34.941392: step 979, loss 0.0397626, acc 0.98
2016-09-07T17:55:35.651364: step 980, loss 0.177977, acc 0.9
2016-09-07T17:55:36.361499: step 981, loss 0.161592, acc 0.92
2016-09-07T17:55:37.039257: step 982, loss 0.152085, acc 0.94
2016-09-07T17:55:37.727208: step 983, loss 0.185944, acc 0.9
2016-09-07T17:55:38.405019: step 984, loss 0.129182, acc 0.98
2016-09-07T17:55:39.082517: step 985, loss 0.0311947, acc 0.98
2016-09-07T17:55:39.765051: step 986, loss 0.155101, acc 0.96
2016-09-07T17:55:40.435191: step 987, loss 0.0966401, acc 0.96
2016-09-07T17:55:41.112388: step 988, loss 0.0176404, acc 1
2016-09-07T17:55:41.798339: step 989, loss 0.160753, acc 0.9
2016-09-07T17:55:42.490087: step 990, loss 0.0276889, acc 1
2016-09-07T17:55:43.162057: step 991, loss 0.0724258, acc 0.96
2016-09-07T17:55:43.831534: step 992, loss 0.176834, acc 0.92
2016-09-07T17:55:44.512113: step 993, loss 0.0469416, acc 1
2016-09-07T17:55:45.195074: step 994, loss 0.130141, acc 0.94
2016-09-07T17:55:45.862393: step 995, loss 0.0705337, acc 0.98
2016-09-07T17:55:46.543560: step 996, loss 0.115898, acc 0.98
2016-09-07T17:55:47.245783: step 997, loss 0.126652, acc 0.94
2016-09-07T17:55:47.941017: step 998, loss 0.090158, acc 0.96
2016-09-07T17:55:48.633151: step 999, loss 0.133195, acc 0.94
2016-09-07T17:55:49.312041: step 1000, loss 0.107844, acc 0.96

Evaluation:
2016-09-07T17:55:52.338561: step 1000, loss 0.850381, acc 0.783

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-1000

2016-09-07T17:55:54.006231: step 1001, loss 0.0810028, acc 0.96
2016-09-07T17:55:54.685859: step 1002, loss 0.117513, acc 0.96
2016-09-07T17:55:55.363360: step 1003, loss 0.11948, acc 0.92
2016-09-07T17:55:56.033244: step 1004, loss 0.107487, acc 0.9
2016-09-07T17:55:56.705550: step 1005, loss 0.129997, acc 0.94
2016-09-07T17:55:57.396216: step 1006, loss 0.0924392, acc 0.94
2016-09-07T17:55:58.107677: step 1007, loss 0.0949236, acc 0.96
2016-09-07T17:55:58.782541: step 1008, loss 0.110583, acc 0.96
2016-09-07T17:55:59.458883: step 1009, loss 0.0632851, acc 0.98
2016-09-07T17:56:00.134028: step 1010, loss 0.0682368, acc 0.98
2016-09-07T17:56:00.857337: step 1011, loss 0.0930331, acc 0.98
2016-09-07T17:56:01.533320: step 1012, loss 0.0412482, acc 0.98
2016-09-07T17:56:02.216634: step 1013, loss 0.0673255, acc 0.96
2016-09-07T17:56:02.920739: step 1014, loss 0.0501113, acc 0.98
2016-09-07T17:56:03.599079: step 1015, loss 0.0378558, acc 0.98
2016-09-07T17:56:04.283323: step 1016, loss 0.056261, acc 0.98
2016-09-07T17:56:04.977072: step 1017, loss 0.188646, acc 0.96
2016-09-07T17:56:05.664958: step 1018, loss 0.0516058, acc 0.98
2016-09-07T17:56:06.338243: step 1019, loss 0.0337213, acc 1
2016-09-07T17:56:07.015231: step 1020, loss 0.142671, acc 0.94
2016-09-07T17:56:07.706668: step 1021, loss 0.0874088, acc 0.96
2016-09-07T17:56:08.358518: step 1022, loss 0.089787, acc 0.96
2016-09-07T17:56:09.050537: step 1023, loss 0.0389378, acc 1
2016-09-07T17:56:09.714323: step 1024, loss 0.0741144, acc 0.96
2016-09-07T17:56:10.385826: step 1025, loss 0.146444, acc 0.94
2016-09-07T17:56:11.082631: step 1026, loss 0.121523, acc 0.94
2016-09-07T17:56:11.782909: step 1027, loss 0.148726, acc 0.92
2016-09-07T17:56:12.501890: step 1028, loss 0.0965475, acc 0.96
2016-09-07T17:56:13.167928: step 1029, loss 0.0571195, acc 0.98
2016-09-07T17:56:13.885047: step 1030, loss 0.0304259, acc 1
2016-09-07T17:56:14.586250: step 1031, loss 0.113566, acc 0.96
2016-09-07T17:56:15.263679: step 1032, loss 0.122099, acc 0.96
2016-09-07T17:56:15.948102: step 1033, loss 0.117046, acc 0.96
2016-09-07T17:56:16.633575: step 1034, loss 0.105351, acc 0.96
2016-09-07T17:56:17.338305: step 1035, loss 0.187953, acc 0.88
2016-09-07T17:56:18.012220: step 1036, loss 0.0581566, acc 0.96
2016-09-07T17:56:18.689241: step 1037, loss 0.00603985, acc 1
2016-09-07T17:56:19.371826: step 1038, loss 0.161169, acc 0.94
2016-09-07T17:56:20.051240: step 1039, loss 0.108617, acc 0.98
2016-09-07T17:56:20.751707: step 1040, loss 0.0781722, acc 0.96
2016-09-07T17:56:21.426515: step 1041, loss 0.0788198, acc 0.98
2016-09-07T17:56:22.109780: step 1042, loss 0.025992, acc 0.98
2016-09-07T17:56:22.784247: step 1043, loss 0.19802, acc 0.92
2016-09-07T17:56:23.483806: step 1044, loss 0.0318093, acc 0.98
2016-09-07T17:56:24.152945: step 1045, loss 0.127109, acc 0.94
2016-09-07T17:56:24.817399: step 1046, loss 0.159016, acc 0.94
2016-09-07T17:56:25.502675: step 1047, loss 0.114829, acc 0.96
2016-09-07T17:56:26.180206: step 1048, loss 0.0550888, acc 0.98
2016-09-07T17:56:26.867405: step 1049, loss 0.0769128, acc 0.96
2016-09-07T17:56:27.547864: step 1050, loss 0.12285, acc 0.94
2016-09-07T17:56:28.269241: step 1051, loss 0.0667306, acc 0.98
2016-09-07T17:56:28.950875: step 1052, loss 0.149095, acc 0.94
2016-09-07T17:56:29.630397: step 1053, loss 0.0863024, acc 0.94
2016-09-07T17:56:30.324030: step 1054, loss 0.0579984, acc 0.98
2016-09-07T17:56:31.015783: step 1055, loss 0.059856, acc 0.98
2016-09-07T17:56:31.676954: step 1056, loss 0.167618, acc 0.96
2016-09-07T17:56:32.329400: step 1057, loss 0.100773, acc 0.94
2016-09-07T17:56:33.030205: step 1058, loss 0.0945946, acc 0.92
2016-09-07T17:56:33.695544: step 1059, loss 0.190107, acc 0.9
2016-09-07T17:56:34.381733: step 1060, loss 0.0831827, acc 1
2016-09-07T17:56:35.067173: step 1061, loss 0.0516375, acc 0.98
2016-09-07T17:56:35.737892: step 1062, loss 0.053607, acc 0.98
2016-09-07T17:56:36.414702: step 1063, loss 0.252401, acc 0.9
2016-09-07T17:56:37.093539: step 1064, loss 0.0706426, acc 0.96
2016-09-07T17:56:37.789429: step 1065, loss 0.0829856, acc 0.98
2016-09-07T17:56:38.455398: step 1066, loss 0.179088, acc 0.92
2016-09-07T17:56:39.137292: step 1067, loss 0.0779742, acc 0.96
2016-09-07T17:56:39.820727: step 1068, loss 0.0300659, acc 1
2016-09-07T17:56:40.500028: step 1069, loss 0.20736, acc 0.92
2016-09-07T17:56:41.178368: step 1070, loss 0.0761947, acc 0.96
2016-09-07T17:56:41.849692: step 1071, loss 0.0317153, acc 1
2016-09-07T17:56:42.529566: step 1072, loss 0.0411537, acc 1
2016-09-07T17:56:43.200644: step 1073, loss 0.133892, acc 0.94
2016-09-07T17:56:43.889547: step 1074, loss 0.0595002, acc 0.98
2016-09-07T17:56:44.561294: step 1075, loss 0.0667507, acc 0.96
2016-09-07T17:56:45.261066: step 1076, loss 0.0863476, acc 0.94
2016-09-07T17:56:45.955070: step 1077, loss 0.181658, acc 0.92
2016-09-07T17:56:46.635709: step 1078, loss 0.159123, acc 0.92
2016-09-07T17:56:47.315644: step 1079, loss 0.129916, acc 0.94
2016-09-07T17:56:48.001432: step 1080, loss 0.110277, acc 0.94
2016-09-07T17:56:48.709402: step 1081, loss 0.0587319, acc 0.98
2016-09-07T17:56:49.383767: step 1082, loss 0.0175904, acc 1
2016-09-07T17:56:50.063467: step 1083, loss 0.144276, acc 0.92
2016-09-07T17:56:50.745020: step 1084, loss 0.0412511, acc 1
2016-09-07T17:56:51.423774: step 1085, loss 0.084416, acc 0.96
2016-09-07T17:56:52.127898: step 1086, loss 0.162439, acc 0.96
2016-09-07T17:56:52.803081: step 1087, loss 0.0935376, acc 0.94
2016-09-07T17:56:53.510973: step 1088, loss 0.0467758, acc 0.98
2016-09-07T17:56:54.178405: step 1089, loss 0.137121, acc 0.94
2016-09-07T17:56:54.864110: step 1090, loss 0.132599, acc 0.92
2016-09-07T17:56:55.548798: step 1091, loss 0.188644, acc 0.9
2016-09-07T17:56:56.231046: step 1092, loss 0.0924946, acc 0.92
2016-09-07T17:56:56.929814: step 1093, loss 0.0663458, acc 0.98
2016-09-07T17:56:57.600418: step 1094, loss 0.0987246, acc 0.94
2016-09-07T17:56:58.311690: step 1095, loss 0.0531216, acc 1
2016-09-07T17:56:58.981949: step 1096, loss 0.127962, acc 0.94
2016-09-07T17:56:59.659145: step 1097, loss 0.208547, acc 0.88
2016-09-07T17:57:00.373895: step 1098, loss 0.0574245, acc 0.96
2016-09-07T17:57:01.067233: step 1099, loss 0.0596889, acc 0.98
2016-09-07T17:57:01.745942: step 1100, loss 0.0629764, acc 1

Evaluation:
2016-09-07T17:57:04.756978: step 1100, loss 0.833647, acc 0.776

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-1100

2016-09-07T17:57:06.459018: step 1101, loss 0.0861377, acc 0.96
2016-09-07T17:57:07.135648: step 1102, loss 0.123069, acc 0.96
2016-09-07T17:57:07.825495: step 1103, loss 0.0648196, acc 0.96
2016-09-07T17:57:08.512794: step 1104, loss 0.0762195, acc 0.98
2016-09-07T17:57:09.183239: step 1105, loss 0.126849, acc 0.96
2016-09-07T17:57:09.894294: step 1106, loss 0.111987, acc 0.96
2016-09-07T17:57:10.568429: step 1107, loss 0.0860523, acc 0.96
2016-09-07T17:57:11.243447: step 1108, loss 0.0183731, acc 1
2016-09-07T17:57:11.923283: step 1109, loss 0.045793, acc 0.98
2016-09-07T17:57:12.636423: step 1110, loss 0.162305, acc 0.94
2016-09-07T17:57:13.318308: step 1111, loss 0.0835079, acc 0.94
2016-09-07T17:57:14.004809: step 1112, loss 0.119807, acc 0.94
2016-09-07T17:57:14.694501: step 1113, loss 0.0151468, acc 1
2016-09-07T17:57:15.390214: step 1114, loss 0.0704487, acc 0.98
2016-09-07T17:57:16.068863: step 1115, loss 0.0247161, acc 1
2016-09-07T17:57:16.743595: step 1116, loss 0.248452, acc 0.86
2016-09-07T17:57:17.446044: step 1117, loss 0.10787, acc 0.96
2016-09-07T17:57:18.116362: step 1118, loss 0.0455318, acc 0.98
2016-09-07T17:57:18.806254: step 1119, loss 0.10945, acc 0.96
2016-09-07T17:57:19.482785: step 1120, loss 0.128218, acc 0.94
2016-09-07T17:57:20.176247: step 1121, loss 0.111817, acc 0.96
2016-09-07T17:57:20.877632: step 1122, loss 0.180234, acc 0.9
2016-09-07T17:57:21.558190: step 1123, loss 0.0631295, acc 0.98
2016-09-07T17:57:22.260259: step 1124, loss 0.111794, acc 0.94
2016-09-07T17:57:22.915479: step 1125, loss 0.0899147, acc 0.98
2016-09-07T17:57:23.610535: step 1126, loss 0.114949, acc 0.98
2016-09-07T17:57:24.289699: step 1127, loss 0.0848681, acc 0.98
2016-09-07T17:57:24.964744: step 1128, loss 0.124049, acc 0.94
2016-09-07T17:57:25.637871: step 1129, loss 0.125311, acc 0.92
2016-09-07T17:57:26.311916: step 1130, loss 0.114551, acc 0.94
2016-09-07T17:57:27.014282: step 1131, loss 0.0600132, acc 0.98
2016-09-07T17:57:27.670762: step 1132, loss 0.120007, acc 0.9
2016-09-07T17:57:28.377911: step 1133, loss 0.0679365, acc 0.98
2016-09-07T17:57:29.064708: step 1134, loss 0.0520803, acc 0.98
2016-09-07T17:57:29.733229: step 1135, loss 0.103022, acc 0.96
2016-09-07T17:57:30.428988: step 1136, loss 0.0412449, acc 1
2016-09-07T17:57:31.127901: step 1137, loss 0.0950419, acc 0.96
2016-09-07T17:57:31.813723: step 1138, loss 0.139782, acc 0.94
2016-09-07T17:57:32.461963: step 1139, loss 0.0404428, acc 0.98
2016-09-07T17:57:33.156514: step 1140, loss 0.111189, acc 0.94
2016-09-07T17:57:33.822004: step 1141, loss 0.0778738, acc 0.96
2016-09-07T17:57:34.518710: step 1142, loss 0.0303594, acc 1
2016-09-07T17:57:35.210361: step 1143, loss 0.114592, acc 0.96
2016-09-07T17:57:35.901916: step 1144, loss 0.031469, acc 1
2016-09-07T17:57:36.578917: step 1145, loss 0.156824, acc 0.96
2016-09-07T17:57:37.236567: step 1146, loss 0.16921, acc 0.92
2016-09-07T17:57:37.937865: step 1147, loss 0.125884, acc 0.92
2016-09-07T17:57:38.630799: step 1148, loss 0.103609, acc 0.94
2016-09-07T17:57:39.317749: step 1149, loss 0.0685987, acc 0.98
2016-09-07T17:57:39.984977: step 1150, loss 0.0638285, acc 0.96
2016-09-07T17:57:40.670086: step 1151, loss 0.122433, acc 0.92
2016-09-07T17:57:41.341731: step 1152, loss 0.0395827, acc 1
2016-09-07T17:57:42.029391: step 1153, loss 0.0808617, acc 0.96
2016-09-07T17:57:42.723502: step 1154, loss 0.0116878, acc 1
2016-09-07T17:57:43.409766: step 1155, loss 0.0566318, acc 0.98
2016-09-07T17:57:44.075408: step 1156, loss 0.0583928, acc 0.98
2016-09-07T17:57:44.773105: step 1157, loss 0.135689, acc 0.92
2016-09-07T17:57:45.456863: step 1158, loss 0.428432, acc 0.9
2016-09-07T17:57:46.135441: step 1159, loss 0.0643317, acc 0.96
2016-09-07T17:57:46.823303: step 1160, loss 0.0765565, acc 0.98
2016-09-07T17:57:47.519375: step 1161, loss 0.0896241, acc 0.94
2016-09-07T17:57:48.181601: step 1162, loss 0.0784557, acc 0.96
2016-09-07T17:57:48.855394: step 1163, loss 0.085847, acc 0.98
2016-09-07T17:57:49.219036: step 1164, loss 0.12109, acc 1
2016-09-07T17:57:49.927020: step 1165, loss 0.100848, acc 0.94
2016-09-07T17:57:50.591189: step 1166, loss 0.0464616, acc 0.98
2016-09-07T17:57:51.270606: step 1167, loss 0.058175, acc 1
2016-09-07T17:57:51.956151: step 1168, loss 0.0583707, acc 0.98
2016-09-07T17:57:52.635570: step 1169, loss 0.124021, acc 0.94
2016-09-07T17:57:53.325278: step 1170, loss 0.246057, acc 0.92
2016-09-07T17:57:54.006142: step 1171, loss 0.0959519, acc 0.96
2016-09-07T17:57:54.718953: step 1172, loss 0.0313474, acc 0.98
2016-09-07T17:57:55.399387: step 1173, loss 0.0323367, acc 1
2016-09-07T17:57:56.093722: step 1174, loss 0.0313834, acc 1
2016-09-07T17:57:56.780241: step 1175, loss 0.079706, acc 0.94
2016-09-07T17:57:57.443831: step 1176, loss 0.199944, acc 0.94
2016-09-07T17:57:58.122542: step 1177, loss 0.0267609, acc 1
2016-09-07T17:57:58.790228: step 1178, loss 0.0209543, acc 1
2016-09-07T17:57:59.499227: step 1179, loss 0.0404753, acc 1
2016-09-07T17:58:00.174456: step 1180, loss 0.0884787, acc 0.96
2016-09-07T17:58:00.888583: step 1181, loss 0.0361858, acc 1
2016-09-07T17:58:01.581127: step 1182, loss 0.0211828, acc 1
2016-09-07T17:58:02.258589: step 1183, loss 0.108654, acc 0.94
2016-09-07T17:58:02.944810: step 1184, loss 0.106214, acc 0.96
2016-09-07T17:58:03.635368: step 1185, loss 0.0604799, acc 0.96
2016-09-07T17:58:04.346250: step 1186, loss 0.122356, acc 0.96
2016-09-07T17:58:05.021545: step 1187, loss 0.028504, acc 0.98
2016-09-07T17:58:05.716704: step 1188, loss 0.0729644, acc 0.96
2016-09-07T17:58:06.392997: step 1189, loss 0.0243772, acc 0.98
2016-09-07T17:58:07.083444: step 1190, loss 0.0414815, acc 0.98
2016-09-07T17:58:07.763921: step 1191, loss 0.0662222, acc 0.96
2016-09-07T17:58:08.440070: step 1192, loss 0.0445502, acc 1
2016-09-07T17:58:09.153154: step 1193, loss 0.0696546, acc 0.96
2016-09-07T17:58:09.827301: step 1194, loss 0.0326056, acc 0.98
2016-09-07T17:58:10.498533: step 1195, loss 0.0295906, acc 0.98
2016-09-07T17:58:11.186088: step 1196, loss 0.0999425, acc 0.94
2016-09-07T17:58:11.856696: step 1197, loss 0.0192863, acc 1
2016-09-07T17:58:12.534823: step 1198, loss 0.0638371, acc 0.96
2016-09-07T17:58:13.202860: step 1199, loss 0.130188, acc 0.94
2016-09-07T17:58:13.880748: step 1200, loss 0.0958576, acc 0.96

Evaluation:
2016-09-07T17:58:16.902179: step 1200, loss 1.14076, acc 0.76

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-1200

2016-09-07T17:58:18.638341: step 1201, loss 0.0490388, acc 0.98
2016-09-07T17:58:19.289457: step 1202, loss 0.0397983, acc 0.98
2016-09-07T17:58:19.988785: step 1203, loss 0.0188176, acc 1
2016-09-07T17:58:20.671185: step 1204, loss 0.0829196, acc 0.98
2016-09-07T17:58:21.327761: step 1205, loss 0.0736889, acc 0.96
2016-09-07T17:58:22.014770: step 1206, loss 0.0952626, acc 0.94
2016-09-07T17:58:22.710752: step 1207, loss 0.108822, acc 0.96
2016-09-07T17:58:23.386230: step 1208, loss 0.0370277, acc 1
2016-09-07T17:58:24.060872: step 1209, loss 0.0487217, acc 0.98
2016-09-07T17:58:24.787933: step 1210, loss 0.374158, acc 0.88
2016-09-07T17:58:25.470995: step 1211, loss 0.0702698, acc 0.96
2016-09-07T17:58:26.188425: step 1212, loss 0.184992, acc 0.92
2016-09-07T17:58:26.854109: step 1213, loss 0.0801233, acc 0.96
2016-09-07T17:58:27.548542: step 1214, loss 0.0734061, acc 0.98
2016-09-07T17:58:28.230312: step 1215, loss 0.156712, acc 0.94
2016-09-07T17:58:28.920005: step 1216, loss 0.0300374, acc 1
2016-09-07T17:58:29.628363: step 1217, loss 0.0550263, acc 0.98
2016-09-07T17:58:30.296603: step 1218, loss 0.0566151, acc 0.98
2016-09-07T17:58:30.978464: step 1219, loss 0.0921845, acc 0.96
2016-09-07T17:58:31.653753: step 1220, loss 0.121704, acc 0.96
2016-09-07T17:58:32.336458: step 1221, loss 0.16793, acc 0.94
2016-09-07T17:58:33.008815: step 1222, loss 0.0802786, acc 0.96
2016-09-07T17:58:33.695823: step 1223, loss 0.0994627, acc 0.96
2016-09-07T17:58:34.386583: step 1224, loss 0.0805727, acc 0.98
2016-09-07T17:58:35.087322: step 1225, loss 0.070361, acc 0.96
2016-09-07T17:58:35.781717: step 1226, loss 0.0543023, acc 0.98
2016-09-07T17:58:36.468408: step 1227, loss 0.0625038, acc 0.96
2016-09-07T17:58:37.151976: step 1228, loss 0.0438124, acc 0.96
2016-09-07T17:58:37.811125: step 1229, loss 0.149363, acc 0.9
2016-09-07T17:58:38.479203: step 1230, loss 0.0906728, acc 0.94
2016-09-07T17:58:39.177968: step 1231, loss 0.038499, acc 0.98
2016-09-07T17:58:39.850620: step 1232, loss 0.0669722, acc 0.98
2016-09-07T17:58:40.545687: step 1233, loss 0.0273725, acc 1
2016-09-07T17:58:41.227587: step 1234, loss 0.0791991, acc 0.96
2016-09-07T17:58:41.892389: step 1235, loss 0.0886726, acc 0.96
2016-09-07T17:58:42.579743: step 1236, loss 0.0628313, acc 0.98
2016-09-07T17:58:43.252240: step 1237, loss 0.0333862, acc 0.98
2016-09-07T17:58:43.944140: step 1238, loss 0.0128446, acc 1
2016-09-07T17:58:44.613791: step 1239, loss 0.0188727, acc 1
2016-09-07T17:58:45.309063: step 1240, loss 0.330206, acc 0.86
2016-09-07T17:58:45.999272: step 1241, loss 0.12925, acc 0.94
2016-09-07T17:58:46.675206: step 1242, loss 0.0809756, acc 0.96
2016-09-07T17:58:47.368012: step 1243, loss 0.0662786, acc 0.96
2016-09-07T17:58:48.054497: step 1244, loss 0.178948, acc 0.92
2016-09-07T17:58:48.745224: step 1245, loss 0.143209, acc 0.92
2016-09-07T17:58:49.416220: step 1246, loss 0.18133, acc 0.96
2016-09-07T17:58:50.099769: step 1247, loss 0.076018, acc 0.96
2016-09-07T17:58:50.781340: step 1248, loss 0.0678933, acc 0.98
2016-09-07T17:58:51.476052: step 1249, loss 0.182108, acc 0.92
2016-09-07T17:58:52.166320: step 1250, loss 0.0546011, acc 1
2016-09-07T17:58:52.845104: step 1251, loss 0.0555651, acc 0.98
2016-09-07T17:58:53.541367: step 1252, loss 0.0508413, acc 0.98
2016-09-07T17:58:54.205458: step 1253, loss 0.242972, acc 0.92
2016-09-07T17:58:54.894286: step 1254, loss 0.04626, acc 0.96
2016-09-07T17:58:55.573858: step 1255, loss 0.137736, acc 0.98
2016-09-07T17:58:56.251176: step 1256, loss 0.0900047, acc 0.96
2016-09-07T17:58:56.940426: step 1257, loss 0.102452, acc 0.94
2016-09-07T17:58:57.614436: step 1258, loss 0.144655, acc 0.98
2016-09-07T17:58:58.290261: step 1259, loss 0.0649217, acc 0.98
2016-09-07T17:58:58.976753: step 1260, loss 0.0851149, acc 0.98
2016-09-07T17:58:59.686403: step 1261, loss 0.0874363, acc 0.94
2016-09-07T17:59:00.406694: step 1262, loss 0.0956193, acc 0.96
2016-09-07T17:59:01.084055: step 1263, loss 0.0720817, acc 0.98
2016-09-07T17:59:01.780244: step 1264, loss 0.0668248, acc 0.96
2016-09-07T17:59:02.447554: step 1265, loss 0.0536415, acc 0.98
2016-09-07T17:59:03.128656: step 1266, loss 0.0881913, acc 0.92
2016-09-07T17:59:03.806432: step 1267, loss 0.0554107, acc 0.94
2016-09-07T17:59:04.518347: step 1268, loss 0.131784, acc 0.96
2016-09-07T17:59:05.191796: step 1269, loss 0.202188, acc 0.92
2016-09-07T17:59:05.869054: step 1270, loss 0.0506441, acc 0.98
2016-09-07T17:59:06.570414: step 1271, loss 0.0336634, acc 0.98
2016-09-07T17:59:07.253110: step 1272, loss 0.041363, acc 0.98
2016-09-07T17:59:07.941520: step 1273, loss 0.111644, acc 0.96
2016-09-07T17:59:08.604308: step 1274, loss 0.0340437, acc 0.98
2016-09-07T17:59:09.302505: step 1275, loss 0.116152, acc 0.96
2016-09-07T17:59:09.967307: step 1276, loss 0.0160495, acc 1
2016-09-07T17:59:10.655903: step 1277, loss 0.102412, acc 0.96
2016-09-07T17:59:11.347915: step 1278, loss 0.0985706, acc 0.96
2016-09-07T17:59:12.025460: step 1279, loss 0.155807, acc 0.94
2016-09-07T17:59:12.699538: step 1280, loss 0.0788486, acc 0.96
2016-09-07T17:59:13.380978: step 1281, loss 0.0443276, acc 0.98
2016-09-07T17:59:14.094037: step 1282, loss 0.0583557, acc 0.98
2016-09-07T17:59:14.760956: step 1283, loss 0.011589, acc 1
2016-09-07T17:59:15.451781: step 1284, loss 0.0529248, acc 0.98
2016-09-07T17:59:16.127253: step 1285, loss 0.123676, acc 0.96
2016-09-07T17:59:16.813969: step 1286, loss 0.137244, acc 0.96
2016-09-07T17:59:17.485721: step 1287, loss 0.0709673, acc 0.98
2016-09-07T17:59:18.173392: step 1288, loss 0.0863676, acc 0.98
2016-09-07T17:59:18.873020: step 1289, loss 0.271804, acc 0.9
2016-09-07T17:59:19.528912: step 1290, loss 0.123221, acc 0.96
2016-09-07T17:59:20.223431: step 1291, loss 0.12485, acc 0.94
2016-09-07T17:59:20.911313: step 1292, loss 0.0736914, acc 0.96
2016-09-07T17:59:21.592347: step 1293, loss 0.0971757, acc 0.96
2016-09-07T17:59:22.246117: step 1294, loss 0.0676341, acc 0.98
2016-09-07T17:59:22.919316: step 1295, loss 0.190772, acc 0.92
2016-09-07T17:59:23.624522: step 1296, loss 0.130452, acc 0.92
2016-09-07T17:59:24.272900: step 1297, loss 0.0242815, acc 0.98
2016-09-07T17:59:24.967361: step 1298, loss 0.149747, acc 0.94
2016-09-07T17:59:25.632978: step 1299, loss 0.0870964, acc 0.96
2016-09-07T17:59:26.335277: step 1300, loss 0.202341, acc 0.9

Evaluation:
2016-09-07T17:59:29.439611: step 1300, loss 0.96424, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-1300

2016-09-07T17:59:31.125230: step 1301, loss 0.190467, acc 0.92
2016-09-07T17:59:31.791271: step 1302, loss 0.056801, acc 0.98
2016-09-07T17:59:32.477198: step 1303, loss 0.111272, acc 0.94
2016-09-07T17:59:33.159361: step 1304, loss 0.0941704, acc 0.94
2016-09-07T17:59:33.817769: step 1305, loss 0.0442315, acc 1
2016-09-07T17:59:34.513870: step 1306, loss 0.0709082, acc 1
2016-09-07T17:59:35.188728: step 1307, loss 0.0779573, acc 0.94
2016-09-07T17:59:35.852043: step 1308, loss 0.102156, acc 0.96
2016-09-07T17:59:36.525655: step 1309, loss 0.126002, acc 0.94
2016-09-07T17:59:37.191503: step 1310, loss 0.0544763, acc 0.98
2016-09-07T17:59:37.882637: step 1311, loss 0.137983, acc 0.94
2016-09-07T17:59:38.569458: step 1312, loss 0.0390087, acc 0.98
2016-09-07T17:59:39.247350: step 1313, loss 0.0955263, acc 0.96
2016-09-07T17:59:39.913974: step 1314, loss 0.0601562, acc 0.96
2016-09-07T17:59:40.589406: step 1315, loss 0.179639, acc 0.94
2016-09-07T17:59:41.249004: step 1316, loss 0.0696706, acc 0.98
2016-09-07T17:59:41.909089: step 1317, loss 0.103833, acc 0.96
2016-09-07T17:59:42.606027: step 1318, loss 0.169825, acc 0.94
2016-09-07T17:59:43.290356: step 1319, loss 0.0326508, acc 1
2016-09-07T17:59:43.972159: step 1320, loss 0.0306663, acc 1
2016-09-07T17:59:44.664586: step 1321, loss 0.0578285, acc 0.98
2016-09-07T17:59:45.388003: step 1322, loss 0.128573, acc 0.92
2016-09-07T17:59:46.043118: step 1323, loss 0.0681509, acc 0.98
2016-09-07T17:59:46.715665: step 1324, loss 0.0950413, acc 0.96
2016-09-07T17:59:47.417975: step 1325, loss 0.220337, acc 0.92
2016-09-07T17:59:48.091070: step 1326, loss 0.0322447, acc 1
2016-09-07T17:59:48.776659: step 1327, loss 0.0266822, acc 1
2016-09-07T17:59:49.473023: step 1328, loss 0.0220519, acc 0.98
2016-09-07T17:59:50.185845: step 1329, loss 0.00649306, acc 1
2016-09-07T17:59:50.863467: step 1330, loss 0.0943633, acc 0.94
2016-09-07T17:59:51.544926: step 1331, loss 0.0884569, acc 0.96
2016-09-07T17:59:52.241717: step 1332, loss 0.0496264, acc 0.98
2016-09-07T17:59:52.921363: step 1333, loss 0.0262181, acc 0.98
2016-09-07T17:59:53.613013: step 1334, loss 0.0563546, acc 1
2016-09-07T17:59:54.305732: step 1335, loss 0.0732405, acc 0.96
2016-09-07T17:59:55.024433: step 1336, loss 0.161766, acc 0.94
2016-09-07T17:59:55.701599: step 1337, loss 0.141995, acc 0.98
2016-09-07T17:59:56.374901: step 1338, loss 0.0155305, acc 1
2016-09-07T17:59:57.055515: step 1339, loss 0.103026, acc 0.94
2016-09-07T17:59:57.736841: step 1340, loss 0.0495845, acc 0.98
2016-09-07T17:59:58.434110: step 1341, loss 0.0132584, acc 1
2016-09-07T17:59:59.103957: step 1342, loss 0.0451552, acc 0.96
2016-09-07T17:59:59.801963: step 1343, loss 0.0630256, acc 0.98
2016-09-07T18:00:00.543618: step 1344, loss 0.0370685, acc 1
2016-09-07T18:00:01.214390: step 1345, loss 0.0353198, acc 0.98
2016-09-07T18:00:01.908098: step 1346, loss 0.0662511, acc 0.98
2016-09-07T18:00:02.609697: step 1347, loss 0.0486159, acc 1
2016-09-07T18:00:03.311064: step 1348, loss 0.0537118, acc 0.98
2016-09-07T18:00:03.988386: step 1349, loss 0.0415865, acc 0.98
2016-09-07T18:00:04.669604: step 1350, loss 0.0115528, acc 1
2016-09-07T18:00:05.355394: step 1351, loss 0.130944, acc 0.94
2016-09-07T18:00:06.028657: step 1352, loss 0.0634081, acc 0.98
2016-09-07T18:00:06.699183: step 1353, loss 0.0142616, acc 1
2016-09-07T18:00:07.372854: step 1354, loss 0.189476, acc 0.92
2016-09-07T18:00:08.067052: step 1355, loss 0.0272314, acc 1
2016-09-07T18:00:08.717867: step 1356, loss 0.0931228, acc 0.96
2016-09-07T18:00:09.408953: step 1357, loss 0.00464558, acc 1
2016-09-07T18:00:09.778828: step 1358, loss 0.129295, acc 0.916667
2016-09-07T18:00:10.474901: step 1359, loss 0.0351385, acc 0.98
2016-09-07T18:00:11.144467: step 1360, loss 0.111177, acc 0.94
2016-09-07T18:00:11.842885: step 1361, loss 0.210149, acc 0.94
2016-09-07T18:00:12.515755: step 1362, loss 0.0151914, acc 1
2016-09-07T18:00:13.206052: step 1363, loss 0.142426, acc 0.94
2016-09-07T18:00:13.895823: step 1364, loss 0.115581, acc 0.92
2016-09-07T18:00:14.587083: step 1365, loss 0.0675411, acc 0.98
2016-09-07T18:00:15.251258: step 1366, loss 0.0844264, acc 0.96
2016-09-07T18:00:15.919428: step 1367, loss 0.0873602, acc 0.96
2016-09-07T18:00:16.619119: step 1368, loss 0.140182, acc 0.96
2016-09-07T18:00:17.288951: step 1369, loss 0.077154, acc 0.98
2016-09-07T18:00:17.953677: step 1370, loss 0.0184855, acc 1
2016-09-07T18:00:18.622181: step 1371, loss 0.148991, acc 0.9
2016-09-07T18:00:19.303634: step 1372, loss 0.110564, acc 0.96
2016-09-07T18:00:19.980380: step 1373, loss 0.0611068, acc 0.98
2016-09-07T18:00:20.666577: step 1374, loss 0.0571578, acc 0.98
2016-09-07T18:00:21.342816: step 1375, loss 0.0175495, acc 1
2016-09-07T18:00:22.003414: step 1376, loss 0.0180015, acc 1
2016-09-07T18:00:22.673604: step 1377, loss 0.0653407, acc 0.98
2016-09-07T18:00:23.331515: step 1378, loss 0.0680535, acc 0.96
2016-09-07T18:00:24.006465: step 1379, loss 0.139349, acc 0.92
2016-09-07T18:00:24.686513: step 1380, loss 0.0829365, acc 0.96
2016-09-07T18:00:25.376132: step 1381, loss 0.0226961, acc 1
2016-09-07T18:00:26.054501: step 1382, loss 0.0333918, acc 0.98
2016-09-07T18:00:26.735938: step 1383, loss 0.0391512, acc 0.98
2016-09-07T18:00:27.437071: step 1384, loss 0.108804, acc 0.92
2016-09-07T18:00:28.113872: step 1385, loss 0.048637, acc 0.98
2016-09-07T18:00:28.791901: step 1386, loss 0.167581, acc 0.9
2016-09-07T18:00:29.483858: step 1387, loss 0.0627144, acc 0.98
2016-09-07T18:00:30.180379: step 1388, loss 0.0307474, acc 0.98
2016-09-07T18:00:30.870570: step 1389, loss 0.0388766, acc 1
2016-09-07T18:00:31.548965: step 1390, loss 0.0893705, acc 0.96
2016-09-07T18:00:32.257468: step 1391, loss 0.0702664, acc 0.98
2016-09-07T18:00:32.926496: step 1392, loss 0.0839363, acc 0.98
2016-09-07T18:00:33.604545: step 1393, loss 0.039221, acc 0.98
2016-09-07T18:00:34.321946: step 1394, loss 0.156106, acc 0.96
2016-09-07T18:00:35.005027: step 1395, loss 0.0349585, acc 1
2016-09-07T18:00:35.665877: step 1396, loss 0.136047, acc 0.94
2016-09-07T18:00:36.346877: step 1397, loss 0.0181561, acc 1
2016-09-07T18:00:37.044416: step 1398, loss 0.0273821, acc 1
2016-09-07T18:00:37.719769: step 1399, loss 0.0421837, acc 1
2016-09-07T18:00:38.399859: step 1400, loss 0.0763531, acc 0.96

Evaluation:
2016-09-07T18:00:41.573021: step 1400, loss 1.21284, acc 0.759

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-1400

2016-09-07T18:00:43.174921: step 1401, loss 0.0335496, acc 0.98
2016-09-07T18:00:43.839008: step 1402, loss 0.0645781, acc 0.96
2016-09-07T18:00:44.523337: step 1403, loss 0.0310462, acc 1
2016-09-07T18:00:45.193486: step 1404, loss 0.0162693, acc 1
2016-09-07T18:00:45.885941: step 1405, loss 0.144593, acc 0.92
2016-09-07T18:00:46.618756: step 1406, loss 0.140424, acc 0.94
2016-09-07T18:00:47.269749: step 1407, loss 0.0473108, acc 1
2016-09-07T18:00:47.958607: step 1408, loss 0.111223, acc 0.94
2016-09-07T18:00:48.647326: step 1409, loss 0.0377302, acc 0.98
2016-09-07T18:00:49.332227: step 1410, loss 0.0170404, acc 1
2016-09-07T18:00:50.033531: step 1411, loss 0.0157454, acc 1
2016-09-07T18:00:50.699812: step 1412, loss 0.246495, acc 0.88
2016-09-07T18:00:51.372481: step 1413, loss 0.029151, acc 0.98
2016-09-07T18:00:52.026222: step 1414, loss 0.0826473, acc 0.94
2016-09-07T18:00:52.723672: step 1415, loss 0.113645, acc 0.96
2016-09-07T18:00:53.424005: step 1416, loss 0.032985, acc 0.98
2016-09-07T18:00:54.104560: step 1417, loss 0.0240404, acc 0.98
2016-09-07T18:00:54.772618: step 1418, loss 0.0491163, acc 0.98
2016-09-07T18:00:55.462560: step 1419, loss 0.091489, acc 0.96
2016-09-07T18:00:56.145119: step 1420, loss 0.0946273, acc 0.96
2016-09-07T18:00:56.820335: step 1421, loss 0.0779081, acc 0.96
2016-09-07T18:00:57.537332: step 1422, loss 0.114437, acc 0.96
2016-09-07T18:00:58.223756: step 1423, loss 0.134153, acc 0.94
2016-09-07T18:00:58.928843: step 1424, loss 0.242961, acc 0.92
2016-09-07T18:00:59.625982: step 1425, loss 0.0655607, acc 0.96
2016-09-07T18:01:00.316048: step 1426, loss 0.0518997, acc 0.96
2016-09-07T18:01:01.007169: step 1427, loss 0.124466, acc 0.98
2016-09-07T18:01:01.673305: step 1428, loss 0.0744868, acc 0.96
2016-09-07T18:01:02.383781: step 1429, loss 0.117734, acc 0.94
2016-09-07T18:01:03.070183: step 1430, loss 0.0403597, acc 0.98
2016-09-07T18:01:03.755373: step 1431, loss 0.059969, acc 0.96
2016-09-07T18:01:04.430293: step 1432, loss 0.0587577, acc 0.98
2016-09-07T18:01:05.097924: step 1433, loss 0.0447202, acc 0.98
2016-09-07T18:01:05.796963: step 1434, loss 0.124642, acc 0.94
2016-09-07T18:01:06.457694: step 1435, loss 0.0710299, acc 0.96
2016-09-07T18:01:07.174618: step 1436, loss 0.0548546, acc 0.96
2016-09-07T18:01:07.865747: step 1437, loss 0.112265, acc 0.96
2016-09-07T18:01:08.557383: step 1438, loss 0.0678646, acc 0.96
2016-09-07T18:01:09.247403: step 1439, loss 0.111489, acc 0.94
2016-09-07T18:01:09.917010: step 1440, loss 0.0921389, acc 0.96
2016-09-07T18:01:10.598292: step 1441, loss 0.0542781, acc 0.98
2016-09-07T18:01:11.267642: step 1442, loss 0.0428551, acc 0.98
2016-09-07T18:01:11.963914: step 1443, loss 0.0344428, acc 0.98
2016-09-07T18:01:12.638051: step 1444, loss 0.0492643, acc 0.96
2016-09-07T18:01:13.325454: step 1445, loss 0.0736149, acc 0.96
2016-09-07T18:01:14.005968: step 1446, loss 0.0220035, acc 1
2016-09-07T18:01:14.684475: step 1447, loss 0.0523866, acc 0.98
2016-09-07T18:01:15.369486: step 1448, loss 0.133478, acc 0.96
2016-09-07T18:01:16.046902: step 1449, loss 0.148667, acc 0.94
2016-09-07T18:01:16.748552: step 1450, loss 0.145207, acc 0.94
2016-09-07T18:01:17.407783: step 1451, loss 0.0512991, acc 0.96
2016-09-07T18:01:18.084554: step 1452, loss 0.0924096, acc 0.96
2016-09-07T18:01:18.743295: step 1453, loss 0.0593375, acc 0.98
2016-09-07T18:01:19.416466: step 1454, loss 0.0515384, acc 0.98
2016-09-07T18:01:20.098150: step 1455, loss 0.0170933, acc 1
2016-09-07T18:01:20.781093: step 1456, loss 0.132166, acc 0.94
2016-09-07T18:01:21.489721: step 1457, loss 0.111454, acc 0.96
2016-09-07T18:01:22.156325: step 1458, loss 0.0441619, acc 0.98
2016-09-07T18:01:22.855589: step 1459, loss 0.0369182, acc 1
2016-09-07T18:01:23.545384: step 1460, loss 0.0508832, acc 0.96
2016-09-07T18:01:24.233351: step 1461, loss 0.0380563, acc 1
2016-09-07T18:01:24.934585: step 1462, loss 0.116907, acc 0.94
2016-09-07T18:01:25.623778: step 1463, loss 0.0312127, acc 0.98
2016-09-07T18:01:26.336937: step 1464, loss 0.220292, acc 0.94
2016-09-07T18:01:27.015765: step 1465, loss 0.0499097, acc 0.96
2016-09-07T18:01:27.705811: step 1466, loss 0.052099, acc 0.98
2016-09-07T18:01:28.399286: step 1467, loss 0.0554626, acc 0.96
2016-09-07T18:01:29.088462: step 1468, loss 0.103198, acc 0.98
2016-09-07T18:01:29.753670: step 1469, loss 0.0568688, acc 0.98
2016-09-07T18:01:30.442424: step 1470, loss 0.00849186, acc 1
2016-09-07T18:01:31.178500: step 1471, loss 0.0548392, acc 0.96
2016-09-07T18:01:31.866973: step 1472, loss 0.0642522, acc 0.96
2016-09-07T18:01:32.543395: step 1473, loss 0.00646685, acc 1
2016-09-07T18:01:33.226045: step 1474, loss 0.0655232, acc 0.96
2016-09-07T18:01:33.901578: step 1475, loss 0.0775943, acc 0.96
2016-09-07T18:01:34.581397: step 1476, loss 0.0635632, acc 0.96
2016-09-07T18:01:35.267476: step 1477, loss 0.0822446, acc 0.96
2016-09-07T18:01:35.961610: step 1478, loss 0.10298, acc 0.94
2016-09-07T18:01:36.626492: step 1479, loss 0.153439, acc 0.94
2016-09-07T18:01:37.300611: step 1480, loss 0.0716546, acc 0.96
2016-09-07T18:01:37.983115: step 1481, loss 0.0510227, acc 0.98
2016-09-07T18:01:38.666505: step 1482, loss 0.0151182, acc 1
2016-09-07T18:01:39.338256: step 1483, loss 0.103557, acc 0.96
2016-09-07T18:01:39.999691: step 1484, loss 0.0370751, acc 1
2016-09-07T18:01:40.663797: step 1485, loss 0.15584, acc 0.94
2016-09-07T18:01:41.339866: step 1486, loss 0.0659935, acc 0.94
2016-09-07T18:01:42.011652: step 1487, loss 0.0596076, acc 0.98
2016-09-07T18:01:42.676967: step 1488, loss 0.00501492, acc 1
2016-09-07T18:01:43.347479: step 1489, loss 0.10105, acc 0.94
2016-09-07T18:01:44.026042: step 1490, loss 0.0832949, acc 0.96
2016-09-07T18:01:44.728387: step 1491, loss 0.0298689, acc 1
2016-09-07T18:01:45.397645: step 1492, loss 0.0492288, acc 0.98
2016-09-07T18:01:46.070398: step 1493, loss 0.160708, acc 0.98
2016-09-07T18:01:46.770684: step 1494, loss 0.101241, acc 0.98
2016-09-07T18:01:47.432280: step 1495, loss 0.0523031, acc 1
2016-09-07T18:01:48.123254: step 1496, loss 0.0549617, acc 0.96
2016-09-07T18:01:48.807894: step 1497, loss 0.0612408, acc 0.98
2016-09-07T18:01:49.493272: step 1498, loss 0.0925493, acc 0.98
2016-09-07T18:01:50.151148: step 1499, loss 0.0481041, acc 0.98
2016-09-07T18:01:50.836033: step 1500, loss 0.035093, acc 0.96

Evaluation:
2016-09-07T18:01:53.999114: step 1500, loss 1.1327, acc 0.764

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-1500

2016-09-07T18:01:55.742536: step 1501, loss 0.0582019, acc 0.96
2016-09-07T18:01:56.444010: step 1502, loss 0.0270601, acc 1
2016-09-07T18:01:57.131156: step 1503, loss 0.0467815, acc 0.98
2016-09-07T18:01:57.824647: step 1504, loss 0.0926491, acc 0.94
2016-09-07T18:01:58.520497: step 1505, loss 0.0440466, acc 0.98
2016-09-07T18:01:59.183793: step 1506, loss 0.141236, acc 0.9
2016-09-07T18:01:59.856302: step 1507, loss 0.0260455, acc 0.98
2016-09-07T18:02:00.558340: step 1508, loss 0.00308351, acc 1
2016-09-07T18:02:01.257142: step 1509, loss 0.0437011, acc 0.96
2016-09-07T18:02:01.935474: step 1510, loss 0.0504579, acc 0.94
2016-09-07T18:02:02.612429: step 1511, loss 0.168859, acc 0.92
2016-09-07T18:02:03.281501: step 1512, loss 0.15959, acc 0.9
2016-09-07T18:02:03.970765: step 1513, loss 0.114718, acc 0.96
2016-09-07T18:02:04.664754: step 1514, loss 0.069549, acc 0.96
2016-09-07T18:02:05.333663: step 1515, loss 0.027498, acc 0.98
2016-09-07T18:02:06.017178: step 1516, loss 0.0916235, acc 0.98
2016-09-07T18:02:06.665366: step 1517, loss 0.0168543, acc 1
2016-09-07T18:02:07.351403: step 1518, loss 0.0640956, acc 0.96
2016-09-07T18:02:08.017086: step 1519, loss 0.0545697, acc 0.96
2016-09-07T18:02:08.680331: step 1520, loss 0.054133, acc 0.96
2016-09-07T18:02:09.355571: step 1521, loss 0.0282254, acc 0.98
2016-09-07T18:02:10.025247: step 1522, loss 0.132998, acc 0.94
2016-09-07T18:02:10.718482: step 1523, loss 0.0376223, acc 1
2016-09-07T18:02:11.397156: step 1524, loss 0.119699, acc 0.94
2016-09-07T18:02:12.094557: step 1525, loss 0.0438028, acc 0.98
2016-09-07T18:02:12.766668: step 1526, loss 0.0131601, acc 1
2016-09-07T18:02:13.436149: step 1527, loss 0.134802, acc 0.96
2016-09-07T18:02:14.123792: step 1528, loss 0.103463, acc 0.96
2016-09-07T18:02:14.807051: step 1529, loss 0.0495054, acc 1
2016-09-07T18:02:15.510511: step 1530, loss 0.0624356, acc 0.98
2016-09-07T18:02:16.166635: step 1531, loss 0.113594, acc 0.94
2016-09-07T18:02:16.867131: step 1532, loss 0.0991264, acc 0.94
2016-09-07T18:02:17.562515: step 1533, loss 0.0503141, acc 1
2016-09-07T18:02:18.219388: step 1534, loss 0.0612171, acc 0.98
2016-09-07T18:02:18.909669: step 1535, loss 0.0592791, acc 0.98
2016-09-07T18:02:19.585603: step 1536, loss 0.0730526, acc 0.98
2016-09-07T18:02:20.262038: step 1537, loss 0.0313163, acc 1
2016-09-07T18:02:20.940156: step 1538, loss 0.135578, acc 0.94
2016-09-07T18:02:21.645824: step 1539, loss 0.042033, acc 0.98
2016-09-07T18:02:22.336921: step 1540, loss 0.0429293, acc 0.98
2016-09-07T18:02:23.009831: step 1541, loss 0.0613384, acc 0.98
2016-09-07T18:02:23.678457: step 1542, loss 0.0193581, acc 1
2016-09-07T18:02:24.380675: step 1543, loss 0.0828247, acc 0.96
2016-09-07T18:02:25.062084: step 1544, loss 0.0335375, acc 0.98
2016-09-07T18:02:25.734882: step 1545, loss 0.108804, acc 0.98
2016-09-07T18:02:26.434418: step 1546, loss 0.066929, acc 0.96
2016-09-07T18:02:27.099207: step 1547, loss 0.151718, acc 0.94
2016-09-07T18:02:27.807021: step 1548, loss 0.041944, acc 0.98
2016-09-07T18:02:28.498672: step 1549, loss 0.0217126, acc 0.98
2016-09-07T18:02:29.186408: step 1550, loss 0.0577962, acc 0.98
2016-09-07T18:02:29.875501: step 1551, loss 0.0809104, acc 0.94
2016-09-07T18:02:30.271226: step 1552, loss 0.000265605, acc 1
2016-09-07T18:02:30.962160: step 1553, loss 0.0629524, acc 0.98
2016-09-07T18:02:31.652875: step 1554, loss 0.0531087, acc 0.96
2016-09-07T18:02:32.350294: step 1555, loss 0.0512566, acc 0.98
2016-09-07T18:02:33.013540: step 1556, loss 0.00729649, acc 1
2016-09-07T18:02:33.704226: step 1557, loss 0.0596713, acc 0.98
2016-09-07T18:02:34.380393: step 1558, loss 0.156556, acc 0.96
2016-09-07T18:02:35.079594: step 1559, loss 0.0315895, acc 0.98
2016-09-07T18:02:35.777833: step 1560, loss 0.0437111, acc 0.96
2016-09-07T18:02:36.452181: step 1561, loss 0.0275357, acc 0.98
2016-09-07T18:02:37.126929: step 1562, loss 0.0532664, acc 0.98
2016-09-07T18:02:37.811754: step 1563, loss 0.0511881, acc 0.98
2016-09-07T18:02:38.499558: step 1564, loss 0.0526066, acc 0.98
2016-09-07T18:02:39.170813: step 1565, loss 0.128962, acc 0.94
2016-09-07T18:02:39.847835: step 1566, loss 0.0522288, acc 0.96
2016-09-07T18:02:40.519871: step 1567, loss 0.0329935, acc 0.98
2016-09-07T18:02:41.205962: step 1568, loss 0.0801608, acc 0.94
2016-09-07T18:02:41.883014: step 1569, loss 0.0916113, acc 0.94
2016-09-07T18:02:42.543910: step 1570, loss 0.0465728, acc 0.96
2016-09-07T18:02:43.238350: step 1571, loss 0.113999, acc 0.96
2016-09-07T18:02:43.889259: step 1572, loss 0.0732618, acc 0.98
2016-09-07T18:02:44.602547: step 1573, loss 0.0225624, acc 0.98
2016-09-07T18:02:45.280546: step 1574, loss 0.0903838, acc 0.96
2016-09-07T18:02:45.984691: step 1575, loss 0.0169982, acc 1
2016-09-07T18:02:46.669665: step 1576, loss 0.0501551, acc 0.96
2016-09-07T18:02:47.343196: step 1577, loss 0.0478671, acc 0.98
2016-09-07T18:02:48.015981: step 1578, loss 0.0152473, acc 1
2016-09-07T18:02:48.687390: step 1579, loss 0.0479507, acc 0.98
2016-09-07T18:02:49.374261: step 1580, loss 0.16107, acc 0.98
2016-09-07T18:02:50.058416: step 1581, loss 0.0240333, acc 1
2016-09-07T18:02:50.726296: step 1582, loss 0.14638, acc 0.96
2016-09-07T18:02:51.403367: step 1583, loss 0.0515986, acc 0.98
2016-09-07T18:02:52.087773: step 1584, loss 0.0818603, acc 0.94
2016-09-07T18:02:52.758980: step 1585, loss 0.062687, acc 0.98
2016-09-07T18:02:53.426562: step 1586, loss 0.0542611, acc 0.98
2016-09-07T18:02:54.116747: step 1587, loss 0.0365873, acc 1
2016-09-07T18:02:54.803844: step 1588, loss 0.0708469, acc 0.98
2016-09-07T18:02:55.478862: step 1589, loss 0.0533167, acc 0.98
2016-09-07T18:02:56.165142: step 1590, loss 0.0757205, acc 0.98
2016-09-07T18:02:56.869237: step 1591, loss 0.0881531, acc 0.96
2016-09-07T18:02:57.559236: step 1592, loss 0.0419126, acc 1
2016-09-07T18:02:58.233828: step 1593, loss 0.142381, acc 0.94
2016-09-07T18:02:58.923669: step 1594, loss 0.0394818, acc 0.98
2016-09-07T18:02:59.622474: step 1595, loss 0.0218622, acc 1
2016-09-07T18:03:00.355391: step 1596, loss 0.0160455, acc 1
2016-09-07T18:03:01.027666: step 1597, loss 0.140673, acc 0.94
2016-09-07T18:03:01.746231: step 1598, loss 0.0633499, acc 0.96
2016-09-07T18:03:02.427666: step 1599, loss 0.0242101, acc 0.98
2016-09-07T18:03:03.126909: step 1600, loss 0.0376857, acc 0.98

Evaluation:
2016-09-07T18:03:06.329811: step 1600, loss 1.30918, acc 0.754

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-1600

2016-09-07T18:03:08.116189: step 1601, loss 0.024307, acc 1
2016-09-07T18:03:08.808140: step 1602, loss 0.039346, acc 0.96
2016-09-07T18:03:09.501729: step 1603, loss 0.0246877, acc 1
2016-09-07T18:03:10.189297: step 1604, loss 0.110116, acc 0.92
2016-09-07T18:03:10.867770: step 1605, loss 0.00330175, acc 1
2016-09-07T18:03:11.534211: step 1606, loss 0.0516529, acc 0.98
2016-09-07T18:03:12.239090: step 1607, loss 0.0368104, acc 1
2016-09-07T18:03:12.917895: step 1608, loss 0.022897, acc 1
2016-09-07T18:03:13.599145: step 1609, loss 0.0748111, acc 0.96
2016-09-07T18:03:14.282923: step 1610, loss 0.00121788, acc 1
2016-09-07T18:03:14.971683: step 1611, loss 0.0283139, acc 1
2016-09-07T18:03:15.652339: step 1612, loss 0.0407573, acc 0.98
2016-09-07T18:03:16.308237: step 1613, loss 0.13075, acc 0.96
2016-09-07T18:03:17.009654: step 1614, loss 0.072246, acc 0.96
2016-09-07T18:03:17.663906: step 1615, loss 0.0274543, acc 0.98
2016-09-07T18:03:18.333953: step 1616, loss 0.168137, acc 0.94
2016-09-07T18:03:19.036305: step 1617, loss 0.0268346, acc 1
2016-09-07T18:03:19.728596: step 1618, loss 0.15244, acc 0.96
2016-09-07T18:03:20.423300: step 1619, loss 0.109719, acc 0.96
2016-09-07T18:03:21.092803: step 1620, loss 0.0579435, acc 0.98
2016-09-07T18:03:21.774297: step 1621, loss 0.0347343, acc 0.98
2016-09-07T18:03:22.427429: step 1622, loss 0.0449164, acc 1
2016-09-07T18:03:23.113037: step 1623, loss 0.0927153, acc 0.96
2016-09-07T18:03:23.791520: step 1624, loss 0.0259068, acc 1
2016-09-07T18:03:24.481206: step 1625, loss 0.0186841, acc 1
2016-09-07T18:03:25.168413: step 1626, loss 0.0193391, acc 1
2016-09-07T18:03:25.852632: step 1627, loss 0.0959664, acc 0.96
2016-09-07T18:03:26.548959: step 1628, loss 0.132565, acc 0.96
2016-09-07T18:03:27.205074: step 1629, loss 0.120659, acc 0.96
2016-09-07T18:03:27.910237: step 1630, loss 0.0648654, acc 0.98
2016-09-07T18:03:28.594728: step 1631, loss 0.0481882, acc 1
2016-09-07T18:03:29.294661: step 1632, loss 0.0712342, acc 0.96
2016-09-07T18:03:29.987268: step 1633, loss 0.0445776, acc 0.98
2016-09-07T18:03:30.676688: step 1634, loss 0.0916415, acc 0.96
2016-09-07T18:03:31.350953: step 1635, loss 0.0468783, acc 1
2016-09-07T18:03:32.013827: step 1636, loss 0.0265224, acc 0.98
2016-09-07T18:03:32.718839: step 1637, loss 0.0810548, acc 0.96
2016-09-07T18:03:33.405440: step 1638, loss 0.0128682, acc 1
2016-09-07T18:03:34.076896: step 1639, loss 0.0156915, acc 1
2016-09-07T18:03:34.778165: step 1640, loss 0.0765297, acc 0.98
2016-09-07T18:03:35.466538: step 1641, loss 0.0804299, acc 0.96
2016-09-07T18:03:36.162561: step 1642, loss 0.132682, acc 0.94
2016-09-07T18:03:36.833746: step 1643, loss 0.0973734, acc 0.96
2016-09-07T18:03:37.539251: step 1644, loss 0.0169436, acc 1
2016-09-07T18:03:38.236189: step 1645, loss 0.0922197, acc 0.94
2016-09-07T18:03:38.902301: step 1646, loss 0.0887824, acc 0.92
2016-09-07T18:03:39.576218: step 1647, loss 0.049451, acc 1
2016-09-07T18:03:40.257956: step 1648, loss 0.00730442, acc 1
2016-09-07T18:03:40.965553: step 1649, loss 0.0369927, acc 0.98
2016-09-07T18:03:41.636414: step 1650, loss 0.0962049, acc 0.96
2016-09-07T18:03:42.335865: step 1651, loss 0.115499, acc 0.92
2016-09-07T18:03:43.030993: step 1652, loss 0.0460611, acc 0.98
2016-09-07T18:03:43.718417: step 1653, loss 0.0080527, acc 1
2016-09-07T18:03:44.401532: step 1654, loss 0.0186619, acc 1
2016-09-07T18:03:45.093995: step 1655, loss 0.0965407, acc 0.96
2016-09-07T18:03:45.787414: step 1656, loss 0.0738074, acc 0.96
2016-09-07T18:03:46.448321: step 1657, loss 0.0378769, acc 0.98
2016-09-07T18:03:47.150653: step 1658, loss 0.136902, acc 0.92
2016-09-07T18:03:47.842625: step 1659, loss 0.123613, acc 0.94
2016-09-07T18:03:48.538836: step 1660, loss 0.0176805, acc 1
2016-09-07T18:03:49.217491: step 1661, loss 0.0634664, acc 0.96
2016-09-07T18:03:49.897611: step 1662, loss 0.0304072, acc 0.98
2016-09-07T18:03:50.585437: step 1663, loss 0.0692356, acc 0.96
2016-09-07T18:03:51.242051: step 1664, loss 0.000968768, acc 1
2016-09-07T18:03:51.938984: step 1665, loss 0.0800726, acc 0.98
2016-09-07T18:03:52.630136: step 1666, loss 0.0370956, acc 0.98
2016-09-07T18:03:53.313834: step 1667, loss 0.126647, acc 0.96
2016-09-07T18:03:53.992975: step 1668, loss 0.100535, acc 0.96
2016-09-07T18:03:54.663079: step 1669, loss 0.0482158, acc 0.98
2016-09-07T18:03:55.342427: step 1670, loss 0.0508783, acc 0.96
2016-09-07T18:03:56.014593: step 1671, loss 0.0648735, acc 0.98
2016-09-07T18:03:56.716579: step 1672, loss 0.0923451, acc 0.96
2016-09-07T18:03:57.384276: step 1673, loss 0.0394445, acc 0.98
2016-09-07T18:03:58.069426: step 1674, loss 0.145344, acc 0.9
2016-09-07T18:03:58.736617: step 1675, loss 0.0293272, acc 1
2016-09-07T18:03:59.411378: step 1676, loss 0.0281811, acc 0.98
2016-09-07T18:04:00.116103: step 1677, loss 0.0808029, acc 0.98
2016-09-07T18:04:00.814236: step 1678, loss 0.062773, acc 0.96
2016-09-07T18:04:01.507345: step 1679, loss 0.185591, acc 0.92
2016-09-07T18:04:02.161227: step 1680, loss 0.00671404, acc 1
2016-09-07T18:04:02.854608: step 1681, loss 0.189819, acc 0.98
2016-09-07T18:04:03.566109: step 1682, loss 0.0545963, acc 1
2016-09-07T18:04:04.253375: step 1683, loss 0.0564343, acc 0.98
2016-09-07T18:04:04.933748: step 1684, loss 0.0853162, acc 0.96
2016-09-07T18:04:05.642271: step 1685, loss 0.0331697, acc 1
2016-09-07T18:04:06.364558: step 1686, loss 0.06336, acc 0.94
2016-09-07T18:04:07.065424: step 1687, loss 0.0969092, acc 0.92
2016-09-07T18:04:07.744016: step 1688, loss 0.0440274, acc 0.96
2016-09-07T18:04:08.431950: step 1689, loss 0.128818, acc 0.9
2016-09-07T18:04:09.128323: step 1690, loss 0.0570072, acc 0.98
2016-09-07T18:04:09.816442: step 1691, loss 0.00506684, acc 1
2016-09-07T18:04:10.486732: step 1692, loss 0.1186, acc 0.96
2016-09-07T18:04:11.166740: step 1693, loss 0.14748, acc 0.94
2016-09-07T18:04:11.841966: step 1694, loss 0.0651201, acc 0.96
2016-09-07T18:04:12.506552: step 1695, loss 0.0341424, acc 1
2016-09-07T18:04:13.194303: step 1696, loss 0.0135688, acc 1
2016-09-07T18:04:13.880268: step 1697, loss 0.00705785, acc 1
2016-09-07T18:04:14.582756: step 1698, loss 0.109027, acc 0.98
2016-09-07T18:04:15.252748: step 1699, loss 0.0354381, acc 1
2016-09-07T18:04:15.952347: step 1700, loss 0.055907, acc 1

Evaluation:
2016-09-07T18:04:19.090684: step 1700, loss 1.22385, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-1700

2016-09-07T18:04:20.730830: step 1701, loss 0.0757319, acc 0.92
2016-09-07T18:04:21.413710: step 1702, loss 0.0197476, acc 1
2016-09-07T18:04:22.094121: step 1703, loss 0.0433317, acc 0.98
2016-09-07T18:04:22.764537: step 1704, loss 0.112386, acc 0.92
2016-09-07T18:04:23.439556: step 1705, loss 0.0610482, acc 0.96
2016-09-07T18:04:24.123068: step 1706, loss 0.126455, acc 0.96
2016-09-07T18:04:24.815960: step 1707, loss 0.103252, acc 0.92
2016-09-07T18:04:25.514325: step 1708, loss 0.017569, acc 1
2016-09-07T18:04:26.197978: step 1709, loss 0.0599206, acc 0.98
2016-09-07T18:04:26.885291: step 1710, loss 0.162839, acc 0.94
2016-09-07T18:04:27.577524: step 1711, loss 0.0850305, acc 0.98
2016-09-07T18:04:28.274735: step 1712, loss 0.0490203, acc 0.94
2016-09-07T18:04:28.957363: step 1713, loss 0.0368929, acc 0.96
2016-09-07T18:04:29.630284: step 1714, loss 0.0204542, acc 1
2016-09-07T18:04:30.324855: step 1715, loss 0.0374782, acc 1
2016-09-07T18:04:30.995735: step 1716, loss 0.00618873, acc 1
2016-09-07T18:04:31.672727: step 1717, loss 0.0309987, acc 0.98
2016-09-07T18:04:32.340754: step 1718, loss 0.0856095, acc 0.94
2016-09-07T18:04:33.026179: step 1719, loss 0.0157748, acc 1
2016-09-07T18:04:33.694984: step 1720, loss 0.071843, acc 0.96
2016-09-07T18:04:34.379278: step 1721, loss 0.0638967, acc 0.96
2016-09-07T18:04:35.076837: step 1722, loss 0.0868576, acc 0.92
2016-09-07T18:04:35.772523: step 1723, loss 0.12894, acc 0.94
2016-09-07T18:04:36.448963: step 1724, loss 0.147445, acc 0.96
2016-09-07T18:04:37.122074: step 1725, loss 0.0548416, acc 0.98
2016-09-07T18:04:37.796181: step 1726, loss 0.00437392, acc 1
2016-09-07T18:04:38.489687: step 1727, loss 0.0179591, acc 1
2016-09-07T18:04:39.189896: step 1728, loss 0.0883474, acc 0.94
2016-09-07T18:04:39.902108: step 1729, loss 0.0270072, acc 0.98
2016-09-07T18:04:40.550281: step 1730, loss 0.0619131, acc 0.96
2016-09-07T18:04:41.256476: step 1731, loss 0.0592049, acc 0.94
2016-09-07T18:04:41.921644: step 1732, loss 0.050884, acc 0.98
2016-09-07T18:04:42.610308: step 1733, loss 0.125309, acc 0.92
2016-09-07T18:04:43.287338: step 1734, loss 0.0406317, acc 1
2016-09-07T18:04:43.976571: step 1735, loss 0.0162452, acc 0.98
2016-09-07T18:04:44.671567: step 1736, loss 0.0219988, acc 0.98
2016-09-07T18:04:45.341192: step 1737, loss 0.043998, acc 0.98
2016-09-07T18:04:46.041034: step 1738, loss 0.0623395, acc 1
2016-09-07T18:04:46.732989: step 1739, loss 0.086274, acc 0.96
2016-09-07T18:04:47.422576: step 1740, loss 0.0238841, acc 1
2016-09-07T18:04:48.112093: step 1741, loss 0.0383416, acc 1
2016-09-07T18:04:48.790918: step 1742, loss 0.0633591, acc 0.96
2016-09-07T18:04:49.462033: step 1743, loss 0.23188, acc 0.9
2016-09-07T18:04:50.131898: step 1744, loss 0.0852507, acc 0.96
2016-09-07T18:04:50.827526: step 1745, loss 0.0594316, acc 0.96
2016-09-07T18:04:51.177216: step 1746, loss 0.00279673, acc 1
2016-09-07T18:04:51.861312: step 1747, loss 0.0796782, acc 0.96
2016-09-07T18:04:52.519808: step 1748, loss 0.150661, acc 0.98
2016-09-07T18:04:53.206283: step 1749, loss 0.16192, acc 0.96
2016-09-07T18:04:53.912974: step 1750, loss 0.0129793, acc 1
2016-09-07T18:04:54.587092: step 1751, loss 0.0658837, acc 0.96
2016-09-07T18:04:55.256336: step 1752, loss 0.0390996, acc 0.98
2016-09-07T18:04:55.954958: step 1753, loss 0.0380571, acc 0.98
2016-09-07T18:04:56.633569: step 1754, loss 0.0845495, acc 0.96
2016-09-07T18:04:57.333457: step 1755, loss 0.0594885, acc 0.96
2016-09-07T18:04:58.036322: step 1756, loss 0.0213511, acc 1
2016-09-07T18:04:58.701333: step 1757, loss 0.0259946, acc 1
2016-09-07T18:04:59.397788: step 1758, loss 0.0491836, acc 0.98
2016-09-07T18:05:00.082787: step 1759, loss 0.038067, acc 0.98
2016-09-07T18:05:00.770822: step 1760, loss 0.0772518, acc 0.94
2016-09-07T18:05:01.483370: step 1761, loss 0.00432992, acc 1
2016-09-07T18:05:02.132221: step 1762, loss 0.0431857, acc 0.96
2016-09-07T18:05:02.828022: step 1763, loss 0.0394872, acc 1
2016-09-07T18:05:03.497465: step 1764, loss 0.124967, acc 0.94
2016-09-07T18:05:04.174923: step 1765, loss 0.0559932, acc 0.96
2016-09-07T18:05:04.844712: step 1766, loss 0.0411701, acc 0.98
2016-09-07T18:05:05.521748: step 1767, loss 0.072631, acc 0.98
2016-09-07T18:05:06.195087: step 1768, loss 0.033229, acc 1
2016-09-07T18:05:06.897971: step 1769, loss 0.0334291, acc 0.98
2016-09-07T18:05:07.613003: step 1770, loss 0.0826349, acc 0.98
2016-09-07T18:05:08.292201: step 1771, loss 0.018993, acc 1
2016-09-07T18:05:08.974752: step 1772, loss 0.029096, acc 0.98
2016-09-07T18:05:09.659593: step 1773, loss 0.0297311, acc 0.98
2016-09-07T18:05:10.352705: step 1774, loss 0.0918087, acc 0.96
2016-09-07T18:05:11.033111: step 1775, loss 0.0801461, acc 0.96
2016-09-07T18:05:11.705902: step 1776, loss 0.0131014, acc 1
2016-09-07T18:05:12.402829: step 1777, loss 0.0367819, acc 0.98
2016-09-07T18:05:13.070530: step 1778, loss 0.043089, acc 0.98
2016-09-07T18:05:13.768892: step 1779, loss 0.0537764, acc 0.98
2016-09-07T18:05:14.451156: step 1780, loss 0.0508603, acc 0.98
2016-09-07T18:05:15.140924: step 1781, loss 0.0306087, acc 1
2016-09-07T18:05:15.827326: step 1782, loss 0.0483813, acc 0.98
2016-09-07T18:05:16.514048: step 1783, loss 0.0976552, acc 0.96
2016-09-07T18:05:17.229567: step 1784, loss 0.0924354, acc 0.96
2016-09-07T18:05:17.923596: step 1785, loss 0.0308236, acc 0.98
2016-09-07T18:05:18.617490: step 1786, loss 0.0423187, acc 0.98
2016-09-07T18:05:19.306167: step 1787, loss 0.0624857, acc 0.98
2016-09-07T18:05:20.006735: step 1788, loss 0.025269, acc 0.98
2016-09-07T18:05:20.676589: step 1789, loss 0.0149264, acc 1
2016-09-07T18:05:21.351534: step 1790, loss 0.0193579, acc 1
2016-09-07T18:05:22.043427: step 1791, loss 0.0110304, acc 1
2016-09-07T18:05:22.715641: step 1792, loss 0.146007, acc 0.94
2016-09-07T18:05:23.390440: step 1793, loss 0.117079, acc 0.92
2016-09-07T18:05:24.061542: step 1794, loss 0.0607307, acc 0.96
2016-09-07T18:05:24.748159: step 1795, loss 0.0271179, acc 0.98
2016-09-07T18:05:25.428607: step 1796, loss 0.0684232, acc 0.94
2016-09-07T18:05:26.119180: step 1797, loss 0.0169735, acc 1
2016-09-07T18:05:26.840255: step 1798, loss 0.0608886, acc 0.98
2016-09-07T18:05:27.526987: step 1799, loss 0.050579, acc 0.96
2016-09-07T18:05:28.218830: step 1800, loss 0.0434666, acc 0.98

Evaluation:
2016-09-07T18:05:31.414530: step 1800, loss 1.6088, acc 0.751

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-1800

2016-09-07T18:05:33.167906: step 1801, loss 0.0699778, acc 0.96
2016-09-07T18:05:33.834369: step 1802, loss 0.105368, acc 0.94
2016-09-07T18:05:34.521872: step 1803, loss 0.045041, acc 0.98
2016-09-07T18:05:35.224738: step 1804, loss 0.0233925, acc 1
2016-09-07T18:05:35.890337: step 1805, loss 0.0535512, acc 0.96
2016-09-07T18:05:36.594858: step 1806, loss 0.0223679, acc 0.98
2016-09-07T18:05:37.284341: step 1807, loss 0.0720498, acc 0.94
2016-09-07T18:05:37.962683: step 1808, loss 0.161661, acc 0.94
2016-09-07T18:05:38.664728: step 1809, loss 0.0655639, acc 0.96
2016-09-07T18:05:39.365681: step 1810, loss 0.033688, acc 1
2016-09-07T18:05:40.075704: step 1811, loss 0.0449719, acc 1
2016-09-07T18:05:40.741499: step 1812, loss 0.066905, acc 0.96
2016-09-07T18:05:41.419255: step 1813, loss 0.00686245, acc 1
2016-09-07T18:05:42.094829: step 1814, loss 0.0648305, acc 0.96
2016-09-07T18:05:42.808720: step 1815, loss 0.0298251, acc 0.98
2016-09-07T18:05:43.484679: step 1816, loss 0.0122646, acc 1
2016-09-07T18:05:44.175458: step 1817, loss 0.0112486, acc 1
2016-09-07T18:05:44.866771: step 1818, loss 0.0346503, acc 0.96
2016-09-07T18:05:45.527966: step 1819, loss 0.0298954, acc 0.98
2016-09-07T18:05:46.229538: step 1820, loss 0.0158581, acc 1
2016-09-07T18:05:46.901238: step 1821, loss 0.0261724, acc 0.98
2016-09-07T18:05:47.573007: step 1822, loss 0.0427189, acc 0.96
2016-09-07T18:05:48.252580: step 1823, loss 0.0963084, acc 0.96
2016-09-07T18:05:48.907691: step 1824, loss 0.0464524, acc 0.98
2016-09-07T18:05:49.585965: step 1825, loss 0.177926, acc 0.96
2016-09-07T18:05:50.240096: step 1826, loss 0.0669258, acc 0.98
2016-09-07T18:05:50.950839: step 1827, loss 0.159701, acc 0.94
2016-09-07T18:05:51.626774: step 1828, loss 0.0147171, acc 1
2016-09-07T18:05:52.292497: step 1829, loss 0.0417356, acc 0.98
2016-09-07T18:05:52.982271: step 1830, loss 0.0151541, acc 1
2016-09-07T18:05:53.670840: step 1831, loss 0.00842084, acc 1
2016-09-07T18:05:54.342356: step 1832, loss 0.0408159, acc 1
2016-09-07T18:05:55.028873: step 1833, loss 0.145289, acc 0.94
2016-09-07T18:05:55.730404: step 1834, loss 0.0427997, acc 1
2016-09-07T18:05:56.395648: step 1835, loss 0.0829756, acc 0.96
2016-09-07T18:05:57.099844: step 1836, loss 0.0123146, acc 1
2016-09-07T18:05:57.798613: step 1837, loss 0.101625, acc 0.96
2016-09-07T18:05:58.489236: step 1838, loss 0.0975376, acc 0.98
2016-09-07T18:05:59.180325: step 1839, loss 0.103106, acc 0.96
2016-09-07T18:05:59.843843: step 1840, loss 0.0197693, acc 0.98
2016-09-07T18:06:00.594434: step 1841, loss 0.0567871, acc 0.98
2016-09-07T18:06:01.267464: step 1842, loss 0.0378256, acc 0.98
2016-09-07T18:06:01.953863: step 1843, loss 0.0448179, acc 0.98
2016-09-07T18:06:02.652494: step 1844, loss 0.0970379, acc 0.94
2016-09-07T18:06:03.354767: step 1845, loss 0.00456133, acc 1
2016-09-07T18:06:04.045278: step 1846, loss 0.100396, acc 0.94
2016-09-07T18:06:04.717386: step 1847, loss 0.0398155, acc 1
2016-09-07T18:06:05.420855: step 1848, loss 0.0198757, acc 1
2016-09-07T18:06:06.123040: step 1849, loss 0.0581979, acc 0.98
2016-09-07T18:06:06.829389: step 1850, loss 0.0242624, acc 0.98
2016-09-07T18:06:07.514133: step 1851, loss 0.0230337, acc 1
2016-09-07T18:06:08.206885: step 1852, loss 0.0755747, acc 0.98
2016-09-07T18:06:08.898949: step 1853, loss 0.0736605, acc 0.96
2016-09-07T18:06:09.546328: step 1854, loss 0.158513, acc 0.96
2016-09-07T18:06:10.227120: step 1855, loss 0.0578723, acc 0.96
2016-09-07T18:06:10.907528: step 1856, loss 0.0419603, acc 0.96
2016-09-07T18:06:11.600415: step 1857, loss 0.172872, acc 0.96
2016-09-07T18:06:12.272618: step 1858, loss 0.0102397, acc 1
2016-09-07T18:06:12.951805: step 1859, loss 0.0588142, acc 0.98
2016-09-07T18:06:13.645716: step 1860, loss 0.0830968, acc 0.96
2016-09-07T18:06:14.311226: step 1861, loss 0.0661109, acc 0.98
2016-09-07T18:06:15.004306: step 1862, loss 0.0361941, acc 0.98
2016-09-07T18:06:15.669752: step 1863, loss 0.0659726, acc 0.98
2016-09-07T18:06:16.342707: step 1864, loss 0.11536, acc 0.96
2016-09-07T18:06:17.030076: step 1865, loss 0.0282588, acc 1
2016-09-07T18:06:17.706008: step 1866, loss 0.0506602, acc 0.96
2016-09-07T18:06:18.393999: step 1867, loss 0.0612776, acc 0.98
2016-09-07T18:06:19.072054: step 1868, loss 0.143383, acc 0.94
2016-09-07T18:06:19.770293: step 1869, loss 0.00553087, acc 1
2016-09-07T18:06:20.445027: step 1870, loss 0.0604123, acc 0.98
2016-09-07T18:06:21.119142: step 1871, loss 0.0991148, acc 0.96
2016-09-07T18:06:21.784798: step 1872, loss 0.0206557, acc 0.98
2016-09-07T18:06:22.459427: step 1873, loss 0.0670197, acc 0.96
2016-09-07T18:06:23.136114: step 1874, loss 0.01304, acc 1
2016-09-07T18:06:23.797408: step 1875, loss 0.0229534, acc 1
2016-09-07T18:06:24.478149: step 1876, loss 0.0575876, acc 1
2016-09-07T18:06:25.163470: step 1877, loss 0.0958544, acc 0.96
2016-09-07T18:06:25.873276: step 1878, loss 0.0641383, acc 0.98
2016-09-07T18:06:26.556112: step 1879, loss 0.089339, acc 0.94
2016-09-07T18:06:27.278026: step 1880, loss 0.0448597, acc 0.96
2016-09-07T18:06:27.971409: step 1881, loss 0.0334001, acc 1
2016-09-07T18:06:28.660049: step 1882, loss 0.0521395, acc 0.96
2016-09-07T18:06:29.363911: step 1883, loss 0.0840991, acc 0.96
2016-09-07T18:06:30.023282: step 1884, loss 0.0348997, acc 0.98
2016-09-07T18:06:30.707949: step 1885, loss 0.0865732, acc 0.96
2016-09-07T18:06:31.414355: step 1886, loss 0.0282685, acc 0.98
2016-09-07T18:06:32.097283: step 1887, loss 0.0857449, acc 0.98
2016-09-07T18:06:32.801500: step 1888, loss 0.0530078, acc 0.98
2016-09-07T18:06:33.488943: step 1889, loss 0.0274782, acc 0.98
2016-09-07T18:06:34.179147: step 1890, loss 0.0219506, acc 1
2016-09-07T18:06:34.848926: step 1891, loss 0.0385562, acc 0.96
2016-09-07T18:06:35.546055: step 1892, loss 0.134266, acc 0.96
2016-09-07T18:06:36.223514: step 1893, loss 0.0475334, acc 0.96
2016-09-07T18:06:36.896592: step 1894, loss 0.0368219, acc 0.98
2016-09-07T18:06:37.579585: step 1895, loss 0.0607027, acc 0.98
2016-09-07T18:06:38.269686: step 1896, loss 0.0936327, acc 0.98
2016-09-07T18:06:38.963698: step 1897, loss 0.112375, acc 0.96
2016-09-07T18:06:39.635738: step 1898, loss 0.173415, acc 0.96
2016-09-07T18:06:40.329726: step 1899, loss 0.0828373, acc 0.94
2016-09-07T18:06:41.015662: step 1900, loss 0.0917159, acc 0.96

Evaluation:
2016-09-07T18:06:44.215582: step 1900, loss 1.23608, acc 0.758

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-1900

2016-09-07T18:06:45.896992: step 1901, loss 0.0434172, acc 1
2016-09-07T18:06:46.623262: step 1902, loss 0.0471748, acc 0.96
2016-09-07T18:06:47.313166: step 1903, loss 0.0337058, acc 0.98
2016-09-07T18:06:47.978245: step 1904, loss 0.0226204, acc 1
2016-09-07T18:06:48.688068: step 1905, loss 0.0049426, acc 1
2016-09-07T18:06:49.359419: step 1906, loss 0.0562073, acc 0.98
2016-09-07T18:06:50.039316: step 1907, loss 0.0577959, acc 0.96
2016-09-07T18:06:50.726067: step 1908, loss 0.013178, acc 1
2016-09-07T18:06:51.408944: step 1909, loss 0.0638681, acc 0.96
2016-09-07T18:06:52.112487: step 1910, loss 0.0340664, acc 0.98
2016-09-07T18:06:52.770781: step 1911, loss 0.035452, acc 0.96
2016-09-07T18:06:53.494855: step 1912, loss 0.0497112, acc 0.96
2016-09-07T18:06:54.189484: step 1913, loss 0.0412888, acc 0.98
2016-09-07T18:06:54.891488: step 1914, loss 0.0520502, acc 0.96
2016-09-07T18:06:55.592881: step 1915, loss 0.0642908, acc 0.96
2016-09-07T18:06:56.285115: step 1916, loss 0.15539, acc 0.94
2016-09-07T18:06:57.019257: step 1917, loss 0.0362357, acc 0.98
2016-09-07T18:06:57.711403: step 1918, loss 0.00488784, acc 1
2016-09-07T18:06:58.402998: step 1919, loss 0.0293689, acc 0.98
2016-09-07T18:06:59.089716: step 1920, loss 0.0078931, acc 1
2016-09-07T18:06:59.782389: step 1921, loss 0.0491884, acc 0.98
2016-09-07T18:07:00.484042: step 1922, loss 0.0667207, acc 0.96
2016-09-07T18:07:01.167089: step 1923, loss 0.0455776, acc 0.98
2016-09-07T18:07:01.866126: step 1924, loss 0.0197541, acc 1
2016-09-07T18:07:02.545101: step 1925, loss 0.0259992, acc 0.98
2016-09-07T18:07:03.245059: step 1926, loss 0.0355653, acc 0.98
2016-09-07T18:07:03.912748: step 1927, loss 0.0868223, acc 0.96
2016-09-07T18:07:04.593619: step 1928, loss 0.140219, acc 0.98
2016-09-07T18:07:05.277003: step 1929, loss 0.0248056, acc 0.98
2016-09-07T18:07:05.964602: step 1930, loss 0.0327654, acc 0.98
2016-09-07T18:07:06.675641: step 1931, loss 0.0146749, acc 1
2016-09-07T18:07:07.341251: step 1932, loss 0.0262222, acc 0.98
2016-09-07T18:07:08.022022: step 1933, loss 0.038438, acc 0.98
2016-09-07T18:07:08.692653: step 1934, loss 0.00786996, acc 1
2016-09-07T18:07:09.364754: step 1935, loss 0.0699616, acc 0.96
2016-09-07T18:07:10.041704: step 1936, loss 0.0596966, acc 0.96
2016-09-07T18:07:10.714134: step 1937, loss 0.0395713, acc 0.98
2016-09-07T18:07:11.397377: step 1938, loss 0.0296928, acc 1
2016-09-07T18:07:12.067540: step 1939, loss 0.0703479, acc 0.98
2016-09-07T18:07:12.448529: step 1940, loss 0.397057, acc 0.833333
2016-09-07T18:07:13.136784: step 1941, loss 0.0340751, acc 0.96
2016-09-07T18:07:13.808747: step 1942, loss 0.110201, acc 0.94
2016-09-07T18:07:14.496815: step 1943, loss 0.0567744, acc 0.96
2016-09-07T18:07:15.186919: step 1944, loss 0.0901994, acc 0.96
2016-09-07T18:07:15.862045: step 1945, loss 0.123257, acc 0.94
2016-09-07T18:07:16.516875: step 1946, loss 0.0605194, acc 0.96
2016-09-07T18:07:17.212008: step 1947, loss 0.107328, acc 0.94
2016-09-07T18:07:17.895721: step 1948, loss 0.0288262, acc 1
2016-09-07T18:07:18.578600: step 1949, loss 0.0203859, acc 1
2016-09-07T18:07:19.237543: step 1950, loss 0.0261334, acc 1
2016-09-07T18:07:19.933008: step 1951, loss 0.0116953, acc 1
2016-09-07T18:07:20.620911: step 1952, loss 0.0501654, acc 0.96
2016-09-07T18:07:21.291565: step 1953, loss 0.177645, acc 0.96
2016-09-07T18:07:21.983091: step 1954, loss 0.112457, acc 0.92
2016-09-07T18:07:22.664398: step 1955, loss 0.172631, acc 0.92
2016-09-07T18:07:23.360442: step 1956, loss 0.110938, acc 0.98
2016-09-07T18:07:24.020761: step 1957, loss 0.238679, acc 0.94
2016-09-07T18:07:24.729512: step 1958, loss 0.0817219, acc 0.96
2016-09-07T18:07:25.422435: step 1959, loss 0.116585, acc 0.94
2016-09-07T18:07:26.106657: step 1960, loss 0.0761773, acc 0.96
2016-09-07T18:07:26.806578: step 1961, loss 0.0457223, acc 1
2016-09-07T18:07:27.502394: step 1962, loss 0.06841, acc 0.96
2016-09-07T18:07:28.194518: step 1963, loss 0.117454, acc 0.98
2016-09-07T18:07:28.874228: step 1964, loss 0.0453562, acc 0.98
2016-09-07T18:07:29.580706: step 1965, loss 0.0278451, acc 1
2016-09-07T18:07:30.284778: step 1966, loss 0.0447231, acc 0.98
2016-09-07T18:07:30.986952: step 1967, loss 0.0153986, acc 1
2016-09-07T18:07:31.683925: step 1968, loss 0.0383652, acc 0.98
2016-09-07T18:07:32.369514: step 1969, loss 0.11312, acc 0.94
2016-09-07T18:07:33.068275: step 1970, loss 0.0463249, acc 1
2016-09-07T18:07:33.750036: step 1971, loss 0.0156444, acc 1
2016-09-07T18:07:34.453868: step 1972, loss 0.0950126, acc 0.94
2016-09-07T18:07:35.136286: step 1973, loss 0.10173, acc 0.96
2016-09-07T18:07:35.814731: step 1974, loss 0.012153, acc 1
2016-09-07T18:07:36.484606: step 1975, loss 0.112689, acc 0.94
2016-09-07T18:07:37.156549: step 1976, loss 0.096526, acc 0.96
2016-09-07T18:07:37.880191: step 1977, loss 0.0713695, acc 0.94
2016-09-07T18:07:38.535245: step 1978, loss 0.0490793, acc 0.96
2016-09-07T18:07:39.226990: step 1979, loss 0.0219224, acc 0.98
2016-09-07T18:07:39.900640: step 1980, loss 0.124897, acc 0.98
2016-09-07T18:07:40.569791: step 1981, loss 0.0634819, acc 0.98
2016-09-07T18:07:41.255444: step 1982, loss 0.0653944, acc 0.96
2016-09-07T18:07:41.949210: step 1983, loss 0.00479878, acc 1
2016-09-07T18:07:42.641107: step 1984, loss 0.0374531, acc 0.96
2016-09-07T18:07:43.286692: step 1985, loss 0.0310501, acc 1
2016-09-07T18:07:44.011090: step 1986, loss 0.181612, acc 0.94
2016-09-07T18:07:44.685151: step 1987, loss 0.0493653, acc 0.96
2016-09-07T18:07:45.385339: step 1988, loss 0.0117864, acc 1
2016-09-07T18:07:46.056392: step 1989, loss 0.0589748, acc 0.98
2016-09-07T18:07:46.740177: step 1990, loss 0.0418751, acc 1
2016-09-07T18:07:47.425377: step 1991, loss 0.0912299, acc 0.92
2016-09-07T18:07:48.095783: step 1992, loss 0.0314556, acc 1
2016-09-07T18:07:48.799068: step 1993, loss 0.0303444, acc 0.98
2016-09-07T18:07:49.471613: step 1994, loss 0.113964, acc 0.96
2016-09-07T18:07:50.157904: step 1995, loss 0.103098, acc 0.98
2016-09-07T18:07:50.831001: step 1996, loss 0.161473, acc 0.94
2016-09-07T18:07:51.511982: step 1997, loss 0.110697, acc 0.94
2016-09-07T18:07:52.210957: step 1998, loss 0.0125745, acc 1
2016-09-07T18:07:52.891123: step 1999, loss 0.0355212, acc 1
2016-09-07T18:07:53.596076: step 2000, loss 0.0609868, acc 0.96

Evaluation:
2016-09-07T18:07:56.792761: step 2000, loss 1.29604, acc 0.752

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-2000

2016-09-07T18:07:58.527185: step 2001, loss 0.0544588, acc 0.96
2016-09-07T18:07:59.223085: step 2002, loss 0.0429757, acc 0.98
2016-09-07T18:07:59.916605: step 2003, loss 0.0726096, acc 0.96
2016-09-07T18:08:00.620557: step 2004, loss 0.182125, acc 0.96
2016-09-07T18:08:01.305341: step 2005, loss 0.057084, acc 0.98
2016-09-07T18:08:02.016668: step 2006, loss 0.073065, acc 0.98
2016-09-07T18:08:02.686979: step 2007, loss 0.105232, acc 0.94
2016-09-07T18:08:03.346342: step 2008, loss 0.0410339, acc 0.98
2016-09-07T18:08:04.046841: step 2009, loss 0.0434896, acc 0.98
2016-09-07T18:08:04.729611: step 2010, loss 0.049868, acc 0.98
2016-09-07T18:08:05.433384: step 2011, loss 0.0566363, acc 0.96
2016-09-07T18:08:06.122099: step 2012, loss 0.0345452, acc 0.98
2016-09-07T18:08:06.829343: step 2013, loss 0.00427658, acc 1
2016-09-07T18:08:07.483276: step 2014, loss 0.0275158, acc 0.98
2016-09-07T18:08:08.177807: step 2015, loss 0.0747906, acc 0.98
2016-09-07T18:08:08.856589: step 2016, loss 0.0584471, acc 0.96
2016-09-07T18:08:09.515776: step 2017, loss 0.071579, acc 0.98
2016-09-07T18:08:10.184495: step 2018, loss 0.0422026, acc 0.98
2016-09-07T18:08:10.865957: step 2019, loss 0.0553743, acc 0.98
2016-09-07T18:08:11.534655: step 2020, loss 0.0881179, acc 0.94
2016-09-07T18:08:12.203602: step 2021, loss 0.0733321, acc 0.96
2016-09-07T18:08:12.892831: step 2022, loss 0.0118339, acc 1
2016-09-07T18:08:13.570314: step 2023, loss 0.0736858, acc 0.94
2016-09-07T18:08:14.245745: step 2024, loss 0.0954097, acc 0.94
2016-09-07T18:08:14.945209: step 2025, loss 0.0172388, acc 1
2016-09-07T18:08:15.623581: step 2026, loss 0.0232031, acc 0.98
2016-09-07T18:08:16.323813: step 2027, loss 0.0160807, acc 1
2016-09-07T18:08:16.998249: step 2028, loss 0.112014, acc 0.96
2016-09-07T18:08:17.689277: step 2029, loss 0.0159821, acc 1
2016-09-07T18:08:18.393952: step 2030, loss 0.158054, acc 0.92
2016-09-07T18:08:19.084447: step 2031, loss 0.121539, acc 0.94
2016-09-07T18:08:19.766098: step 2032, loss 0.0302654, acc 1
2016-09-07T18:08:20.462025: step 2033, loss 0.0295752, acc 1
2016-09-07T18:08:21.182807: step 2034, loss 0.0776416, acc 0.98
2016-09-07T18:08:21.859537: step 2035, loss 0.0496199, acc 0.96
2016-09-07T18:08:22.561428: step 2036, loss 0.035476, acc 1
2016-09-07T18:08:23.236270: step 2037, loss 0.0278852, acc 1
2016-09-07T18:08:23.912749: step 2038, loss 0.029491, acc 1
2016-09-07T18:08:24.612334: step 2039, loss 0.103389, acc 0.94
2016-09-07T18:08:25.300283: step 2040, loss 0.0786194, acc 0.96
2016-09-07T18:08:26.004519: step 2041, loss 0.0231111, acc 1
2016-09-07T18:08:26.659063: step 2042, loss 0.0644998, acc 0.98
2016-09-07T18:08:27.353824: step 2043, loss 0.0370226, acc 0.98
2016-09-07T18:08:28.041506: step 2044, loss 0.1851, acc 0.96
2016-09-07T18:08:28.739241: step 2045, loss 0.0250062, acc 1
2016-09-07T18:08:29.420958: step 2046, loss 0.0997816, acc 0.96
2016-09-07T18:08:30.134542: step 2047, loss 0.0641136, acc 0.96
2016-09-07T18:08:30.834378: step 2048, loss 0.0198836, acc 1
2016-09-07T18:08:31.480145: step 2049, loss 0.101182, acc 0.94
2016-09-07T18:08:32.173081: step 2050, loss 0.0404776, acc 0.98
2016-09-07T18:08:32.876604: step 2051, loss 0.0915575, acc 0.96
2016-09-07T18:08:33.581073: step 2052, loss 0.0376071, acc 0.98
2016-09-07T18:08:34.257946: step 2053, loss 0.0709186, acc 0.98
2016-09-07T18:08:34.930159: step 2054, loss 0.0306414, acc 1
2016-09-07T18:08:35.629897: step 2055, loss 0.0589971, acc 0.96
2016-09-07T18:08:36.289684: step 2056, loss 0.0323008, acc 1
2016-09-07T18:08:36.997084: step 2057, loss 0.00653085, acc 1
2016-09-07T18:08:37.671438: step 2058, loss 0.105028, acc 0.94
2016-09-07T18:08:38.347724: step 2059, loss 0.0273269, acc 1
2016-09-07T18:08:39.026936: step 2060, loss 0.154556, acc 0.98
2016-09-07T18:08:39.692691: step 2061, loss 0.0123061, acc 1
2016-09-07T18:08:40.356292: step 2062, loss 0.104332, acc 0.96
2016-09-07T18:08:41.017305: step 2063, loss 0.100784, acc 0.98
2016-09-07T18:08:41.713882: step 2064, loss 0.0416424, acc 0.98
2016-09-07T18:08:42.378207: step 2065, loss 0.027814, acc 0.98
2016-09-07T18:08:43.074923: step 2066, loss 0.0377449, acc 1
2016-09-07T18:08:43.774249: step 2067, loss 0.0517588, acc 0.98
2016-09-07T18:08:44.455939: step 2068, loss 0.0391492, acc 0.98
2016-09-07T18:08:45.155539: step 2069, loss 0.0254478, acc 1
2016-09-07T18:08:45.805721: step 2070, loss 0.0450828, acc 0.98
2016-09-07T18:08:46.500546: step 2071, loss 0.0760802, acc 0.96
2016-09-07T18:08:47.165151: step 2072, loss 0.0716443, acc 0.98
2016-09-07T18:08:47.896086: step 2073, loss 0.0148764, acc 1
2016-09-07T18:08:48.583739: step 2074, loss 0.0215412, acc 0.98
2016-09-07T18:08:49.278812: step 2075, loss 0.0643351, acc 0.96
2016-09-07T18:08:49.960698: step 2076, loss 0.0234015, acc 0.98
2016-09-07T18:08:50.634581: step 2077, loss 0.15838, acc 0.96
2016-09-07T18:08:51.336338: step 2078, loss 0.00246176, acc 1
2016-09-07T18:08:52.042945: step 2079, loss 0.0257861, acc 1
2016-09-07T18:08:52.739631: step 2080, loss 0.0270043, acc 0.98
2016-09-07T18:08:53.436768: step 2081, loss 0.0725456, acc 0.98
2016-09-07T18:08:54.130601: step 2082, loss 0.023561, acc 0.98
2016-09-07T18:08:54.802646: step 2083, loss 0.0310265, acc 1
2016-09-07T18:08:55.465292: step 2084, loss 0.0359472, acc 1
2016-09-07T18:08:56.161506: step 2085, loss 0.0974928, acc 0.94
2016-09-07T18:08:56.838065: step 2086, loss 0.01861, acc 1
2016-09-07T18:08:57.514250: step 2087, loss 0.0334515, acc 1
2016-09-07T18:08:58.200700: step 2088, loss 0.0541832, acc 0.96
2016-09-07T18:08:58.904650: step 2089, loss 0.103603, acc 0.96
2016-09-07T18:08:59.592745: step 2090, loss 0.0117119, acc 1
2016-09-07T18:09:00.264740: step 2091, loss 0.0468798, acc 0.98
2016-09-07T18:09:00.957207: step 2092, loss 0.0409902, acc 0.98
2016-09-07T18:09:01.625080: step 2093, loss 0.133711, acc 0.96
2016-09-07T18:09:02.316942: step 2094, loss 0.0862389, acc 0.96
2016-09-07T18:09:02.981876: step 2095, loss 0.0323261, acc 0.98
2016-09-07T18:09:03.651616: step 2096, loss 0.0209953, acc 1
2016-09-07T18:09:04.327309: step 2097, loss 0.00579, acc 1
2016-09-07T18:09:05.014862: step 2098, loss 0.0392929, acc 0.98
2016-09-07T18:09:05.709852: step 2099, loss 0.179857, acc 0.9
2016-09-07T18:09:06.389845: step 2100, loss 0.0612738, acc 0.98

Evaluation:
2016-09-07T18:09:09.641936: step 2100, loss 1.35608, acc 0.763

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-2100

2016-09-07T18:09:11.421764: step 2101, loss 0.0395543, acc 0.98
2016-09-07T18:09:12.130139: step 2102, loss 0.0200905, acc 1
2016-09-07T18:09:12.830355: step 2103, loss 0.010306, acc 1
2016-09-07T18:09:13.505786: step 2104, loss 0.0469478, acc 0.98
2016-09-07T18:09:14.199059: step 2105, loss 0.0257865, acc 0.98
2016-09-07T18:09:14.909861: step 2106, loss 0.0275968, acc 1
2016-09-07T18:09:15.611613: step 2107, loss 0.0588963, acc 0.96
2016-09-07T18:09:16.289905: step 2108, loss 0.00658673, acc 1
2016-09-07T18:09:16.973782: step 2109, loss 0.076719, acc 0.94
2016-09-07T18:09:17.674878: step 2110, loss 0.0466853, acc 0.96
2016-09-07T18:09:18.354065: step 2111, loss 0.064614, acc 0.98
2016-09-07T18:09:19.060424: step 2112, loss 0.0485023, acc 1
2016-09-07T18:09:19.744640: step 2113, loss 0.0382713, acc 0.98
2016-09-07T18:09:20.429821: step 2114, loss 0.0549404, acc 0.96
2016-09-07T18:09:21.105489: step 2115, loss 0.0839981, acc 0.98
2016-09-07T18:09:21.795462: step 2116, loss 0.0417181, acc 0.98
2016-09-07T18:09:22.517100: step 2117, loss 0.0485101, acc 0.96
2016-09-07T18:09:23.193633: step 2118, loss 0.0019379, acc 1
2016-09-07T18:09:23.881669: step 2119, loss 0.0732562, acc 0.96
2016-09-07T18:09:24.579386: step 2120, loss 0.0869377, acc 0.96
2016-09-07T18:09:25.263044: step 2121, loss 0.0616044, acc 0.98
2016-09-07T18:09:25.933413: step 2122, loss 0.0301001, acc 0.98
2016-09-07T18:09:26.633033: step 2123, loss 0.0853629, acc 0.94
2016-09-07T18:09:27.319744: step 2124, loss 0.0557418, acc 0.96
2016-09-07T18:09:27.973682: step 2125, loss 0.0246194, acc 1
2016-09-07T18:09:28.673846: step 2126, loss 0.00591442, acc 1
2016-09-07T18:09:29.338334: step 2127, loss 0.113687, acc 0.9
2016-09-07T18:09:30.014149: step 2128, loss 0.0299859, acc 0.98
2016-09-07T18:09:30.684163: step 2129, loss 0.0329129, acc 0.98
2016-09-07T18:09:31.379340: step 2130, loss 0.016761, acc 1
2016-09-07T18:09:32.061812: step 2131, loss 0.0358305, acc 1
2016-09-07T18:09:32.723460: step 2132, loss 0.0766101, acc 0.98
2016-09-07T18:09:33.422041: step 2133, loss 0.0111642, acc 1
2016-09-07T18:09:33.790857: step 2134, loss 0.00100449, acc 1
2016-09-07T18:09:34.494581: step 2135, loss 0.0489113, acc 0.98
2016-09-07T18:09:35.150486: step 2136, loss 0.0679637, acc 0.96
2016-09-07T18:09:35.844729: step 2137, loss 0.030165, acc 0.98
2016-09-07T18:09:36.515439: step 2138, loss 0.0570127, acc 0.96
2016-09-07T18:09:37.199279: step 2139, loss 0.0349714, acc 0.98
2016-09-07T18:09:37.878303: step 2140, loss 0.0397037, acc 1
2016-09-07T18:09:38.584397: step 2141, loss 0.0128655, acc 1
2016-09-07T18:09:39.292737: step 2142, loss 0.0123586, acc 1
2016-09-07T18:09:39.959273: step 2143, loss 0.00385912, acc 1
2016-09-07T18:09:40.651866: step 2144, loss 0.0353048, acc 0.96
2016-09-07T18:09:41.330979: step 2145, loss 0.16217, acc 0.96
2016-09-07T18:09:42.021913: step 2146, loss 0.0443037, acc 1
2016-09-07T18:09:42.706346: step 2147, loss 0.0229636, acc 0.98
2016-09-07T18:09:43.409105: step 2148, loss 0.0732884, acc 0.96
2016-09-07T18:09:44.122045: step 2149, loss 0.0145808, acc 1
2016-09-07T18:09:44.803545: step 2150, loss 0.0837066, acc 0.98
2016-09-07T18:09:45.501375: step 2151, loss 0.0849407, acc 0.98
2016-09-07T18:09:46.195945: step 2152, loss 0.00239245, acc 1
2016-09-07T18:09:46.895586: step 2153, loss 0.0628577, acc 0.96
2016-09-07T18:09:47.576051: step 2154, loss 0.0334029, acc 1
2016-09-07T18:09:48.275748: step 2155, loss 0.0168768, acc 1
2016-09-07T18:09:48.978138: step 2156, loss 0.0125942, acc 1
2016-09-07T18:09:49.671869: step 2157, loss 0.0949413, acc 0.96
2016-09-07T18:09:50.389003: step 2158, loss 0.0620086, acc 0.96
2016-09-07T18:09:51.056860: step 2159, loss 0.0571868, acc 0.98
2016-09-07T18:09:51.736734: step 2160, loss 0.0558569, acc 0.96
2016-09-07T18:09:52.428131: step 2161, loss 0.0169363, acc 1
2016-09-07T18:09:53.112444: step 2162, loss 0.00417421, acc 1
2016-09-07T18:09:53.812953: step 2163, loss 0.108018, acc 0.96
2016-09-07T18:09:54.490127: step 2164, loss 0.0161447, acc 1
2016-09-07T18:09:55.165053: step 2165, loss 0.133522, acc 0.92
2016-09-07T18:09:55.843764: step 2166, loss 0.0777849, acc 0.96
2016-09-07T18:09:56.561973: step 2167, loss 0.00684713, acc 1
2016-09-07T18:09:57.238974: step 2168, loss 0.0384046, acc 0.98
2016-09-07T18:09:57.930056: step 2169, loss 0.0240985, acc 1
2016-09-07T18:09:58.632896: step 2170, loss 0.0141033, acc 1
2016-09-07T18:09:59.304433: step 2171, loss 0.0157448, acc 1
2016-09-07T18:09:59.975663: step 2172, loss 0.119211, acc 0.94
2016-09-07T18:10:00.692277: step 2173, loss 0.0754564, acc 0.96
2016-09-07T18:10:01.393146: step 2174, loss 0.0275789, acc 1
2016-09-07T18:10:02.101883: step 2175, loss 0.125481, acc 0.96
2016-09-07T18:10:02.781741: step 2176, loss 0.0552842, acc 0.98
2016-09-07T18:10:03.481506: step 2177, loss 0.026321, acc 1
2016-09-07T18:10:04.124590: step 2178, loss 0.0203812, acc 1
2016-09-07T18:10:04.820042: step 2179, loss 0.0736039, acc 0.96
2016-09-07T18:10:05.510337: step 2180, loss 0.0497513, acc 0.98
2016-09-07T18:10:06.187705: step 2181, loss 0.0379156, acc 0.98
2016-09-07T18:10:06.878763: step 2182, loss 0.0633418, acc 0.98
2016-09-07T18:10:07.548433: step 2183, loss 0.031522, acc 0.98
2016-09-07T18:10:08.251328: step 2184, loss 0.160196, acc 0.98
2016-09-07T18:10:08.910096: step 2185, loss 0.022196, acc 1
2016-09-07T18:10:09.577605: step 2186, loss 0.0170195, acc 1
2016-09-07T18:10:10.265095: step 2187, loss 0.00450483, acc 1
2016-09-07T18:10:10.943945: step 2188, loss 0.0398387, acc 0.98
2016-09-07T18:10:11.639425: step 2189, loss 0.160963, acc 0.94
2016-09-07T18:10:12.318542: step 2190, loss 0.055127, acc 0.96
2016-09-07T18:10:13.007201: step 2191, loss 0.065517, acc 0.96
2016-09-07T18:10:13.666547: step 2192, loss 0.037641, acc 0.98
2016-09-07T18:10:14.373147: step 2193, loss 0.0259125, acc 1
2016-09-07T18:10:15.081044: step 2194, loss 0.0159435, acc 1
2016-09-07T18:10:15.769644: step 2195, loss 0.0883932, acc 0.96
2016-09-07T18:10:16.455545: step 2196, loss 0.0238163, acc 0.98
2016-09-07T18:10:17.126211: step 2197, loss 0.0937029, acc 0.96
2016-09-07T18:10:17.829065: step 2198, loss 0.0856743, acc 0.98
2016-09-07T18:10:18.500289: step 2199, loss 0.0133606, acc 1
2016-09-07T18:10:19.175512: step 2200, loss 0.0550455, acc 0.98

Evaluation:
2016-09-07T18:10:22.428260: step 2200, loss 1.34179, acc 0.762

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-2200

2016-09-07T18:10:24.151885: step 2201, loss 0.0591406, acc 0.98
2016-09-07T18:10:24.824915: step 2202, loss 0.0448977, acc 0.98
2016-09-07T18:10:25.515056: step 2203, loss 0.0465466, acc 0.98
2016-09-07T18:10:26.222009: step 2204, loss 0.0751605, acc 0.94
2016-09-07T18:10:26.926949: step 2205, loss 0.0175502, acc 0.98
2016-09-07T18:10:27.605513: step 2206, loss 0.0070919, acc 1
2016-09-07T18:10:28.284987: step 2207, loss 0.0369117, acc 0.98
2016-09-07T18:10:28.977189: step 2208, loss 0.0157173, acc 1
2016-09-07T18:10:29.659647: step 2209, loss 0.037477, acc 0.98
2016-09-07T18:10:30.337853: step 2210, loss 0.0642946, acc 0.94
2016-09-07T18:10:31.059566: step 2211, loss 0.0470413, acc 0.98
2016-09-07T18:10:31.753345: step 2212, loss 0.0501893, acc 0.96
2016-09-07T18:10:32.441652: step 2213, loss 0.0360492, acc 0.98
2016-09-07T18:10:33.122385: step 2214, loss 0.0115334, acc 1
2016-09-07T18:10:33.802167: step 2215, loss 0.0310719, acc 1
2016-09-07T18:10:34.480475: step 2216, loss 0.0582554, acc 0.98
2016-09-07T18:10:35.165643: step 2217, loss 0.0225615, acc 0.98
2016-09-07T18:10:35.862335: step 2218, loss 0.0487277, acc 0.98
2016-09-07T18:10:36.530133: step 2219, loss 0.00795877, acc 1
2016-09-07T18:10:37.236362: step 2220, loss 0.0556517, acc 0.96
2016-09-07T18:10:37.931291: step 2221, loss 0.0594131, acc 0.96
2016-09-07T18:10:38.605625: step 2222, loss 0.0120664, acc 1
2016-09-07T18:10:39.277969: step 2223, loss 0.0412008, acc 0.98
2016-09-07T18:10:39.956042: step 2224, loss 0.0571787, acc 0.96
2016-09-07T18:10:40.637776: step 2225, loss 0.0189457, acc 0.98
2016-09-07T18:10:41.316641: step 2226, loss 0.0559815, acc 0.98
2016-09-07T18:10:42.026727: step 2227, loss 0.0171027, acc 1
2016-09-07T18:10:42.700791: step 2228, loss 0.032252, acc 1
2016-09-07T18:10:43.398980: step 2229, loss 0.126002, acc 0.96
2016-09-07T18:10:44.070871: step 2230, loss 0.0398058, acc 0.98
2016-09-07T18:10:44.770826: step 2231, loss 0.0590599, acc 0.96
2016-09-07T18:10:45.481600: step 2232, loss 0.0432876, acc 0.96
2016-09-07T18:10:46.137121: step 2233, loss 0.0106862, acc 1
2016-09-07T18:10:46.836665: step 2234, loss 0.0330275, acc 1
2016-09-07T18:10:47.517130: step 2235, loss 0.0429451, acc 0.96
2016-09-07T18:10:48.222093: step 2236, loss 0.0184864, acc 0.98
2016-09-07T18:10:48.897587: step 2237, loss 0.0603263, acc 0.96
2016-09-07T18:10:49.562113: step 2238, loss 0.0651299, acc 0.94
2016-09-07T18:10:50.271531: step 2239, loss 0.0938979, acc 0.96
2016-09-07T18:10:50.933104: step 2240, loss 0.103975, acc 0.98
2016-09-07T18:10:51.627669: step 2241, loss 0.0285327, acc 0.98
2016-09-07T18:10:52.310209: step 2242, loss 0.0367726, acc 1
2016-09-07T18:10:52.970993: step 2243, loss 0.0593867, acc 0.98
2016-09-07T18:10:53.650162: step 2244, loss 0.28998, acc 0.92
2016-09-07T18:10:54.333734: step 2245, loss 0.0422756, acc 1
2016-09-07T18:10:55.039135: step 2246, loss 0.00987658, acc 1
2016-09-07T18:10:55.697505: step 2247, loss 0.120864, acc 0.94
2016-09-07T18:10:56.377949: step 2248, loss 0.0270231, acc 1
2016-09-07T18:10:57.067195: step 2249, loss 0.0460073, acc 0.96
2016-09-07T18:10:57.749134: step 2250, loss 0.10686, acc 0.96
2016-09-07T18:10:58.427749: step 2251, loss 0.0231532, acc 0.98
2016-09-07T18:10:59.139818: step 2252, loss 0.104992, acc 0.96
2016-09-07T18:10:59.811179: step 2253, loss 0.0715329, acc 0.98
2016-09-07T18:11:00.508543: step 2254, loss 0.115309, acc 0.96
2016-09-07T18:11:01.211276: step 2255, loss 0.0916547, acc 0.96
2016-09-07T18:11:01.907847: step 2256, loss 0.0324889, acc 0.98
2016-09-07T18:11:02.587520: step 2257, loss 0.0165676, acc 1
2016-09-07T18:11:03.282428: step 2258, loss 0.099654, acc 0.98
2016-09-07T18:11:03.974206: step 2259, loss 0.0609773, acc 0.96
2016-09-07T18:11:04.655659: step 2260, loss 0.0778519, acc 0.96
2016-09-07T18:11:05.322895: step 2261, loss 0.0175333, acc 1
2016-09-07T18:11:06.018784: step 2262, loss 0.0212259, acc 1
2016-09-07T18:11:06.703106: step 2263, loss 0.0311458, acc 0.98
2016-09-07T18:11:07.388960: step 2264, loss 0.114065, acc 0.96
2016-09-07T18:11:08.050418: step 2265, loss 0.0137047, acc 1
2016-09-07T18:11:08.739725: step 2266, loss 0.0503455, acc 0.98
2016-09-07T18:11:09.433854: step 2267, loss 0.0816115, acc 0.98
2016-09-07T18:11:10.088870: step 2268, loss 0.0583317, acc 0.96
2016-09-07T18:11:10.782414: step 2269, loss 0.0215231, acc 1
2016-09-07T18:11:11.453303: step 2270, loss 0.0335365, acc 1
2016-09-07T18:11:12.123057: step 2271, loss 0.0555053, acc 0.98
2016-09-07T18:11:12.787873: step 2272, loss 0.051129, acc 0.98
2016-09-07T18:11:13.457126: step 2273, loss 0.0368018, acc 1
2016-09-07T18:11:14.154437: step 2274, loss 0.0674889, acc 0.94
2016-09-07T18:11:14.835525: step 2275, loss 0.0787199, acc 0.96
2016-09-07T18:11:15.537195: step 2276, loss 0.0839536, acc 0.98
2016-09-07T18:11:16.208244: step 2277, loss 0.0355913, acc 0.98
2016-09-07T18:11:16.920109: step 2278, loss 0.0442901, acc 0.96
2016-09-07T18:11:17.605766: step 2279, loss 0.0655954, acc 0.96
2016-09-07T18:11:18.311505: step 2280, loss 0.0398743, acc 0.98
2016-09-07T18:11:19.016662: step 2281, loss 0.126016, acc 0.98
2016-09-07T18:11:19.703830: step 2282, loss 0.0828649, acc 0.96
2016-09-07T18:11:20.394804: step 2283, loss 0.0848318, acc 0.96
2016-09-07T18:11:21.063195: step 2284, loss 0.052898, acc 0.94
2016-09-07T18:11:21.747162: step 2285, loss 0.0376616, acc 0.96
2016-09-07T18:11:22.423800: step 2286, loss 0.0416803, acc 0.98
2016-09-07T18:11:23.117054: step 2287, loss 0.0674878, acc 0.96
2016-09-07T18:11:23.786437: step 2288, loss 0.0531155, acc 0.96
2016-09-07T18:11:24.473116: step 2289, loss 0.114204, acc 0.96
2016-09-07T18:11:25.181565: step 2290, loss 0.0527151, acc 0.96
2016-09-07T18:11:25.852422: step 2291, loss 0.0237344, acc 0.98
2016-09-07T18:11:26.542869: step 2292, loss 0.0711704, acc 0.98
2016-09-07T18:11:27.215709: step 2293, loss 0.0123343, acc 1
2016-09-07T18:11:27.940495: step 2294, loss 0.042555, acc 0.98
2016-09-07T18:11:28.635201: step 2295, loss 0.0828211, acc 0.98
2016-09-07T18:11:29.304913: step 2296, loss 0.0695013, acc 0.96
2016-09-07T18:11:29.991165: step 2297, loss 0.0781381, acc 0.98
2016-09-07T18:11:30.666777: step 2298, loss 0.0727945, acc 0.98
2016-09-07T18:11:31.342138: step 2299, loss 0.0625486, acc 0.96
2016-09-07T18:11:32.022589: step 2300, loss 0.186293, acc 0.9

Evaluation:
2016-09-07T18:11:35.229303: step 2300, loss 1.33876, acc 0.754

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-2300

2016-09-07T18:11:36.953654: step 2301, loss 0.0282964, acc 1
2016-09-07T18:11:37.635990: step 2302, loss 0.0540056, acc 0.96
2016-09-07T18:11:38.316107: step 2303, loss 0.0800635, acc 0.96
2016-09-07T18:11:38.985875: step 2304, loss 0.11414, acc 0.98
2016-09-07T18:11:39.683683: step 2305, loss 0.0251724, acc 1
2016-09-07T18:11:40.361452: step 2306, loss 0.0216168, acc 1
2016-09-07T18:11:41.021580: step 2307, loss 0.0148942, acc 1
2016-09-07T18:11:41.717501: step 2308, loss 0.0615354, acc 0.98
2016-09-07T18:11:42.392651: step 2309, loss 0.0346621, acc 1
2016-09-07T18:11:43.066004: step 2310, loss 0.0597152, acc 0.98
2016-09-07T18:11:43.733679: step 2311, loss 0.0766532, acc 0.94
2016-09-07T18:11:44.408233: step 2312, loss 0.038225, acc 0.98
2016-09-07T18:11:45.086620: step 2313, loss 0.0508009, acc 0.98
2016-09-07T18:11:45.773515: step 2314, loss 0.0487992, acc 0.98
2016-09-07T18:11:46.451783: step 2315, loss 0.0582273, acc 0.96
2016-09-07T18:11:47.138162: step 2316, loss 0.0045279, acc 1
2016-09-07T18:11:47.820333: step 2317, loss 0.0250174, acc 1
2016-09-07T18:11:48.499711: step 2318, loss 0.023453, acc 1
2016-09-07T18:11:49.185655: step 2319, loss 0.0738213, acc 0.96
2016-09-07T18:11:49.853527: step 2320, loss 0.0456871, acc 0.98
2016-09-07T18:11:50.534053: step 2321, loss 0.0461794, acc 0.98
2016-09-07T18:11:51.206483: step 2322, loss 0.0245453, acc 1
2016-09-07T18:11:51.906354: step 2323, loss 0.0346745, acc 1
2016-09-07T18:11:52.612417: step 2324, loss 0.0151783, acc 1
2016-09-07T18:11:53.308328: step 2325, loss 0.0774577, acc 0.94
2016-09-07T18:11:54.015106: step 2326, loss 0.0404848, acc 0.98
2016-09-07T18:11:54.658095: step 2327, loss 0.0261451, acc 0.98
2016-09-07T18:11:55.031790: step 2328, loss 0.026804, acc 1
2016-09-07T18:11:55.733594: step 2329, loss 0.0244648, acc 1
2016-09-07T18:11:56.434278: step 2330, loss 0.0479987, acc 0.98
2016-09-07T18:11:57.110934: step 2331, loss 0.0515108, acc 0.96
2016-09-07T18:11:57.811345: step 2332, loss 0.0226735, acc 0.98
2016-09-07T18:11:58.502524: step 2333, loss 0.06804, acc 0.98
2016-09-07T18:11:59.190051: step 2334, loss 0.0410866, acc 0.98
2016-09-07T18:11:59.879348: step 2335, loss 0.0588785, acc 0.96
2016-09-07T18:12:00.578697: step 2336, loss 0.0411071, acc 0.98
2016-09-07T18:12:01.271413: step 2337, loss 0.0228711, acc 0.98
2016-09-07T18:12:01.945738: step 2338, loss 0.0321634, acc 0.98
2016-09-07T18:12:02.615172: step 2339, loss 0.0976382, acc 0.98
2016-09-07T18:12:03.279294: step 2340, loss 0.0309786, acc 0.98
2016-09-07T18:12:03.953401: step 2341, loss 0.0284676, acc 1
2016-09-07T18:12:04.632918: step 2342, loss 0.0467159, acc 0.96
2016-09-07T18:12:05.301533: step 2343, loss 0.00736564, acc 1
2016-09-07T18:12:05.970547: step 2344, loss 0.00292419, acc 1
2016-09-07T18:12:06.625924: step 2345, loss 0.0749516, acc 0.96
2016-09-07T18:12:07.333783: step 2346, loss 0.134927, acc 0.98
2016-09-07T18:12:08.023645: step 2347, loss 0.0583025, acc 0.98
2016-09-07T18:12:08.724812: step 2348, loss 0.010601, acc 1
2016-09-07T18:12:09.432816: step 2349, loss 0.0112064, acc 1
2016-09-07T18:12:10.119987: step 2350, loss 0.0864731, acc 0.96
2016-09-07T18:12:10.807197: step 2351, loss 0.00747496, acc 1
2016-09-07T18:12:11.475194: step 2352, loss 0.0187833, acc 1
2016-09-07T18:12:12.190560: step 2353, loss 0.016787, acc 1
2016-09-07T18:12:12.880686: step 2354, loss 0.0187366, acc 1
2016-09-07T18:12:13.555579: step 2355, loss 0.0304506, acc 0.98
2016-09-07T18:12:14.251348: step 2356, loss 0.0013728, acc 1
2016-09-07T18:12:14.942529: step 2357, loss 0.0672655, acc 0.96
2016-09-07T18:12:15.661267: step 2358, loss 0.0853438, acc 0.96
2016-09-07T18:12:16.359269: step 2359, loss 0.0898435, acc 0.94
2016-09-07T18:12:17.031854: step 2360, loss 0.0783, acc 0.98
2016-09-07T18:12:17.719864: step 2361, loss 0.0549835, acc 0.98
2016-09-07T18:12:18.380047: step 2362, loss 0.123897, acc 0.92
2016-09-07T18:12:19.075260: step 2363, loss 0.0338188, acc 0.98
2016-09-07T18:12:19.760125: step 2364, loss 0.0803012, acc 0.92
2016-09-07T18:12:20.469130: step 2365, loss 0.044725, acc 0.96
2016-09-07T18:12:21.151296: step 2366, loss 0.104217, acc 0.94
2016-09-07T18:12:21.818581: step 2367, loss 0.0280557, acc 0.98
2016-09-07T18:12:22.500470: step 2368, loss 0.0810452, acc 0.98
2016-09-07T18:12:23.184980: step 2369, loss 0.0850739, acc 0.94
2016-09-07T18:12:23.868628: step 2370, loss 0.0644045, acc 0.98
2016-09-07T18:12:24.551023: step 2371, loss 0.0633069, acc 0.96
2016-09-07T18:12:25.248668: step 2372, loss 0.049544, acc 0.96
2016-09-07T18:12:25.925425: step 2373, loss 0.0380428, acc 1
2016-09-07T18:12:26.623924: step 2374, loss 0.0228116, acc 0.98
2016-09-07T18:12:27.310697: step 2375, loss 0.0636569, acc 0.98
2016-09-07T18:12:27.996674: step 2376, loss 0.0730681, acc 0.98
2016-09-07T18:12:28.681388: step 2377, loss 0.01917, acc 1
2016-09-07T18:12:29.376808: step 2378, loss 0.014552, acc 1
2016-09-07T18:12:30.078685: step 2379, loss 0.01741, acc 1
2016-09-07T18:12:30.750159: step 2380, loss 0.0361515, acc 0.98
2016-09-07T18:12:31.452315: step 2381, loss 0.0256645, acc 1
2016-09-07T18:12:32.133605: step 2382, loss 0.00320138, acc 1
2016-09-07T18:12:32.812164: step 2383, loss 0.0407226, acc 0.98
2016-09-07T18:12:33.503038: step 2384, loss 0.0335815, acc 1
2016-09-07T18:12:34.190318: step 2385, loss 0.0133246, acc 1
2016-09-07T18:12:34.884273: step 2386, loss 0.0910204, acc 0.96
2016-09-07T18:12:35.545010: step 2387, loss 0.0201528, acc 1
2016-09-07T18:12:36.230254: step 2388, loss 0.0298522, acc 0.98
2016-09-07T18:12:36.905626: step 2389, loss 0.102394, acc 0.98
2016-09-07T18:12:37.587898: step 2390, loss 0.0921003, acc 0.98
2016-09-07T18:12:38.271542: step 2391, loss 0.0251712, acc 1
2016-09-07T18:12:38.950768: step 2392, loss 0.0403051, acc 1
2016-09-07T18:12:39.639654: step 2393, loss 0.139631, acc 0.94
2016-09-07T18:12:40.302739: step 2394, loss 0.0414167, acc 0.98
2016-09-07T18:12:41.003920: step 2395, loss 0.0455028, acc 0.98
2016-09-07T18:12:41.698471: step 2396, loss 0.0175976, acc 1
2016-09-07T18:12:42.396227: step 2397, loss 0.0241287, acc 0.98
2016-09-07T18:12:43.074088: step 2398, loss 0.0653952, acc 0.96
2016-09-07T18:12:43.759878: step 2399, loss 0.0215402, acc 0.98
2016-09-07T18:12:44.482589: step 2400, loss 0.0322584, acc 1

Evaluation:
2016-09-07T18:12:47.721944: step 2400, loss 1.60499, acc 0.753

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-2400

2016-09-07T18:12:49.515187: step 2401, loss 0.0222411, acc 0.98
2016-09-07T18:12:50.182985: step 2402, loss 0.024722, acc 0.98
2016-09-07T18:12:50.846809: step 2403, loss 0.0599136, acc 0.96
2016-09-07T18:12:51.542542: step 2404, loss 0.0537695, acc 0.96
2016-09-07T18:12:52.242954: step 2405, loss 0.245793, acc 0.94
2016-09-07T18:12:52.956973: step 2406, loss 0.0467357, acc 0.98
2016-09-07T18:12:53.626093: step 2407, loss 0.0735546, acc 0.94
2016-09-07T18:12:54.291486: step 2408, loss 0.0479183, acc 0.96
2016-09-07T18:12:54.969979: step 2409, loss 0.0208443, acc 0.98
2016-09-07T18:12:55.672522: step 2410, loss 0.00180311, acc 1
2016-09-07T18:12:56.357807: step 2411, loss 0.050398, acc 0.98
2016-09-07T18:12:57.032958: step 2412, loss 0.0260622, acc 1
2016-09-07T18:12:57.720931: step 2413, loss 0.0227678, acc 0.98
2016-09-07T18:12:58.372969: step 2414, loss 0.0853439, acc 0.96
2016-09-07T18:12:59.079963: step 2415, loss 0.00684379, acc 1
2016-09-07T18:12:59.766310: step 2416, loss 0.0175356, acc 0.98
2016-09-07T18:13:00.495955: step 2417, loss 0.0492871, acc 0.98
2016-09-07T18:13:01.197494: step 2418, loss 0.072062, acc 0.98
2016-09-07T18:13:01.890529: step 2419, loss 0.118145, acc 0.98
2016-09-07T18:13:02.584675: step 2420, loss 0.0167887, acc 0.98
2016-09-07T18:13:03.252754: step 2421, loss 0.0233216, acc 0.98
2016-09-07T18:13:03.915069: step 2422, loss 0.0292408, acc 1
2016-09-07T18:13:04.603769: step 2423, loss 0.0221763, acc 0.98
2016-09-07T18:13:05.273388: step 2424, loss 0.0581177, acc 0.98
2016-09-07T18:13:05.946042: step 2425, loss 0.0412377, acc 0.98
2016-09-07T18:13:06.633988: step 2426, loss 0.0790481, acc 0.96
2016-09-07T18:13:07.351882: step 2427, loss 0.0273758, acc 0.98
2016-09-07T18:13:08.014617: step 2428, loss 0.0750523, acc 0.98
2016-09-07T18:13:08.696284: step 2429, loss 0.018155, acc 1
2016-09-07T18:13:09.400207: step 2430, loss 0.0708003, acc 0.96
2016-09-07T18:13:10.097884: step 2431, loss 0.0313836, acc 1
2016-09-07T18:13:10.779247: step 2432, loss 0.116991, acc 0.98
2016-09-07T18:13:11.459082: step 2433, loss 0.101593, acc 0.94
2016-09-07T18:13:12.161723: step 2434, loss 0.0569095, acc 1
2016-09-07T18:13:12.834698: step 2435, loss 0.111567, acc 0.96
2016-09-07T18:13:13.533115: step 2436, loss 0.0179111, acc 1
2016-09-07T18:13:14.213019: step 2437, loss 0.0243333, acc 1
2016-09-07T18:13:14.890814: step 2438, loss 0.0214557, acc 1
2016-09-07T18:13:15.567532: step 2439, loss 0.0600888, acc 0.96
2016-09-07T18:13:16.237407: step 2440, loss 0.00920988, acc 1
2016-09-07T18:13:16.937605: step 2441, loss 0.0228109, acc 1
2016-09-07T18:13:17.597590: step 2442, loss 0.0825161, acc 0.94
2016-09-07T18:13:18.292939: step 2443, loss 0.0150431, acc 1
2016-09-07T18:13:18.966324: step 2444, loss 0.0488583, acc 0.98
2016-09-07T18:13:19.645601: step 2445, loss 0.063289, acc 0.92
2016-09-07T18:13:20.318155: step 2446, loss 0.0433943, acc 0.98
2016-09-07T18:13:21.005006: step 2447, loss 0.106677, acc 0.96
2016-09-07T18:13:21.718845: step 2448, loss 0.0718676, acc 0.98
2016-09-07T18:13:22.379378: step 2449, loss 0.0397392, acc 0.98
2016-09-07T18:13:23.105097: step 2450, loss 0.048645, acc 0.96
2016-09-07T18:13:23.778798: step 2451, loss 0.00647445, acc 1
2016-09-07T18:13:24.461673: step 2452, loss 0.0131762, acc 1
2016-09-07T18:13:25.137492: step 2453, loss 0.0178701, acc 1
2016-09-07T18:13:25.814448: step 2454, loss 0.0190149, acc 0.98
2016-09-07T18:13:26.499978: step 2455, loss 0.0221004, acc 0.98
2016-09-07T18:13:27.172070: step 2456, loss 0.00969513, acc 1
2016-09-07T18:13:27.883963: step 2457, loss 0.130681, acc 0.94
2016-09-07T18:13:28.576135: step 2458, loss 0.0335627, acc 0.98
2016-09-07T18:13:29.270662: step 2459, loss 0.00217777, acc 1
2016-09-07T18:13:29.970457: step 2460, loss 0.0595271, acc 0.96
2016-09-07T18:13:30.645996: step 2461, loss 0.0174784, acc 0.98
2016-09-07T18:13:31.335432: step 2462, loss 0.0962391, acc 0.96
2016-09-07T18:13:32.007168: step 2463, loss 0.0208287, acc 0.98
2016-09-07T18:13:32.702281: step 2464, loss 0.0102693, acc 1
2016-09-07T18:13:33.386518: step 2465, loss 0.0367134, acc 0.98
2016-09-07T18:13:34.071760: step 2466, loss 0.0558441, acc 0.98
2016-09-07T18:13:34.765689: step 2467, loss 0.0227969, acc 0.98
2016-09-07T18:13:35.434641: step 2468, loss 0.0589, acc 1
2016-09-07T18:13:36.115294: step 2469, loss 0.065251, acc 0.96
2016-09-07T18:13:36.765837: step 2470, loss 0.0350692, acc 0.98
2016-09-07T18:13:37.469312: step 2471, loss 0.0136906, acc 1
2016-09-07T18:13:38.133289: step 2472, loss 0.0261766, acc 0.98
2016-09-07T18:13:38.821426: step 2473, loss 0.0488597, acc 0.98
2016-09-07T18:13:39.509853: step 2474, loss 0.110204, acc 0.94
2016-09-07T18:13:40.216195: step 2475, loss 0.0417288, acc 1
2016-09-07T18:13:40.915813: step 2476, loss 0.0630974, acc 0.96
2016-09-07T18:13:41.587453: step 2477, loss 0.0155276, acc 1
2016-09-07T18:13:42.269519: step 2478, loss 0.0510772, acc 0.98
2016-09-07T18:13:42.935646: step 2479, loss 0.0746658, acc 0.96
2016-09-07T18:13:43.605091: step 2480, loss 0.106203, acc 0.96
2016-09-07T18:13:44.271655: step 2481, loss 0.0408318, acc 0.96
2016-09-07T18:13:44.936605: step 2482, loss 0.0306262, acc 1
2016-09-07T18:13:45.630098: step 2483, loss 0.188025, acc 0.96
2016-09-07T18:13:46.325051: step 2484, loss 0.00260381, acc 1
2016-09-07T18:13:47.039768: step 2485, loss 0.00188662, acc 1
2016-09-07T18:13:47.718519: step 2486, loss 0.0400183, acc 0.98
2016-09-07T18:13:48.379058: step 2487, loss 0.0215525, acc 1
2016-09-07T18:13:49.045980: step 2488, loss 0.0678711, acc 0.98
2016-09-07T18:13:49.701283: step 2489, loss 0.0720553, acc 0.98
2016-09-07T18:13:50.381668: step 2490, loss 0.0811331, acc 0.98
2016-09-07T18:13:51.073613: step 2491, loss 0.00405482, acc 1
2016-09-07T18:13:51.774144: step 2492, loss 0.0145133, acc 1
2016-09-07T18:13:52.446533: step 2493, loss 0.0279098, acc 0.98
2016-09-07T18:13:53.156958: step 2494, loss 0.237516, acc 0.96
2016-09-07T18:13:53.869185: step 2495, loss 0.0107298, acc 1
2016-09-07T18:13:54.554327: step 2496, loss 0.0194104, acc 1
2016-09-07T18:13:55.243636: step 2497, loss 0.0494581, acc 0.98
2016-09-07T18:13:55.914089: step 2498, loss 0.0584462, acc 0.96
2016-09-07T18:13:56.612678: step 2499, loss 0.0904948, acc 0.94
2016-09-07T18:13:57.268264: step 2500, loss 0.0435113, acc 1

Evaluation:
2016-09-07T18:14:00.579452: step 2500, loss 1.51031, acc 0.752

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-2500

2016-09-07T18:14:02.313652: step 2501, loss 0.0273647, acc 0.98
2016-09-07T18:14:02.997570: step 2502, loss 0.0398476, acc 0.96
2016-09-07T18:14:03.681072: step 2503, loss 0.0450716, acc 1
2016-09-07T18:14:04.359459: step 2504, loss 0.0562208, acc 0.98
2016-09-07T18:14:05.077187: step 2505, loss 0.0403574, acc 0.98
2016-09-07T18:14:05.746666: step 2506, loss 0.0221005, acc 1
2016-09-07T18:14:06.413087: step 2507, loss 0.0431224, acc 0.96
2016-09-07T18:14:07.101615: step 2508, loss 0.0824274, acc 0.96
2016-09-07T18:14:07.780599: step 2509, loss 0.0597303, acc 0.98
2016-09-07T18:14:08.474970: step 2510, loss 0.0486784, acc 0.96
2016-09-07T18:14:09.149090: step 2511, loss 0.0093908, acc 1
2016-09-07T18:14:09.849972: step 2512, loss 0.0329221, acc 0.98
2016-09-07T18:14:10.524576: step 2513, loss 0.0533633, acc 0.98
2016-09-07T18:14:11.222800: step 2514, loss 0.0800711, acc 0.96
2016-09-07T18:14:11.907680: step 2515, loss 0.0740257, acc 0.94
2016-09-07T18:14:12.593489: step 2516, loss 0.0201292, acc 1
2016-09-07T18:14:13.321240: step 2517, loss 0.0418827, acc 0.96
2016-09-07T18:14:14.063638: step 2518, loss 0.015684, acc 1
2016-09-07T18:14:14.760377: step 2519, loss 0.0578181, acc 0.98
2016-09-07T18:14:15.422733: step 2520, loss 0.0790673, acc 0.94
2016-09-07T18:14:16.098279: step 2521, loss 0.0139645, acc 1
2016-09-07T18:14:16.458297: step 2522, loss 0.0535049, acc 1
2016-09-07T18:14:17.168332: step 2523, loss 0.0179462, acc 1
2016-09-07T18:14:17.849246: step 2524, loss 0.0216663, acc 1
2016-09-07T18:14:18.528902: step 2525, loss 0.0286715, acc 0.98
2016-09-07T18:14:19.231298: step 2526, loss 0.0144122, acc 1
2016-09-07T18:14:19.919862: step 2527, loss 0.00893473, acc 1
2016-09-07T18:14:20.594198: step 2528, loss 0.0918728, acc 0.96
2016-09-07T18:14:21.272708: step 2529, loss 0.0108852, acc 1
2016-09-07T18:14:21.968357: step 2530, loss 0.039083, acc 0.98
2016-09-07T18:14:22.629261: step 2531, loss 0.0232037, acc 0.98
2016-09-07T18:14:23.318820: step 2532, loss 0.0352124, acc 1
2016-09-07T18:14:24.008587: step 2533, loss 0.0130429, acc 1
2016-09-07T18:14:24.699703: step 2534, loss 0.00576338, acc 1
2016-09-07T18:14:25.388814: step 2535, loss 0.175006, acc 0.96
2016-09-07T18:14:26.050147: step 2536, loss 0.163905, acc 0.96
2016-09-07T18:14:26.757433: step 2537, loss 0.0103438, acc 1
2016-09-07T18:14:27.443250: step 2538, loss 0.0647597, acc 0.98
2016-09-07T18:14:28.120202: step 2539, loss 0.0511352, acc 0.98
2016-09-07T18:14:28.793081: step 2540, loss 0.00214625, acc 1
2016-09-07T18:14:29.461060: step 2541, loss 0.0114028, acc 1
2016-09-07T18:14:30.155350: step 2542, loss 0.0265517, acc 0.98
2016-09-07T18:14:30.845874: step 2543, loss 0.015117, acc 1
2016-09-07T18:14:31.540182: step 2544, loss 0.0714849, acc 0.96
2016-09-07T18:14:32.207220: step 2545, loss 0.0248283, acc 1
2016-09-07T18:14:32.908537: step 2546, loss 0.00384931, acc 1
2016-09-07T18:14:33.630974: step 2547, loss 0.0393976, acc 0.98
2016-09-07T18:14:34.312718: step 2548, loss 0.00189201, acc 1
2016-09-07T18:14:35.003157: step 2549, loss 0.0207851, acc 1
2016-09-07T18:14:35.681711: step 2550, loss 0.00368036, acc 1
2016-09-07T18:14:36.386771: step 2551, loss 0.00854548, acc 1
2016-09-07T18:14:37.087225: step 2552, loss 0.0025098, acc 1
2016-09-07T18:14:37.759880: step 2553, loss 0.0975917, acc 0.96
2016-09-07T18:14:38.439378: step 2554, loss 0.0269524, acc 1
2016-09-07T18:14:39.122431: step 2555, loss 0.0221824, acc 0.98
2016-09-07T18:14:39.805784: step 2556, loss 0.000794344, acc 1
2016-09-07T18:14:40.481393: step 2557, loss 0.00359092, acc 1
2016-09-07T18:14:41.188555: step 2558, loss 0.125159, acc 0.98
2016-09-07T18:14:41.847963: step 2559, loss 0.0305345, acc 0.98
2016-09-07T18:14:42.524195: step 2560, loss 0.119731, acc 0.96
2016-09-07T18:14:43.203202: step 2561, loss 0.0427774, acc 0.96
2016-09-07T18:14:43.873546: step 2562, loss 0.0232985, acc 0.98
2016-09-07T18:14:44.561289: step 2563, loss 0.102847, acc 0.98
2016-09-07T18:14:45.246418: step 2564, loss 0.016258, acc 1
2016-09-07T18:14:45.928616: step 2565, loss 0.0395271, acc 0.98
2016-09-07T18:14:46.586832: step 2566, loss 0.0216787, acc 1
2016-09-07T18:14:47.278148: step 2567, loss 0.0195346, acc 1
2016-09-07T18:14:47.973815: step 2568, loss 0.117237, acc 0.96
2016-09-07T18:14:48.663036: step 2569, loss 0.0560254, acc 0.98
2016-09-07T18:14:49.356811: step 2570, loss 0.0117304, acc 1
2016-09-07T18:14:50.049440: step 2571, loss 0.0115854, acc 1
2016-09-07T18:14:50.770744: step 2572, loss 0.0323961, acc 1
2016-09-07T18:14:51.427601: step 2573, loss 0.0113389, acc 1
2016-09-07T18:14:52.128028: step 2574, loss 0.0207714, acc 0.98
2016-09-07T18:14:52.823239: step 2575, loss 0.0123555, acc 1
2016-09-07T18:14:53.511899: step 2576, loss 0.00245095, acc 1
2016-09-07T18:14:54.183675: step 2577, loss 0.00475768, acc 1
2016-09-07T18:14:54.880309: step 2578, loss 0.0178391, acc 1
2016-09-07T18:14:55.584508: step 2579, loss 0.0230667, acc 1
2016-09-07T18:14:56.247812: step 2580, loss 0.0182839, acc 0.98
2016-09-07T18:14:56.946026: step 2581, loss 0.0198084, acc 0.98
2016-09-07T18:14:57.608513: step 2582, loss 0.0214632, acc 1
2016-09-07T18:14:58.279558: step 2583, loss 0.0332894, acc 0.98
2016-09-07T18:14:58.973769: step 2584, loss 0.0115552, acc 1
2016-09-07T18:14:59.655700: step 2585, loss 0.0505807, acc 0.98
2016-09-07T18:15:00.394813: step 2586, loss 0.0337555, acc 1
2016-09-07T18:15:01.066517: step 2587, loss 0.0467547, acc 0.98
2016-09-07T18:15:01.766519: step 2588, loss 0.0227171, acc 1
2016-09-07T18:15:02.455603: step 2589, loss 0.069717, acc 0.96
2016-09-07T18:15:03.126059: step 2590, loss 0.000700969, acc 1
2016-09-07T18:15:03.814647: step 2591, loss 0.0386822, acc 0.98
2016-09-07T18:15:04.494071: step 2592, loss 0.0319628, acc 0.98
2016-09-07T18:15:05.175853: step 2593, loss 0.0495237, acc 0.96
2016-09-07T18:15:05.854865: step 2594, loss 0.0396565, acc 0.98
2016-09-07T18:15:06.580179: step 2595, loss 0.00719547, acc 1
2016-09-07T18:15:07.290854: step 2596, loss 0.0247394, acc 1
2016-09-07T18:15:07.976922: step 2597, loss 0.0137997, acc 1
2016-09-07T18:15:08.666032: step 2598, loss 0.0835276, acc 0.94
2016-09-07T18:15:09.340978: step 2599, loss 0.00999888, acc 1
2016-09-07T18:15:10.048871: step 2600, loss 0.00851628, acc 1

Evaluation:
2016-09-07T18:15:13.341005: step 2600, loss 1.67184, acc 0.743

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-2600

2016-09-07T18:15:15.097224: step 2601, loss 0.006251, acc 1
2016-09-07T18:15:15.800056: step 2602, loss 0.0637965, acc 0.96
2016-09-07T18:15:16.484580: step 2603, loss 0.134312, acc 0.96
2016-09-07T18:15:17.216661: step 2604, loss 0.0346436, acc 0.98
2016-09-07T18:15:17.875442: step 2605, loss 0.00364317, acc 1
2016-09-07T18:15:18.573988: step 2606, loss 0.000985203, acc 1
2016-09-07T18:15:19.262847: step 2607, loss 0.0560919, acc 0.98
2016-09-07T18:15:19.942608: step 2608, loss 0.0257037, acc 0.98
2016-09-07T18:15:20.630726: step 2609, loss 0.0660825, acc 0.96
2016-09-07T18:15:21.326236: step 2610, loss 0.0447016, acc 0.96
2016-09-07T18:15:22.026650: step 2611, loss 0.02678, acc 0.98
2016-09-07T18:15:22.716474: step 2612, loss 0.05809, acc 0.98
2016-09-07T18:15:23.405713: step 2613, loss 0.028837, acc 0.98
2016-09-07T18:15:24.092082: step 2614, loss 0.0125648, acc 1
2016-09-07T18:15:24.788324: step 2615, loss 0.0128291, acc 1
2016-09-07T18:15:25.459574: step 2616, loss 0.00971586, acc 1
2016-09-07T18:15:26.160501: step 2617, loss 0.174227, acc 0.94
2016-09-07T18:15:26.859427: step 2618, loss 0.0916482, acc 0.98
2016-09-07T18:15:27.523623: step 2619, loss 0.0649602, acc 0.96
2016-09-07T18:15:28.208531: step 2620, loss 0.0249224, acc 1
2016-09-07T18:15:28.891067: step 2621, loss 0.0603512, acc 0.98
2016-09-07T18:15:29.563609: step 2622, loss 0.0616589, acc 0.98
2016-09-07T18:15:30.240466: step 2623, loss 0.00811165, acc 1
2016-09-07T18:15:30.911272: step 2624, loss 0.0120954, acc 1
2016-09-07T18:15:31.589130: step 2625, loss 0.0218792, acc 1
2016-09-07T18:15:32.232835: step 2626, loss 0.158412, acc 0.96
2016-09-07T18:15:32.942974: step 2627, loss 0.0118221, acc 1
2016-09-07T18:15:33.595156: step 2628, loss 0.0572352, acc 0.96
2016-09-07T18:15:34.284541: step 2629, loss 0.040723, acc 0.98
2016-09-07T18:15:34.956178: step 2630, loss 0.0194197, acc 0.98
2016-09-07T18:15:35.630654: step 2631, loss 0.0744743, acc 0.96
2016-09-07T18:15:36.331660: step 2632, loss 0.0315872, acc 1
2016-09-07T18:15:37.014166: step 2633, loss 0.0872553, acc 0.94
2016-09-07T18:15:37.706085: step 2634, loss 0.0192713, acc 1
2016-09-07T18:15:38.387514: step 2635, loss 0.0137678, acc 1
2016-09-07T18:15:39.073027: step 2636, loss 0.0166852, acc 1
2016-09-07T18:15:39.757706: step 2637, loss 0.0136358, acc 1
2016-09-07T18:15:40.436220: step 2638, loss 0.0575076, acc 0.98
2016-09-07T18:15:41.112993: step 2639, loss 0.100865, acc 0.94
2016-09-07T18:15:41.799014: step 2640, loss 0.153887, acc 0.96
2016-09-07T18:15:42.486595: step 2641, loss 0.0149525, acc 1
2016-09-07T18:15:43.167717: step 2642, loss 0.0337785, acc 0.98
2016-09-07T18:15:43.875819: step 2643, loss 0.0146998, acc 1
2016-09-07T18:15:44.565015: step 2644, loss 0.108999, acc 0.96
2016-09-07T18:15:45.255300: step 2645, loss 0.0247471, acc 0.98
2016-09-07T18:15:45.938648: step 2646, loss 0.0564097, acc 0.98
2016-09-07T18:15:46.621676: step 2647, loss 0.0132267, acc 1
2016-09-07T18:15:47.305008: step 2648, loss 0.0307881, acc 0.98
2016-09-07T18:15:47.982263: step 2649, loss 0.118967, acc 0.96
2016-09-07T18:15:48.659394: step 2650, loss 0.0437031, acc 0.98
2016-09-07T18:15:49.350694: step 2651, loss 0.00195938, acc 1
2016-09-07T18:15:50.059099: step 2652, loss 0.0217453, acc 1
2016-09-07T18:15:50.731899: step 2653, loss 0.0588276, acc 0.96
2016-09-07T18:15:51.422141: step 2654, loss 0.0205122, acc 0.98
2016-09-07T18:15:52.135568: step 2655, loss 0.0394703, acc 0.98
2016-09-07T18:15:52.800607: step 2656, loss 0.0427255, acc 1
2016-09-07T18:15:53.475238: step 2657, loss 0.0535452, acc 0.96
2016-09-07T18:15:54.139907: step 2658, loss 0.0332817, acc 0.98
2016-09-07T18:15:54.808006: step 2659, loss 0.0497336, acc 0.98
2016-09-07T18:15:55.493150: step 2660, loss 0.00908953, acc 1
2016-09-07T18:15:56.162900: step 2661, loss 0.0312943, acc 0.98
2016-09-07T18:15:56.837904: step 2662, loss 0.094755, acc 0.94
2016-09-07T18:15:57.482640: step 2663, loss 0.010916, acc 1
2016-09-07T18:15:58.176108: step 2664, loss 0.0476394, acc 0.96
2016-09-07T18:15:58.847801: step 2665, loss 0.0535432, acc 0.98
2016-09-07T18:15:59.513297: step 2666, loss 0.0139655, acc 1
2016-09-07T18:16:00.213465: step 2667, loss 0.030535, acc 0.98
2016-09-07T18:16:00.918700: step 2668, loss 0.0363956, acc 0.98
2016-09-07T18:16:01.595004: step 2669, loss 0.0159163, acc 1
2016-09-07T18:16:02.256552: step 2670, loss 0.0417899, acc 0.98
2016-09-07T18:16:02.949285: step 2671, loss 0.0417957, acc 1
2016-09-07T18:16:03.655301: step 2672, loss 0.0335695, acc 0.96
2016-09-07T18:16:04.356026: step 2673, loss 0.0500495, acc 0.96
2016-09-07T18:16:05.021072: step 2674, loss 0.0500741, acc 0.96
2016-09-07T18:16:05.714933: step 2675, loss 0.0323172, acc 1
2016-09-07T18:16:06.393310: step 2676, loss 0.0390474, acc 0.98
2016-09-07T18:16:07.075689: step 2677, loss 0.0276989, acc 0.98
2016-09-07T18:16:07.766097: step 2678, loss 0.0199438, acc 1
2016-09-07T18:16:08.455766: step 2679, loss 0.00532856, acc 1
2016-09-07T18:16:09.128913: step 2680, loss 0.00163018, acc 1
2016-09-07T18:16:09.816423: step 2681, loss 0.000707894, acc 1
2016-09-07T18:16:10.504907: step 2682, loss 0.0778227, acc 0.98
2016-09-07T18:16:11.195386: step 2683, loss 0.0506087, acc 0.98
2016-09-07T18:16:11.877184: step 2684, loss 0.00401935, acc 1
2016-09-07T18:16:12.571429: step 2685, loss 0.0013587, acc 1
2016-09-07T18:16:13.262372: step 2686, loss 0.00555414, acc 1
2016-09-07T18:16:13.951625: step 2687, loss 0.00947202, acc 1
2016-09-07T18:16:14.620910: step 2688, loss 0.0178166, acc 0.98
2016-09-07T18:16:15.316172: step 2689, loss 0.010978, acc 1
2016-09-07T18:16:16.001056: step 2690, loss 0.028557, acc 1
2016-09-07T18:16:16.675803: step 2691, loss 0.00958771, acc 1
2016-09-07T18:16:17.404969: step 2692, loss 0.0769645, acc 0.96
2016-09-07T18:16:18.092728: step 2693, loss 0.0243633, acc 1
2016-09-07T18:16:18.778029: step 2694, loss 0.022599, acc 1
2016-09-07T18:16:19.478901: step 2695, loss 0.00727929, acc 1
2016-09-07T18:16:20.182591: step 2696, loss 0.0694273, acc 0.98
2016-09-07T18:16:20.880396: step 2697, loss 0.0464099, acc 0.98
2016-09-07T18:16:21.551322: step 2698, loss 0.0892187, acc 0.94
2016-09-07T18:16:22.255188: step 2699, loss 0.063149, acc 0.96
2016-09-07T18:16:22.953672: step 2700, loss 0.0185244, acc 0.98

Evaluation:
2016-09-07T18:16:26.240313: step 2700, loss 1.64724, acc 0.738

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-2700

2016-09-07T18:16:27.965354: step 2701, loss 0.0301015, acc 0.98
2016-09-07T18:16:28.656628: step 2702, loss 0.0519259, acc 0.98
2016-09-07T18:16:29.326059: step 2703, loss 0.0433233, acc 0.98
2016-09-07T18:16:29.989219: step 2704, loss 0.0114279, acc 1
2016-09-07T18:16:30.692041: step 2705, loss 0.0224967, acc 1
2016-09-07T18:16:31.366180: step 2706, loss 0.0456696, acc 0.96
2016-09-07T18:16:32.038769: step 2707, loss 0.0349728, acc 0.98
2016-09-07T18:16:32.731334: step 2708, loss 0.0262097, acc 0.98
2016-09-07T18:16:33.403987: step 2709, loss 0.0542133, acc 0.94
2016-09-07T18:16:34.108021: step 2710, loss 0.00477125, acc 1
2016-09-07T18:16:34.783332: step 2711, loss 0.0311037, acc 0.98
2016-09-07T18:16:35.493025: step 2712, loss 0.097733, acc 0.94
2016-09-07T18:16:36.186110: step 2713, loss 0.0220097, acc 1
2016-09-07T18:16:36.860935: step 2714, loss 0.0366241, acc 0.98
2016-09-07T18:16:37.525244: step 2715, loss 0.0793554, acc 0.98
2016-09-07T18:16:37.894364: step 2716, loss 0.0948458, acc 0.916667
2016-09-07T18:16:38.564160: step 2717, loss 0.0518863, acc 0.98
2016-09-07T18:16:39.257553: step 2718, loss 0.0410461, acc 0.96
2016-09-07T18:16:39.941747: step 2719, loss 0.0938588, acc 0.94
2016-09-07T18:16:40.601475: step 2720, loss 0.0172914, acc 1
2016-09-07T18:16:41.322198: step 2721, loss 0.0162878, acc 1
2016-09-07T18:16:41.973638: step 2722, loss 0.0365519, acc 0.98
2016-09-07T18:16:42.678342: step 2723, loss 0.00992871, acc 1
2016-09-07T18:16:43.337945: step 2724, loss 0.0249674, acc 1
2016-09-07T18:16:44.042667: step 2725, loss 0.0307516, acc 0.98
2016-09-07T18:16:44.720058: step 2726, loss 0.00796465, acc 1
2016-09-07T18:16:45.404563: step 2727, loss 0.0395395, acc 0.98
2016-09-07T18:16:46.101768: step 2728, loss 0.0321971, acc 0.96
2016-09-07T18:16:46.789483: step 2729, loss 0.0682156, acc 0.94
2016-09-07T18:16:47.490733: step 2730, loss 0.00861872, acc 1
2016-09-07T18:16:48.164266: step 2731, loss 0.00256889, acc 1
2016-09-07T18:16:48.854146: step 2732, loss 0.0565209, acc 0.98
2016-09-07T18:16:49.551325: step 2733, loss 0.00506721, acc 1
2016-09-07T18:16:50.237051: step 2734, loss 0.0448215, acc 0.96
2016-09-07T18:16:50.942793: step 2735, loss 0.0695207, acc 0.96
2016-09-07T18:16:51.608679: step 2736, loss 0.0864895, acc 0.98
2016-09-07T18:16:52.350231: step 2737, loss 0.0213714, acc 0.98
2016-09-07T18:16:53.024555: step 2738, loss 0.0113794, acc 1
2016-09-07T18:16:53.695999: step 2739, loss 0.0376683, acc 0.98
2016-09-07T18:16:54.379583: step 2740, loss 0.0277711, acc 1
2016-09-07T18:16:55.062592: step 2741, loss 0.0842632, acc 0.96
2016-09-07T18:16:55.755456: step 2742, loss 0.0128324, acc 1
2016-09-07T18:16:56.406882: step 2743, loss 0.0267754, acc 0.98
2016-09-07T18:16:57.094994: step 2744, loss 0.00420886, acc 1
2016-09-07T18:16:57.767830: step 2745, loss 0.00943169, acc 1
2016-09-07T18:16:58.461933: step 2746, loss 0.123502, acc 0.98
2016-09-07T18:16:59.156024: step 2747, loss 0.0140965, acc 1
2016-09-07T18:16:59.837127: step 2748, loss 0.123466, acc 0.94
2016-09-07T18:17:00.547700: step 2749, loss 0.0439093, acc 0.98
2016-09-07T18:17:01.225484: step 2750, loss 0.0976424, acc 0.96
2016-09-07T18:17:01.958727: step 2751, loss 0.0647594, acc 0.96
2016-09-07T18:17:02.652708: step 2752, loss 0.0329032, acc 0.98
2016-09-07T18:17:03.341570: step 2753, loss 0.0210146, acc 1
2016-09-07T18:17:04.022752: step 2754, loss 0.0520503, acc 0.94
2016-09-07T18:17:04.709774: step 2755, loss 0.0789871, acc 0.96
2016-09-07T18:17:05.401786: step 2756, loss 0.000754023, acc 1
2016-09-07T18:17:06.080734: step 2757, loss 0.0478062, acc 0.96
2016-09-07T18:17:06.792623: step 2758, loss 0.106374, acc 0.98
2016-09-07T18:17:07.482175: step 2759, loss 0.0612786, acc 0.96
2016-09-07T18:17:08.159811: step 2760, loss 0.045833, acc 0.98
2016-09-07T18:17:08.834059: step 2761, loss 0.00961558, acc 1
2016-09-07T18:17:09.519968: step 2762, loss 0.171815, acc 0.94
2016-09-07T18:17:10.212807: step 2763, loss 0.0311907, acc 0.98
2016-09-07T18:17:10.895828: step 2764, loss 0.0612632, acc 0.96
2016-09-07T18:17:11.587855: step 2765, loss 0.015288, acc 1
2016-09-07T18:17:12.256099: step 2766, loss 0.0180607, acc 1
2016-09-07T18:17:12.950528: step 2767, loss 0.00838195, acc 1
2016-09-07T18:17:13.629823: step 2768, loss 0.0397407, acc 0.98
2016-09-07T18:17:14.315132: step 2769, loss 0.040814, acc 1
2016-09-07T18:17:14.983900: step 2770, loss 0.0187401, acc 0.98
2016-09-07T18:17:15.639950: step 2771, loss 0.00702042, acc 1
2016-09-07T18:17:16.337938: step 2772, loss 0.0204663, acc 1
2016-09-07T18:17:17.028760: step 2773, loss 0.04325, acc 1
2016-09-07T18:17:17.709399: step 2774, loss 0.00211327, acc 1
2016-09-07T18:17:18.412208: step 2775, loss 0.0111411, acc 1
2016-09-07T18:17:19.112415: step 2776, loss 0.0197635, acc 1
2016-09-07T18:17:19.801900: step 2777, loss 0.033468, acc 1
2016-09-07T18:17:20.462019: step 2778, loss 0.00790493, acc 1
2016-09-07T18:17:21.162127: step 2779, loss 0.045101, acc 0.98
2016-09-07T18:17:21.822023: step 2780, loss 0.0709803, acc 0.96
2016-09-07T18:17:22.492691: step 2781, loss 0.0304862, acc 1
2016-09-07T18:17:23.171271: step 2782, loss 0.0312111, acc 0.98
2016-09-07T18:17:23.853589: step 2783, loss 0.10617, acc 0.96
2016-09-07T18:17:24.537556: step 2784, loss 0.0420489, acc 0.98
2016-09-07T18:17:25.219890: step 2785, loss 0.008189, acc 1
2016-09-07T18:17:25.926057: step 2786, loss 0.0437831, acc 0.96
2016-09-07T18:17:26.603940: step 2787, loss 0.00228121, acc 1
2016-09-07T18:17:27.300097: step 2788, loss 0.00235774, acc 1
2016-09-07T18:17:27.984486: step 2789, loss 0.00113791, acc 1
2016-09-07T18:17:28.677466: step 2790, loss 0.0267026, acc 0.98
2016-09-07T18:17:29.373299: step 2791, loss 0.0305027, acc 0.98
2016-09-07T18:17:30.070896: step 2792, loss 0.0323578, acc 0.98
2016-09-07T18:17:30.773740: step 2793, loss 0.0575135, acc 0.96
2016-09-07T18:17:31.437369: step 2794, loss 0.00195017, acc 1
2016-09-07T18:17:32.122658: step 2795, loss 0.0293382, acc 0.98
2016-09-07T18:17:32.798067: step 2796, loss 0.0488616, acc 0.98
2016-09-07T18:17:33.481598: step 2797, loss 0.0126035, acc 1
2016-09-07T18:17:34.179197: step 2798, loss 0.03943, acc 0.98
2016-09-07T18:17:34.871270: step 2799, loss 0.0768096, acc 0.92
2016-09-07T18:17:35.565245: step 2800, loss 0.0102495, acc 1

Evaluation:
2016-09-07T18:17:38.857344: step 2800, loss 1.78202, acc 0.744

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-2800

2016-09-07T18:17:40.665415: step 2801, loss 0.0236515, acc 1
2016-09-07T18:17:41.346982: step 2802, loss 0.0233844, acc 0.98
2016-09-07T18:17:42.037975: step 2803, loss 0.0188007, acc 0.98
2016-09-07T18:17:42.723471: step 2804, loss 0.010622, acc 1
2016-09-07T18:17:43.383672: step 2805, loss 0.0354678, acc 0.98
2016-09-07T18:17:44.090325: step 2806, loss 0.0451313, acc 1
2016-09-07T18:17:44.782036: step 2807, loss 0.0822712, acc 0.98
2016-09-07T18:17:45.487421: step 2808, loss 0.0876611, acc 0.98
2016-09-07T18:17:46.176247: step 2809, loss 0.0778638, acc 0.98
2016-09-07T18:17:46.869871: step 2810, loss 0.015427, acc 1
2016-09-07T18:17:47.580151: step 2811, loss 0.031804, acc 0.98
2016-09-07T18:17:48.270599: step 2812, loss 0.017745, acc 1
2016-09-07T18:17:48.945731: step 2813, loss 0.0315102, acc 0.98
2016-09-07T18:17:49.640737: step 2814, loss 0.0312906, acc 0.98
2016-09-07T18:17:50.324748: step 2815, loss 0.0694122, acc 0.96
2016-09-07T18:17:51.021611: step 2816, loss 0.0658818, acc 0.96
2016-09-07T18:17:51.699638: step 2817, loss 0.0163421, acc 1
2016-09-07T18:17:52.400833: step 2818, loss 0.0390036, acc 0.98
2016-09-07T18:17:53.100674: step 2819, loss 0.1394, acc 0.98
2016-09-07T18:17:53.772453: step 2820, loss 0.00528746, acc 1
2016-09-07T18:17:54.469790: step 2821, loss 0.0252222, acc 1
2016-09-07T18:17:55.212261: step 2822, loss 0.074444, acc 0.96
2016-09-07T18:17:55.925300: step 2823, loss 0.0143918, acc 1
2016-09-07T18:17:56.609113: step 2824, loss 0.0387999, acc 0.98
2016-09-07T18:17:57.318417: step 2825, loss 0.00703223, acc 1
2016-09-07T18:17:58.020173: step 2826, loss 0.0245201, acc 0.98
2016-09-07T18:17:58.694107: step 2827, loss 0.0148009, acc 1
2016-09-07T18:17:59.380982: step 2828, loss 0.0469406, acc 0.98
2016-09-07T18:18:00.059741: step 2829, loss 0.0445929, acc 0.98
2016-09-07T18:18:00.781335: step 2830, loss 0.0255867, acc 0.98
2016-09-07T18:18:01.466982: step 2831, loss 0.0881711, acc 0.96
2016-09-07T18:18:02.147787: step 2832, loss 0.0953074, acc 0.96
2016-09-07T18:18:02.828953: step 2833, loss 0.0119115, acc 1
2016-09-07T18:18:03.509940: step 2834, loss 0.0336053, acc 0.98
2016-09-07T18:18:04.209381: step 2835, loss 0.068734, acc 0.98
2016-09-07T18:18:04.902703: step 2836, loss 0.149059, acc 0.96
2016-09-07T18:18:05.607311: step 2837, loss 0.0515637, acc 0.98
2016-09-07T18:18:06.283215: step 2838, loss 0.0474527, acc 0.98
2016-09-07T18:18:06.962631: step 2839, loss 0.00828329, acc 1
2016-09-07T18:18:07.646584: step 2840, loss 0.0203517, acc 1
2016-09-07T18:18:08.331143: step 2841, loss 0.0924294, acc 0.94
2016-09-07T18:18:09.012298: step 2842, loss 0.0341078, acc 0.98
2016-09-07T18:18:09.698772: step 2843, loss 0.0628564, acc 0.98
2016-09-07T18:18:10.391297: step 2844, loss 0.00623575, acc 1
2016-09-07T18:18:11.056183: step 2845, loss 0.0320156, acc 0.98
2016-09-07T18:18:11.753938: step 2846, loss 0.0342876, acc 0.98
2016-09-07T18:18:12.436031: step 2847, loss 0.0182046, acc 1
2016-09-07T18:18:13.130433: step 2848, loss 0.0338099, acc 1
2016-09-07T18:18:13.814849: step 2849, loss 0.00210668, acc 1
2016-09-07T18:18:14.487615: step 2850, loss 0.0259437, acc 0.98
2016-09-07T18:18:15.172688: step 2851, loss 0.0142114, acc 1
2016-09-07T18:18:15.847467: step 2852, loss 0.0480367, acc 0.98
2016-09-07T18:18:16.586674: step 2853, loss 0.00379921, acc 1
2016-09-07T18:18:17.271623: step 2854, loss 0.000169492, acc 1
2016-09-07T18:18:17.954269: step 2855, loss 0.0627186, acc 0.96
2016-09-07T18:18:18.649673: step 2856, loss 0.0233102, acc 0.98
2016-09-07T18:18:19.349152: step 2857, loss 0.0346134, acc 0.96
2016-09-07T18:18:20.050223: step 2858, loss 0.0395278, acc 0.98
2016-09-07T18:18:20.744475: step 2859, loss 0.031552, acc 0.98
2016-09-07T18:18:21.421215: step 2860, loss 0.00253442, acc 1
2016-09-07T18:18:22.129246: step 2861, loss 0.0209233, acc 1
2016-09-07T18:18:22.828528: step 2862, loss 0.0455709, acc 0.98
2016-09-07T18:18:23.511611: step 2863, loss 0.120755, acc 0.94
2016-09-07T18:18:24.177862: step 2864, loss 0.00820409, acc 1
2016-09-07T18:18:24.898176: step 2865, loss 0.0178237, acc 1
2016-09-07T18:18:25.580479: step 2866, loss 0.000421187, acc 1
2016-09-07T18:18:26.275853: step 2867, loss 0.0835074, acc 0.96
2016-09-07T18:18:26.983527: step 2868, loss 0.12903, acc 0.9
2016-09-07T18:18:27.682251: step 2869, loss 0.00945764, acc 1
2016-09-07T18:18:28.376770: step 2870, loss 0.126988, acc 0.98
2016-09-07T18:18:29.041083: step 2871, loss 0.0321352, acc 0.98
2016-09-07T18:18:29.750843: step 2872, loss 0.00175765, acc 1
2016-09-07T18:18:30.436195: step 2873, loss 0.00225122, acc 1
2016-09-07T18:18:31.125997: step 2874, loss 0.0245319, acc 1
2016-09-07T18:18:31.823452: step 2875, loss 0.00282572, acc 1
2016-09-07T18:18:32.497333: step 2876, loss 0.0441479, acc 0.96
2016-09-07T18:18:33.182432: step 2877, loss 0.0439537, acc 0.98
2016-09-07T18:18:33.850079: step 2878, loss 0.0626298, acc 0.98
2016-09-07T18:18:34.544047: step 2879, loss 0.00234814, acc 1
2016-09-07T18:18:35.226740: step 2880, loss 0.0459703, acc 0.98
2016-09-07T18:18:35.910220: step 2881, loss 0.00289148, acc 1
2016-09-07T18:18:36.604574: step 2882, loss 0.019901, acc 1
2016-09-07T18:18:37.281811: step 2883, loss 0.000812961, acc 1
2016-09-07T18:18:37.961970: step 2884, loss 0.0124149, acc 1
2016-09-07T18:18:38.619333: step 2885, loss 0.0811383, acc 0.94
2016-09-07T18:18:39.310803: step 2886, loss 0.0590826, acc 0.98
2016-09-07T18:18:39.972586: step 2887, loss 0.0756407, acc 0.94
2016-09-07T18:18:40.659687: step 2888, loss 0.0370299, acc 1
2016-09-07T18:18:41.357832: step 2889, loss 0.0508289, acc 0.96
2016-09-07T18:18:42.054498: step 2890, loss 0.0485599, acc 0.98
2016-09-07T18:18:42.727931: step 2891, loss 0.0351812, acc 0.98
2016-09-07T18:18:43.408304: step 2892, loss 0.0219095, acc 1
2016-09-07T18:18:44.102786: step 2893, loss 0.0217607, acc 0.98
2016-09-07T18:18:44.771629: step 2894, loss 0.000850822, acc 1
2016-09-07T18:18:45.448317: step 2895, loss 0.0609356, acc 0.98
2016-09-07T18:18:46.145527: step 2896, loss 0.0614311, acc 1
2016-09-07T18:18:46.818273: step 2897, loss 0.124956, acc 0.94
2016-09-07T18:18:47.510159: step 2898, loss 0.0149971, acc 1
2016-09-07T18:18:48.193277: step 2899, loss 0.0284582, acc 0.98
2016-09-07T18:18:48.895878: step 2900, loss 0.0414044, acc 0.98

Evaluation:
2016-09-07T18:18:52.199646: step 2900, loss 1.7822, acc 0.743

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-2900

2016-09-07T18:18:53.986817: step 2901, loss 0.0146767, acc 1
2016-09-07T18:18:54.684080: step 2902, loss 0.0924149, acc 0.92
2016-09-07T18:18:55.372547: step 2903, loss 0.0392993, acc 0.98
2016-09-07T18:18:56.078999: step 2904, loss 0.00710696, acc 1
2016-09-07T18:18:56.742735: step 2905, loss 0.0499722, acc 0.98
2016-09-07T18:18:57.438157: step 2906, loss 0.0182627, acc 1
2016-09-07T18:18:58.117902: step 2907, loss 0.0253713, acc 1
2016-09-07T18:18:58.792113: step 2908, loss 0.022027, acc 1
2016-09-07T18:18:59.482283: step 2909, loss 0.00857913, acc 1
2016-09-07T18:18:59.851363: step 2910, loss 0.164535, acc 0.916667
2016-09-07T18:19:00.577965: step 2911, loss 0.0360015, acc 0.98
2016-09-07T18:19:01.253337: step 2912, loss 0.0292588, acc 0.98
2016-09-07T18:19:01.925020: step 2913, loss 0.11752, acc 0.94
2016-09-07T18:19:02.619803: step 2914, loss 0.0286578, acc 0.98
2016-09-07T18:19:03.327408: step 2915, loss 0.0480192, acc 0.98
2016-09-07T18:19:04.001905: step 2916, loss 0.0536435, acc 0.98
2016-09-07T18:19:04.667774: step 2917, loss 0.0342586, acc 1
2016-09-07T18:19:05.361706: step 2918, loss 0.0563889, acc 0.94
2016-09-07T18:19:06.052038: step 2919, loss 0.0566401, acc 0.98
2016-09-07T18:19:06.748219: step 2920, loss 0.0199793, acc 1
2016-09-07T18:19:07.431855: step 2921, loss 0.0305141, acc 0.98
2016-09-07T18:19:08.147586: step 2922, loss 0.0249229, acc 0.98
2016-09-07T18:19:08.825913: step 2923, loss 0.0624719, acc 1
2016-09-07T18:19:09.507347: step 2924, loss 0.0363104, acc 0.98
2016-09-07T18:19:10.196280: step 2925, loss 0.0104408, acc 1
2016-09-07T18:19:10.898554: step 2926, loss 0.0201955, acc 1
2016-09-07T18:19:11.612616: step 2927, loss 0.0983719, acc 0.98
2016-09-07T18:19:12.289330: step 2928, loss 0.0271001, acc 0.98
2016-09-07T18:19:12.986852: step 2929, loss 0.0375373, acc 0.98
2016-09-07T18:19:13.671597: step 2930, loss 0.0528368, acc 0.96
2016-09-07T18:19:14.347644: step 2931, loss 0.0184663, acc 0.98
2016-09-07T18:19:15.042627: step 2932, loss 0.0219203, acc 0.98
2016-09-07T18:19:15.735817: step 2933, loss 0.0168082, acc 1
2016-09-07T18:19:16.421837: step 2934, loss 0.00762528, acc 1
2016-09-07T18:19:17.080872: step 2935, loss 0.0235874, acc 1
2016-09-07T18:19:17.807215: step 2936, loss 0.0388873, acc 0.98
2016-09-07T18:19:18.476239: step 2937, loss 0.012609, acc 1
2016-09-07T18:19:19.171039: step 2938, loss 0.0294063, acc 1
2016-09-07T18:19:19.859147: step 2939, loss 0.0289696, acc 0.98
2016-09-07T18:19:20.543344: step 2940, loss 0.0216202, acc 0.98
2016-09-07T18:19:21.237910: step 2941, loss 0.0265509, acc 0.98
2016-09-07T18:19:21.938741: step 2942, loss 0.0786121, acc 0.94
2016-09-07T18:19:22.636507: step 2943, loss 0.0353325, acc 0.98
2016-09-07T18:19:23.324727: step 2944, loss 0.0140588, acc 1
2016-09-07T18:19:24.014282: step 2945, loss 0.0197616, acc 0.98
2016-09-07T18:19:24.694162: step 2946, loss 0.0254211, acc 1
2016-09-07T18:19:25.378162: step 2947, loss 0.0227411, acc 0.98
2016-09-07T18:19:26.058779: step 2948, loss 0.0125555, acc 1
2016-09-07T18:19:26.728777: step 2949, loss 0.0224094, acc 0.98
2016-09-07T18:19:27.418958: step 2950, loss 0.000266883, acc 1
2016-09-07T18:19:28.077564: step 2951, loss 0.0883986, acc 0.96
2016-09-07T18:19:28.748058: step 2952, loss 0.00253601, acc 1
2016-09-07T18:19:29.412310: step 2953, loss 0.00474299, acc 1
2016-09-07T18:19:30.085196: step 2954, loss 0.0167001, acc 1
2016-09-07T18:19:30.754838: step 2955, loss 0.0558415, acc 0.96
2016-09-07T18:19:31.441876: step 2956, loss 0.0363854, acc 0.98
2016-09-07T18:19:32.152766: step 2957, loss 0.0050809, acc 1
2016-09-07T18:19:32.838965: step 2958, loss 0.0658892, acc 0.98
2016-09-07T18:19:33.535112: step 2959, loss 0.0643153, acc 0.96
2016-09-07T18:19:34.206524: step 2960, loss 0.00551716, acc 1
2016-09-07T18:19:34.875280: step 2961, loss 0.0255365, acc 1
2016-09-07T18:19:35.569346: step 2962, loss 0.000302867, acc 1
2016-09-07T18:19:36.261192: step 2963, loss 0.00248502, acc 1
2016-09-07T18:19:36.971112: step 2964, loss 0.0463548, acc 0.96
2016-09-07T18:19:37.645541: step 2965, loss 0.00816474, acc 1
2016-09-07T18:19:38.327335: step 2966, loss 0.0429036, acc 0.98
2016-09-07T18:19:39.018644: step 2967, loss 0.0202315, acc 0.98
2016-09-07T18:19:39.701246: step 2968, loss 0.00197157, acc 1
2016-09-07T18:19:40.377411: step 2969, loss 0.00275854, acc 1
2016-09-07T18:19:41.057945: step 2970, loss 0.0132387, acc 1
2016-09-07T18:19:41.749107: step 2971, loss 0.0447508, acc 0.98
2016-09-07T18:19:42.399548: step 2972, loss 0.0303301, acc 0.98
2016-09-07T18:19:43.077521: step 2973, loss 0.0240331, acc 0.98
2016-09-07T18:19:43.741700: step 2974, loss 0.0323679, acc 0.98
2016-09-07T18:19:44.413501: step 2975, loss 0.0615168, acc 0.98
2016-09-07T18:19:45.090827: step 2976, loss 0.176665, acc 0.96
2016-09-07T18:19:45.776656: step 2977, loss 0.00598929, acc 1
2016-09-07T18:19:46.473215: step 2978, loss 0.0143253, acc 1
2016-09-07T18:19:47.147447: step 2979, loss 0.0454776, acc 0.98
2016-09-07T18:19:47.861264: step 2980, loss 0.0571883, acc 0.98
2016-09-07T18:19:48.547156: step 2981, loss 0.0012446, acc 1
2016-09-07T18:19:49.232126: step 2982, loss 0.101344, acc 0.96
2016-09-07T18:19:49.927796: step 2983, loss 0.0314857, acc 0.98
2016-09-07T18:19:50.636591: step 2984, loss 0.0284953, acc 0.98
2016-09-07T18:19:51.335806: step 2985, loss 0.0283406, acc 0.98
2016-09-07T18:19:52.008367: step 2986, loss 0.0697267, acc 0.94
2016-09-07T18:19:52.710375: step 2987, loss 0.0332355, acc 0.98
2016-09-07T18:19:53.389073: step 2988, loss 0.000137553, acc 1
2016-09-07T18:19:54.081259: step 2989, loss 0.0764704, acc 0.98
2016-09-07T18:19:54.772580: step 2990, loss 0.0289557, acc 0.98
2016-09-07T18:19:55.463408: step 2991, loss 0.0136901, acc 1
2016-09-07T18:19:56.211173: step 2992, loss 0.0267638, acc 0.98
2016-09-07T18:19:56.911351: step 2993, loss 0.0147813, acc 1
2016-09-07T18:19:57.638168: step 2994, loss 0.0254192, acc 0.98
2016-09-07T18:19:58.316478: step 2995, loss 0.00859433, acc 1
2016-09-07T18:19:59.027375: step 2996, loss 0.0153812, acc 1
2016-09-07T18:19:59.717662: step 2997, loss 0.0326964, acc 0.98
2016-09-07T18:20:00.446104: step 2998, loss 0.0130363, acc 1
2016-09-07T18:20:01.148936: step 2999, loss 0.0129788, acc 1
2016-09-07T18:20:01.845801: step 3000, loss 0.0368986, acc 0.98

Evaluation:
2016-09-07T18:20:05.126427: step 3000, loss 1.87725, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-3000

2016-09-07T18:20:07.024440: step 3001, loss 0.0188664, acc 0.98
2016-09-07T18:20:07.707859: step 3002, loss 0.0343469, acc 0.96
2016-09-07T18:20:08.370227: step 3003, loss 0.00603196, acc 1
2016-09-07T18:20:09.044880: step 3004, loss 0.108182, acc 0.98
2016-09-07T18:20:09.731837: step 3005, loss 0.0278887, acc 0.98
2016-09-07T18:20:10.397892: step 3006, loss 0.0675282, acc 0.98
2016-09-07T18:20:11.086044: step 3007, loss 0.00647566, acc 1
2016-09-07T18:20:11.785300: step 3008, loss 0.0889465, acc 0.96
2016-09-07T18:20:12.450523: step 3009, loss 0.0539529, acc 0.98
2016-09-07T18:20:13.153870: step 3010, loss 0.0437876, acc 0.98
2016-09-07T18:20:13.849581: step 3011, loss 0.0188042, acc 1
2016-09-07T18:20:14.549655: step 3012, loss 0.106677, acc 0.96
2016-09-07T18:20:15.220820: step 3013, loss 0.00201768, acc 1
2016-09-07T18:20:15.882146: step 3014, loss 0.0372948, acc 0.98
2016-09-07T18:20:16.550570: step 3015, loss 0.0130379, acc 1
2016-09-07T18:20:17.214192: step 3016, loss 0.0345229, acc 0.98
2016-09-07T18:20:17.923666: step 3017, loss 0.0149572, acc 1
2016-09-07T18:20:18.603931: step 3018, loss 0.00754256, acc 1
2016-09-07T18:20:19.302255: step 3019, loss 0.200442, acc 0.94
2016-09-07T18:20:19.987403: step 3020, loss 0.0183548, acc 0.98
2016-09-07T18:20:20.669103: step 3021, loss 0.0543021, acc 0.98
2016-09-07T18:20:21.339452: step 3022, loss 0.0160547, acc 1
2016-09-07T18:20:21.985214: step 3023, loss 0.00442385, acc 1
2016-09-07T18:20:22.676730: step 3024, loss 0.139564, acc 0.96
2016-09-07T18:20:23.340360: step 3025, loss 0.0142484, acc 1
2016-09-07T18:20:24.028641: step 3026, loss 0.0900426, acc 0.98
2016-09-07T18:20:24.718002: step 3027, loss 0.0734502, acc 0.98
2016-09-07T18:20:25.389884: step 3028, loss 0.0541996, acc 0.96
2016-09-07T18:20:26.089815: step 3029, loss 0.00714604, acc 1
2016-09-07T18:20:26.776328: step 3030, loss 0.0157765, acc 1
2016-09-07T18:20:27.467317: step 3031, loss 0.0202453, acc 0.98
2016-09-07T18:20:28.164589: step 3032, loss 0.0266214, acc 0.98
2016-09-07T18:20:28.845008: step 3033, loss 0.00648679, acc 1
2016-09-07T18:20:29.539533: step 3034, loss 0.0189365, acc 0.98
2016-09-07T18:20:30.202467: step 3035, loss 0.0443778, acc 0.98
2016-09-07T18:20:30.883158: step 3036, loss 0.0450088, acc 0.98
2016-09-07T18:20:31.562283: step 3037, loss 0.0427209, acc 0.96
2016-09-07T18:20:32.227935: step 3038, loss 0.0117289, acc 1
2016-09-07T18:20:32.867413: step 3039, loss 0.00510383, acc 1
2016-09-07T18:20:33.572007: step 3040, loss 0.0714032, acc 0.96
2016-09-07T18:20:34.249264: step 3041, loss 0.108001, acc 0.98
2016-09-07T18:20:34.921031: step 3042, loss 0.0294944, acc 0.98
2016-09-07T18:20:35.585616: step 3043, loss 0.0690378, acc 0.98
2016-09-07T18:20:36.267939: step 3044, loss 0.0346832, acc 0.98
2016-09-07T18:20:36.943867: step 3045, loss 0.0546689, acc 0.96
2016-09-07T18:20:37.617562: step 3046, loss 0.0038053, acc 1
2016-09-07T18:20:38.333403: step 3047, loss 0.00776525, acc 1
2016-09-07T18:20:39.013939: step 3048, loss 0.00088263, acc 1
2016-09-07T18:20:39.701429: step 3049, loss 0.0707935, acc 0.98
2016-09-07T18:20:40.402917: step 3050, loss 0.0698281, acc 0.96
2016-09-07T18:20:41.100081: step 3051, loss 0.0309039, acc 1
2016-09-07T18:20:41.822051: step 3052, loss 0.0237536, acc 0.98
2016-09-07T18:20:42.487181: step 3053, loss 0.0295413, acc 1
2016-09-07T18:20:43.174521: step 3054, loss 0.0390029, acc 0.98
2016-09-07T18:20:43.872653: step 3055, loss 0.0155451, acc 0.98
2016-09-07T18:20:44.555689: step 3056, loss 0.00936229, acc 1
2016-09-07T18:20:45.227122: step 3057, loss 0.028296, acc 0.98
2016-09-07T18:20:45.902417: step 3058, loss 0.0297385, acc 1
2016-09-07T18:20:46.592510: step 3059, loss 0.0463417, acc 0.96
2016-09-07T18:20:47.238437: step 3060, loss 0.00406313, acc 1
2016-09-07T18:20:47.922902: step 3061, loss 0.0412048, acc 1
2016-09-07T18:20:48.602844: step 3062, loss 0.0538685, acc 0.98
2016-09-07T18:20:49.280766: step 3063, loss 0.128925, acc 0.96
2016-09-07T18:20:49.968930: step 3064, loss 0.0107248, acc 1
2016-09-07T18:20:50.657790: step 3065, loss 0.0409359, acc 0.98
2016-09-07T18:20:51.344548: step 3066, loss 0.0247108, acc 1
2016-09-07T18:20:52.037200: step 3067, loss 0.0103372, acc 1
2016-09-07T18:20:52.735579: step 3068, loss 0.0569891, acc 0.98
2016-09-07T18:20:53.419193: step 3069, loss 0.0600789, acc 0.98
2016-09-07T18:20:54.114331: step 3070, loss 0.0334998, acc 0.98
2016-09-07T18:20:54.805526: step 3071, loss 0.0374743, acc 0.98
2016-09-07T18:20:55.492591: step 3072, loss 0.0364304, acc 0.96
2016-09-07T18:20:56.186816: step 3073, loss 0.00268459, acc 1
2016-09-07T18:20:56.856231: step 3074, loss 0.0276648, acc 1
2016-09-07T18:20:57.569157: step 3075, loss 0.0282262, acc 1
2016-09-07T18:20:58.261469: step 3076, loss 0.0632551, acc 0.96
2016-09-07T18:20:58.956194: step 3077, loss 0.00587952, acc 1
2016-09-07T18:20:59.643398: step 3078, loss 0.0591909, acc 0.98
2016-09-07T18:21:00.359527: step 3079, loss 0.0407589, acc 0.98
2016-09-07T18:21:01.044694: step 3080, loss 0.0953598, acc 0.94
2016-09-07T18:21:01.729238: step 3081, loss 0.0346423, acc 0.98
2016-09-07T18:21:02.443379: step 3082, loss 0.0309609, acc 0.98
2016-09-07T18:21:03.129502: step 3083, loss 0.0137662, acc 1
2016-09-07T18:21:03.831113: step 3084, loss 0.0220042, acc 0.98
2016-09-07T18:21:04.546659: step 3085, loss 0.0243137, acc 1
2016-09-07T18:21:05.220141: step 3086, loss 0.000775792, acc 1
2016-09-07T18:21:05.928146: step 3087, loss 0.0083092, acc 1
2016-09-07T18:21:06.587669: step 3088, loss 0.0849783, acc 0.96
2016-09-07T18:21:07.301758: step 3089, loss 0.0463793, acc 0.98
2016-09-07T18:21:07.992298: step 3090, loss 0.0154541, acc 1
2016-09-07T18:21:08.656816: step 3091, loss 0.0101763, acc 1
2016-09-07T18:21:09.351695: step 3092, loss 0.0387088, acc 0.96
2016-09-07T18:21:10.046677: step 3093, loss 0.0235657, acc 0.98
2016-09-07T18:21:10.742153: step 3094, loss 0.0169694, acc 1
2016-09-07T18:21:11.398265: step 3095, loss 0.0098491, acc 1
2016-09-07T18:21:12.106964: step 3096, loss 0.0220675, acc 0.98
2016-09-07T18:21:12.796222: step 3097, loss 0.026035, acc 1
2016-09-07T18:21:13.472909: step 3098, loss 0.0683539, acc 0.98
2016-09-07T18:21:14.167519: step 3099, loss 0.0356584, acc 0.98
2016-09-07T18:21:14.875087: step 3100, loss 0.0391928, acc 0.98

Evaluation:
2016-09-07T18:21:18.204603: step 3100, loss 2.04187, acc 0.733

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-3100

2016-09-07T18:21:19.931617: step 3101, loss 0.16886, acc 0.92
2016-09-07T18:21:20.622351: step 3102, loss 0.0131786, acc 1
2016-09-07T18:21:21.295616: step 3103, loss 0.000126292, acc 1
2016-09-07T18:21:21.672150: step 3104, loss 4.98175e-05, acc 1
2016-09-07T18:21:22.360213: step 3105, loss 0.0457133, acc 0.98
2016-09-07T18:21:23.036694: step 3106, loss 0.00449511, acc 1
2016-09-07T18:21:23.742447: step 3107, loss 0.0557393, acc 0.98
2016-09-07T18:21:24.436580: step 3108, loss 0.0630061, acc 0.98
2016-09-07T18:21:25.136001: step 3109, loss 0.0662543, acc 0.98
2016-09-07T18:21:25.803314: step 3110, loss 0.0157028, acc 1
2016-09-07T18:21:26.496692: step 3111, loss 0.0344844, acc 0.98
2016-09-07T18:21:27.200085: step 3112, loss 0.135993, acc 0.96
2016-09-07T18:21:27.876846: step 3113, loss 0.0357912, acc 0.98
2016-09-07T18:21:28.562321: step 3114, loss 0.0466598, acc 0.98
2016-09-07T18:21:29.249077: step 3115, loss 0.0398581, acc 0.98
2016-09-07T18:21:29.942822: step 3116, loss 0.00964446, acc 1
2016-09-07T18:21:30.636694: step 3117, loss 0.114169, acc 0.96
2016-09-07T18:21:31.327466: step 3118, loss 0.144128, acc 0.94
2016-09-07T18:21:32.012966: step 3119, loss 0.0611617, acc 0.96
2016-09-07T18:21:32.706516: step 3120, loss 0.0700845, acc 0.96
2016-09-07T18:21:33.381101: step 3121, loss 0.0718988, acc 0.98
2016-09-07T18:21:34.056064: step 3122, loss 0.0362399, acc 0.98
2016-09-07T18:21:34.745311: step 3123, loss 0.089872, acc 0.96
2016-09-07T18:21:35.408078: step 3124, loss 0.0182103, acc 1
2016-09-07T18:21:36.098148: step 3125, loss 0.0348901, acc 0.98
2016-09-07T18:21:36.794592: step 3126, loss 0.0689479, acc 0.94
2016-09-07T18:21:37.480401: step 3127, loss 0.0144241, acc 1
2016-09-07T18:21:38.177819: step 3128, loss 0.00762424, acc 1
2016-09-07T18:21:38.860856: step 3129, loss 0.111506, acc 0.98
2016-09-07T18:21:39.546571: step 3130, loss 0.00264057, acc 1
2016-09-07T18:21:40.215468: step 3131, loss 0.0367482, acc 0.98
2016-09-07T18:21:40.909622: step 3132, loss 0.0145189, acc 1
2016-09-07T18:21:41.599205: step 3133, loss 0.0118405, acc 1
2016-09-07T18:21:42.296196: step 3134, loss 0.0291709, acc 0.98
2016-09-07T18:21:42.970812: step 3135, loss 0.0246549, acc 1
2016-09-07T18:21:43.682324: step 3136, loss 0.105205, acc 0.96
2016-09-07T18:21:44.386280: step 3137, loss 0.0531663, acc 0.96
2016-09-07T18:21:45.068111: step 3138, loss 0.03692, acc 0.98
2016-09-07T18:21:45.773768: step 3139, loss 0.0126258, acc 1
2016-09-07T18:21:46.489060: step 3140, loss 0.146753, acc 0.96
2016-09-07T18:21:47.153899: step 3141, loss 0.104287, acc 0.96
2016-09-07T18:21:47.834099: step 3142, loss 0.0679301, acc 0.98
2016-09-07T18:21:48.536975: step 3143, loss 0.00621932, acc 1
2016-09-07T18:21:49.245019: step 3144, loss 0.000112213, acc 1
2016-09-07T18:21:49.924907: step 3145, loss 0.0689507, acc 0.96
2016-09-07T18:21:50.617417: step 3146, loss 0.0232894, acc 1
2016-09-07T18:21:51.305238: step 3147, loss 0.000828852, acc 1
2016-09-07T18:21:52.011738: step 3148, loss 0.0552163, acc 0.96
2016-09-07T18:21:52.681841: step 3149, loss 0.0974055, acc 0.98
2016-09-07T18:21:53.341956: step 3150, loss 0.108424, acc 0.96
2016-09-07T18:21:54.039933: step 3151, loss 0.0797651, acc 0.94
2016-09-07T18:21:54.699183: step 3152, loss 0.0567229, acc 0.96
2016-09-07T18:21:55.382764: step 3153, loss 0.00978476, acc 1
2016-09-07T18:21:56.067619: step 3154, loss 0.0339352, acc 0.98
2016-09-07T18:21:56.777315: step 3155, loss 0.0121535, acc 1
2016-09-07T18:21:57.471126: step 3156, loss 0.0183353, acc 0.98
2016-09-07T18:21:58.161401: step 3157, loss 0.0102703, acc 1
2016-09-07T18:21:58.876592: step 3158, loss 0.0116437, acc 1
2016-09-07T18:21:59.552079: step 3159, loss 0.0956985, acc 0.94
2016-09-07T18:22:00.268997: step 3160, loss 0.0686976, acc 0.98
2016-09-07T18:22:00.969653: step 3161, loss 0.0210578, acc 0.98
2016-09-07T18:22:01.640038: step 3162, loss 0.0745786, acc 0.98
2016-09-07T18:22:02.348908: step 3163, loss 0.0152284, acc 1
2016-09-07T18:22:03.020370: step 3164, loss 0.0473698, acc 0.98
2016-09-07T18:22:03.725219: step 3165, loss 0.0211307, acc 1
2016-09-07T18:22:04.433343: step 3166, loss 0.00213877, acc 1
2016-09-07T18:22:05.144942: step 3167, loss 0.0513437, acc 1
2016-09-07T18:22:05.828084: step 3168, loss 0.0188551, acc 1
2016-09-07T18:22:06.504150: step 3169, loss 0.0605518, acc 0.96
2016-09-07T18:22:07.196032: step 3170, loss 0.0634719, acc 0.98
2016-09-07T18:22:07.874170: step 3171, loss 0.00537912, acc 1
2016-09-07T18:22:08.593731: step 3172, loss 0.0156968, acc 1
2016-09-07T18:22:09.300390: step 3173, loss 0.0904443, acc 0.96
2016-09-07T18:22:10.004453: step 3174, loss 0.0452214, acc 0.98
2016-09-07T18:22:10.693811: step 3175, loss 0.00222165, acc 1
2016-09-07T18:22:11.384749: step 3176, loss 0.0271223, acc 0.98
2016-09-07T18:22:12.094411: step 3177, loss 0.00293088, acc 1
2016-09-07T18:22:12.782782: step 3178, loss 0.00105988, acc 1
2016-09-07T18:22:13.477956: step 3179, loss 0.0549546, acc 0.98
2016-09-07T18:22:14.165837: step 3180, loss 0.0340276, acc 0.98
2016-09-07T18:22:14.851297: step 3181, loss 0.0257064, acc 1
2016-09-07T18:22:15.586963: step 3182, loss 0.0137222, acc 1
2016-09-07T18:22:16.263524: step 3183, loss 0.0258141, acc 0.98
2016-09-07T18:22:16.952967: step 3184, loss 0.00633103, acc 1
2016-09-07T18:22:17.624117: step 3185, loss 0.0161776, acc 0.98
2016-09-07T18:22:18.296784: step 3186, loss 0.105816, acc 0.94
2016-09-07T18:22:18.979921: step 3187, loss 0.0922736, acc 0.96
2016-09-07T18:22:19.686169: step 3188, loss 0.0210543, acc 1
2016-09-07T18:22:20.384128: step 3189, loss 0.030042, acc 0.98
2016-09-07T18:22:21.054664: step 3190, loss 0.0437865, acc 1
2016-09-07T18:22:21.765459: step 3191, loss 0.0217502, acc 0.98
2016-09-07T18:22:22.460077: step 3192, loss 0.0462954, acc 0.98
2016-09-07T18:22:23.156185: step 3193, loss 0.00669285, acc 1
2016-09-07T18:22:23.850480: step 3194, loss 0.0361975, acc 0.98
2016-09-07T18:22:24.529861: step 3195, loss 0.00518914, acc 1
2016-09-07T18:22:25.215440: step 3196, loss 0.190897, acc 0.94
2016-09-07T18:22:25.894699: step 3197, loss 0.00715635, acc 1
2016-09-07T18:22:26.602446: step 3198, loss 0.0055969, acc 1
2016-09-07T18:22:27.287145: step 3199, loss 0.0211241, acc 1
2016-09-07T18:22:27.959889: step 3200, loss 0.105196, acc 0.96

Evaluation:
2016-09-07T18:22:31.295439: step 3200, loss 1.92393, acc 0.748

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-3200

2016-09-07T18:22:33.124225: step 3201, loss 0.0145872, acc 1
2016-09-07T18:22:33.839632: step 3202, loss 0.0836793, acc 0.94
2016-09-07T18:22:34.546236: step 3203, loss 0.0161994, acc 1
2016-09-07T18:22:35.244397: step 3204, loss 0.0328298, acc 0.98
2016-09-07T18:22:35.949680: step 3205, loss 0.0268101, acc 0.98
2016-09-07T18:22:36.655675: step 3206, loss 0.0146618, acc 1
2016-09-07T18:22:37.352173: step 3207, loss 0.0900732, acc 0.98
2016-09-07T18:22:38.016823: step 3208, loss 0.0112986, acc 1
2016-09-07T18:22:38.711710: step 3209, loss 0.019547, acc 1
2016-09-07T18:22:39.395336: step 3210, loss 0.00843887, acc 1
2016-09-07T18:22:40.084548: step 3211, loss 0.0440218, acc 0.96
2016-09-07T18:22:40.797148: step 3212, loss 0.052778, acc 0.98
2016-09-07T18:22:41.473849: step 3213, loss 0.0305435, acc 1
2016-09-07T18:22:42.182459: step 3214, loss 0.0222412, acc 1
2016-09-07T18:22:42.850356: step 3215, loss 0.0816741, acc 0.98
2016-09-07T18:22:43.532536: step 3216, loss 0.0140506, acc 1
2016-09-07T18:22:44.235937: step 3217, loss 0.0353867, acc 1
2016-09-07T18:22:44.945562: step 3218, loss 0.0256398, acc 0.98
2016-09-07T18:22:45.640360: step 3219, loss 0.0625538, acc 0.96
2016-09-07T18:22:46.303493: step 3220, loss 0.0285812, acc 1
2016-09-07T18:22:47.015878: step 3221, loss 0.0721791, acc 0.96
2016-09-07T18:22:47.708450: step 3222, loss 0.0209562, acc 1
2016-09-07T18:22:48.394950: step 3223, loss 0.0703841, acc 0.98
2016-09-07T18:22:49.087877: step 3224, loss 0.0450053, acc 0.98
2016-09-07T18:22:49.769568: step 3225, loss 0.040857, acc 0.96
2016-09-07T18:22:50.455406: step 3226, loss 0.0163722, acc 1
2016-09-07T18:22:51.130132: step 3227, loss 0.0239197, acc 0.98
2016-09-07T18:22:51.822161: step 3228, loss 0.0321927, acc 0.98
2016-09-07T18:22:52.500176: step 3229, loss 0.171813, acc 0.98
2016-09-07T18:22:53.187358: step 3230, loss 0.102652, acc 0.98
2016-09-07T18:22:53.859725: step 3231, loss 0.0449505, acc 0.98
2016-09-07T18:22:54.559026: step 3232, loss 0.0123107, acc 1
2016-09-07T18:22:55.236263: step 3233, loss 0.016312, acc 1
2016-09-07T18:22:55.903519: step 3234, loss 0.0441232, acc 0.98
2016-09-07T18:22:56.896470: step 3235, loss 0.0453457, acc 0.98
2016-09-07T18:22:57.848475: step 3236, loss 0.016988, acc 1
2016-09-07T18:22:58.781753: step 3237, loss 0.0019723, acc 1
2016-09-07T18:22:59.697400: step 3238, loss 0.0361242, acc 0.98
2016-09-07T18:23:00.680896: step 3239, loss 0.0238479, acc 0.98
2016-09-07T18:23:01.633157: step 3240, loss 0.084978, acc 0.98
2016-09-07T18:23:02.630859: step 3241, loss 0.0474696, acc 0.98
2016-09-07T18:23:03.606095: step 3242, loss 0.0010156, acc 1
2016-09-07T18:23:04.583237: step 3243, loss 0.0206091, acc 0.98
2016-09-07T18:23:05.551107: step 3244, loss 0.0541079, acc 0.98
2016-09-07T18:23:06.420721: step 3245, loss 0.0182065, acc 1
2016-09-07T18:23:07.082624: step 3246, loss 0.027911, acc 0.98
2016-09-07T18:23:07.742913: step 3247, loss 0.0195498, acc 1
2016-09-07T18:23:08.402416: step 3248, loss 0.0534935, acc 0.98
2016-09-07T18:23:09.082738: step 3249, loss 0.0395632, acc 0.98
2016-09-07T18:23:09.754096: step 3250, loss 0.0076799, acc 1
2016-09-07T18:23:10.426768: step 3251, loss 0.0595054, acc 0.98
2016-09-07T18:23:11.108904: step 3252, loss 0.0920174, acc 0.98
2016-09-07T18:23:11.774680: step 3253, loss 0.102256, acc 0.98
2016-09-07T18:23:12.444747: step 3254, loss 0.00883595, acc 1
2016-09-07T18:23:13.101239: step 3255, loss 0.045815, acc 1
2016-09-07T18:23:13.788130: step 3256, loss 0.158978, acc 0.96
2016-09-07T18:23:14.446257: step 3257, loss 0.0677921, acc 0.94
2016-09-07T18:23:15.109632: step 3258, loss 0.089104, acc 0.94
2016-09-07T18:23:15.777352: step 3259, loss 0.0676457, acc 0.98
2016-09-07T18:23:16.451158: step 3260, loss 0.0283377, acc 0.98
2016-09-07T18:23:17.114968: step 3261, loss 0.0824388, acc 0.98
2016-09-07T18:23:17.775453: step 3262, loss 0.0289253, acc 1
2016-09-07T18:23:18.430751: step 3263, loss 0.0190106, acc 0.98
2016-09-07T18:23:19.111198: step 3264, loss 0.0362498, acc 1
2016-09-07T18:23:19.797633: step 3265, loss 0.0194421, acc 1
2016-09-07T18:23:20.506415: step 3266, loss 0.0273115, acc 0.98
2016-09-07T18:23:21.188681: step 3267, loss 0.0409131, acc 0.98
2016-09-07T18:23:21.868243: step 3268, loss 0.00588536, acc 1
2016-09-07T18:23:22.527829: step 3269, loss 0.0124872, acc 1
2016-09-07T18:23:23.198094: step 3270, loss 0.0283082, acc 0.98
2016-09-07T18:23:23.871386: step 3271, loss 0.00584949, acc 1
2016-09-07T18:23:24.528694: step 3272, loss 0.0232135, acc 1
2016-09-07T18:23:25.203243: step 3273, loss 0.0203074, acc 0.98
2016-09-07T18:23:25.886031: step 3274, loss 0.0394473, acc 1
2016-09-07T18:23:26.553615: step 3275, loss 0.0138124, acc 1
2016-09-07T18:23:27.234660: step 3276, loss 0.0515689, acc 0.96
2016-09-07T18:23:27.897613: step 3277, loss 0.0129798, acc 1
2016-09-07T18:23:28.567600: step 3278, loss 0.0107296, acc 1
2016-09-07T18:23:29.212247: step 3279, loss 0.034385, acc 1
2016-09-07T18:23:29.887943: step 3280, loss 0.0943208, acc 0.96
2016-09-07T18:23:30.554818: step 3281, loss 0.00210143, acc 1
2016-09-07T18:23:31.217846: step 3282, loss 0.0236879, acc 0.98
2016-09-07T18:23:31.891830: step 3283, loss 0.019767, acc 1
2016-09-07T18:23:32.566901: step 3284, loss 0.0687438, acc 0.94
2016-09-07T18:23:33.248615: step 3285, loss 0.0746173, acc 0.96
2016-09-07T18:23:33.916578: step 3286, loss 0.0810233, acc 0.96
2016-09-07T18:23:34.596406: step 3287, loss 0.105903, acc 0.96
2016-09-07T18:23:35.275204: step 3288, loss 0.00686391, acc 1
2016-09-07T18:23:35.944317: step 3289, loss 0.00119989, acc 1
2016-09-07T18:23:36.609815: step 3290, loss 0.0513978, acc 0.98
2016-09-07T18:23:37.278511: step 3291, loss 0.0576595, acc 0.96
2016-09-07T18:23:37.951323: step 3292, loss 0.0473611, acc 0.96
2016-09-07T18:23:38.621740: step 3293, loss 0.0535501, acc 0.98
2016-09-07T18:23:39.297531: step 3294, loss 0.0273103, acc 0.98
2016-09-07T18:23:39.972999: step 3295, loss 0.0394644, acc 0.98
2016-09-07T18:23:40.646506: step 3296, loss 0.0479253, acc 0.96
2016-09-07T18:23:41.315872: step 3297, loss 0.0147569, acc 1
2016-09-07T18:23:41.686167: step 3298, loss 0.0472056, acc 1
2016-09-07T18:23:42.350149: step 3299, loss 0.0695573, acc 0.94
2016-09-07T18:23:43.033646: step 3300, loss 0.0263514, acc 0.98

Evaluation:
2016-09-07T18:23:46.322497: step 3300, loss 1.52487, acc 0.744

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-3300

2016-09-07T18:23:48.126672: step 3301, loss 0.0289916, acc 0.98
2016-09-07T18:23:48.796411: step 3302, loss 0.0268957, acc 1
2016-09-07T18:23:49.470994: step 3303, loss 0.0360438, acc 0.98
2016-09-07T18:23:50.141853: step 3304, loss 0.0405984, acc 0.96
2016-09-07T18:23:50.815672: step 3305, loss 0.131809, acc 0.96
2016-09-07T18:23:51.500104: step 3306, loss 0.0597851, acc 0.96
2016-09-07T18:23:52.217449: step 3307, loss 0.00391926, acc 1
2016-09-07T18:23:52.880031: step 3308, loss 0.0230518, acc 1
2016-09-07T18:23:53.548367: step 3309, loss 0.0018321, acc 1
2016-09-07T18:23:54.231303: step 3310, loss 0.0525155, acc 0.96
2016-09-07T18:23:54.892645: step 3311, loss 0.109347, acc 0.94
2016-09-07T18:23:55.567274: step 3312, loss 0.0678752, acc 0.96
2016-09-07T18:23:56.224880: step 3313, loss 0.027238, acc 1
2016-09-07T18:23:56.915609: step 3314, loss 0.015577, acc 1
2016-09-07T18:23:57.589058: step 3315, loss 0.0344337, acc 0.98
2016-09-07T18:23:58.254577: step 3316, loss 0.0675445, acc 0.98
2016-09-07T18:23:58.920335: step 3317, loss 0.0321998, acc 0.98
2016-09-07T18:23:59.603104: step 3318, loss 0.0523233, acc 0.96
2016-09-07T18:24:00.305776: step 3319, loss 0.0134558, acc 1
2016-09-07T18:24:00.975072: step 3320, loss 0.080711, acc 0.94
2016-09-07T18:24:01.626857: step 3321, loss 0.0612389, acc 0.98
2016-09-07T18:24:02.291045: step 3322, loss 0.0140572, acc 1
2016-09-07T18:24:02.968647: step 3323, loss 0.0278526, acc 1
2016-09-07T18:24:03.631447: step 3324, loss 0.0285018, acc 0.98
2016-09-07T18:24:04.323615: step 3325, loss 0.00253072, acc 1
2016-09-07T18:24:05.004052: step 3326, loss 0.0135797, acc 1
2016-09-07T18:24:05.683325: step 3327, loss 0.0199801, acc 0.98
2016-09-07T18:24:06.375362: step 3328, loss 0.022539, acc 1
2016-09-07T18:24:07.043667: step 3329, loss 0.0131738, acc 1
2016-09-07T18:24:07.717327: step 3330, loss 0.0754396, acc 0.96
2016-09-07T18:24:08.399306: step 3331, loss 0.0103618, acc 1
2016-09-07T18:24:09.065982: step 3332, loss 0.0223984, acc 0.98
2016-09-07T18:24:09.741611: step 3333, loss 0.0393908, acc 0.98
2016-09-07T18:24:10.412237: step 3334, loss 0.076476, acc 0.94
2016-09-07T18:24:11.086366: step 3335, loss 0.00727533, acc 1
2016-09-07T18:24:11.754277: step 3336, loss 0.127754, acc 0.98
2016-09-07T18:24:12.431536: step 3337, loss 0.00262845, acc 1
2016-09-07T18:24:13.119678: step 3338, loss 0.000563869, acc 1
2016-09-07T18:24:13.803804: step 3339, loss 0.0330286, acc 1
2016-09-07T18:24:14.509624: step 3340, loss 0.0444134, acc 0.98
2016-09-07T18:24:15.197776: step 3341, loss 0.00175226, acc 1
2016-09-07T18:24:15.870096: step 3342, loss 0.000327255, acc 1
2016-09-07T18:24:16.544166: step 3343, loss 0.0982953, acc 0.96
2016-09-07T18:24:17.212508: step 3344, loss 0.0305476, acc 0.98
2016-09-07T18:24:17.904126: step 3345, loss 0.00556028, acc 1
2016-09-07T18:24:18.579188: step 3346, loss 0.0744447, acc 0.94
2016-09-07T18:24:19.244984: step 3347, loss 0.015049, acc 0.98
2016-09-07T18:24:19.910826: step 3348, loss 0.0286614, acc 0.98
2016-09-07T18:24:20.586977: step 3349, loss 0.0171519, acc 1
2016-09-07T18:24:21.242408: step 3350, loss 0.0215508, acc 0.98
2016-09-07T18:24:21.900261: step 3351, loss 0.0284825, acc 1
2016-09-07T18:24:22.577291: step 3352, loss 0.00703699, acc 1
2016-09-07T18:24:23.233328: step 3353, loss 0.033756, acc 0.98
2016-09-07T18:24:23.922589: step 3354, loss 0.0928366, acc 0.96
2016-09-07T18:24:24.584033: step 3355, loss 0.0299716, acc 0.98
2016-09-07T18:24:25.268059: step 3356, loss 0.0416143, acc 0.98
2016-09-07T18:24:25.933527: step 3357, loss 0.017353, acc 1
2016-09-07T18:24:26.582254: step 3358, loss 0.00735237, acc 1
2016-09-07T18:24:27.266480: step 3359, loss 0.0303697, acc 0.98
2016-09-07T18:24:27.935975: step 3360, loss 0.0117782, acc 1
2016-09-07T18:24:28.614025: step 3361, loss 0.03262, acc 0.98
2016-09-07T18:24:29.262696: step 3362, loss 0.0779397, acc 0.96
2016-09-07T18:24:29.933373: step 3363, loss 0.098024, acc 0.94
2016-09-07T18:24:30.598547: step 3364, loss 0.0612871, acc 0.94
2016-09-07T18:24:31.273726: step 3365, loss 0.0126189, acc 1
2016-09-07T18:24:31.943362: step 3366, loss 0.00148696, acc 1
2016-09-07T18:24:32.614192: step 3367, loss 0.0128486, acc 1
2016-09-07T18:24:33.292638: step 3368, loss 0.0298945, acc 1
2016-09-07T18:24:33.970133: step 3369, loss 0.00143335, acc 1
2016-09-07T18:24:34.654372: step 3370, loss 0.0247073, acc 1
2016-09-07T18:24:35.335487: step 3371, loss 0.00212727, acc 1
2016-09-07T18:24:35.998556: step 3372, loss 0.0181534, acc 1
2016-09-07T18:24:36.725534: step 3373, loss 0.0262987, acc 0.98
2016-09-07T18:24:37.369162: step 3374, loss 0.0423138, acc 0.98
2016-09-07T18:24:38.038403: step 3375, loss 0.0007909, acc 1
2016-09-07T18:24:38.701251: step 3376, loss 0.0269596, acc 1
2016-09-07T18:24:39.371391: step 3377, loss 0.0488247, acc 0.98
2016-09-07T18:24:40.061976: step 3378, loss 0.00988818, acc 1
2016-09-07T18:24:40.731115: step 3379, loss 0.046162, acc 0.98
2016-09-07T18:24:41.407122: step 3380, loss 0.0627959, acc 0.98
2016-09-07T18:24:42.084191: step 3381, loss 0.0126669, acc 1
2016-09-07T18:24:42.770923: step 3382, loss 0.00455776, acc 1
2016-09-07T18:24:43.446294: step 3383, loss 0.00102588, acc 1
2016-09-07T18:24:44.127676: step 3384, loss 0.00542602, acc 1
2016-09-07T18:24:44.798477: step 3385, loss 0.0644227, acc 0.96
2016-09-07T18:24:45.450534: step 3386, loss 0.0527371, acc 0.96
2016-09-07T18:24:46.131212: step 3387, loss 0.0372431, acc 0.98
2016-09-07T18:24:46.806433: step 3388, loss 0.014187, acc 1
2016-09-07T18:24:47.477082: step 3389, loss 0.0321313, acc 0.98
2016-09-07T18:24:48.152051: step 3390, loss 0.0480753, acc 0.98
2016-09-07T18:24:48.829487: step 3391, loss 0.0642472, acc 0.96
2016-09-07T18:24:49.487261: step 3392, loss 0.0013413, acc 1
2016-09-07T18:24:50.156150: step 3393, loss 0.0417846, acc 0.96
2016-09-07T18:24:50.826231: step 3394, loss 0.014131, acc 1
2016-09-07T18:24:51.507234: step 3395, loss 0.0165462, acc 1
2016-09-07T18:24:52.170054: step 3396, loss 0.0783341, acc 0.96
2016-09-07T18:24:52.851080: step 3397, loss 0.01521, acc 1
2016-09-07T18:24:53.539469: step 3398, loss 0.0199729, acc 1
2016-09-07T18:24:54.208494: step 3399, loss 0.0293322, acc 0.98
2016-09-07T18:24:54.889379: step 3400, loss 0.00637149, acc 1

Evaluation:
2016-09-07T18:24:58.205261: step 3400, loss 2.06948, acc 0.749

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-3400

2016-09-07T18:24:59.877749: step 3401, loss 0.0268993, acc 0.98
2016-09-07T18:25:00.578639: step 3402, loss 0.0303783, acc 0.98
2016-09-07T18:25:01.259525: step 3403, loss 0.0118327, acc 1
2016-09-07T18:25:01.950371: step 3404, loss 0.0169537, acc 0.98
2016-09-07T18:25:02.623636: step 3405, loss 0.189399, acc 0.96
2016-09-07T18:25:03.297782: step 3406, loss 0.017941, acc 1
2016-09-07T18:25:03.976883: step 3407, loss 0.0272761, acc 0.98
2016-09-07T18:25:04.635558: step 3408, loss 0.00184008, acc 1
2016-09-07T18:25:05.297132: step 3409, loss 0.0156008, acc 1
2016-09-07T18:25:05.982346: step 3410, loss 0.0945922, acc 0.96
2016-09-07T18:25:06.654040: step 3411, loss 0.0357417, acc 0.96
2016-09-07T18:25:07.319583: step 3412, loss 0.20778, acc 0.94
2016-09-07T18:25:07.977299: step 3413, loss 0.0196604, acc 1
2016-09-07T18:25:08.648539: step 3414, loss 0.0560345, acc 1
2016-09-07T18:25:09.334549: step 3415, loss 0.00642683, acc 1
2016-09-07T18:25:10.011269: step 3416, loss 0.00114429, acc 1
2016-09-07T18:25:10.666298: step 3417, loss 0.00248454, acc 1
2016-09-07T18:25:11.322006: step 3418, loss 0.0115583, acc 1
2016-09-07T18:25:12.004268: step 3419, loss 0.00362394, acc 1
2016-09-07T18:25:12.672853: step 3420, loss 0.0398907, acc 0.96
2016-09-07T18:25:13.334234: step 3421, loss 0.0107027, acc 1
2016-09-07T18:25:14.005467: step 3422, loss 0.0068441, acc 1
2016-09-07T18:25:14.660254: step 3423, loss 0.135661, acc 0.96
2016-09-07T18:25:15.326279: step 3424, loss 0.0589407, acc 0.96
2016-09-07T18:25:16.014521: step 3425, loss 0.0926009, acc 0.98
2016-09-07T18:25:16.692236: step 3426, loss 0.0132888, acc 1
2016-09-07T18:25:17.360763: step 3427, loss 0.201031, acc 0.98
2016-09-07T18:25:18.019760: step 3428, loss 0.117989, acc 0.98
2016-09-07T18:25:18.691268: step 3429, loss 0.0251355, acc 0.98
2016-09-07T18:25:19.368195: step 3430, loss 0.0309301, acc 0.98
2016-09-07T18:25:20.015397: step 3431, loss 0.0483362, acc 0.98
2016-09-07T18:25:20.699165: step 3432, loss 0.0199216, acc 1
2016-09-07T18:25:21.359390: step 3433, loss 0.0264804, acc 1
2016-09-07T18:25:22.025437: step 3434, loss 0.0145354, acc 1
2016-09-07T18:25:22.701682: step 3435, loss 0.0304686, acc 0.98
2016-09-07T18:25:23.426875: step 3436, loss 0.0013906, acc 1
2016-09-07T18:25:24.105524: step 3437, loss 0.00712342, acc 1
2016-09-07T18:25:24.783559: step 3438, loss 0.0605074, acc 0.96
2016-09-07T18:25:25.453310: step 3439, loss 0.0881317, acc 0.96
2016-09-07T18:25:26.117070: step 3440, loss 0.0234242, acc 0.98
2016-09-07T18:25:26.796046: step 3441, loss 0.0324512, acc 0.98
2016-09-07T18:25:27.467941: step 3442, loss 0.0296947, acc 0.98
2016-09-07T18:25:28.147749: step 3443, loss 0.037909, acc 1
2016-09-07T18:25:28.813890: step 3444, loss 0.0290881, acc 0.98
2016-09-07T18:25:29.476284: step 3445, loss 0.0763065, acc 0.96
2016-09-07T18:25:30.150501: step 3446, loss 0.0606421, acc 0.98
2016-09-07T18:25:30.841731: step 3447, loss 0.0356284, acc 0.98
2016-09-07T18:25:31.518439: step 3448, loss 0.000909023, acc 1
2016-09-07T18:25:32.197368: step 3449, loss 0.037217, acc 0.98
2016-09-07T18:25:32.864069: step 3450, loss 0.039085, acc 0.98
2016-09-07T18:25:33.541035: step 3451, loss 0.035047, acc 0.98
2016-09-07T18:25:34.225474: step 3452, loss 0.0662582, acc 0.98
2016-09-07T18:25:34.915235: step 3453, loss 0.0377716, acc 0.98
2016-09-07T18:25:35.584791: step 3454, loss 0.030824, acc 0.98
2016-09-07T18:25:36.243220: step 3455, loss 0.0128728, acc 1
2016-09-07T18:25:36.909324: step 3456, loss 0.00873885, acc 1
2016-09-07T18:25:37.585054: step 3457, loss 0.0060758, acc 1
2016-09-07T18:25:38.249213: step 3458, loss 0.0353485, acc 0.98
2016-09-07T18:25:38.909536: step 3459, loss 0.103825, acc 0.96
2016-09-07T18:25:39.583232: step 3460, loss 0.0442585, acc 0.98
2016-09-07T18:25:40.253650: step 3461, loss 0.0525057, acc 0.98
2016-09-07T18:25:40.927012: step 3462, loss 0.00691654, acc 1
2016-09-07T18:25:41.585119: step 3463, loss 0.0281404, acc 0.98
2016-09-07T18:25:42.240648: step 3464, loss 0.03512, acc 1
2016-09-07T18:25:42.910498: step 3465, loss 0.00340173, acc 1
2016-09-07T18:25:43.575860: step 3466, loss 0.0145848, acc 0.98
2016-09-07T18:25:44.250666: step 3467, loss 0.0736976, acc 0.96
2016-09-07T18:25:44.935342: step 3468, loss 0.0514056, acc 0.96
2016-09-07T18:25:45.616252: step 3469, loss 0.043265, acc 0.98
2016-09-07T18:25:46.299826: step 3470, loss 0.0753669, acc 0.98
2016-09-07T18:25:46.967940: step 3471, loss 0.0311622, acc 0.98
2016-09-07T18:25:47.645777: step 3472, loss 0.0164092, acc 0.98
2016-09-07T18:25:48.305468: step 3473, loss 0.0320555, acc 1
2016-09-07T18:25:48.982446: step 3474, loss 0.0276961, acc 1
2016-09-07T18:25:49.656421: step 3475, loss 0.0433487, acc 0.98
2016-09-07T18:25:50.330073: step 3476, loss 0.00560661, acc 1
2016-09-07T18:25:51.005057: step 3477, loss 0.02611, acc 0.98
2016-09-07T18:25:51.671744: step 3478, loss 0.0289396, acc 0.98
2016-09-07T18:25:52.345362: step 3479, loss 0.0638299, acc 0.98
2016-09-07T18:25:53.033438: step 3480, loss 0.0255547, acc 0.98
2016-09-07T18:25:53.703516: step 3481, loss 0.0215184, acc 1
2016-09-07T18:25:54.367270: step 3482, loss 0.0103162, acc 1
2016-09-07T18:25:55.049353: step 3483, loss 0.0298543, acc 0.98
2016-09-07T18:25:55.719880: step 3484, loss 0.00245509, acc 1
2016-09-07T18:25:56.375717: step 3485, loss 0.0872238, acc 0.94
2016-09-07T18:25:57.061650: step 3486, loss 0.00176676, acc 1
2016-09-07T18:25:57.752739: step 3487, loss 0.0223439, acc 0.98
2016-09-07T18:25:58.413220: step 3488, loss 0.0352824, acc 0.98
2016-09-07T18:25:59.079358: step 3489, loss 0.0181015, acc 1
2016-09-07T18:25:59.755902: step 3490, loss 0.0518007, acc 0.94
2016-09-07T18:26:00.455467: step 3491, loss 0.059037, acc 0.98
2016-09-07T18:26:00.801006: step 3492, loss 0.0592195, acc 1
2016-09-07T18:26:01.453597: step 3493, loss 0.0297144, acc 1
2016-09-07T18:26:02.118267: step 3494, loss 0.0411856, acc 0.98
2016-09-07T18:26:02.791885: step 3495, loss 0.0355463, acc 0.98
2016-09-07T18:26:03.462398: step 3496, loss 0.0019491, acc 1
2016-09-07T18:26:04.142739: step 3497, loss 0.000404616, acc 1
2016-09-07T18:26:04.795978: step 3498, loss 0.0745069, acc 0.98
2016-09-07T18:26:05.470358: step 3499, loss 0.0253579, acc 0.98
2016-09-07T18:26:06.159456: step 3500, loss 0.0214876, acc 1

Evaluation:
2016-09-07T18:26:09.455686: step 3500, loss 1.89101, acc 0.753

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-3500

2016-09-07T18:26:11.235604: step 3501, loss 0.0873983, acc 0.98
2016-09-07T18:26:11.918197: step 3502, loss 0.0038446, acc 1
2016-09-07T18:26:12.599755: step 3503, loss 0.0199956, acc 0.98
2016-09-07T18:26:13.255470: step 3504, loss 0.039167, acc 0.98
2016-09-07T18:26:13.923321: step 3505, loss 0.11996, acc 0.98
2016-09-07T18:26:14.581746: step 3506, loss 0.0943393, acc 0.94
2016-09-07T18:26:15.256650: step 3507, loss 0.00371264, acc 1
2016-09-07T18:26:15.919394: step 3508, loss 0.0146173, acc 1
2016-09-07T18:26:16.576285: step 3509, loss 0.00436243, acc 1
2016-09-07T18:26:17.233802: step 3510, loss 0.00259508, acc 1
2016-09-07T18:26:17.887554: step 3511, loss 0.00932494, acc 1
2016-09-07T18:26:18.558156: step 3512, loss 0.000401903, acc 1
2016-09-07T18:26:19.224999: step 3513, loss 0.0290274, acc 1
2016-09-07T18:26:19.905862: step 3514, loss 0.0269128, acc 1
2016-09-07T18:26:20.577791: step 3515, loss 0.00367475, acc 1
2016-09-07T18:26:21.250971: step 3516, loss 0.0422, acc 0.98
2016-09-07T18:26:21.933969: step 3517, loss 0.0643343, acc 0.96
2016-09-07T18:26:22.603437: step 3518, loss 0.0518729, acc 0.98
2016-09-07T18:26:23.261797: step 3519, loss 0.00317272, acc 1
2016-09-07T18:26:23.925352: step 3520, loss 0.00732528, acc 1
2016-09-07T18:26:24.587958: step 3521, loss 0.0612684, acc 0.96
2016-09-07T18:26:25.259583: step 3522, loss 0.0325238, acc 0.98
2016-09-07T18:26:25.922836: step 3523, loss 0.0541706, acc 0.96
2016-09-07T18:26:26.592748: step 3524, loss 0.0450649, acc 0.98
2016-09-07T18:26:27.272073: step 3525, loss 0.044327, acc 0.98
2016-09-07T18:26:27.948727: step 3526, loss 0.0453488, acc 0.96
2016-09-07T18:26:28.629882: step 3527, loss 0.0195843, acc 0.98
2016-09-07T18:26:29.333747: step 3528, loss 0.117043, acc 0.96
2016-09-07T18:26:30.003097: step 3529, loss 0.00018292, acc 1
2016-09-07T18:26:30.660665: step 3530, loss 0.11127, acc 0.98
2016-09-07T18:26:31.338524: step 3531, loss 0.00138225, acc 1
2016-09-07T18:26:32.015541: step 3532, loss 0.00667113, acc 1
2016-09-07T18:26:32.677252: step 3533, loss 0.0178412, acc 0.98
2016-09-07T18:26:33.361565: step 3534, loss 0.0157862, acc 1
2016-09-07T18:26:34.060050: step 3535, loss 0.0445971, acc 0.98
2016-09-07T18:26:34.741366: step 3536, loss 0.0420575, acc 0.98
2016-09-07T18:26:35.410489: step 3537, loss 0.0444023, acc 0.98
2016-09-07T18:26:36.074060: step 3538, loss 0.0698505, acc 0.94
2016-09-07T18:26:36.739905: step 3539, loss 0.0613871, acc 0.96
2016-09-07T18:26:37.404102: step 3540, loss 0.00433409, acc 1
2016-09-07T18:26:38.077549: step 3541, loss 0.0205075, acc 0.98
2016-09-07T18:26:38.741022: step 3542, loss 0.0477206, acc 0.96
2016-09-07T18:26:39.415531: step 3543, loss 0.0300584, acc 1
2016-09-07T18:26:40.088685: step 3544, loss 0.100598, acc 0.96
2016-09-07T18:26:40.770178: step 3545, loss 0.00535363, acc 1
2016-09-07T18:26:41.443146: step 3546, loss 0.0232039, acc 1
2016-09-07T18:26:42.111905: step 3547, loss 0.0747814, acc 0.94
2016-09-07T18:26:42.793431: step 3548, loss 0.0039619, acc 1
2016-09-07T18:26:43.465722: step 3549, loss 0.0618404, acc 0.98
2016-09-07T18:26:44.133706: step 3550, loss 0.00397424, acc 1
2016-09-07T18:26:44.808267: step 3551, loss 0.0141494, acc 1
2016-09-07T18:26:45.484252: step 3552, loss 0.00164506, acc 1
2016-09-07T18:26:46.160568: step 3553, loss 0.0979992, acc 0.96
2016-09-07T18:26:46.834422: step 3554, loss 0.0234571, acc 1
2016-09-07T18:26:47.505052: step 3555, loss 0.0674817, acc 0.96
2016-09-07T18:26:48.172821: step 3556, loss 0.057704, acc 0.98
2016-09-07T18:26:48.848909: step 3557, loss 0.0126848, acc 1
2016-09-07T18:26:49.518451: step 3558, loss 0.0399711, acc 0.98
2016-09-07T18:26:50.193300: step 3559, loss 0.0032716, acc 1
2016-09-07T18:26:50.853141: step 3560, loss 0.00869285, acc 1
2016-09-07T18:26:51.539062: step 3561, loss 0.00693535, acc 1
2016-09-07T18:26:52.204947: step 3562, loss 0.0204509, acc 1
2016-09-07T18:26:52.898417: step 3563, loss 0.057302, acc 0.98
2016-09-07T18:26:53.568966: step 3564, loss 0.0434227, acc 1
2016-09-07T18:26:54.236447: step 3565, loss 0.000564228, acc 1
2016-09-07T18:26:54.939278: step 3566, loss 0.00710778, acc 1
2016-09-07T18:26:55.607234: step 3567, loss 0.00536728, acc 1
2016-09-07T18:26:56.284041: step 3568, loss 0.0398596, acc 0.98
2016-09-07T18:26:56.955404: step 3569, loss 0.00941967, acc 1
2016-09-07T18:26:57.624373: step 3570, loss 0.0872214, acc 0.98
2016-09-07T18:26:58.303356: step 3571, loss 0.0178574, acc 1
2016-09-07T18:26:58.978487: step 3572, loss 0.0257715, acc 0.98
2016-09-07T18:26:59.655037: step 3573, loss 0.0340052, acc 0.98
2016-09-07T18:27:00.355698: step 3574, loss 0.00670523, acc 1
2016-09-07T18:27:01.027389: step 3575, loss 0.0323519, acc 0.98
2016-09-07T18:27:01.685198: step 3576, loss 0.00759946, acc 1
2016-09-07T18:27:02.350683: step 3577, loss 0.00398967, acc 1
2016-09-07T18:27:03.030215: step 3578, loss 0.107774, acc 0.94
2016-09-07T18:27:03.700152: step 3579, loss 0.0205917, acc 0.98
2016-09-07T18:27:04.375123: step 3580, loss 0.0151213, acc 1
2016-09-07T18:27:05.068547: step 3581, loss 0.032726, acc 1
2016-09-07T18:27:05.779773: step 3582, loss 0.0179094, acc 1
2016-09-07T18:27:06.441926: step 3583, loss 0.0220955, acc 0.98
2016-09-07T18:27:07.105938: step 3584, loss 0.00295178, acc 1
2016-09-07T18:27:07.774289: step 3585, loss 0.0366618, acc 0.98
2016-09-07T18:27:08.459124: step 3586, loss 0.0181371, acc 1
2016-09-07T18:27:09.125718: step 3587, loss 0.0408824, acc 0.98
2016-09-07T18:27:09.785701: step 3588, loss 0.0491914, acc 0.96
2016-09-07T18:27:10.466906: step 3589, loss 0.0190523, acc 0.98
2016-09-07T18:27:11.163657: step 3590, loss 0.0197859, acc 1
2016-09-07T18:27:11.836804: step 3591, loss 0.0173337, acc 0.98
2016-09-07T18:27:12.521578: step 3592, loss 0.0502037, acc 0.96
2016-09-07T18:27:13.194762: step 3593, loss 0.0462812, acc 0.98
2016-09-07T18:27:13.865566: step 3594, loss 0.0131, acc 1
2016-09-07T18:27:14.541607: step 3595, loss 0.0136176, acc 1
2016-09-07T18:27:15.193098: step 3596, loss 0.00668343, acc 1
2016-09-07T18:27:15.875504: step 3597, loss 0.00757382, acc 1
2016-09-07T18:27:16.543624: step 3598, loss 0.0674895, acc 0.98
2016-09-07T18:27:17.227248: step 3599, loss 0.0146112, acc 1
2016-09-07T18:27:17.878082: step 3600, loss 0.0335045, acc 0.98

Evaluation:
2016-09-07T18:27:21.183943: step 3600, loss 1.9262, acc 0.754

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-3600

2016-09-07T18:27:22.872629: step 3601, loss 0.0142506, acc 1
2016-09-07T18:27:23.583477: step 3602, loss 0.00688287, acc 1
2016-09-07T18:27:24.292409: step 3603, loss 0.07183, acc 0.96
2016-09-07T18:27:24.957374: step 3604, loss 0.000937575, acc 1
2016-09-07T18:27:25.622187: step 3605, loss 0.00988906, acc 1
2016-09-07T18:27:26.286837: step 3606, loss 0.00903336, acc 1
2016-09-07T18:27:26.957290: step 3607, loss 0.0144918, acc 1
2016-09-07T18:27:27.614280: step 3608, loss 0.030158, acc 0.98
2016-09-07T18:27:28.272823: step 3609, loss 0.00470835, acc 1
2016-09-07T18:27:28.950023: step 3610, loss 0.0502056, acc 0.96
2016-09-07T18:27:29.613106: step 3611, loss 0.0285837, acc 0.98
2016-09-07T18:27:30.278124: step 3612, loss 0.0693932, acc 0.96
2016-09-07T18:27:30.961463: step 3613, loss 0.000983175, acc 1
2016-09-07T18:27:31.642249: step 3614, loss 0.0227646, acc 1
2016-09-07T18:27:32.320678: step 3615, loss 0.0178795, acc 0.98
2016-09-07T18:27:33.012628: step 3616, loss 0.0417078, acc 0.98
2016-09-07T18:27:33.686425: step 3617, loss 0.0422739, acc 0.98
2016-09-07T18:27:34.356179: step 3618, loss 0.023528, acc 0.98
2016-09-07T18:27:35.035532: step 3619, loss 0.0407895, acc 0.98
2016-09-07T18:27:35.720032: step 3620, loss 0.0350529, acc 0.98
2016-09-07T18:27:36.376467: step 3621, loss 0.00540913, acc 1
2016-09-07T18:27:37.043437: step 3622, loss 0.0402184, acc 0.98
2016-09-07T18:27:37.700900: step 3623, loss 0.0494098, acc 0.96
2016-09-07T18:27:38.367037: step 3624, loss 0.00687858, acc 1
2016-09-07T18:27:39.025684: step 3625, loss 0.0138888, acc 1
2016-09-07T18:27:39.730054: step 3626, loss 0.00242391, acc 1
2016-09-07T18:27:40.397189: step 3627, loss 0.0479499, acc 0.96
2016-09-07T18:27:41.067743: step 3628, loss 0.00092912, acc 1
2016-09-07T18:27:41.726177: step 3629, loss 0.0475221, acc 0.98
2016-09-07T18:27:42.409618: step 3630, loss 0.00157468, acc 1
2016-09-07T18:27:43.089960: step 3631, loss 0.0322286, acc 0.98
2016-09-07T18:27:43.756264: step 3632, loss 0.196902, acc 0.98
2016-09-07T18:27:44.422503: step 3633, loss 0.10601, acc 0.96
2016-09-07T18:27:45.110340: step 3634, loss 0.0134622, acc 1
2016-09-07T18:27:45.793108: step 3635, loss 0.00724406, acc 1
2016-09-07T18:27:46.461743: step 3636, loss 0.0318015, acc 0.98
2016-09-07T18:27:47.136645: step 3637, loss 0.0182195, acc 1
2016-09-07T18:27:47.815345: step 3638, loss 0.0119339, acc 1
2016-09-07T18:27:48.474120: step 3639, loss 0.135167, acc 0.94
2016-09-07T18:27:49.133386: step 3640, loss 0.0129243, acc 1
2016-09-07T18:27:49.804300: step 3641, loss 0.0499274, acc 0.98
2016-09-07T18:27:50.477314: step 3642, loss 0.0396387, acc 0.96
2016-09-07T18:27:51.148940: step 3643, loss 0.0180038, acc 0.98
2016-09-07T18:27:51.837480: step 3644, loss 0.0611507, acc 0.98
2016-09-07T18:27:52.511200: step 3645, loss 0.0385724, acc 0.96
2016-09-07T18:27:53.211481: step 3646, loss 0.00823731, acc 1
2016-09-07T18:27:53.894847: step 3647, loss 0.0616591, acc 0.94
2016-09-07T18:27:54.556602: step 3648, loss 0.0220981, acc 0.98
2016-09-07T18:27:55.245977: step 3649, loss 0.205372, acc 0.92
2016-09-07T18:27:55.903386: step 3650, loss 0.0700901, acc 0.96
2016-09-07T18:27:56.555761: step 3651, loss 0.0381715, acc 0.98
2016-09-07T18:27:57.216307: step 3652, loss 0.105927, acc 0.98
2016-09-07T18:27:57.891103: step 3653, loss 0.0123214, acc 1
2016-09-07T18:27:58.574904: step 3654, loss 0.0434446, acc 0.98
2016-09-07T18:27:59.245879: step 3655, loss 0.007009, acc 1
2016-09-07T18:27:59.927668: step 3656, loss 0.0718673, acc 0.98
2016-09-07T18:28:00.620597: step 3657, loss 0.00126696, acc 1
2016-09-07T18:28:01.310304: step 3658, loss 0.0575403, acc 0.96
2016-09-07T18:28:01.995314: step 3659, loss 0.00142385, acc 1
2016-09-07T18:28:02.670555: step 3660, loss 0.0273207, acc 0.98
2016-09-07T18:28:03.355263: step 3661, loss 0.0322446, acc 0.98
2016-09-07T18:28:04.027273: step 3662, loss 0.0605518, acc 0.96
2016-09-07T18:28:04.701022: step 3663, loss 0.00790444, acc 1
2016-09-07T18:28:05.385781: step 3664, loss 0.0084756, acc 1
2016-09-07T18:28:06.073445: step 3665, loss 0.0313537, acc 0.98
2016-09-07T18:28:06.738755: step 3666, loss 0.124293, acc 0.96
2016-09-07T18:28:07.402610: step 3667, loss 0.0787141, acc 0.96
2016-09-07T18:28:08.063844: step 3668, loss 0.0424315, acc 0.98
2016-09-07T18:28:08.751266: step 3669, loss 0.0115148, acc 1
2016-09-07T18:28:09.410556: step 3670, loss 0.0151391, acc 1
2016-09-07T18:28:10.078964: step 3671, loss 0.0378906, acc 1
2016-09-07T18:28:10.745371: step 3672, loss 0.0177359, acc 0.98
2016-09-07T18:28:11.422105: step 3673, loss 0.0647916, acc 0.96
2016-09-07T18:28:12.093776: step 3674, loss 0.0889504, acc 0.96
2016-09-07T18:28:12.758618: step 3675, loss 0.000770358, acc 1
2016-09-07T18:28:13.428881: step 3676, loss 0.0359819, acc 0.96
2016-09-07T18:28:14.102966: step 3677, loss 0.00567466, acc 1
2016-09-07T18:28:14.762023: step 3678, loss 0.0791172, acc 0.98
2016-09-07T18:28:15.419509: step 3679, loss 0.0353302, acc 0.98
2016-09-07T18:28:16.090045: step 3680, loss 0.0291926, acc 0.98
2016-09-07T18:28:16.779041: step 3681, loss 0.00270412, acc 1
2016-09-07T18:28:17.443204: step 3682, loss 0.0266515, acc 1
2016-09-07T18:28:18.125568: step 3683, loss 0.0251719, acc 1
2016-09-07T18:28:18.787327: step 3684, loss 0.0165309, acc 1
2016-09-07T18:28:19.456484: step 3685, loss 0.0146713, acc 1
2016-09-07T18:28:19.812138: step 3686, loss 0.0181084, acc 1
2016-09-07T18:28:20.474698: step 3687, loss 0.0147104, acc 1
2016-09-07T18:28:21.146305: step 3688, loss 0.0088322, acc 1
2016-09-07T18:28:21.833932: step 3689, loss 0.0705902, acc 0.94
2016-09-07T18:28:22.509832: step 3690, loss 0.0183066, acc 1
2016-09-07T18:28:23.217189: step 3691, loss 0.00454316, acc 1
2016-09-07T18:28:23.883270: step 3692, loss 0.0482497, acc 0.96
2016-09-07T18:28:24.538804: step 3693, loss 0.0050912, acc 1
2016-09-07T18:28:25.217015: step 3694, loss 0.0898051, acc 0.98
2016-09-07T18:28:25.892056: step 3695, loss 0.0394653, acc 0.98
2016-09-07T18:28:26.572823: step 3696, loss 0.0366691, acc 0.98
2016-09-07T18:28:27.242592: step 3697, loss 0.0289433, acc 0.98
2016-09-07T18:28:27.921710: step 3698, loss 0.0154531, acc 1
2016-09-07T18:28:28.586928: step 3699, loss 0.0158741, acc 1
2016-09-07T18:28:29.254147: step 3700, loss 0.0408496, acc 0.96

Evaluation:
2016-09-07T18:28:32.574481: step 3700, loss 1.68538, acc 0.739

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-3700

2016-09-07T18:28:34.262243: step 3701, loss 0.00285461, acc 1
2016-09-07T18:28:34.937607: step 3702, loss 0.00624203, acc 1
2016-09-07T18:28:35.632671: step 3703, loss 0.0185773, acc 1
2016-09-07T18:28:36.302242: step 3704, loss 0.0847398, acc 0.96
2016-09-07T18:28:36.966949: step 3705, loss 0.0313266, acc 0.96
2016-09-07T18:28:37.639896: step 3706, loss 0.048718, acc 0.96
2016-09-07T18:28:38.324755: step 3707, loss 0.0429372, acc 0.98
2016-09-07T18:28:39.005070: step 3708, loss 0.0205332, acc 1
2016-09-07T18:28:39.666663: step 3709, loss 0.049448, acc 0.98
2016-09-07T18:28:40.324212: step 3710, loss 0.0191377, acc 1
2016-09-07T18:28:40.991839: step 3711, loss 0.00227227, acc 1
2016-09-07T18:28:41.661333: step 3712, loss 0.00384085, acc 1
2016-09-07T18:28:42.320477: step 3713, loss 0.00695708, acc 1
2016-09-07T18:28:42.993217: step 3714, loss 0.000683859, acc 1
2016-09-07T18:28:43.676813: step 3715, loss 0.00451999, acc 1
2016-09-07T18:28:44.367998: step 3716, loss 0.00328081, acc 1
2016-09-07T18:28:45.052556: step 3717, loss 0.0251139, acc 0.98
2016-09-07T18:28:45.736809: step 3718, loss 0.0183911, acc 1
2016-09-07T18:28:46.423618: step 3719, loss 0.0110289, acc 1
2016-09-07T18:28:47.088442: step 3720, loss 0.00696441, acc 1
2016-09-07T18:28:47.770696: step 3721, loss 0.0803614, acc 0.98
2016-09-07T18:28:48.438489: step 3722, loss 0.0180702, acc 1
2016-09-07T18:28:49.117588: step 3723, loss 0.0257534, acc 1
2016-09-07T18:28:49.809962: step 3724, loss 0.11124, acc 0.96
2016-09-07T18:28:50.482600: step 3725, loss 0.000987593, acc 1
2016-09-07T18:28:51.141842: step 3726, loss 0.000586508, acc 1
2016-09-07T18:28:51.812638: step 3727, loss 0.0955185, acc 0.98
2016-09-07T18:28:52.478740: step 3728, loss 0.0328275, acc 0.98
2016-09-07T18:28:53.159153: step 3729, loss 0.00188809, acc 1
2016-09-07T18:28:53.816034: step 3730, loss 0.0952424, acc 0.96
2016-09-07T18:28:54.515274: step 3731, loss 0.00981646, acc 1
2016-09-07T18:28:55.180113: step 3732, loss 0.00702576, acc 1
2016-09-07T18:28:55.852158: step 3733, loss 0.0620003, acc 0.98
2016-09-07T18:28:56.531372: step 3734, loss 0.08584, acc 0.96
2016-09-07T18:28:57.199887: step 3735, loss 0.00886347, acc 1
2016-09-07T18:28:57.880831: step 3736, loss 0.0142961, acc 1
2016-09-07T18:28:58.554478: step 3737, loss 0.145263, acc 0.96
2016-09-07T18:28:59.248005: step 3738, loss 0.0366783, acc 0.96
2016-09-07T18:28:59.911516: step 3739, loss 0.0106187, acc 1
2016-09-07T18:29:00.613232: step 3740, loss 0.0122478, acc 1
2016-09-07T18:29:01.299568: step 3741, loss 0.0343253, acc 0.98
2016-09-07T18:29:01.957917: step 3742, loss 0.13991, acc 0.98
2016-09-07T18:29:02.630049: step 3743, loss 0.02021, acc 1
2016-09-07T18:29:03.305874: step 3744, loss 0.000455281, acc 1
2016-09-07T18:29:03.974115: step 3745, loss 0.00779949, acc 1
2016-09-07T18:29:04.662975: step 3746, loss 0.00563253, acc 1
2016-09-07T18:29:05.361347: step 3747, loss 0.0884519, acc 0.96
2016-09-07T18:29:06.035970: step 3748, loss 0.0240453, acc 0.98
2016-09-07T18:29:06.718244: step 3749, loss 0.00624159, acc 1
2016-09-07T18:29:07.394004: step 3750, loss 0.042352, acc 0.98
2016-09-07T18:29:08.073376: step 3751, loss 0.0581038, acc 0.98
2016-09-07T18:29:08.775415: step 3752, loss 0.0177389, acc 1
2016-09-07T18:29:09.429700: step 3753, loss 0.027129, acc 0.98
2016-09-07T18:29:10.124849: step 3754, loss 0.00223694, acc 1
2016-09-07T18:29:10.821849: step 3755, loss 0.00194887, acc 1
2016-09-07T18:29:11.479552: step 3756, loss 0.0133252, acc 1
2016-09-07T18:29:12.147578: step 3757, loss 0.0134932, acc 1
2016-09-07T18:29:12.828164: step 3758, loss 0.0510864, acc 0.96
2016-09-07T18:29:13.497204: step 3759, loss 0.0022473, acc 1
2016-09-07T18:29:14.190458: step 3760, loss 0.034642, acc 0.96
2016-09-07T18:29:14.879350: step 3761, loss 0.0488751, acc 0.96
2016-09-07T18:29:15.543453: step 3762, loss 0.0583983, acc 0.98
2016-09-07T18:29:16.230306: step 3763, loss 0.0550559, acc 0.96
2016-09-07T18:29:16.896168: step 3764, loss 0.012941, acc 1
2016-09-07T18:29:17.568150: step 3765, loss 0.0798885, acc 0.96
2016-09-07T18:29:18.242875: step 3766, loss 0.038527, acc 0.98
2016-09-07T18:29:18.921371: step 3767, loss 0.104189, acc 0.96
2016-09-07T18:29:19.611144: step 3768, loss 0.0456503, acc 0.98
2016-09-07T18:29:20.274252: step 3769, loss 0.062897, acc 0.96
2016-09-07T18:29:20.945391: step 3770, loss 0.00461637, acc 1
2016-09-07T18:29:21.615640: step 3771, loss 0.0494407, acc 0.96
2016-09-07T18:29:22.294730: step 3772, loss 0.00544133, acc 1
2016-09-07T18:29:22.963171: step 3773, loss 0.00457653, acc 1
2016-09-07T18:29:23.625113: step 3774, loss 0.0164006, acc 1
2016-09-07T18:29:24.291917: step 3775, loss 0.0327828, acc 1
2016-09-07T18:29:24.987464: step 3776, loss 0.0386672, acc 0.98
2016-09-07T18:29:25.680432: step 3777, loss 0.0411844, acc 0.98
2016-09-07T18:29:26.358863: step 3778, loss 0.0224732, acc 0.98
2016-09-07T18:29:27.032867: step 3779, loss 0.196923, acc 0.94
2016-09-07T18:29:27.705147: step 3780, loss 0.106553, acc 0.96
2016-09-07T18:29:28.384967: step 3781, loss 0.0398775, acc 0.96
2016-09-07T18:29:29.084430: step 3782, loss 0.00338576, acc 1
2016-09-07T18:29:29.770599: step 3783, loss 0.0768777, acc 0.94
2016-09-07T18:29:30.439047: step 3784, loss 0.0317756, acc 0.98
2016-09-07T18:29:31.121186: step 3785, loss 0.0285363, acc 0.98
2016-09-07T18:29:31.792414: step 3786, loss 0.0271405, acc 0.98
2016-09-07T18:29:32.464275: step 3787, loss 0.00852546, acc 1
2016-09-07T18:29:33.141421: step 3788, loss 0.0244464, acc 1
2016-09-07T18:29:33.814807: step 3789, loss 0.020523, acc 0.98
2016-09-07T18:29:34.521778: step 3790, loss 0.0754864, acc 0.98
2016-09-07T18:29:35.188623: step 3791, loss 0.0336838, acc 0.98
2016-09-07T18:29:35.874569: step 3792, loss 0.0268608, acc 1
2016-09-07T18:29:36.541601: step 3793, loss 0.0276097, acc 0.98
2016-09-07T18:29:37.211241: step 3794, loss 0.00615034, acc 1
2016-09-07T18:29:37.878983: step 3795, loss 0.0193402, acc 1
2016-09-07T18:29:38.538224: step 3796, loss 0.00316825, acc 1
2016-09-07T18:29:39.219365: step 3797, loss 0.0319077, acc 1
2016-09-07T18:29:39.908953: step 3798, loss 0.0127491, acc 1
2016-09-07T18:29:40.582730: step 3799, loss 0.0626183, acc 0.98
2016-09-07T18:29:41.235219: step 3800, loss 0.0180898, acc 0.98

Evaluation:
2016-09-07T18:29:44.578762: step 3800, loss 1.75881, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-3800

2016-09-07T18:29:46.400277: step 3801, loss 0.0180181, acc 1
2016-09-07T18:29:47.066847: step 3802, loss 0.0708547, acc 0.96
2016-09-07T18:29:47.753517: step 3803, loss 0.00794434, acc 1
2016-09-07T18:29:48.421355: step 3804, loss 0.0216632, acc 1
2016-09-07T18:29:49.094601: step 3805, loss 0.000547917, acc 1
2016-09-07T18:29:49.772825: step 3806, loss 0.0506856, acc 0.96
2016-09-07T18:29:50.434299: step 3807, loss 0.00237769, acc 1
2016-09-07T18:29:51.087320: step 3808, loss 0.00741273, acc 1
2016-09-07T18:29:51.745401: step 3809, loss 0.000526446, acc 1
2016-09-07T18:29:52.397990: step 3810, loss 0.0129163, acc 1
2016-09-07T18:29:53.063018: step 3811, loss 0.0586064, acc 0.94
2016-09-07T18:29:53.728690: step 3812, loss 0.0018835, acc 1
2016-09-07T18:29:54.384701: step 3813, loss 0.00748726, acc 1
2016-09-07T18:29:55.063408: step 3814, loss 0.0210973, acc 1
2016-09-07T18:29:55.724383: step 3815, loss 0.0231105, acc 1
2016-09-07T18:29:56.409447: step 3816, loss 0.0391564, acc 0.96
2016-09-07T18:29:57.085280: step 3817, loss 0.0330787, acc 0.98
2016-09-07T18:29:57.763901: step 3818, loss 0.0488807, acc 0.96
2016-09-07T18:29:58.443907: step 3819, loss 0.0051104, acc 1
2016-09-07T18:29:59.126032: step 3820, loss 0.0222678, acc 0.98
2016-09-07T18:29:59.807818: step 3821, loss 0.0306712, acc 0.96
2016-09-07T18:30:00.509847: step 3822, loss 0.000692099, acc 1
2016-09-07T18:30:01.193367: step 3823, loss 0.0435096, acc 0.98
2016-09-07T18:30:01.863913: step 3824, loss 4.47587e-05, acc 1
2016-09-07T18:30:02.524706: step 3825, loss 0.00899436, acc 1
2016-09-07T18:30:03.196456: step 3826, loss 0.107297, acc 0.94
2016-09-07T18:30:03.865804: step 3827, loss 0.00287147, acc 1
2016-09-07T18:30:04.539431: step 3828, loss 0.0195155, acc 0.98
2016-09-07T18:30:05.224895: step 3829, loss 0.0275458, acc 0.98
2016-09-07T18:30:05.885884: step 3830, loss 0.0271563, acc 1
2016-09-07T18:30:06.542649: step 3831, loss 6.72775e-05, acc 1
2016-09-07T18:30:07.197538: step 3832, loss 0.0434351, acc 0.98
2016-09-07T18:30:07.869015: step 3833, loss 0.000158158, acc 1
2016-09-07T18:30:08.556067: step 3834, loss 0.0153259, acc 0.98
2016-09-07T18:30:09.218980: step 3835, loss 0.0053325, acc 1
2016-09-07T18:30:09.873446: step 3836, loss 0.00547879, acc 1
2016-09-07T18:30:10.525959: step 3837, loss 0.0295724, acc 0.98
2016-09-07T18:30:11.182464: step 3838, loss 0.0645326, acc 0.94
2016-09-07T18:30:11.865266: step 3839, loss 0.0124631, acc 1
2016-09-07T18:30:12.524904: step 3840, loss 0.02975, acc 1
2016-09-07T18:30:13.198956: step 3841, loss 0.0188239, acc 1
2016-09-07T18:30:13.854282: step 3842, loss 0.00194248, acc 1
2016-09-07T18:30:14.519951: step 3843, loss 8.69023e-05, acc 1
2016-09-07T18:30:15.194531: step 3844, loss 0.00286172, acc 1
2016-09-07T18:30:15.860781: step 3845, loss 0.0134193, acc 1
2016-09-07T18:30:16.545474: step 3846, loss 0.00153448, acc 1
2016-09-07T18:30:17.233475: step 3847, loss 0.0715978, acc 0.98
2016-09-07T18:30:17.900737: step 3848, loss 0.00936863, acc 1
2016-09-07T18:30:18.586102: step 3849, loss 0.00441598, acc 1
2016-09-07T18:30:19.254966: step 3850, loss 0.00110655, acc 1
2016-09-07T18:30:19.918606: step 3851, loss 0.0184005, acc 0.98
2016-09-07T18:30:20.579557: step 3852, loss 0.0291686, acc 0.98
2016-09-07T18:30:21.253141: step 3853, loss 0.0384582, acc 0.98
2016-09-07T18:30:21.916839: step 3854, loss 0.0187726, acc 0.98
2016-09-07T18:30:22.575376: step 3855, loss 0.0314096, acc 0.98
2016-09-07T18:30:23.228073: step 3856, loss 0.259859, acc 0.92
2016-09-07T18:30:23.920840: step 3857, loss 0.00669082, acc 1
2016-09-07T18:30:24.598722: step 3858, loss 0.0193126, acc 0.98
2016-09-07T18:30:25.265263: step 3859, loss 0.0505072, acc 0.98
2016-09-07T18:30:25.934441: step 3860, loss 0.000314226, acc 1
2016-09-07T18:30:26.595517: step 3861, loss 0.0337945, acc 0.98
2016-09-07T18:30:27.277521: step 3862, loss 0.038709, acc 0.98
2016-09-07T18:30:27.944196: step 3863, loss 0.0137986, acc 1
2016-09-07T18:30:28.613468: step 3864, loss 0.000183216, acc 1
2016-09-07T18:30:29.296066: step 3865, loss 0.0217592, acc 1
2016-09-07T18:30:29.952005: step 3866, loss 0.0761922, acc 0.98
2016-09-07T18:30:30.623454: step 3867, loss 0.017843, acc 0.98
2016-09-07T18:30:31.298423: step 3868, loss 0.049993, acc 0.98
2016-09-07T18:30:31.966078: step 3869, loss 0.00562706, acc 1
2016-09-07T18:30:32.626487: step 3870, loss 0.0152539, acc 0.98
2016-09-07T18:30:33.293231: step 3871, loss 0.00327114, acc 1
2016-09-07T18:30:33.954911: step 3872, loss 0.00417656, acc 1
2016-09-07T18:30:34.629933: step 3873, loss 0.0272123, acc 0.98
2016-09-07T18:30:35.295221: step 3874, loss 0.00455893, acc 1
2016-09-07T18:30:35.959856: step 3875, loss 0.0604144, acc 0.98
2016-09-07T18:30:36.631888: step 3876, loss 0.0306466, acc 0.98
2016-09-07T18:30:37.306736: step 3877, loss 0.0147098, acc 1
2016-09-07T18:30:37.997462: step 3878, loss 0.0112041, acc 1
2016-09-07T18:30:38.673566: step 3879, loss 0.00939749, acc 1
2016-09-07T18:30:39.023022: step 3880, loss 0.0796511, acc 1
2016-09-07T18:30:39.705097: step 3881, loss 0.0120061, acc 1
2016-09-07T18:30:40.413627: step 3882, loss 0.0122459, acc 1
2016-09-07T18:30:41.083760: step 3883, loss 0.0190364, acc 1
2016-09-07T18:30:41.768831: step 3884, loss 0.116066, acc 0.96
2016-09-07T18:30:42.456377: step 3885, loss 0.00946325, acc 1
2016-09-07T18:30:43.124693: step 3886, loss 0.00928966, acc 1
2016-09-07T18:30:43.788955: step 3887, loss 0.230647, acc 0.96
2016-09-07T18:30:44.473510: step 3888, loss 0.0274857, acc 0.98
2016-09-07T18:30:45.149965: step 3889, loss 0.0357642, acc 0.98
2016-09-07T18:30:45.821745: step 3890, loss 0.0528358, acc 0.98
2016-09-07T18:30:46.515198: step 3891, loss 1.73793e-05, acc 1
2016-09-07T18:30:47.180848: step 3892, loss 0.0270967, acc 0.98
2016-09-07T18:30:47.845335: step 3893, loss 0.0440559, acc 0.98
2016-09-07T18:30:48.504932: step 3894, loss 0.0111898, acc 1
2016-09-07T18:30:49.183994: step 3895, loss 0.0057128, acc 1
2016-09-07T18:30:49.858446: step 3896, loss 0.0220578, acc 1
2016-09-07T18:30:50.521066: step 3897, loss 0.0074903, acc 1
2016-09-07T18:30:51.176124: step 3898, loss 0.00455285, acc 1
2016-09-07T18:30:51.839150: step 3899, loss 0.0197139, acc 1
2016-09-07T18:30:52.506868: step 3900, loss 0.124899, acc 0.94

Evaluation:
2016-09-07T18:30:55.799153: step 3900, loss 2.03295, acc 0.764

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-3900

2016-09-07T18:30:57.558147: step 3901, loss 0.0292782, acc 0.98
2016-09-07T18:30:58.251282: step 3902, loss 0.0159679, acc 1
2016-09-07T18:30:58.922343: step 3903, loss 0.0170254, acc 0.98
2016-09-07T18:30:59.592780: step 3904, loss 0.00570604, acc 1
2016-09-07T18:31:00.271754: step 3905, loss 0.00583028, acc 1
2016-09-07T18:31:00.934702: step 3906, loss 0.00137939, acc 1
2016-09-07T18:31:01.608990: step 3907, loss 0.00430694, acc 1
2016-09-07T18:31:02.296045: step 3908, loss 0.109173, acc 0.98
2016-09-07T18:31:02.958093: step 3909, loss 0.0684021, acc 0.98
2016-09-07T18:31:03.623419: step 3910, loss 0.0341939, acc 0.98
2016-09-07T18:31:04.310967: step 3911, loss 0.127211, acc 0.98
2016-09-07T18:31:04.970859: step 3912, loss 0.108167, acc 0.94
2016-09-07T18:31:05.630289: step 3913, loss 0.0131089, acc 1
2016-09-07T18:31:06.302050: step 3914, loss 0.0954436, acc 0.98
2016-09-07T18:31:06.962390: step 3915, loss 0.0267635, acc 0.98
2016-09-07T18:31:07.634237: step 3916, loss 0.0762888, acc 0.96
2016-09-07T18:31:08.331189: step 3917, loss 0.0128817, acc 1
2016-09-07T18:31:09.016305: step 3918, loss 0.0139538, acc 1
2016-09-07T18:31:09.680757: step 3919, loss 0.00021828, acc 1
2016-09-07T18:31:10.354782: step 3920, loss 0.00940305, acc 1
2016-09-07T18:31:11.034854: step 3921, loss 0.0509372, acc 0.98
2016-09-07T18:31:11.702791: step 3922, loss 0.0480064, acc 0.98
2016-09-07T18:31:12.380539: step 3923, loss 0.000803101, acc 1
2016-09-07T18:31:13.046203: step 3924, loss 0.0450367, acc 0.98
2016-09-07T18:31:13.709141: step 3925, loss 0.0423025, acc 0.98
2016-09-07T18:31:14.389801: step 3926, loss 0.0530194, acc 0.96
2016-09-07T18:31:15.064407: step 3927, loss 0.0657705, acc 0.96
2016-09-07T18:31:15.740123: step 3928, loss 0.000986225, acc 1
2016-09-07T18:31:16.395773: step 3929, loss 0.00633394, acc 1
2016-09-07T18:31:17.071020: step 3930, loss 0.0426271, acc 0.98
2016-09-07T18:31:17.764335: step 3931, loss 0.0162179, acc 1
2016-09-07T18:31:18.428199: step 3932, loss 0.0208085, acc 0.98
2016-09-07T18:31:19.103162: step 3933, loss 0.00240682, acc 1
2016-09-07T18:31:19.772758: step 3934, loss 0.0193274, acc 0.98
2016-09-07T18:31:20.482036: step 3935, loss 0.0234287, acc 1
2016-09-07T18:31:21.151016: step 3936, loss 0.0335451, acc 0.98
2016-09-07T18:31:21.828796: step 3937, loss 0.0153874, acc 1
2016-09-07T18:31:22.527358: step 3938, loss 0.00631345, acc 1
2016-09-07T18:31:23.223714: step 3939, loss 0.087956, acc 0.98
2016-09-07T18:31:23.899615: step 3940, loss 0.000645609, acc 1
2016-09-07T18:31:24.569900: step 3941, loss 0.00924005, acc 1
2016-09-07T18:31:25.248318: step 3942, loss 0.0247572, acc 1
2016-09-07T18:31:25.930326: step 3943, loss 0.00674516, acc 1
2016-09-07T18:31:26.645364: step 3944, loss 0.159888, acc 0.94
2016-09-07T18:31:27.316954: step 3945, loss 0.0307777, acc 1
2016-09-07T18:31:27.992343: step 3946, loss 0.00789324, acc 1
2016-09-07T18:31:28.667574: step 3947, loss 0.00978361, acc 1
2016-09-07T18:31:29.346189: step 3948, loss 0.053762, acc 0.98
2016-09-07T18:31:30.018320: step 3949, loss 0.0125383, acc 1
2016-09-07T18:31:30.685550: step 3950, loss 0.0489316, acc 0.98
2016-09-07T18:31:31.350141: step 3951, loss 0.02836, acc 0.98
2016-09-07T18:31:32.018241: step 3952, loss 0.0342418, acc 0.98
2016-09-07T18:31:32.686776: step 3953, loss 0.00541183, acc 1
2016-09-07T18:31:33.356146: step 3954, loss 0.00162577, acc 1
2016-09-07T18:31:34.035731: step 3955, loss 0.029029, acc 0.98
2016-09-07T18:31:34.698448: step 3956, loss 0.0755815, acc 0.94
2016-09-07T18:31:35.386301: step 3957, loss 0.0524381, acc 0.98
2016-09-07T18:31:36.060483: step 3958, loss 0.00986923, acc 1
2016-09-07T18:31:36.747623: step 3959, loss 0.0880488, acc 0.96
2016-09-07T18:31:37.436375: step 3960, loss 0.00786672, acc 1
2016-09-07T18:31:38.118239: step 3961, loss 0.021104, acc 1
2016-09-07T18:31:38.803239: step 3962, loss 0.000810863, acc 1
2016-09-07T18:31:39.515678: step 3963, loss 0.023413, acc 0.98
2016-09-07T18:31:40.182506: step 3964, loss 0.0303054, acc 0.98
2016-09-07T18:31:40.881040: step 3965, loss 0.0854217, acc 0.96
2016-09-07T18:31:41.541711: step 3966, loss 0.0527863, acc 0.96
2016-09-07T18:31:42.210493: step 3967, loss 0.00851425, acc 1
2016-09-07T18:31:42.875476: step 3968, loss 0.00651772, acc 1
2016-09-07T18:31:43.553717: step 3969, loss 0.0265101, acc 1
2016-09-07T18:31:44.226718: step 3970, loss 0.0814338, acc 0.96
2016-09-07T18:31:44.901948: step 3971, loss 0.179175, acc 0.96
2016-09-07T18:31:45.581535: step 3972, loss 0.0183062, acc 1
2016-09-07T18:31:46.237868: step 3973, loss 0.00252255, acc 1
2016-09-07T18:31:46.914937: step 3974, loss 0.0191136, acc 1
2016-09-07T18:31:47.588210: step 3975, loss 0.0146745, acc 1
2016-09-07T18:31:48.259206: step 3976, loss 0.22322, acc 0.96
2016-09-07T18:31:48.928725: step 3977, loss 0.0161328, acc 0.98
2016-09-07T18:31:49.601658: step 3978, loss 0.159805, acc 0.98
2016-09-07T18:31:50.284537: step 3979, loss 0.0281209, acc 0.98
2016-09-07T18:31:50.964526: step 3980, loss 0.0303851, acc 0.98
2016-09-07T18:31:51.632009: step 3981, loss 0.0042548, acc 1
2016-09-07T18:31:52.322954: step 3982, loss 0.00776682, acc 1
2016-09-07T18:31:52.994954: step 3983, loss 0.0272052, acc 0.98
2016-09-07T18:31:53.677675: step 3984, loss 0.00492804, acc 1
2016-09-07T18:31:54.354842: step 3985, loss 0.0520119, acc 0.98
2016-09-07T18:31:55.043888: step 3986, loss 0.0396354, acc 1
2016-09-07T18:31:55.727052: step 3987, loss 0.0277644, acc 0.98
2016-09-07T18:31:56.401670: step 3988, loss 0.114135, acc 0.94
2016-09-07T18:31:57.073236: step 3989, loss 0.0512641, acc 0.98
2016-09-07T18:31:57.758472: step 3990, loss 0.0418494, acc 0.98
2016-09-07T18:31:58.433110: step 3991, loss 0.0633906, acc 0.98
2016-09-07T18:31:59.102534: step 3992, loss 0.0118707, acc 1
2016-09-07T18:31:59.765273: step 3993, loss 0.0340019, acc 0.98
2016-09-07T18:32:00.494271: step 3994, loss 0.0697442, acc 0.96
2016-09-07T18:32:01.161465: step 3995, loss 0.00999137, acc 1
2016-09-07T18:32:01.842195: step 3996, loss 0.0284085, acc 0.98
2016-09-07T18:32:02.516475: step 3997, loss 0.0277903, acc 0.98
2016-09-07T18:32:03.177441: step 3998, loss 0.0448835, acc 0.98
2016-09-07T18:32:03.857756: step 3999, loss 0.0796088, acc 0.96
2016-09-07T18:32:04.548856: step 4000, loss 0.0711444, acc 0.96

Evaluation:
2016-09-07T18:32:07.859371: step 4000, loss 1.61425, acc 0.737

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-4000

2016-09-07T18:32:09.642190: step 4001, loss 0.0184473, acc 0.98
2016-09-07T18:32:10.317953: step 4002, loss 0.131041, acc 0.94
2016-09-07T18:32:10.986485: step 4003, loss 0.0259689, acc 1
2016-09-07T18:32:11.656151: step 4004, loss 0.0044543, acc 1
2016-09-07T18:32:12.324531: step 4005, loss 0.0735631, acc 0.98
2016-09-07T18:32:13.006783: step 4006, loss 0.00402636, acc 1
2016-09-07T18:32:13.692710: step 4007, loss 0.0324448, acc 0.98
2016-09-07T18:32:14.371263: step 4008, loss 0.00294755, acc 1
2016-09-07T18:32:15.062108: step 4009, loss 0.00206979, acc 1
2016-09-07T18:32:15.736736: step 4010, loss 0.0254073, acc 0.98
2016-09-07T18:32:16.412385: step 4011, loss 0.066889, acc 0.96
2016-09-07T18:32:17.077256: step 4012, loss 0.0543695, acc 0.98
2016-09-07T18:32:17.749375: step 4013, loss 0.0137853, acc 1
2016-09-07T18:32:18.434088: step 4014, loss 0.0722218, acc 0.98
2016-09-07T18:32:19.131974: step 4015, loss 0.054847, acc 0.98
2016-09-07T18:32:19.817133: step 4016, loss 0.089083, acc 0.98
2016-09-07T18:32:20.503682: step 4017, loss 0.023415, acc 0.98
2016-09-07T18:32:21.177080: step 4018, loss 0.00551266, acc 1
2016-09-07T18:32:21.856401: step 4019, loss 0.0531359, acc 0.98
2016-09-07T18:32:22.533356: step 4020, loss 0.0453388, acc 0.98
2016-09-07T18:32:23.220915: step 4021, loss 0.036027, acc 0.98
2016-09-07T18:32:23.893091: step 4022, loss 0.0356442, acc 0.96
2016-09-07T18:32:24.574246: step 4023, loss 0.0049516, acc 1
2016-09-07T18:32:25.233998: step 4024, loss 0.00101879, acc 1
2016-09-07T18:32:25.910210: step 4025, loss 0.019534, acc 1
2016-09-07T18:32:26.577526: step 4026, loss 0.00843732, acc 1
2016-09-07T18:32:27.258949: step 4027, loss 0.0236629, acc 1
2016-09-07T18:32:27.931924: step 4028, loss 0.0228506, acc 0.98
2016-09-07T18:32:28.624882: step 4029, loss 0.0361023, acc 1
2016-09-07T18:32:29.310285: step 4030, loss 0.00225187, acc 1
2016-09-07T18:32:29.967803: step 4031, loss 0.041626, acc 0.98
2016-09-07T18:32:30.645968: step 4032, loss 0.0269566, acc 1
2016-09-07T18:32:31.333222: step 4033, loss 0.0287515, acc 1
2016-09-07T18:32:31.993904: step 4034, loss 0.0384022, acc 0.98
2016-09-07T18:32:32.654583: step 4035, loss 0.0260694, acc 1
2016-09-07T18:32:33.325521: step 4036, loss 0.0442276, acc 0.98
2016-09-07T18:32:34.008854: step 4037, loss 0.0580789, acc 0.96
2016-09-07T18:32:34.665681: step 4038, loss 0.0233827, acc 0.98
2016-09-07T18:32:35.338522: step 4039, loss 0.0607021, acc 0.96
2016-09-07T18:32:36.006621: step 4040, loss 0.0198671, acc 0.98
2016-09-07T18:32:36.679084: step 4041, loss 0.00826335, acc 1
2016-09-07T18:32:37.355305: step 4042, loss 0.0953512, acc 0.92
2016-09-07T18:32:38.040141: step 4043, loss 0.00100553, acc 1
2016-09-07T18:32:38.723254: step 4044, loss 0.00605261, acc 1
2016-09-07T18:32:39.388690: step 4045, loss 0.00781193, acc 1
2016-09-07T18:32:40.054540: step 4046, loss 0.0402892, acc 0.96
2016-09-07T18:32:40.706422: step 4047, loss 0.00810386, acc 1
2016-09-07T18:32:41.369418: step 4048, loss 0.0136528, acc 1
2016-09-07T18:32:42.043823: step 4049, loss 0.0858619, acc 0.96
2016-09-07T18:32:42.722228: step 4050, loss 0.0260135, acc 1
2016-09-07T18:32:43.398371: step 4051, loss 0.023847, acc 0.98
2016-09-07T18:32:44.081248: step 4052, loss 0.146454, acc 0.96
2016-09-07T18:32:44.743541: step 4053, loss 0.054688, acc 0.96
2016-09-07T18:32:45.413908: step 4054, loss 0.0019451, acc 1
2016-09-07T18:32:46.066769: step 4055, loss 0.00162657, acc 1
2016-09-07T18:32:46.728968: step 4056, loss 0.0418416, acc 0.98
2016-09-07T18:32:47.401643: step 4057, loss 0.00339169, acc 1
2016-09-07T18:32:48.070462: step 4058, loss 0.0412626, acc 0.98
2016-09-07T18:32:48.742921: step 4059, loss 0.0179038, acc 0.98
2016-09-07T18:32:49.411834: step 4060, loss 0.0189598, acc 0.98
2016-09-07T18:32:50.087733: step 4061, loss 0.0728139, acc 0.98
2016-09-07T18:32:50.781184: step 4062, loss 0.0191437, acc 1
2016-09-07T18:32:51.464636: step 4063, loss 0.0134689, acc 1
2016-09-07T18:32:52.151731: step 4064, loss 0.000252579, acc 1
2016-09-07T18:32:52.819758: step 4065, loss 0.00310024, acc 1
2016-09-07T18:32:53.480568: step 4066, loss 0.016139, acc 1
2016-09-07T18:32:54.171411: step 4067, loss 0.0336106, acc 0.96
2016-09-07T18:32:54.844115: step 4068, loss 0.0850728, acc 0.98
2016-09-07T18:32:55.496581: step 4069, loss 0.00647215, acc 1
2016-09-07T18:32:56.176976: step 4070, loss 0.0464019, acc 0.96
2016-09-07T18:32:56.839937: step 4071, loss 0.0609343, acc 0.96
2016-09-07T18:32:57.533901: step 4072, loss 0.0130039, acc 1
2016-09-07T18:32:58.206034: step 4073, loss 0.0450557, acc 0.96
2016-09-07T18:32:58.566426: step 4074, loss 0.169616, acc 0.916667
2016-09-07T18:32:59.254749: step 4075, loss 0.0298611, acc 0.98
2016-09-07T18:32:59.937896: step 4076, loss 0.0340616, acc 0.98
2016-09-07T18:33:00.642209: step 4077, loss 0.0190449, acc 0.98
2016-09-07T18:33:01.316397: step 4078, loss 0.00119333, acc 1
2016-09-07T18:33:01.997743: step 4079, loss 0.0813662, acc 0.96
2016-09-07T18:33:02.655717: step 4080, loss 0.111613, acc 0.96
2016-09-07T18:33:03.320191: step 4081, loss 0.00442489, acc 1
2016-09-07T18:33:03.996740: step 4082, loss 0.0159544, acc 1
2016-09-07T18:33:04.646896: step 4083, loss 0.0110297, acc 1
2016-09-07T18:33:05.314125: step 4084, loss 0.0290227, acc 1
2016-09-07T18:33:05.990572: step 4085, loss 0.0166939, acc 1
2016-09-07T18:33:06.651316: step 4086, loss 0.036056, acc 0.98
2016-09-07T18:33:07.308664: step 4087, loss 0.0411456, acc 1
2016-09-07T18:33:07.994025: step 4088, loss 0.0360875, acc 0.98
2016-09-07T18:33:08.650421: step 4089, loss 0.0360202, acc 0.98
2016-09-07T18:33:09.305089: step 4090, loss 0.0158406, acc 0.98
2016-09-07T18:33:09.981122: step 4091, loss 0.0315497, acc 1
2016-09-07T18:33:10.642502: step 4092, loss 0.0221751, acc 1
2016-09-07T18:33:11.318548: step 4093, loss 0.0139896, acc 1
2016-09-07T18:33:11.981191: step 4094, loss 0.0190143, acc 0.98
2016-09-07T18:33:12.643642: step 4095, loss 0.0202532, acc 0.98
2016-09-07T18:33:13.323346: step 4096, loss 0.0352738, acc 1
2016-09-07T18:33:13.989770: step 4097, loss 0.131304, acc 0.94
2016-09-07T18:33:14.648287: step 4098, loss 0.0330278, acc 0.98
2016-09-07T18:33:15.314202: step 4099, loss 0.0120119, acc 1
2016-09-07T18:33:15.994551: step 4100, loss 0.0116442, acc 1

Evaluation:
2016-09-07T18:33:18.886895: step 4100, loss 1.73666, acc 0.752

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-4100

2016-09-07T18:33:20.575521: step 4101, loss 0.00915533, acc 1
2016-09-07T18:33:21.272677: step 4102, loss 0.00247557, acc 1
2016-09-07T18:33:21.955124: step 4103, loss 0.0271086, acc 0.98
2016-09-07T18:33:22.616317: step 4104, loss 0.0210568, acc 1
2016-09-07T18:33:23.282308: step 4105, loss 0.0796486, acc 0.96
2016-09-07T18:33:23.937167: step 4106, loss 0.0566507, acc 0.96
2016-09-07T18:33:24.585186: step 4107, loss 0.00298994, acc 1
2016-09-07T18:33:25.225302: step 4108, loss 0.0107642, acc 1
2016-09-07T18:33:25.870949: step 4109, loss 0.00883905, acc 1
2016-09-07T18:33:26.542130: step 4110, loss 0.0178004, acc 1
2016-09-07T18:33:27.210000: step 4111, loss 0.0051813, acc 1
2016-09-07T18:33:27.873270: step 4112, loss 0.00135252, acc 1
2016-09-07T18:33:28.556483: step 4113, loss 0.035228, acc 0.98
2016-09-07T18:33:29.213759: step 4114, loss 0.0432446, acc 0.98
2016-09-07T18:33:29.875154: step 4115, loss 0.0244564, acc 0.98
2016-09-07T18:33:30.546612: step 4116, loss 0.00864315, acc 1
2016-09-07T18:33:31.229512: step 4117, loss 0.000366167, acc 1
2016-09-07T18:33:31.886528: step 4118, loss 0.0380638, acc 0.98
2016-09-07T18:33:32.564608: step 4119, loss 0.0388076, acc 0.98
2016-09-07T18:33:33.231180: step 4120, loss 0.030289, acc 0.98
2016-09-07T18:33:33.891331: step 4121, loss 0.00698734, acc 1
2016-09-07T18:33:34.567610: step 4122, loss 0.066412, acc 0.96
2016-09-07T18:33:35.229835: step 4123, loss 0.00231556, acc 1
2016-09-07T18:33:35.890544: step 4124, loss 0.0100865, acc 1
2016-09-07T18:33:36.553945: step 4125, loss 0.0189534, acc 1
2016-09-07T18:33:37.205990: step 4126, loss 0.0055173, acc 1
2016-09-07T18:33:37.862586: step 4127, loss 0.00498251, acc 1
2016-09-07T18:33:38.516794: step 4128, loss 0.00651262, acc 1
2016-09-07T18:33:39.185389: step 4129, loss 0.0457402, acc 0.96
2016-09-07T18:33:39.845150: step 4130, loss 0.0305434, acc 1
2016-09-07T18:33:40.497615: step 4131, loss 0.0541203, acc 0.96
2016-09-07T18:33:41.158431: step 4132, loss 0.011731, acc 1
2016-09-07T18:33:41.833195: step 4133, loss 0.0039123, acc 1
2016-09-07T18:33:42.501929: step 4134, loss 0.0183505, acc 1
2016-09-07T18:33:43.192313: step 4135, loss 0.0404301, acc 0.98
2016-09-07T18:33:43.865869: step 4136, loss 0.0332087, acc 0.98
2016-09-07T18:33:44.529352: step 4137, loss 0.0485845, acc 0.96
2016-09-07T18:33:45.200411: step 4138, loss 0.0194882, acc 1
2016-09-07T18:33:45.850452: step 4139, loss 0.038076, acc 0.96
2016-09-07T18:33:46.516691: step 4140, loss 0.0104483, acc 1
2016-09-07T18:33:47.175826: step 4141, loss 0.00250239, acc 1
2016-09-07T18:33:47.831487: step 4142, loss 0.00550153, acc 1
2016-09-07T18:33:48.495471: step 4143, loss 0.00780866, acc 1
2016-09-07T18:33:49.175151: step 4144, loss 0.0219872, acc 1
2016-09-07T18:33:49.829036: step 4145, loss 0.00216254, acc 1
2016-09-07T18:33:50.505795: step 4146, loss 0.0493574, acc 0.96
2016-09-07T18:33:51.191942: step 4147, loss 0.0036545, acc 1
2016-09-07T18:33:51.867463: step 4148, loss 0.0264766, acc 0.98
2016-09-07T18:33:52.517852: step 4149, loss 0.00353704, acc 1
2016-09-07T18:33:53.187531: step 4150, loss 0.0398462, acc 0.96
2016-09-07T18:33:53.860018: step 4151, loss 0.03569, acc 0.98
2016-09-07T18:33:54.534248: step 4152, loss 0.0101016, acc 1
2016-09-07T18:33:55.199543: step 4153, loss 0.0219954, acc 0.98
2016-09-07T18:33:55.864591: step 4154, loss 0.0533129, acc 0.96
2016-09-07T18:33:56.521631: step 4155, loss 0.067306, acc 0.98
2016-09-07T18:33:57.172915: step 4156, loss 0.000145979, acc 1
2016-09-07T18:33:57.852533: step 4157, loss 0.0113006, acc 1
2016-09-07T18:33:58.506440: step 4158, loss 0.0087364, acc 1
2016-09-07T18:33:59.178607: step 4159, loss 0.0717372, acc 0.98
2016-09-07T18:33:59.829375: step 4160, loss 0.000260572, acc 1
2016-09-07T18:34:00.535625: step 4161, loss 0.0464944, acc 0.98
2016-09-07T18:34:01.207217: step 4162, loss 0.0246644, acc 1
2016-09-07T18:34:01.898998: step 4163, loss 0.00681012, acc 1
2016-09-07T18:34:02.559786: step 4164, loss 0.0455015, acc 0.96
2016-09-07T18:34:03.225398: step 4165, loss 0.0188055, acc 1
2016-09-07T18:34:03.876933: step 4166, loss 0.0766632, acc 0.94
2016-09-07T18:34:04.542439: step 4167, loss 0.00503227, acc 1
2016-09-07T18:34:05.229924: step 4168, loss 0.00198666, acc 1
2016-09-07T18:34:05.892582: step 4169, loss 0.0459911, acc 0.98
2016-09-07T18:34:06.551339: step 4170, loss 0.00873506, acc 1
2016-09-07T18:34:07.218268: step 4171, loss 0.000543773, acc 1
2016-09-07T18:34:07.868432: step 4172, loss 0.066844, acc 0.98
2016-09-07T18:34:08.520979: step 4173, loss 0.0141866, acc 1
2016-09-07T18:34:09.181693: step 4174, loss 0.00567814, acc 1
2016-09-07T18:34:09.834283: step 4175, loss 0.000700928, acc 1
2016-09-07T18:34:10.488904: step 4176, loss 0.0438994, acc 0.98
2016-09-07T18:34:11.153819: step 4177, loss 0.0312528, acc 0.98
2016-09-07T18:34:11.823521: step 4178, loss 0.0174905, acc 1
2016-09-07T18:34:12.492200: step 4179, loss 0.0693574, acc 0.98
2016-09-07T18:34:13.179884: step 4180, loss 0.0129813, acc 1
2016-09-07T18:34:13.827464: step 4181, loss 0.00537688, acc 1
2016-09-07T18:34:14.484112: step 4182, loss 0.0738668, acc 0.96
2016-09-07T18:34:15.168248: step 4183, loss 0.0161444, acc 1
2016-09-07T18:34:15.830945: step 4184, loss 0.000199529, acc 1
2016-09-07T18:34:16.506290: step 4185, loss 0.00649013, acc 1
2016-09-07T18:34:17.174853: step 4186, loss 0.028334, acc 0.98
2016-09-07T18:34:17.847260: step 4187, loss 0.0330648, acc 1
2016-09-07T18:34:18.514157: step 4188, loss 0.029808, acc 0.98
2016-09-07T18:34:19.167065: step 4189, loss 0.0188273, acc 0.98
2016-09-07T18:34:19.825267: step 4190, loss 0.148263, acc 0.94
2016-09-07T18:34:20.500951: step 4191, loss 0.108198, acc 0.96
2016-09-07T18:34:21.168924: step 4192, loss 0.0160327, acc 0.98
2016-09-07T18:34:21.826513: step 4193, loss 0.00455036, acc 1
2016-09-07T18:34:22.498387: step 4194, loss 0.0468449, acc 0.98
2016-09-07T18:34:23.160902: step 4195, loss 0.0200355, acc 0.98
2016-09-07T18:34:23.849481: step 4196, loss 0.000579558, acc 1
2016-09-07T18:34:24.510861: step 4197, loss 0.00727199, acc 1
2016-09-07T18:34:25.176925: step 4198, loss 0.0374566, acc 0.98
2016-09-07T18:34:25.836912: step 4199, loss 0.0139017, acc 1
2016-09-07T18:34:26.553971: step 4200, loss 0.0357725, acc 0.98

Evaluation:
2016-09-07T18:34:29.408989: step 4200, loss 1.83149, acc 0.755

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-4200

2016-09-07T18:34:31.056660: step 4201, loss 0.220468, acc 0.98
2016-09-07T18:34:31.712429: step 4202, loss 0.0294408, acc 0.98
2016-09-07T18:34:32.374668: step 4203, loss 0.0189276, acc 0.98
2016-09-07T18:34:33.048604: step 4204, loss 0.128443, acc 0.98
2016-09-07T18:34:33.707890: step 4205, loss 0.0924171, acc 0.96
2016-09-07T18:34:34.396754: step 4206, loss 0.0766365, acc 0.96
2016-09-07T18:34:35.067319: step 4207, loss 0.00726747, acc 1
2016-09-07T18:34:35.744793: step 4208, loss 0.0349452, acc 0.98
2016-09-07T18:34:36.391970: step 4209, loss 0.0444438, acc 0.98
2016-09-07T18:34:37.058871: step 4210, loss 0.0765469, acc 0.96
2016-09-07T18:34:37.713007: step 4211, loss 0.0113808, acc 1
2016-09-07T18:34:38.396525: step 4212, loss 0.00458087, acc 1
2016-09-07T18:34:39.068322: step 4213, loss 0.00432186, acc 1
2016-09-07T18:34:39.743163: step 4214, loss 0.0402939, acc 0.98
2016-09-07T18:34:40.432167: step 4215, loss 0.00185656, acc 1
2016-09-07T18:34:41.098869: step 4216, loss 0.0308456, acc 1
2016-09-07T18:34:41.774301: step 4217, loss 0.105821, acc 0.94
2016-09-07T18:34:42.428991: step 4218, loss 0.00575876, acc 1
2016-09-07T18:34:43.115471: step 4219, loss 0.0115413, acc 1
2016-09-07T18:34:43.786969: step 4220, loss 0.00437075, acc 1
2016-09-07T18:34:44.467099: step 4221, loss 0.0421175, acc 0.98
2016-09-07T18:34:45.153740: step 4222, loss 0.04105, acc 0.96
2016-09-07T18:34:45.818137: step 4223, loss 0.0182246, acc 0.98
2016-09-07T18:34:46.484431: step 4224, loss 0.0277296, acc 0.98
2016-09-07T18:34:47.159291: step 4225, loss 0.114254, acc 0.98
2016-09-07T18:34:47.824861: step 4226, loss 0.0283488, acc 0.98
2016-09-07T18:34:48.492329: step 4227, loss 0.0134459, acc 1
2016-09-07T18:34:49.151668: step 4228, loss 0.0322225, acc 0.98
2016-09-07T18:34:49.826087: step 4229, loss 0.0228294, acc 0.98
2016-09-07T18:34:50.497752: step 4230, loss 0.0123985, acc 1
2016-09-07T18:34:51.178695: step 4231, loss 0.00600934, acc 1
2016-09-07T18:34:51.869363: step 4232, loss 0.0253603, acc 0.98
2016-09-07T18:34:52.540554: step 4233, loss 0.00368155, acc 1
2016-09-07T18:34:53.198859: step 4234, loss 0.0294378, acc 0.98
2016-09-07T18:34:53.875385: step 4235, loss 0.0229012, acc 1
2016-09-07T18:34:54.548242: step 4236, loss 0.0234716, acc 0.98
2016-09-07T18:34:55.209196: step 4237, loss 0.0163539, acc 1
2016-09-07T18:34:55.879675: step 4238, loss 0.0145191, acc 1
2016-09-07T18:34:56.531683: step 4239, loss 0.00477879, acc 1
2016-09-07T18:34:57.183475: step 4240, loss 0.0897189, acc 0.96
2016-09-07T18:34:57.846669: step 4241, loss 0.0316659, acc 0.98
2016-09-07T18:34:58.511506: step 4242, loss 0.00672101, acc 1
2016-09-07T18:34:59.166873: step 4243, loss 0.0171932, acc 1
2016-09-07T18:34:59.825682: step 4244, loss 0.13303, acc 0.98
2016-09-07T18:35:00.546509: step 4245, loss 0.0401484, acc 0.98
2016-09-07T18:35:01.218294: step 4246, loss 0.016468, acc 1
2016-09-07T18:35:01.888323: step 4247, loss 0.175806, acc 0.94
2016-09-07T18:35:02.546816: step 4248, loss 0.0143416, acc 1
2016-09-07T18:35:03.194248: step 4249, loss 0.0561899, acc 0.98
2016-09-07T18:35:03.872433: step 4250, loss 0.0310465, acc 1
2016-09-07T18:35:04.542596: step 4251, loss 0.0300742, acc 0.98
2016-09-07T18:35:05.229106: step 4252, loss 0.00797008, acc 1
2016-09-07T18:35:05.888640: step 4253, loss 0.0388565, acc 0.98
2016-09-07T18:35:06.555693: step 4254, loss 0.0148343, acc 1
2016-09-07T18:35:07.241455: step 4255, loss 0.0185401, acc 1
2016-09-07T18:35:07.907665: step 4256, loss 0.00110731, acc 1
2016-09-07T18:35:08.579972: step 4257, loss 0.0196929, acc 0.98
2016-09-07T18:35:09.261920: step 4258, loss 0.0870849, acc 0.96
2016-09-07T18:35:09.941780: step 4259, loss 0.0181521, acc 1
2016-09-07T18:35:10.615157: step 4260, loss 0.00872913, acc 1
2016-09-07T18:35:11.276564: step 4261, loss 0.0221184, acc 0.98
2016-09-07T18:35:11.953775: step 4262, loss 0.000164692, acc 1
2016-09-07T18:35:12.663828: step 4263, loss 0.0278662, acc 1
2016-09-07T18:35:13.306737: step 4264, loss 0.0213888, acc 1
2016-09-07T18:35:13.963221: step 4265, loss 0.0638831, acc 0.98
2016-09-07T18:35:14.627299: step 4266, loss 0.0800963, acc 0.98
2016-09-07T18:35:15.302355: step 4267, loss 0.0869779, acc 0.98
2016-09-07T18:35:15.661153: step 4268, loss 0.029094, acc 1
2016-09-07T18:35:16.335736: step 4269, loss 0.0195851, acc 1
2016-09-07T18:35:17.011658: step 4270, loss 0.00246233, acc 1
2016-09-07T18:35:17.681426: step 4271, loss 0.0974035, acc 0.96
2016-09-07T18:35:18.359837: step 4272, loss 0.000872439, acc 1
2016-09-07T18:35:19.015993: step 4273, loss 0.0134121, acc 1
2016-09-07T18:35:19.697921: step 4274, loss 0.0815859, acc 0.98
2016-09-07T18:35:20.362213: step 4275, loss 0.0435262, acc 0.96
2016-09-07T18:35:21.032044: step 4276, loss 0.0664942, acc 0.98
2016-09-07T18:35:21.696386: step 4277, loss 0.0105853, acc 1
2016-09-07T18:35:22.376002: step 4278, loss 0.0589748, acc 0.96
2016-09-07T18:35:23.038873: step 4279, loss 0.0510646, acc 0.96
2016-09-07T18:35:23.696621: step 4280, loss 0.0872589, acc 0.96
2016-09-07T18:35:24.374350: step 4281, loss 0.0267447, acc 1
2016-09-07T18:35:25.048417: step 4282, loss 0.0550373, acc 0.98
2016-09-07T18:35:25.711972: step 4283, loss 0.00751927, acc 1
2016-09-07T18:35:26.366159: step 4284, loss 0.0161319, acc 1
2016-09-07T18:35:27.023661: step 4285, loss 0.0638126, acc 0.98
2016-09-07T18:35:27.689825: step 4286, loss 0.00119419, acc 1
2016-09-07T18:35:28.369787: step 4287, loss 0.0219608, acc 1
2016-09-07T18:35:29.035513: step 4288, loss 0.0359456, acc 0.98
2016-09-07T18:35:29.710794: step 4289, loss 0.0119928, acc 1
2016-09-07T18:35:30.386765: step 4290, loss 0.0300601, acc 0.98
2016-09-07T18:35:31.048209: step 4291, loss 0.0190138, acc 1
2016-09-07T18:35:31.735073: step 4292, loss 0.054073, acc 0.98
2016-09-07T18:35:32.406055: step 4293, loss 0.0561026, acc 0.96
2016-09-07T18:35:33.079527: step 4294, loss 0.0111271, acc 1
2016-09-07T18:35:33.742552: step 4295, loss 0.00748806, acc 1
2016-09-07T18:35:34.411685: step 4296, loss 0.0183061, acc 1
2016-09-07T18:35:35.077624: step 4297, loss 0.01313, acc 1
2016-09-07T18:35:35.727059: step 4298, loss 0.00295692, acc 1
2016-09-07T18:35:36.392048: step 4299, loss 0.0276216, acc 0.98
2016-09-07T18:35:37.056511: step 4300, loss 0.0787049, acc 0.98

Evaluation:
2016-09-07T18:35:39.930171: step 4300, loss 1.60117, acc 0.76

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-4300

2016-09-07T18:35:41.539454: step 4301, loss 0.0384355, acc 0.98
2016-09-07T18:35:42.208343: step 4302, loss 0.0172836, acc 1
2016-09-07T18:35:42.874720: step 4303, loss 0.00113771, acc 1
2016-09-07T18:35:43.548865: step 4304, loss 0.029583, acc 0.98
2016-09-07T18:35:44.224133: step 4305, loss 0.0156831, acc 0.98
2016-09-07T18:35:44.907161: step 4306, loss 0.0578601, acc 0.96
2016-09-07T18:35:45.591902: step 4307, loss 0.000739433, acc 1
2016-09-07T18:35:46.253542: step 4308, loss 0.00134326, acc 1
2016-09-07T18:35:46.910220: step 4309, loss 0.0184105, acc 0.98
2016-09-07T18:35:47.568108: step 4310, loss 0.0123047, acc 1
2016-09-07T18:35:48.228591: step 4311, loss 0.0370009, acc 0.98
2016-09-07T18:35:48.891222: step 4312, loss 0.0268074, acc 0.98
2016-09-07T18:35:49.551263: step 4313, loss 0.00177779, acc 1
2016-09-07T18:35:50.216061: step 4314, loss 0.0641617, acc 0.96
2016-09-07T18:35:50.890538: step 4315, loss 0.0141416, acc 1
2016-09-07T18:35:51.541573: step 4316, loss 0.0165486, acc 1
2016-09-07T18:35:52.215937: step 4317, loss 0.00292165, acc 1
2016-09-07T18:35:52.895863: step 4318, loss 0.0438361, acc 0.98
2016-09-07T18:35:53.575625: step 4319, loss 0.0128078, acc 1
2016-09-07T18:35:54.258172: step 4320, loss 0.0057686, acc 1
2016-09-07T18:35:54.931756: step 4321, loss 0.012854, acc 1
2016-09-07T18:35:55.598050: step 4322, loss 0.00151921, acc 1
2016-09-07T18:35:56.278310: step 4323, loss 0.0239909, acc 1
2016-09-07T18:35:56.928858: step 4324, loss 0.03249, acc 0.98
2016-09-07T18:35:57.643580: step 4325, loss 0.00593094, acc 1
2016-09-07T18:35:58.375346: step 4326, loss 0.000132067, acc 1
2016-09-07T18:35:59.079017: step 4327, loss 0.0131416, acc 1
2016-09-07T18:35:59.731650: step 4328, loss 0.0232766, acc 0.98
2016-09-07T18:36:00.440876: step 4329, loss 0.0930864, acc 0.96
2016-09-07T18:36:01.096412: step 4330, loss 0.0403336, acc 0.98
2016-09-07T18:36:01.736070: step 4331, loss 0.0297604, acc 1
2016-09-07T18:36:02.407813: step 4332, loss 0.0142406, acc 1
2016-09-07T18:36:03.062075: step 4333, loss 0.0569769, acc 0.96
2016-09-07T18:36:03.733231: step 4334, loss 0.0367024, acc 0.98
2016-09-07T18:36:04.412160: step 4335, loss 0.0563652, acc 0.96
2016-09-07T18:36:05.067030: step 4336, loss 0.0105146, acc 1
2016-09-07T18:36:05.727816: step 4337, loss 0.00203521, acc 1
2016-09-07T18:36:06.400508: step 4338, loss 0.0475123, acc 0.98
2016-09-07T18:36:07.073946: step 4339, loss 0.0033495, acc 1
2016-09-07T18:36:07.733291: step 4340, loss 0.0632859, acc 0.94
2016-09-07T18:36:08.432780: step 4341, loss 0.000130423, acc 1
2016-09-07T18:36:09.116196: step 4342, loss 0.00302113, acc 1
2016-09-07T18:36:09.766925: step 4343, loss 0.128515, acc 0.98
2016-09-07T18:36:10.431676: step 4344, loss 0.0389332, acc 0.96
2016-09-07T18:36:11.099679: step 4345, loss 0.0150138, acc 1
2016-09-07T18:36:11.760006: step 4346, loss 0.0179832, acc 1
2016-09-07T18:36:12.415005: step 4347, loss 0.00913232, acc 1
2016-09-07T18:36:13.079934: step 4348, loss 0.0218503, acc 1
2016-09-07T18:36:13.769612: step 4349, loss 0.021711, acc 1
2016-09-07T18:36:14.428352: step 4350, loss 0.00767055, acc 1
2016-09-07T18:36:15.119120: step 4351, loss 0.0115499, acc 1
2016-09-07T18:36:15.785386: step 4352, loss 0.008309, acc 1
2016-09-07T18:36:16.459956: step 4353, loss 0.000829456, acc 1
2016-09-07T18:36:17.116244: step 4354, loss 0.0271897, acc 0.98
2016-09-07T18:36:17.801151: step 4355, loss 0.0116805, acc 1
2016-09-07T18:36:18.462019: step 4356, loss 0.0449301, acc 0.98
2016-09-07T18:36:19.124754: step 4357, loss 0.0469258, acc 0.96
2016-09-07T18:36:19.787784: step 4358, loss 0.0295471, acc 1
2016-09-07T18:36:20.457412: step 4359, loss 0.0637719, acc 0.98
2016-09-07T18:36:21.110498: step 4360, loss 0.00278952, acc 1
2016-09-07T18:36:21.776602: step 4361, loss 0.0147792, acc 1
2016-09-07T18:36:22.441843: step 4362, loss 0.0226906, acc 0.98
2016-09-07T18:36:23.101688: step 4363, loss 0.0203587, acc 1
2016-09-07T18:36:23.772562: step 4364, loss 0.00555316, acc 1
2016-09-07T18:36:24.474543: step 4365, loss 0.00191023, acc 1
2016-09-07T18:36:25.139575: step 4366, loss 0.0277608, acc 0.98
2016-09-07T18:36:25.817605: step 4367, loss 0.0742021, acc 0.98
2016-09-07T18:36:26.490952: step 4368, loss 0.0206962, acc 1
2016-09-07T18:36:27.168131: step 4369, loss 0.00598066, acc 1
2016-09-07T18:36:27.853489: step 4370, loss 0.00867513, acc 1
2016-09-07T18:36:28.533101: step 4371, loss 0.0148673, acc 1
2016-09-07T18:36:29.196455: step 4372, loss 0.0900766, acc 0.96
2016-09-07T18:36:29.865468: step 4373, loss 0.0452418, acc 0.98
2016-09-07T18:36:30.525713: step 4374, loss 0.00223415, acc 1
2016-09-07T18:36:31.198749: step 4375, loss 0.0399287, acc 0.98
2016-09-07T18:36:31.865319: step 4376, loss 0.0219458, acc 1
2016-09-07T18:36:32.506776: step 4377, loss 0.0736511, acc 0.96
2016-09-07T18:36:33.161557: step 4378, loss 0.00222795, acc 1
2016-09-07T18:36:33.850539: step 4379, loss 0.0439657, acc 0.98
2016-09-07T18:36:34.512373: step 4380, loss 0.0073208, acc 1
2016-09-07T18:36:35.174905: step 4381, loss 0.000638002, acc 1
2016-09-07T18:36:35.855904: step 4382, loss 0.0211222, acc 0.98
2016-09-07T18:36:36.511211: step 4383, loss 0.0107942, acc 1
2016-09-07T18:36:37.179398: step 4384, loss 0.052481, acc 0.94
2016-09-07T18:36:37.864030: step 4385, loss 0.0528921, acc 0.98
2016-09-07T18:36:38.520853: step 4386, loss 0.0505836, acc 0.96
2016-09-07T18:36:39.178045: step 4387, loss 0.011943, acc 1
2016-09-07T18:36:39.857405: step 4388, loss 0.0167739, acc 1
2016-09-07T18:36:40.530796: step 4389, loss 0.0149343, acc 0.98
2016-09-07T18:36:41.203398: step 4390, loss 0.00997123, acc 1
2016-09-07T18:36:41.909211: step 4391, loss 0.027286, acc 0.98
2016-09-07T18:36:42.579581: step 4392, loss 0.0522333, acc 0.98
2016-09-07T18:36:43.249289: step 4393, loss 0.0266084, acc 1
2016-09-07T18:36:43.933336: step 4394, loss 0.0194433, acc 1
2016-09-07T18:36:44.614936: step 4395, loss 0.0156699, acc 1
2016-09-07T18:36:45.275892: step 4396, loss 0.0445761, acc 0.98
2016-09-07T18:36:45.943554: step 4397, loss 0.00352004, acc 1
2016-09-07T18:36:46.618286: step 4398, loss 0.0301456, acc 1
2016-09-07T18:36:47.279478: step 4399, loss 0.0469636, acc 0.98
2016-09-07T18:36:47.934721: step 4400, loss 0.0405892, acc 0.98

Evaluation:
2016-09-07T18:36:50.823548: step 4400, loss 1.83333, acc 0.761

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-4400

2016-09-07T18:36:52.497092: step 4401, loss 0.0175457, acc 1
2016-09-07T18:36:53.162487: step 4402, loss 0.019889, acc 1
2016-09-07T18:36:53.851714: step 4403, loss 0.106334, acc 0.98
2016-09-07T18:36:54.532529: step 4404, loss 0.00453682, acc 1
2016-09-07T18:36:55.204130: step 4405, loss 0.0535319, acc 0.98
2016-09-07T18:36:55.873835: step 4406, loss 0.0210749, acc 0.98
2016-09-07T18:36:56.525100: step 4407, loss 0.00352309, acc 1
2016-09-07T18:36:57.193368: step 4408, loss 0.0413001, acc 0.98
2016-09-07T18:36:57.847911: step 4409, loss 0.000753924, acc 1
2016-09-07T18:36:58.507802: step 4410, loss 0.0315601, acc 0.98
2016-09-07T18:36:59.173113: step 4411, loss 0.0034514, acc 1
2016-09-07T18:36:59.839550: step 4412, loss 0.00458699, acc 1
2016-09-07T18:37:00.549342: step 4413, loss 0.0102479, acc 1
2016-09-07T18:37:01.213938: step 4414, loss 0.00250567, acc 1
2016-09-07T18:37:01.881978: step 4415, loss 0.00114071, acc 1
2016-09-07T18:37:02.534862: step 4416, loss 0.141073, acc 0.94
2016-09-07T18:37:03.207164: step 4417, loss 0.00610066, acc 1
2016-09-07T18:37:03.876895: step 4418, loss 0.00843665, acc 1
2016-09-07T18:37:04.550460: step 4419, loss 0.0313797, acc 0.96
2016-09-07T18:37:05.227625: step 4420, loss 0.0674914, acc 0.94
2016-09-07T18:37:05.912793: step 4421, loss 0.0182044, acc 0.98
2016-09-07T18:37:06.593387: step 4422, loss 0.0188566, acc 1
2016-09-07T18:37:07.246356: step 4423, loss 0.00175891, acc 1
2016-09-07T18:37:07.922519: step 4424, loss 0.0070478, acc 1
2016-09-07T18:37:08.603556: step 4425, loss 0.00116635, acc 1
2016-09-07T18:37:09.276269: step 4426, loss 0.0447246, acc 1
2016-09-07T18:37:09.961875: step 4427, loss 0.0546352, acc 0.98
2016-09-07T18:37:10.641147: step 4428, loss 0.0209269, acc 0.98
2016-09-07T18:37:11.321064: step 4429, loss 0.133344, acc 0.98
2016-09-07T18:37:12.000419: step 4430, loss 0.0136619, acc 1
2016-09-07T18:37:12.673532: step 4431, loss 0.00588848, acc 1
2016-09-07T18:37:13.329808: step 4432, loss 0.0628608, acc 0.98
2016-09-07T18:37:13.993539: step 4433, loss 0.00109143, acc 1
2016-09-07T18:37:14.687104: step 4434, loss 0.0315808, acc 0.98
2016-09-07T18:37:15.359065: step 4435, loss 0.052018, acc 0.98
2016-09-07T18:37:16.004128: step 4436, loss 0.01759, acc 1
2016-09-07T18:37:16.668520: step 4437, loss 0.0258137, acc 1
2016-09-07T18:37:17.342322: step 4438, loss 0.0549641, acc 0.98
2016-09-07T18:37:18.023864: step 4439, loss 0.0243697, acc 0.98
2016-09-07T18:37:18.700690: step 4440, loss 0.0512905, acc 0.98
2016-09-07T18:37:19.383029: step 4441, loss 0.0434671, acc 0.98
2016-09-07T18:37:20.037102: step 4442, loss 0.000133856, acc 1
2016-09-07T18:37:20.687505: step 4443, loss 0.0158043, acc 0.98
2016-09-07T18:37:21.367791: step 4444, loss 0.00659955, acc 1
2016-09-07T18:37:22.064123: step 4445, loss 0.0190174, acc 1
2016-09-07T18:37:22.726860: step 4446, loss 0.010229, acc 1
2016-09-07T18:37:23.388143: step 4447, loss 0.0542964, acc 0.98
2016-09-07T18:37:24.068831: step 4448, loss 0.0139257, acc 1
2016-09-07T18:37:24.740374: step 4449, loss 0.0246958, acc 1
2016-09-07T18:37:25.411750: step 4450, loss 0.030083, acc 0.98
2016-09-07T18:37:26.069546: step 4451, loss 0.0143804, acc 0.98
2016-09-07T18:37:26.729062: step 4452, loss 0.0149219, acc 1
2016-09-07T18:37:27.399121: step 4453, loss 0.0204065, acc 0.98
2016-09-07T18:37:28.098885: step 4454, loss 0.00642522, acc 1
2016-09-07T18:37:28.761732: step 4455, loss 0.0154782, acc 1
2016-09-07T18:37:29.422154: step 4456, loss 0.0096861, acc 1
2016-09-07T18:37:30.100147: step 4457, loss 0.0164763, acc 1
2016-09-07T18:37:30.768509: step 4458, loss 0.0158747, acc 1
2016-09-07T18:37:31.417397: step 4459, loss 0.0493884, acc 0.96
2016-09-07T18:37:32.070239: step 4460, loss 0.00620346, acc 1
2016-09-07T18:37:32.742830: step 4461, loss 0.00259951, acc 1
2016-09-07T18:37:33.102519: step 4462, loss 0.115261, acc 0.916667
2016-09-07T18:37:33.770622: step 4463, loss 0.0396202, acc 0.98
2016-09-07T18:37:34.444989: step 4464, loss 0.0545489, acc 0.96
2016-09-07T18:37:35.120778: step 4465, loss 0.0109315, acc 1
2016-09-07T18:37:35.792547: step 4466, loss 0.0148552, acc 1
2016-09-07T18:37:36.450160: step 4467, loss 0.00444171, acc 1
2016-09-07T18:37:37.131684: step 4468, loss 0.0177065, acc 1
2016-09-07T18:37:37.804966: step 4469, loss 0.0376932, acc 0.98
2016-09-07T18:37:38.455037: step 4470, loss 0.16857, acc 0.96
2016-09-07T18:37:39.141998: step 4471, loss 0.00945621, acc 1
2016-09-07T18:37:39.817859: step 4472, loss 0.00912394, acc 1
2016-09-07T18:37:40.483174: step 4473, loss 0.0366437, acc 0.98
2016-09-07T18:37:41.144850: step 4474, loss 0.0463918, acc 0.98
2016-09-07T18:37:41.803390: step 4475, loss 0.130426, acc 0.94
2016-09-07T18:37:42.473600: step 4476, loss 0.0298259, acc 0.98
2016-09-07T18:37:43.168743: step 4477, loss 0.0198703, acc 0.98
2016-09-07T18:37:43.833742: step 4478, loss 0.00194379, acc 1
2016-09-07T18:37:44.519441: step 4479, loss 0.0220413, acc 0.98
2016-09-07T18:37:45.212527: step 4480, loss 0.0127347, acc 1
2016-09-07T18:37:45.880900: step 4481, loss 0.00188058, acc 1
2016-09-07T18:37:46.552995: step 4482, loss 0.0129605, acc 1
2016-09-07T18:37:47.224643: step 4483, loss 0.0284278, acc 0.98
2016-09-07T18:37:47.899046: step 4484, loss 0.0158139, acc 1
2016-09-07T18:37:48.588772: step 4485, loss 0.12222, acc 0.96
2016-09-07T18:37:49.284178: step 4486, loss 0.00375674, acc 1
2016-09-07T18:37:49.939299: step 4487, loss 0.0400501, acc 0.96
2016-09-07T18:37:50.602441: step 4488, loss 0.0931119, acc 0.96
2016-09-07T18:37:51.283098: step 4489, loss 0.00112836, acc 1
2016-09-07T18:37:51.948335: step 4490, loss 0.0117629, acc 1
2016-09-07T18:37:52.603711: step 4491, loss 0.00104573, acc 1
2016-09-07T18:37:53.276226: step 4492, loss 0.0285892, acc 0.98
2016-09-07T18:37:53.945305: step 4493, loss 0.00745724, acc 1
2016-09-07T18:37:54.608700: step 4494, loss 0.0427561, acc 0.98
2016-09-07T18:37:55.282705: step 4495, loss 0.0104915, acc 1
2016-09-07T18:37:55.926939: step 4496, loss 0.0117937, acc 1
2016-09-07T18:37:56.598002: step 4497, loss 0.046874, acc 0.98
2016-09-07T18:37:57.261876: step 4498, loss 0.0496328, acc 0.98
2016-09-07T18:37:57.945687: step 4499, loss 0.0098297, acc 1
2016-09-07T18:37:58.610406: step 4500, loss 0.0442893, acc 0.98

Evaluation:
2016-09-07T18:38:01.512310: step 4500, loss 1.66943, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-4500

2016-09-07T18:38:03.257493: step 4501, loss 0.0172346, acc 1
2016-09-07T18:38:03.923941: step 4502, loss 0.000106526, acc 1
2016-09-07T18:38:04.593354: step 4503, loss 0.0905212, acc 0.96
2016-09-07T18:38:05.266639: step 4504, loss 0.0432331, acc 0.98
2016-09-07T18:38:05.935061: step 4505, loss 0.00582966, acc 1
2016-09-07T18:38:06.605450: step 4506, loss 0.0225719, acc 0.98
2016-09-07T18:38:07.255256: step 4507, loss 0.0871175, acc 0.98
2016-09-07T18:38:07.926536: step 4508, loss 0.0158761, acc 1
2016-09-07T18:38:08.602428: step 4509, loss 0.014794, acc 1
2016-09-07T18:38:09.275384: step 4510, loss 0.0424067, acc 0.98
2016-09-07T18:38:09.943984: step 4511, loss 0.00466023, acc 1
2016-09-07T18:38:10.601870: step 4512, loss 0.0801825, acc 0.94
2016-09-07T18:38:11.263152: step 4513, loss 0.0384048, acc 0.98
2016-09-07T18:38:11.934194: step 4514, loss 0.0690483, acc 0.98
2016-09-07T18:38:12.581749: step 4515, loss 0.00816007, acc 1
2016-09-07T18:38:13.286500: step 4516, loss 0.0345421, acc 0.98
2016-09-07T18:38:13.942611: step 4517, loss 0.00445784, acc 1
2016-09-07T18:38:14.597022: step 4518, loss 0.00636005, acc 1
2016-09-07T18:38:15.259949: step 4519, loss 0.0429339, acc 0.98
2016-09-07T18:38:15.916875: step 4520, loss 0.000125426, acc 1
2016-09-07T18:38:16.586567: step 4521, loss 0.0645871, acc 0.96
2016-09-07T18:38:17.241119: step 4522, loss 0.00814419, acc 1
2016-09-07T18:38:17.902398: step 4523, loss 0.016461, acc 1
2016-09-07T18:38:18.569509: step 4524, loss 0.00233998, acc 1
2016-09-07T18:38:19.243127: step 4525, loss 0.0267407, acc 0.98
2016-09-07T18:38:19.910467: step 4526, loss 0.061756, acc 0.98
2016-09-07T18:38:20.571487: step 4527, loss 0.0147749, acc 1
2016-09-07T18:38:21.238636: step 4528, loss 0.0721436, acc 0.96
2016-09-07T18:38:21.900399: step 4529, loss 0.0141227, acc 1
2016-09-07T18:38:22.574275: step 4530, loss 6.5842e-05, acc 1
2016-09-07T18:38:23.243004: step 4531, loss 0.0064914, acc 1
2016-09-07T18:38:23.934981: step 4532, loss 0.0627784, acc 0.98
2016-09-07T18:38:24.614840: step 4533, loss 0.130916, acc 0.98
2016-09-07T18:38:25.265363: step 4534, loss 0.00618395, acc 1
2016-09-07T18:38:25.917640: step 4535, loss 0.0118286, acc 1
2016-09-07T18:38:26.580266: step 4536, loss 0.0489489, acc 0.96
2016-09-07T18:38:27.266696: step 4537, loss 0.00608493, acc 1
2016-09-07T18:38:27.940296: step 4538, loss 0.0709723, acc 0.94
2016-09-07T18:38:28.609675: step 4539, loss 0.11811, acc 0.98
2016-09-07T18:38:29.267989: step 4540, loss 0.0184928, acc 1
2016-09-07T18:38:29.932862: step 4541, loss 0.0501036, acc 0.96
2016-09-07T18:38:30.609801: step 4542, loss 0.0294641, acc 0.98
2016-09-07T18:38:31.279923: step 4543, loss 0.00684212, acc 1
2016-09-07T18:38:31.931907: step 4544, loss 0.0576606, acc 0.98
2016-09-07T18:38:32.616734: step 4545, loss 0.00160226, acc 1
2016-09-07T18:38:33.295009: step 4546, loss 0.0378847, acc 0.98
2016-09-07T18:38:33.977185: step 4547, loss 0.00426372, acc 1
2016-09-07T18:38:34.649198: step 4548, loss 0.0028469, acc 1
2016-09-07T18:38:35.308392: step 4549, loss 0.0333003, acc 0.98
2016-09-07T18:38:35.985260: step 4550, loss 0.065149, acc 0.96
2016-09-07T18:38:36.661244: step 4551, loss 0.126264, acc 0.94
2016-09-07T18:38:37.324944: step 4552, loss 0.0132733, acc 1
2016-09-07T18:38:37.988280: step 4553, loss 0.00439976, acc 1
2016-09-07T18:38:38.683143: step 4554, loss 0.0567275, acc 0.98
2016-09-07T18:38:39.355373: step 4555, loss 0.0405782, acc 0.98
2016-09-07T18:38:40.014041: step 4556, loss 0.0410862, acc 0.98
2016-09-07T18:38:40.689625: step 4557, loss 0.0264045, acc 1
2016-09-07T18:38:41.360008: step 4558, loss 0.0279287, acc 1
2016-09-07T18:38:42.002164: step 4559, loss 0.0299875, acc 0.98
2016-09-07T18:38:42.674965: step 4560, loss 0.0227881, acc 0.98
2016-09-07T18:38:43.353494: step 4561, loss 0.0454966, acc 0.98
2016-09-07T18:38:44.019568: step 4562, loss 0.0315473, acc 1
2016-09-07T18:38:44.693948: step 4563, loss 0.0228174, acc 1
2016-09-07T18:38:45.356541: step 4564, loss 0.0220244, acc 0.98
2016-09-07T18:38:46.007456: step 4565, loss 0.0226043, acc 0.98
2016-09-07T18:38:46.647797: step 4566, loss 0.0136205, acc 1
2016-09-07T18:38:47.312869: step 4567, loss 0.063718, acc 0.96
2016-09-07T18:38:47.983694: step 4568, loss 0.0154308, acc 1
2016-09-07T18:38:48.641096: step 4569, loss 0.0445669, acc 0.98
2016-09-07T18:38:49.320231: step 4570, loss 0.029419, acc 0.98
2016-09-07T18:38:50.020413: step 4571, loss 0.00173705, acc 1
2016-09-07T18:38:50.701449: step 4572, loss 0.0147719, acc 1
2016-09-07T18:38:51.370540: step 4573, loss 0.0397673, acc 0.96
2016-09-07T18:38:52.045975: step 4574, loss 0.00529999, acc 1
2016-09-07T18:38:52.716315: step 4575, loss 0.0161932, acc 1
2016-09-07T18:38:53.393739: step 4576, loss 0.040098, acc 0.98
2016-09-07T18:38:54.055994: step 4577, loss 0.00393052, acc 1
2016-09-07T18:38:54.726318: step 4578, loss 0.00628256, acc 1
2016-09-07T18:38:55.402226: step 4579, loss 0.0116167, acc 1
2016-09-07T18:38:56.061974: step 4580, loss 0.0114643, acc 1
2016-09-07T18:38:56.720265: step 4581, loss 0.06601, acc 0.98
2016-09-07T18:38:57.416280: step 4582, loss 0.0957829, acc 0.96
2016-09-07T18:38:58.101805: step 4583, loss 0.0152641, acc 1
2016-09-07T18:38:58.773751: step 4584, loss 0.0315379, acc 0.96
2016-09-07T18:38:59.457753: step 4585, loss 0.00117313, acc 1
2016-09-07T18:39:00.126274: step 4586, loss 0.013537, acc 1
2016-09-07T18:39:00.832793: step 4587, loss 0.0612751, acc 0.98
2016-09-07T18:39:01.512579: step 4588, loss 0.0262351, acc 0.98
2016-09-07T18:39:02.175282: step 4589, loss 0.0216303, acc 0.98
2016-09-07T18:39:02.841973: step 4590, loss 0.00935936, acc 1
2016-09-07T18:39:03.489968: step 4591, loss 0.00934858, acc 1
2016-09-07T18:39:04.148531: step 4592, loss 0.00621055, acc 1
2016-09-07T18:39:04.824489: step 4593, loss 0.0130629, acc 1
2016-09-07T18:39:05.512430: step 4594, loss 0.0228256, acc 1
2016-09-07T18:39:06.180383: step 4595, loss 0.040381, acc 0.98
2016-09-07T18:39:06.833528: step 4596, loss 0.0180431, acc 1
2016-09-07T18:39:07.484034: step 4597, loss 0.0246049, acc 0.98
2016-09-07T18:39:08.150687: step 4598, loss 0.0202897, acc 1
2016-09-07T18:39:08.817198: step 4599, loss 0.00147239, acc 1
2016-09-07T18:39:09.483058: step 4600, loss 0.00707373, acc 1

Evaluation:
2016-09-07T18:39:12.356308: step 4600, loss 1.94147, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-4600

2016-09-07T18:39:13.986118: step 4601, loss 0.00750376, acc 1
2016-09-07T18:39:14.656248: step 4602, loss 0.112394, acc 0.98
2016-09-07T18:39:15.307224: step 4603, loss 0.00521497, acc 1
2016-09-07T18:39:15.972000: step 4604, loss 0.00525229, acc 1
2016-09-07T18:39:16.657979: step 4605, loss 0.0149583, acc 1
2016-09-07T18:39:17.331395: step 4606, loss 0.0181624, acc 0.98
2016-09-07T18:39:17.981056: step 4607, loss 0.106338, acc 0.94
2016-09-07T18:39:18.648953: step 4608, loss 0.0887706, acc 0.98
2016-09-07T18:39:19.316158: step 4609, loss 0.00798483, acc 1
2016-09-07T18:39:19.990555: step 4610, loss 0.118674, acc 0.98
2016-09-07T18:39:20.659143: step 4611, loss 0.0222633, acc 0.98
2016-09-07T18:39:21.328661: step 4612, loss 0.0208829, acc 0.98
2016-09-07T18:39:22.008472: step 4613, loss 0.0204686, acc 0.98
2016-09-07T18:39:22.690700: step 4614, loss 0.0200041, acc 1
2016-09-07T18:39:23.352267: step 4615, loss 0.0158776, acc 0.98
2016-09-07T18:39:24.026055: step 4616, loss 0.0266923, acc 1
2016-09-07T18:39:24.695696: step 4617, loss 0.0643557, acc 0.96
2016-09-07T18:39:25.370919: step 4618, loss 0.00581769, acc 1
2016-09-07T18:39:26.062538: step 4619, loss 0.000902319, acc 1
2016-09-07T18:39:26.741445: step 4620, loss 0.0383085, acc 0.96
2016-09-07T18:39:27.408927: step 4621, loss 0.00140398, acc 1
2016-09-07T18:39:28.078502: step 4622, loss 0.00646377, acc 1
2016-09-07T18:39:28.739309: step 4623, loss 0.0286521, acc 0.98
2016-09-07T18:39:29.410779: step 4624, loss 0.0230125, acc 0.98
2016-09-07T18:39:30.106107: step 4625, loss 0.00587622, acc 1
2016-09-07T18:39:30.777665: step 4626, loss 0.0300363, acc 1
2016-09-07T18:39:31.425868: step 4627, loss 0.0135543, acc 1
2016-09-07T18:39:32.095695: step 4628, loss 0.0108905, acc 1
2016-09-07T18:39:32.743430: step 4629, loss 0.00501136, acc 1
2016-09-07T18:39:33.394104: step 4630, loss 0.0545049, acc 0.96
2016-09-07T18:39:34.063736: step 4631, loss 0.0515637, acc 0.98
2016-09-07T18:39:34.713365: step 4632, loss 0.0249502, acc 1
2016-09-07T18:39:35.375046: step 4633, loss 0.00215065, acc 1
2016-09-07T18:39:36.042609: step 4634, loss 0.0126249, acc 1
2016-09-07T18:39:36.719711: step 4635, loss 0.0202455, acc 1
2016-09-07T18:39:37.389051: step 4636, loss 0.0625387, acc 0.96
2016-09-07T18:39:38.054331: step 4637, loss 0.0888721, acc 0.98
2016-09-07T18:39:38.711491: step 4638, loss 0.0107354, acc 1
2016-09-07T18:39:39.381276: step 4639, loss 0.0214108, acc 1
2016-09-07T18:39:40.033334: step 4640, loss 0.0174748, acc 1
2016-09-07T18:39:40.697698: step 4641, loss 0.017917, acc 1
2016-09-07T18:39:41.358819: step 4642, loss 0.0112642, acc 1
2016-09-07T18:39:42.021256: step 4643, loss 0.0319406, acc 0.98
2016-09-07T18:39:42.714610: step 4644, loss 0.071383, acc 0.96
2016-09-07T18:39:43.379965: step 4645, loss 0.0020808, acc 1
2016-09-07T18:39:44.037934: step 4646, loss 0.00139576, acc 1
2016-09-07T18:39:44.713802: step 4647, loss 0.00265037, acc 1
2016-09-07T18:39:45.361773: step 4648, loss 0.0393203, acc 0.98
2016-09-07T18:39:46.022020: step 4649, loss 0.0159286, acc 1
2016-09-07T18:39:46.671097: step 4650, loss 0.0389142, acc 0.98
2016-09-07T18:39:47.326781: step 4651, loss 0.0376034, acc 0.98
2016-09-07T18:39:47.998694: step 4652, loss 0.0227138, acc 1
2016-09-07T18:39:48.674455: step 4653, loss 0.0306607, acc 1
2016-09-07T18:39:49.344676: step 4654, loss 0.0632309, acc 0.98
2016-09-07T18:39:50.016870: step 4655, loss 0.0145821, acc 1
2016-09-07T18:39:50.370606: step 4656, loss 0.0105143, acc 1
2016-09-07T18:39:51.038049: step 4657, loss 0.00801013, acc 1
2016-09-07T18:39:51.682414: step 4658, loss 0.0495845, acc 0.98
2016-09-07T18:39:52.331807: step 4659, loss 0.00186966, acc 1
2016-09-07T18:39:52.987210: step 4660, loss 0.0887555, acc 0.96
2016-09-07T18:39:53.670515: step 4661, loss 0.0112776, acc 1
2016-09-07T18:39:54.322413: step 4662, loss 0.0429915, acc 0.98
2016-09-07T18:39:54.987806: step 4663, loss 0.0481087, acc 0.98
2016-09-07T18:39:55.673195: step 4664, loss 0.00445818, acc 1
2016-09-07T18:39:56.351051: step 4665, loss 0.00122002, acc 1
2016-09-07T18:39:57.018242: step 4666, loss 0.0200407, acc 1
2016-09-07T18:39:57.670927: step 4667, loss 0.0115147, acc 1
2016-09-07T18:39:58.345651: step 4668, loss 0.0485557, acc 0.98
2016-09-07T18:39:59.014630: step 4669, loss 0.0658719, acc 0.96
2016-09-07T18:39:59.685235: step 4670, loss 0.0429288, acc 0.96
2016-09-07T18:40:00.386884: step 4671, loss 0.017327, acc 1
2016-09-07T18:40:01.041915: step 4672, loss 0.00986272, acc 1
2016-09-07T18:40:01.714500: step 4673, loss 0.0235216, acc 1
2016-09-07T18:40:02.366866: step 4674, loss 0.00097249, acc 1
2016-09-07T18:40:03.025738: step 4675, loss 0.045643, acc 0.96
2016-09-07T18:40:03.684434: step 4676, loss 0.0947924, acc 0.98
2016-09-07T18:40:04.345046: step 4677, loss 0.0059586, acc 1
2016-09-07T18:40:05.016804: step 4678, loss 0.0119182, acc 1
2016-09-07T18:40:05.678303: step 4679, loss 0.000255187, acc 1
2016-09-07T18:40:06.353157: step 4680, loss 0.036855, acc 0.98
2016-09-07T18:40:07.027600: step 4681, loss 0.00844701, acc 1
2016-09-07T18:40:07.711514: step 4682, loss 0.0246517, acc 1
2016-09-07T18:40:08.400744: step 4683, loss 0.0022812, acc 1
2016-09-07T18:40:09.070367: step 4684, loss 0.00179677, acc 1
2016-09-07T18:40:09.729000: step 4685, loss 0.0258284, acc 1
2016-09-07T18:40:10.394054: step 4686, loss 0.0379227, acc 0.98
2016-09-07T18:40:11.068744: step 4687, loss 0.099358, acc 0.94
2016-09-07T18:40:11.729522: step 4688, loss 0.0409625, acc 0.96
2016-09-07T18:40:12.405128: step 4689, loss 0.0112101, acc 1
2016-09-07T18:40:13.059103: step 4690, loss 0.0452381, acc 0.98
2016-09-07T18:40:13.721863: step 4691, loss 0.0035494, acc 1
2016-09-07T18:40:14.401054: step 4692, loss 0.0176175, acc 0.98
2016-09-07T18:40:15.053558: step 4693, loss 0.0814616, acc 0.98
2016-09-07T18:40:15.716424: step 4694, loss 0.00456385, acc 1
2016-09-07T18:40:16.394368: step 4695, loss 0.00558638, acc 1
2016-09-07T18:40:17.043418: step 4696, loss 0.00630863, acc 1
2016-09-07T18:40:17.700309: step 4697, loss 0.0362529, acc 1
2016-09-07T18:40:18.358602: step 4698, loss 0.00632787, acc 1
2016-09-07T18:40:19.012270: step 4699, loss 0.0188555, acc 1
2016-09-07T18:40:19.668938: step 4700, loss 0.0198809, acc 1

Evaluation:
2016-09-07T18:40:22.556895: step 4700, loss 1.98359, acc 0.746

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-4700

2016-09-07T18:40:24.161816: step 4701, loss 0.015184, acc 1
2016-09-07T18:40:24.834921: step 4702, loss 0.0335237, acc 0.98
2016-09-07T18:40:25.490221: step 4703, loss 0.0764232, acc 0.96
2016-09-07T18:40:26.151524: step 4704, loss 0.0257894, acc 1
2016-09-07T18:40:26.811113: step 4705, loss 0.00103261, acc 1
2016-09-07T18:40:27.470303: step 4706, loss 0.029412, acc 0.98
2016-09-07T18:40:28.173491: step 4707, loss 0.0346481, acc 0.98
2016-09-07T18:40:28.842874: step 4708, loss 0.0117475, acc 1
2016-09-07T18:40:29.501790: step 4709, loss 0.00368813, acc 1
2016-09-07T18:40:30.162652: step 4710, loss 0.0194522, acc 1
2016-09-07T18:40:30.829050: step 4711, loss 0.000459816, acc 1
2016-09-07T18:40:31.485934: step 4712, loss 0.00604128, acc 1
2016-09-07T18:40:32.145777: step 4713, loss 0.481157, acc 0.94
2016-09-07T18:40:32.815066: step 4714, loss 0.000598611, acc 1
2016-09-07T18:40:33.474936: step 4715, loss 0.0136669, acc 1
2016-09-07T18:40:34.149634: step 4716, loss 0.0199871, acc 1
2016-09-07T18:40:34.820085: step 4717, loss 0.0314671, acc 1
2016-09-07T18:40:35.481138: step 4718, loss 0.0253798, acc 0.98
2016-09-07T18:40:36.149655: step 4719, loss 0.0410957, acc 0.98
2016-09-07T18:40:36.805629: step 4720, loss 0.0153129, acc 1
2016-09-07T18:40:37.475277: step 4721, loss 0.0525566, acc 0.96
2016-09-07T18:40:38.142863: step 4722, loss 0.0156474, acc 1
2016-09-07T18:40:38.799830: step 4723, loss 0.0999796, acc 0.96
2016-09-07T18:40:39.478504: step 4724, loss 0.0121615, acc 1
2016-09-07T18:40:40.149891: step 4725, loss 0.0413049, acc 0.98
2016-09-07T18:40:40.828407: step 4726, loss 0.0286039, acc 0.98
2016-09-07T18:40:41.508224: step 4727, loss 0.00778953, acc 1
2016-09-07T18:40:42.188987: step 4728, loss 0.028399, acc 0.98
2016-09-07T18:40:42.839466: step 4729, loss 0.00139384, acc 1
2016-09-07T18:40:43.513656: step 4730, loss 0.000417485, acc 1
2016-09-07T18:40:44.157595: step 4731, loss 0.0229298, acc 0.98
2016-09-07T18:40:44.810297: step 4732, loss 0.043151, acc 0.98
2016-09-07T18:40:45.491703: step 4733, loss 0.00183607, acc 1
2016-09-07T18:40:46.142045: step 4734, loss 0.00389033, acc 1
2016-09-07T18:40:46.824500: step 4735, loss 0.0238479, acc 1
2016-09-07T18:40:47.494978: step 4736, loss 0.0322651, acc 0.98
2016-09-07T18:40:48.177872: step 4737, loss 0.0102396, acc 1
2016-09-07T18:40:48.842228: step 4738, loss 0.00601134, acc 1
2016-09-07T18:40:49.499836: step 4739, loss 0.0533383, acc 0.96
2016-09-07T18:40:50.163321: step 4740, loss 0.00252919, acc 1
2016-09-07T18:40:50.815618: step 4741, loss 0.0165252, acc 1
2016-09-07T18:40:51.504133: step 4742, loss 0.000415842, acc 1
2016-09-07T18:40:52.168099: step 4743, loss 0.00673026, acc 1
2016-09-07T18:40:52.830746: step 4744, loss 0.00645778, acc 1
2016-09-07T18:40:53.493327: step 4745, loss 0.00518296, acc 1
2016-09-07T18:40:54.164428: step 4746, loss 0.0132904, acc 1
2016-09-07T18:40:54.862723: step 4747, loss 0.00507143, acc 1
2016-09-07T18:40:55.531779: step 4748, loss 0.0351704, acc 0.98
2016-09-07T18:40:56.196601: step 4749, loss 0.0087581, acc 1
2016-09-07T18:40:56.869863: step 4750, loss 0.0045647, acc 1
2016-09-07T18:40:57.530018: step 4751, loss 0.0913529, acc 0.96
2016-09-07T18:40:58.197211: step 4752, loss 0.00438426, acc 1
2016-09-07T18:40:58.859210: step 4753, loss 0.0122945, acc 1
2016-09-07T18:40:59.516599: step 4754, loss 0.0240254, acc 1
2016-09-07T18:41:00.166568: step 4755, loss 0.00339189, acc 1
2016-09-07T18:41:00.881528: step 4756, loss 0.0481828, acc 0.98
2016-09-07T18:41:01.564445: step 4757, loss 0.00282819, acc 1
2016-09-07T18:41:02.244440: step 4758, loss 0.017124, acc 1
2016-09-07T18:41:02.910635: step 4759, loss 0.034423, acc 0.96
2016-09-07T18:41:03.584110: step 4760, loss 0.0270869, acc 0.98
2016-09-07T18:41:04.254389: step 4761, loss 0.00136042, acc 1
2016-09-07T18:41:04.914071: step 4762, loss 0.0200525, acc 1
2016-09-07T18:41:05.569817: step 4763, loss 0.00825814, acc 1
2016-09-07T18:41:06.238066: step 4764, loss 0.028536, acc 0.98
2016-09-07T18:41:06.903265: step 4765, loss 0.00155199, acc 1
2016-09-07T18:41:07.563343: step 4766, loss 0.0424937, acc 0.96
2016-09-07T18:41:08.246674: step 4767, loss 0.0373029, acc 0.98
2016-09-07T18:41:08.903173: step 4768, loss 0.000390618, acc 1
2016-09-07T18:41:09.583354: step 4769, loss 0.0263326, acc 1
2016-09-07T18:41:10.252125: step 4770, loss 0.0575726, acc 0.96
2016-09-07T18:41:10.903929: step 4771, loss 0.0114077, acc 1
2016-09-07T18:41:11.579390: step 4772, loss 0.0969959, acc 0.96
2016-09-07T18:41:12.273343: step 4773, loss 0.0142099, acc 1
2016-09-07T18:41:12.953759: step 4774, loss 0.0411992, acc 0.98
2016-09-07T18:41:13.632134: step 4775, loss 0.0214773, acc 0.98
2016-09-07T18:41:14.293529: step 4776, loss 0.0150168, acc 1
2016-09-07T18:41:14.944516: step 4777, loss 0.0263614, acc 0.98
2016-09-07T18:41:15.616553: step 4778, loss 0.00402882, acc 1
2016-09-07T18:41:16.296852: step 4779, loss 0.0436629, acc 0.98
2016-09-07T18:41:16.960360: step 4780, loss 0.00710715, acc 1
2016-09-07T18:41:17.656895: step 4781, loss 0.0239319, acc 1
2016-09-07T18:41:18.322154: step 4782, loss 0.0180509, acc 1
2016-09-07T18:41:18.979547: step 4783, loss 0.0474459, acc 0.98
2016-09-07T18:41:19.648324: step 4784, loss 9.52809e-05, acc 1
2016-09-07T18:41:20.288369: step 4785, loss 0.0123164, acc 1
2016-09-07T18:41:20.942329: step 4786, loss 0.0434216, acc 0.98
2016-09-07T18:41:21.598351: step 4787, loss 0.0399217, acc 0.98
2016-09-07T18:41:22.251896: step 4788, loss 0.226217, acc 0.96
2016-09-07T18:41:22.915525: step 4789, loss 0.155809, acc 0.98
2016-09-07T18:41:23.577130: step 4790, loss 0.0540088, acc 0.96
2016-09-07T18:41:24.235661: step 4791, loss 0.0367598, acc 1
2016-09-07T18:41:24.899630: step 4792, loss 0.0161981, acc 0.98
2016-09-07T18:41:25.585282: step 4793, loss 0.0320579, acc 0.96
2016-09-07T18:41:26.265095: step 4794, loss 0.0153253, acc 0.98
2016-09-07T18:41:26.926818: step 4795, loss 0.000367591, acc 1
2016-09-07T18:41:27.623171: step 4796, loss 0.0396146, acc 0.98
2016-09-07T18:41:28.302726: step 4797, loss 0.0253187, acc 0.98
2016-09-07T18:41:28.966749: step 4798, loss 0.00500231, acc 1
2016-09-07T18:41:29.633111: step 4799, loss 0.0271277, acc 0.98
2016-09-07T18:41:30.298376: step 4800, loss 8.42343e-05, acc 1

Evaluation:
2016-09-07T18:41:33.178146: step 4800, loss 1.89456, acc 0.752

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-4800

2016-09-07T18:41:34.789495: step 4801, loss 0.0521994, acc 0.96
2016-09-07T18:41:35.464804: step 4802, loss 0.0153015, acc 1
2016-09-07T18:41:36.146599: step 4803, loss 0.0121855, acc 1
2016-09-07T18:41:36.809217: step 4804, loss 0.00870968, acc 1
2016-09-07T18:41:37.473085: step 4805, loss 0.00095006, acc 1
2016-09-07T18:41:38.130026: step 4806, loss 0.00464118, acc 1
2016-09-07T18:41:38.798275: step 4807, loss 0.0195663, acc 0.98
2016-09-07T18:41:39.464937: step 4808, loss 0.0412768, acc 0.98
2016-09-07T18:41:40.131651: step 4809, loss 0.0275574, acc 1
2016-09-07T18:41:40.793365: step 4810, loss 0.0525956, acc 0.98
2016-09-07T18:41:41.438888: step 4811, loss 0.0220634, acc 0.98
2016-09-07T18:41:42.084592: step 4812, loss 0.002931, acc 1
2016-09-07T18:41:42.742683: step 4813, loss 0.00471562, acc 1
2016-09-07T18:41:43.423353: step 4814, loss 0.0506973, acc 0.98
2016-09-07T18:41:44.097216: step 4815, loss 0.0360536, acc 0.98
2016-09-07T18:41:44.751886: step 4816, loss 0.00265563, acc 1
2016-09-07T18:41:45.395892: step 4817, loss 0.0209475, acc 0.98
2016-09-07T18:41:46.070078: step 4818, loss 0.0414667, acc 0.98
2016-09-07T18:41:46.750862: step 4819, loss 0.0358243, acc 0.98
2016-09-07T18:41:47.420797: step 4820, loss 0.068087, acc 0.96
2016-09-07T18:41:48.084096: step 4821, loss 0.000573103, acc 1
2016-09-07T18:41:48.744225: step 4822, loss 0.0902112, acc 0.96
2016-09-07T18:41:49.420464: step 4823, loss 0.0677951, acc 0.96
2016-09-07T18:41:50.076031: step 4824, loss 0.00997082, acc 1
2016-09-07T18:41:50.746527: step 4825, loss 0.0211115, acc 0.98
2016-09-07T18:41:51.405511: step 4826, loss 0.0396112, acc 0.98
2016-09-07T18:41:52.061515: step 4827, loss 0.0445657, acc 0.98
2016-09-07T18:41:52.724622: step 4828, loss 0.0191631, acc 0.98
2016-09-07T18:41:53.387374: step 4829, loss 0.00517021, acc 1
2016-09-07T18:41:54.049943: step 4830, loss 0.00152577, acc 1
2016-09-07T18:41:54.712498: step 4831, loss 0.0469078, acc 0.98
2016-09-07T18:41:55.359712: step 4832, loss 0.0564416, acc 0.98
2016-09-07T18:41:56.028961: step 4833, loss 0.00158503, acc 1
2016-09-07T18:41:56.667728: step 4834, loss 0.000700537, acc 1
2016-09-07T18:41:57.369322: step 4835, loss 0.0390652, acc 0.98
2016-09-07T18:41:58.030846: step 4836, loss 0.0341597, acc 0.98
2016-09-07T18:41:58.686141: step 4837, loss 0.0420427, acc 0.96
2016-09-07T18:41:59.358414: step 4838, loss 0.000739287, acc 1
2016-09-07T18:42:00.033365: step 4839, loss 0.013655, acc 1
2016-09-07T18:42:00.755987: step 4840, loss 0.00790458, acc 1
2016-09-07T18:42:01.405952: step 4841, loss 0.00247501, acc 1
2016-09-07T18:42:02.069018: step 4842, loss 0.0158307, acc 1
2016-09-07T18:42:02.742699: step 4843, loss 0.0734911, acc 0.96
2016-09-07T18:42:03.410601: step 4844, loss 0.00825579, acc 1
2016-09-07T18:42:04.080525: step 4845, loss 0.0144455, acc 1
2016-09-07T18:42:04.749399: step 4846, loss 0.171556, acc 0.98
2016-09-07T18:42:05.432090: step 4847, loss 0.0155126, acc 1
2016-09-07T18:42:06.104045: step 4848, loss 0.0114669, acc 1
2016-09-07T18:42:06.782030: step 4849, loss 0.00232598, acc 1
2016-09-07T18:42:07.139009: step 4850, loss 0.0415056, acc 1
2016-09-07T18:42:07.825214: step 4851, loss 0.0703051, acc 0.94
2016-09-07T18:42:08.505122: step 4852, loss 0.0282705, acc 1
2016-09-07T18:42:09.163987: step 4853, loss 0.0525132, acc 0.98
2016-09-07T18:42:09.817193: step 4854, loss 0.0336944, acc 0.98
2016-09-07T18:42:10.484539: step 4855, loss 0.0223774, acc 0.98
2016-09-07T18:42:11.136866: step 4856, loss 0.0556473, acc 0.98
2016-09-07T18:42:11.794683: step 4857, loss 0.0112855, acc 1
2016-09-07T18:42:12.452329: step 4858, loss 0.0161685, acc 1
2016-09-07T18:42:13.115372: step 4859, loss 0.0131751, acc 1
2016-09-07T18:42:13.794857: step 4860, loss 0.0138956, acc 1
2016-09-07T18:42:14.456374: step 4861, loss 0.0146979, acc 1
2016-09-07T18:42:15.135674: step 4862, loss 0.0205081, acc 0.98
2016-09-07T18:42:15.805816: step 4863, loss 0.0564537, acc 0.96
2016-09-07T18:42:16.480506: step 4864, loss 0.0796556, acc 0.96
2016-09-07T18:42:17.143742: step 4865, loss 0.00214385, acc 1
2016-09-07T18:42:17.817510: step 4866, loss 0.0833372, acc 0.96
2016-09-07T18:42:18.494082: step 4867, loss 0.0155561, acc 1
2016-09-07T18:42:19.168125: step 4868, loss 0.00369706, acc 1
2016-09-07T18:42:19.824933: step 4869, loss 0.0715792, acc 0.94
2016-09-07T18:42:20.475480: step 4870, loss 0.0300057, acc 0.98
2016-09-07T18:42:21.162207: step 4871, loss 0.0329052, acc 0.98
2016-09-07T18:42:21.827264: step 4872, loss 0.0166143, acc 1
2016-09-07T18:42:22.492823: step 4873, loss 0.0114027, acc 1
2016-09-07T18:42:23.143996: step 4874, loss 0.028387, acc 0.98
2016-09-07T18:42:23.821890: step 4875, loss 0.00894629, acc 1
2016-09-07T18:42:24.495666: step 4876, loss 0.0022576, acc 1
2016-09-07T18:42:25.165079: step 4877, loss 0.00124248, acc 1
2016-09-07T18:42:25.837914: step 4878, loss 0.0510708, acc 0.98
2016-09-07T18:42:26.513733: step 4879, loss 0.0468778, acc 0.98
2016-09-07T18:42:27.208908: step 4880, loss 0.0443807, acc 0.96
2016-09-07T18:42:27.890423: step 4881, loss 0.0243863, acc 0.98
2016-09-07T18:42:28.553377: step 4882, loss 0.0317308, acc 1
2016-09-07T18:42:29.228331: step 4883, loss 0.025288, acc 1
2016-09-07T18:42:29.891915: step 4884, loss 0.0451206, acc 0.96
2016-09-07T18:42:30.561901: step 4885, loss 0.019944, acc 1
2016-09-07T18:42:31.235286: step 4886, loss 0.0287427, acc 0.98
2016-09-07T18:42:31.908720: step 4887, loss 0.0251647, acc 1
2016-09-07T18:42:32.565459: step 4888, loss 0.0274297, acc 0.98
2016-09-07T18:42:33.221288: step 4889, loss 0.000359147, acc 1
2016-09-07T18:42:33.874287: step 4890, loss 0.00259057, acc 1
2016-09-07T18:42:34.538679: step 4891, loss 0.0142614, acc 1
2016-09-07T18:42:35.210663: step 4892, loss 0.017833, acc 1
2016-09-07T18:42:35.873847: step 4893, loss 0.0154935, acc 0.98
2016-09-07T18:42:36.533627: step 4894, loss 0.00748149, acc 1
2016-09-07T18:42:37.208181: step 4895, loss 0.0296839, acc 0.98
2016-09-07T18:42:37.898663: step 4896, loss 0.017177, acc 1
2016-09-07T18:42:38.558157: step 4897, loss 0.0325752, acc 0.98
2016-09-07T18:42:39.205998: step 4898, loss 0.0390007, acc 0.98
2016-09-07T18:42:39.871340: step 4899, loss 0.109811, acc 0.98
2016-09-07T18:42:40.534253: step 4900, loss 0.0338751, acc 0.98

Evaluation:
2016-09-07T18:42:43.440080: step 4900, loss 2.13045, acc 0.74

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-4900

2016-09-07T18:42:45.195133: step 4901, loss 0.00418036, acc 1
2016-09-07T18:42:45.860130: step 4902, loss 0.0962506, acc 0.98
2016-09-07T18:42:46.515294: step 4903, loss 0.0632256, acc 0.98
2016-09-07T18:42:47.176829: step 4904, loss 0.011254, acc 1
2016-09-07T18:42:47.841368: step 4905, loss 0.0139051, acc 1
2016-09-07T18:42:48.507451: step 4906, loss 0.22135, acc 0.92
2016-09-07T18:42:49.177571: step 4907, loss 0.00620486, acc 1
2016-09-07T18:42:49.853546: step 4908, loss 0.00178084, acc 1
2016-09-07T18:42:50.521304: step 4909, loss 0.0076392, acc 1
2016-09-07T18:42:51.206640: step 4910, loss 0.02479, acc 0.98
2016-09-07T18:42:51.884483: step 4911, loss 0.0136444, acc 1
2016-09-07T18:42:52.546130: step 4912, loss 0.0239275, acc 0.98
2016-09-07T18:42:53.198900: step 4913, loss 0.0563942, acc 0.98
2016-09-07T18:42:53.853504: step 4914, loss 0.0504123, acc 0.96
2016-09-07T18:42:54.514211: step 4915, loss 0.0218315, acc 0.98
2016-09-07T18:42:55.197247: step 4916, loss 0.0259167, acc 1
2016-09-07T18:42:55.884219: step 4917, loss 0.0126722, acc 1
2016-09-07T18:42:56.565520: step 4918, loss 0.0476332, acc 0.98
2016-09-07T18:42:57.246676: step 4919, loss 0.0046849, acc 1
2016-09-07T18:42:57.923291: step 4920, loss 0.112457, acc 0.96
2016-09-07T18:42:58.620860: step 4921, loss 0.0342143, acc 0.98
2016-09-07T18:42:59.303421: step 4922, loss 0.0163746, acc 1
2016-09-07T18:42:59.997557: step 4923, loss 0.000119353, acc 1
2016-09-07T18:43:00.702154: step 4924, loss 0.0188129, acc 1
2016-09-07T18:43:01.393107: step 4925, loss 0.00460196, acc 1
2016-09-07T18:43:02.044747: step 4926, loss 0.0447062, acc 0.96
2016-09-07T18:43:02.727250: step 4927, loss 0.0295896, acc 0.98
2016-09-07T18:43:03.404929: step 4928, loss 0.00753547, acc 1
2016-09-07T18:43:04.092896: step 4929, loss 0.0102792, acc 1
2016-09-07T18:43:04.766590: step 4930, loss 0.0203685, acc 0.98
2016-09-07T18:43:05.413698: step 4931, loss 0.0637869, acc 0.98
2016-09-07T18:43:06.063688: step 4932, loss 0.0478919, acc 0.96
2016-09-07T18:43:06.739509: step 4933, loss 0.0115219, acc 1
2016-09-07T18:43:07.398922: step 4934, loss 0.0103149, acc 1
2016-09-07T18:43:08.082030: step 4935, loss 0.00287702, acc 1
2016-09-07T18:43:08.746365: step 4936, loss 0.0116436, acc 1
2016-09-07T18:43:09.397863: step 4937, loss 0.0574115, acc 0.96
2016-09-07T18:43:10.053629: step 4938, loss 0.0394231, acc 0.98
2016-09-07T18:43:10.745880: step 4939, loss 0.00993638, acc 1
2016-09-07T18:43:11.402142: step 4940, loss 0.0355566, acc 0.98
2016-09-07T18:43:12.056679: step 4941, loss 0.00544588, acc 1
2016-09-07T18:43:12.727484: step 4942, loss 0.0124195, acc 1
2016-09-07T18:43:13.403597: step 4943, loss 0.00305117, acc 1
2016-09-07T18:43:14.059851: step 4944, loss 0.0240268, acc 0.98
2016-09-07T18:43:14.723381: step 4945, loss 0.000826481, acc 1
2016-09-07T18:43:15.393075: step 4946, loss 0.0149652, acc 0.98
2016-09-07T18:43:16.058318: step 4947, loss 0.0361231, acc 0.98
2016-09-07T18:43:16.737337: step 4948, loss 0.0493704, acc 0.98
2016-09-07T18:43:17.408338: step 4949, loss 0.0111201, acc 1
2016-09-07T18:43:18.081521: step 4950, loss 0.0651369, acc 0.96
2016-09-07T18:43:18.746957: step 4951, loss 0.00547935, acc 1
2016-09-07T18:43:19.434777: step 4952, loss 0.00908783, acc 1
2016-09-07T18:43:20.101111: step 4953, loss 0.0215891, acc 0.98
2016-09-07T18:43:20.765330: step 4954, loss 0.00179255, acc 1
2016-09-07T18:43:21.434798: step 4955, loss 0.0221974, acc 1
2016-09-07T18:43:22.120901: step 4956, loss 0.0223651, acc 0.98
2016-09-07T18:43:22.798229: step 4957, loss 0.0190545, acc 1
2016-09-07T18:43:23.471559: step 4958, loss 0.0090856, acc 1
2016-09-07T18:43:24.138623: step 4959, loss 0.00888996, acc 1
2016-09-07T18:43:24.820221: step 4960, loss 0.0169289, acc 1
2016-09-07T18:43:25.493178: step 4961, loss 0.0725519, acc 0.96
2016-09-07T18:43:26.171842: step 4962, loss 0.000110494, acc 1
2016-09-07T18:43:26.862235: step 4963, loss 0.00376436, acc 1
2016-09-07T18:43:27.561129: step 4964, loss 0.0203822, acc 1
2016-09-07T18:43:28.212411: step 4965, loss 0.00288495, acc 1
2016-09-07T18:43:28.884004: step 4966, loss 0.0146793, acc 1
2016-09-07T18:43:29.553918: step 4967, loss 0.011411, acc 1
2016-09-07T18:43:30.208631: step 4968, loss 0.0332274, acc 0.98
2016-09-07T18:43:30.878420: step 4969, loss 0.000177874, acc 1
2016-09-07T18:43:31.523946: step 4970, loss 0.0402448, acc 0.96
2016-09-07T18:43:32.189260: step 4971, loss 0.00563806, acc 1
2016-09-07T18:43:32.860781: step 4972, loss 0.0355045, acc 0.98
2016-09-07T18:43:33.531052: step 4973, loss 0.0153221, acc 0.98
2016-09-07T18:43:34.225069: step 4974, loss 0.0395945, acc 0.98
2016-09-07T18:43:34.912268: step 4975, loss 0.0132615, acc 1
2016-09-07T18:43:35.578385: step 4976, loss 0.118835, acc 0.98
2016-09-07T18:43:36.236694: step 4977, loss 0.0312507, acc 0.98
2016-09-07T18:43:36.888368: step 4978, loss 0.009212, acc 1
2016-09-07T18:43:37.545910: step 4979, loss 0.0107171, acc 1
2016-09-07T18:43:38.207262: step 4980, loss 0.0139929, acc 1
2016-09-07T18:43:38.861257: step 4981, loss 0.0226212, acc 1
2016-09-07T18:43:39.530368: step 4982, loss 0.060597, acc 0.98
2016-09-07T18:43:40.199578: step 4983, loss 0.0240686, acc 0.98
2016-09-07T18:43:40.882739: step 4984, loss 0.0139259, acc 1
2016-09-07T18:43:41.566567: step 4985, loss 0.00212314, acc 1
2016-09-07T18:43:42.237075: step 4986, loss 0.00133661, acc 1
2016-09-07T18:43:42.880036: step 4987, loss 0.0128273, acc 1
2016-09-07T18:43:43.558080: step 4988, loss 0.000129768, acc 1
2016-09-07T18:43:44.225553: step 4989, loss 0.0403295, acc 0.96
2016-09-07T18:43:44.863310: step 4990, loss 0.0632002, acc 0.98
2016-09-07T18:43:45.522185: step 4991, loss 0.0162636, acc 1
2016-09-07T18:43:46.185977: step 4992, loss 0.0164631, acc 0.98
2016-09-07T18:43:46.877454: step 4993, loss 0.00389661, acc 1
2016-09-07T18:43:47.556031: step 4994, loss 0.00542923, acc 1
2016-09-07T18:43:48.238403: step 4995, loss 0.106884, acc 0.98
2016-09-07T18:43:48.900323: step 4996, loss 0.0272035, acc 0.98
2016-09-07T18:43:49.573550: step 4997, loss 0.0148231, acc 0.98
2016-09-07T18:43:50.229096: step 4998, loss 0.0263087, acc 0.98
2016-09-07T18:43:50.890369: step 4999, loss 0.0904696, acc 0.98
2016-09-07T18:43:51.575293: step 5000, loss 0.000111388, acc 1

Evaluation:
2016-09-07T18:43:54.417034: step 5000, loss 2.18911, acc 0.734

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-5000

2016-09-07T18:43:55.992719: step 5001, loss 0.0168319, acc 0.98
2016-09-07T18:43:56.654878: step 5002, loss 0.0314122, acc 0.98
2016-09-07T18:43:57.320940: step 5003, loss 0.027209, acc 1
2016-09-07T18:43:57.988659: step 5004, loss 0.104426, acc 0.94
2016-09-07T18:43:58.667615: step 5005, loss 0.0361108, acc 0.98
2016-09-07T18:43:59.350615: step 5006, loss 0.0015161, acc 1
2016-09-07T18:44:00.015409: step 5007, loss 0.0152082, acc 0.98
2016-09-07T18:44:00.720537: step 5008, loss 0.0290535, acc 1
2016-09-07T18:44:01.387050: step 5009, loss 0.0752969, acc 0.98
2016-09-07T18:44:02.061515: step 5010, loss 0.0225388, acc 0.98
2016-09-07T18:44:02.728490: step 5011, loss 0.0292604, acc 0.98
2016-09-07T18:44:03.393606: step 5012, loss 0.0316316, acc 0.98
2016-09-07T18:44:04.062603: step 5013, loss 0.00318853, acc 1
2016-09-07T18:44:04.725504: step 5014, loss 0.0907802, acc 0.96
2016-09-07T18:44:05.385900: step 5015, loss 0.00901513, acc 1
2016-09-07T18:44:06.064230: step 5016, loss 0.0542634, acc 0.98
2016-09-07T18:44:06.723406: step 5017, loss 0.103269, acc 0.98
2016-09-07T18:44:07.399264: step 5018, loss 0.0650127, acc 0.96
2016-09-07T18:44:08.057848: step 5019, loss 0.0589789, acc 0.98
2016-09-07T18:44:08.733459: step 5020, loss 0.046265, acc 1
2016-09-07T18:44:09.400903: step 5021, loss 0.0120662, acc 1
2016-09-07T18:44:10.065209: step 5022, loss 0.0133948, acc 1
2016-09-07T18:44:10.737611: step 5023, loss 0.000349708, acc 1
2016-09-07T18:44:11.409515: step 5024, loss 0.00248983, acc 1
2016-09-07T18:44:12.073577: step 5025, loss 0.0373607, acc 0.98
2016-09-07T18:44:12.763707: step 5026, loss 0.0426093, acc 0.98
2016-09-07T18:44:13.406238: step 5027, loss 0.0029381, acc 1
2016-09-07T18:44:14.062664: step 5028, loss 0.00249159, acc 1
2016-09-07T18:44:14.745445: step 5029, loss 0.0388517, acc 0.98
2016-09-07T18:44:15.417348: step 5030, loss 0.0238351, acc 1
2016-09-07T18:44:16.079495: step 5031, loss 0.0168784, acc 1
2016-09-07T18:44:16.743329: step 5032, loss 0.00753627, acc 1
2016-09-07T18:44:17.412644: step 5033, loss 0.00995597, acc 1
2016-09-07T18:44:18.079121: step 5034, loss 0.00673635, acc 1
2016-09-07T18:44:18.759825: step 5035, loss 0.0110075, acc 1
2016-09-07T18:44:19.425153: step 5036, loss 0.0137618, acc 1
2016-09-07T18:44:20.097568: step 5037, loss 0.0205251, acc 1
2016-09-07T18:44:20.763810: step 5038, loss 0.00900385, acc 1
2016-09-07T18:44:21.423660: step 5039, loss 0.0206801, acc 0.98
2016-09-07T18:44:22.083464: step 5040, loss 0.12771, acc 0.96
2016-09-07T18:44:22.735352: step 5041, loss 0.00701005, acc 1
2016-09-07T18:44:23.393255: step 5042, loss 0.14601, acc 0.96
2016-09-07T18:44:24.076904: step 5043, loss 0.06251, acc 0.98
2016-09-07T18:44:24.424310: step 5044, loss 0.00245738, acc 1
2016-09-07T18:44:25.090093: step 5045, loss 0.0291376, acc 0.98
2016-09-07T18:44:25.746658: step 5046, loss 0.0247045, acc 0.98
2016-09-07T18:44:26.414729: step 5047, loss 0.0475466, acc 0.98
2016-09-07T18:44:27.096708: step 5048, loss 0.0227503, acc 0.98
2016-09-07T18:44:27.766707: step 5049, loss 0.00933741, acc 1
2016-09-07T18:44:28.444077: step 5050, loss 0.0392265, acc 0.98
2016-09-07T18:44:29.144232: step 5051, loss 0.0197995, acc 1
2016-09-07T18:44:29.796662: step 5052, loss 0.0225638, acc 0.98
2016-09-07T18:44:30.480090: step 5053, loss 0.028354, acc 0.98
2016-09-07T18:44:31.172645: step 5054, loss 0.0265473, acc 1
2016-09-07T18:44:31.836152: step 5055, loss 0.0892028, acc 0.96
2016-09-07T18:44:32.502750: step 5056, loss 0.0238169, acc 1
2016-09-07T18:44:33.167341: step 5057, loss 0.0122642, acc 1
2016-09-07T18:44:33.825665: step 5058, loss 0.0127992, acc 1
2016-09-07T18:44:34.481213: step 5059, loss 0.0138857, acc 1
2016-09-07T18:44:35.138766: step 5060, loss 0.00627643, acc 1
2016-09-07T18:44:35.789541: step 5061, loss 0.03033, acc 0.98
2016-09-07T18:44:36.465283: step 5062, loss 0.00441606, acc 1
2016-09-07T18:44:37.125076: step 5063, loss 0.0285252, acc 0.98
2016-09-07T18:44:37.791395: step 5064, loss 0.00372369, acc 1
2016-09-07T18:44:38.475539: step 5065, loss 0.0147394, acc 1
2016-09-07T18:44:39.160513: step 5066, loss 0.0564366, acc 0.96
2016-09-07T18:44:39.827955: step 5067, loss 0.0181662, acc 1
2016-09-07T18:44:40.491248: step 5068, loss 0.00635363, acc 1
2016-09-07T18:44:41.150808: step 5069, loss 0.000598236, acc 1
2016-09-07T18:44:41.814846: step 5070, loss 0.00156831, acc 1
2016-09-07T18:44:42.476357: step 5071, loss 0.0282753, acc 0.98
2016-09-07T18:44:43.130422: step 5072, loss 0.000242343, acc 1
2016-09-07T18:44:43.783051: step 5073, loss 0.00672349, acc 1
2016-09-07T18:44:44.441881: step 5074, loss 0.0353999, acc 0.96
2016-09-07T18:44:45.111573: step 5075, loss 0.00241971, acc 1
2016-09-07T18:44:45.772336: step 5076, loss 0.0855647, acc 0.94
2016-09-07T18:44:46.448609: step 5077, loss 0.0251712, acc 0.98
2016-09-07T18:44:47.141122: step 5078, loss 0.0470139, acc 0.96
2016-09-07T18:44:47.815016: step 5079, loss 0.00364447, acc 1
2016-09-07T18:44:48.485830: step 5080, loss 0.0106556, acc 1
2016-09-07T18:44:49.173717: step 5081, loss 0.0191568, acc 0.98
2016-09-07T18:44:49.835075: step 5082, loss 0.00110798, acc 1
2016-09-07T18:44:50.490372: step 5083, loss 0.00409314, acc 1
2016-09-07T18:44:51.141388: step 5084, loss 0.00505909, acc 1
2016-09-07T18:44:51.819578: step 5085, loss 0.0193098, acc 0.98
2016-09-07T18:44:52.506242: step 5086, loss 0.0226083, acc 0.98
2016-09-07T18:44:53.172866: step 5087, loss 0.0149443, acc 0.98
2016-09-07T18:44:53.841756: step 5088, loss 0.00487238, acc 1
2016-09-07T18:44:54.504331: step 5089, loss 0.0272058, acc 1
2016-09-07T18:44:55.154743: step 5090, loss 0.00370119, acc 1
2016-09-07T18:44:55.830928: step 5091, loss 0.0212455, acc 0.98
2016-09-07T18:44:56.546320: step 5092, loss 0.0824603, acc 0.96
2016-09-07T18:44:57.206595: step 5093, loss 0.0112609, acc 1
2016-09-07T18:44:57.854730: step 5094, loss 0.0205064, acc 1
2016-09-07T18:44:58.507332: step 5095, loss 0.00764646, acc 1
2016-09-07T18:44:59.184799: step 5096, loss 0.0650164, acc 0.98
2016-09-07T18:44:59.850834: step 5097, loss 0.020397, acc 1
2016-09-07T18:45:00.557749: step 5098, loss 0.29787, acc 0.98
2016-09-07T18:45:01.220862: step 5099, loss 0.0066077, acc 1
2016-09-07T18:45:01.899751: step 5100, loss 0.0177451, acc 1

Evaluation:
2016-09-07T18:45:04.762250: step 5100, loss 2.37904, acc 0.74

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-5100

2016-09-07T18:45:06.430392: step 5101, loss 0.0828584, acc 0.98
2016-09-07T18:45:07.095868: step 5102, loss 0.0205738, acc 0.98
2016-09-07T18:45:07.778206: step 5103, loss 0.0391738, acc 0.98
2016-09-07T18:45:08.441130: step 5104, loss 0.0165492, acc 1
2016-09-07T18:45:09.100932: step 5105, loss 0.0140533, acc 1
2016-09-07T18:45:09.763923: step 5106, loss 0.00827197, acc 1
2016-09-07T18:45:10.457129: step 5107, loss 0.10971, acc 0.96
2016-09-07T18:45:11.135874: step 5108, loss 0.00748651, acc 1
2016-09-07T18:45:11.803515: step 5109, loss 0.0443504, acc 0.98
2016-09-07T18:45:12.472558: step 5110, loss 0.0142696, acc 1
2016-09-07T18:45:13.140770: step 5111, loss 0.0295931, acc 0.98
2016-09-07T18:45:13.801578: step 5112, loss 0.00921282, acc 1
2016-09-07T18:45:14.455179: step 5113, loss 0.0209937, acc 0.98
2016-09-07T18:45:15.121270: step 5114, loss 0.0360911, acc 0.98
2016-09-07T18:45:15.786569: step 5115, loss 0.0128343, acc 1
2016-09-07T18:45:16.454282: step 5116, loss 0.0158561, acc 1
2016-09-07T18:45:17.114822: step 5117, loss 0.00196861, acc 1
2016-09-07T18:45:17.765143: step 5118, loss 0.00463235, acc 1
2016-09-07T18:45:18.443563: step 5119, loss 0.0163907, acc 0.98
2016-09-07T18:45:19.108847: step 5120, loss 0.0457846, acc 0.98
2016-09-07T18:45:19.774852: step 5121, loss 0.0373674, acc 0.98
2016-09-07T18:45:20.452103: step 5122, loss 0.00821185, acc 1
2016-09-07T18:45:21.117329: step 5123, loss 0.0165534, acc 0.98
2016-09-07T18:45:21.787802: step 5124, loss 0.0402058, acc 0.98
2016-09-07T18:45:22.466381: step 5125, loss 0.000528833, acc 1
2016-09-07T18:45:23.127021: step 5126, loss 0.0245042, acc 0.98
2016-09-07T18:45:23.805994: step 5127, loss 0.0312454, acc 1
2016-09-07T18:45:24.478806: step 5128, loss 0.0751683, acc 0.98
2016-09-07T18:45:25.139364: step 5129, loss 0.0381473, acc 0.98
2016-09-07T18:45:25.794322: step 5130, loss 0.033362, acc 1
2016-09-07T18:45:26.453562: step 5131, loss 0.000173908, acc 1
2016-09-07T18:45:27.114651: step 5132, loss 0.0570713, acc 0.96
2016-09-07T18:45:27.789775: step 5133, loss 0.0445013, acc 0.98
2016-09-07T18:45:28.449251: step 5134, loss 0.00909209, acc 1
2016-09-07T18:45:29.116473: step 5135, loss 0.0335473, acc 0.98
2016-09-07T18:45:29.774155: step 5136, loss 0.0599108, acc 0.94
2016-09-07T18:45:30.449353: step 5137, loss 0.0786552, acc 0.96
2016-09-07T18:45:31.148668: step 5138, loss 0.00834799, acc 1
2016-09-07T18:45:31.824734: step 5139, loss 0.0818174, acc 0.96
2016-09-07T18:45:32.500111: step 5140, loss 0.248475, acc 0.98
2016-09-07T18:45:33.160356: step 5141, loss 0.00935457, acc 1
2016-09-07T18:45:33.817678: step 5142, loss 0.0394919, acc 1
2016-09-07T18:45:34.490913: step 5143, loss 0.00378768, acc 1
2016-09-07T18:45:35.170331: step 5144, loss 0.0327967, acc 0.98
2016-09-07T18:45:35.835974: step 5145, loss 0.0412661, acc 0.98
2016-09-07T18:45:36.493117: step 5146, loss 0.00529652, acc 1
2016-09-07T18:45:37.181514: step 5147, loss 0.00845437, acc 1
2016-09-07T18:45:37.861217: step 5148, loss 0.00737664, acc 1
2016-09-07T18:45:38.533390: step 5149, loss 0.00550101, acc 1
2016-09-07T18:45:39.200902: step 5150, loss 0.0460634, acc 0.98
2016-09-07T18:45:39.863637: step 5151, loss 0.0117285, acc 1
2016-09-07T18:45:40.521530: step 5152, loss 0.00890823, acc 1
2016-09-07T18:45:41.186206: step 5153, loss 0.0962041, acc 0.96
2016-09-07T18:45:41.896468: step 5154, loss 0.0565897, acc 0.98
2016-09-07T18:45:42.559113: step 5155, loss 0.145508, acc 0.98
2016-09-07T18:45:43.227370: step 5156, loss 0.0178821, acc 0.98
2016-09-07T18:45:43.908211: step 5157, loss 0.082182, acc 0.98
2016-09-07T18:45:44.592795: step 5158, loss 0.0311156, acc 0.98
2016-09-07T18:45:45.268606: step 5159, loss 0.029089, acc 1
2016-09-07T18:45:45.923240: step 5160, loss 0.0102015, acc 1
2016-09-07T18:45:46.589137: step 5161, loss 0.0254197, acc 1
2016-09-07T18:45:47.254524: step 5162, loss 0.0114112, acc 1
2016-09-07T18:45:47.942949: step 5163, loss 0.00755966, acc 1
2016-09-07T18:45:48.622759: step 5164, loss 0.0601143, acc 0.96
2016-09-07T18:45:49.301762: step 5165, loss 0.0047443, acc 1
2016-09-07T18:45:49.972860: step 5166, loss 0.0541338, acc 0.98
2016-09-07T18:45:50.650156: step 5167, loss 0.0184437, acc 0.98
2016-09-07T18:45:51.316737: step 5168, loss 0.0246507, acc 0.98
2016-09-07T18:45:52.001731: step 5169, loss 0.0835705, acc 0.96
2016-09-07T18:45:52.689521: step 5170, loss 0.00264869, acc 1
2016-09-07T18:45:53.382429: step 5171, loss 0.038644, acc 0.98
2016-09-07T18:45:54.045255: step 5172, loss 0.0170317, acc 1
2016-09-07T18:45:54.731246: step 5173, loss 0.00184451, acc 1
2016-09-07T18:45:55.410827: step 5174, loss 0.0147775, acc 1
2016-09-07T18:45:56.083913: step 5175, loss 0.0424572, acc 0.98
2016-09-07T18:45:56.744002: step 5176, loss 0.011735, acc 1
2016-09-07T18:45:57.429731: step 5177, loss 0.00910759, acc 1
2016-09-07T18:45:58.106427: step 5178, loss 0.022248, acc 1
2016-09-07T18:45:58.782947: step 5179, loss 0.0456765, acc 0.98
2016-09-07T18:45:59.452212: step 5180, loss 0.0129819, acc 1
2016-09-07T18:46:00.120731: step 5181, loss 0.00284093, acc 1
2016-09-07T18:46:00.812417: step 5182, loss 0.000190536, acc 1
2016-09-07T18:46:01.469493: step 5183, loss 0.00613908, acc 1
2016-09-07T18:46:02.151414: step 5184, loss 0.0292416, acc 0.98
2016-09-07T18:46:02.821590: step 5185, loss 0.0233161, acc 0.98
2016-09-07T18:46:03.499654: step 5186, loss 0.0163605, acc 0.98
2016-09-07T18:46:04.178190: step 5187, loss 0.0263828, acc 0.98
2016-09-07T18:46:04.847131: step 5188, loss 0.0262819, acc 1
2016-09-07T18:46:05.522895: step 5189, loss 0.00788241, acc 1
2016-09-07T18:46:06.192798: step 5190, loss 0.00734315, acc 1
2016-09-07T18:46:06.862328: step 5191, loss 0.0127905, acc 1
2016-09-07T18:46:07.519616: step 5192, loss 0.0886282, acc 0.98
2016-09-07T18:46:08.203319: step 5193, loss 0.0121818, acc 1
2016-09-07T18:46:08.888798: step 5194, loss 0.0562254, acc 0.96
2016-09-07T18:46:09.580812: step 5195, loss 0.0201398, acc 1
2016-09-07T18:46:10.243380: step 5196, loss 0.0476047, acc 0.96
2016-09-07T18:46:10.913286: step 5197, loss 0.0154082, acc 1
2016-09-07T18:46:11.589514: step 5198, loss 0.0152488, acc 1
2016-09-07T18:46:12.262252: step 5199, loss 0.033885, acc 0.98
2016-09-07T18:46:12.928725: step 5200, loss 0.00912724, acc 1

Evaluation:
2016-09-07T18:46:15.807455: step 5200, loss 2.0497, acc 0.751

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-5200

2016-09-07T18:46:17.463837: step 5201, loss 0.00305571, acc 1
2016-09-07T18:46:18.122063: step 5202, loss 0.0367255, acc 0.98
2016-09-07T18:46:18.772324: step 5203, loss 0.0529148, acc 0.98
2016-09-07T18:46:19.413198: step 5204, loss 0.00562304, acc 1
2016-09-07T18:46:20.086778: step 5205, loss 0.0280605, acc 0.98
2016-09-07T18:46:20.765795: step 5206, loss 0.00513871, acc 1
2016-09-07T18:46:21.452925: step 5207, loss 0.000264344, acc 1
2016-09-07T18:46:22.139711: step 5208, loss 0.0181882, acc 1
2016-09-07T18:46:22.806054: step 5209, loss 0.0110496, acc 1
2016-09-07T18:46:23.479232: step 5210, loss 0.00603094, acc 1
2016-09-07T18:46:24.148128: step 5211, loss 0.031661, acc 0.98
2016-09-07T18:46:24.829466: step 5212, loss 0.0263943, acc 0.98
2016-09-07T18:46:25.501009: step 5213, loss 0.000656375, acc 1
2016-09-07T18:46:26.159889: step 5214, loss 0.0328782, acc 0.98
2016-09-07T18:46:26.823068: step 5215, loss 0.022022, acc 1
2016-09-07T18:46:27.463032: step 5216, loss 0.0188792, acc 0.98
2016-09-07T18:46:28.164746: step 5217, loss 0.0225102, acc 1
2016-09-07T18:46:28.830405: step 5218, loss 0.00414181, acc 1
2016-09-07T18:46:29.505621: step 5219, loss 0.00861127, acc 1
2016-09-07T18:46:30.181063: step 5220, loss 0.00268373, acc 1
2016-09-07T18:46:30.845992: step 5221, loss 0.0108572, acc 1
2016-09-07T18:46:31.516214: step 5222, loss 0.0201644, acc 1
2016-09-07T18:46:32.180168: step 5223, loss 0.00662087, acc 1
2016-09-07T18:46:32.855792: step 5224, loss 0.00666087, acc 1
2016-09-07T18:46:33.516178: step 5225, loss 0.0513299, acc 0.98
2016-09-07T18:46:34.186720: step 5226, loss 0.0623632, acc 0.98
2016-09-07T18:46:34.859100: step 5227, loss 0.0116648, acc 1
2016-09-07T18:46:35.529127: step 5228, loss 0.000275953, acc 1
2016-09-07T18:46:36.181811: step 5229, loss 0.0163444, acc 1
2016-09-07T18:46:36.849797: step 5230, loss 0.0147264, acc 1
2016-09-07T18:46:37.517034: step 5231, loss 0.0251347, acc 1
2016-09-07T18:46:38.189772: step 5232, loss 0.00133495, acc 1
2016-09-07T18:46:38.859973: step 5233, loss 0.0076288, acc 1
2016-09-07T18:46:39.516764: step 5234, loss 0.0559273, acc 0.96
2016-09-07T18:46:40.197553: step 5235, loss 0.118992, acc 0.96
2016-09-07T18:46:40.854291: step 5236, loss 0.0161742, acc 0.98
2016-09-07T18:46:41.512926: step 5237, loss 0.0316929, acc 0.98
2016-09-07T18:46:41.865152: step 5238, loss 0.0176557, acc 1
2016-09-07T18:46:42.538784: step 5239, loss 0.00562722, acc 1
2016-09-07T18:46:43.194565: step 5240, loss 0.0156305, acc 1
2016-09-07T18:46:43.843445: step 5241, loss 0.0564507, acc 0.96
2016-09-07T18:46:44.505939: step 5242, loss 0.0139695, acc 1
2016-09-07T18:46:45.176641: step 5243, loss 0.0310999, acc 0.98
2016-09-07T18:46:45.832129: step 5244, loss 0.0385278, acc 0.96
2016-09-07T18:46:46.521091: step 5245, loss 0.00642549, acc 1
2016-09-07T18:46:47.172619: step 5246, loss 0.0204412, acc 1
2016-09-07T18:46:47.830770: step 5247, loss 0.00356186, acc 1
2016-09-07T18:46:48.492897: step 5248, loss 0.00165609, acc 1
2016-09-07T18:46:49.161262: step 5249, loss 0.0244012, acc 0.98
2016-09-07T18:46:49.815997: step 5250, loss 0.0856239, acc 0.98
2016-09-07T18:46:50.473180: step 5251, loss 0.0158714, acc 1
2016-09-07T18:46:51.122935: step 5252, loss 0.0494144, acc 0.98
2016-09-07T18:46:51.774568: step 5253, loss 0.0145959, acc 1
2016-09-07T18:46:52.452143: step 5254, loss 0.00445885, acc 1
2016-09-07T18:46:53.133269: step 5255, loss 0.00403377, acc 1
2016-09-07T18:46:53.795793: step 5256, loss 0.000660877, acc 1
2016-09-07T18:46:54.467074: step 5257, loss 0.0222934, acc 0.98
2016-09-07T18:46:55.121104: step 5258, loss 0.0358007, acc 0.98
2016-09-07T18:46:55.782066: step 5259, loss 0.0658702, acc 0.98
2016-09-07T18:46:56.444201: step 5260, loss 0.0326384, acc 0.98
2016-09-07T18:46:57.105976: step 5261, loss 0.0192272, acc 0.98
2016-09-07T18:46:57.765206: step 5262, loss 8.23135e-05, acc 1
2016-09-07T18:46:58.429814: step 5263, loss 0.0140646, acc 1
2016-09-07T18:46:59.109547: step 5264, loss 0.00336974, acc 1
2016-09-07T18:46:59.788346: step 5265, loss 0.0242699, acc 0.98
2016-09-07T18:47:00.488376: step 5266, loss 0.0370001, acc 0.98
2016-09-07T18:47:01.164003: step 5267, loss 0.00308162, acc 1
2016-09-07T18:47:01.836040: step 5268, loss 0.0191149, acc 1
2016-09-07T18:47:02.502599: step 5269, loss 0.00170674, acc 1
2016-09-07T18:47:03.181864: step 5270, loss 0.0599942, acc 0.98
2016-09-07T18:47:03.855081: step 5271, loss 0.013083, acc 1
2016-09-07T18:47:04.526375: step 5272, loss 0.0292047, acc 0.96
2016-09-07T18:47:05.188413: step 5273, loss 0.0277557, acc 0.98
2016-09-07T18:47:05.847385: step 5274, loss 0.00412955, acc 1
2016-09-07T18:47:06.483864: step 5275, loss 0.0581293, acc 0.98
2016-09-07T18:47:07.144816: step 5276, loss 0.0501587, acc 0.98
2016-09-07T18:47:07.795818: step 5277, loss 0.000633185, acc 1
2016-09-07T18:47:08.464460: step 5278, loss 0.0144214, acc 1
2016-09-07T18:47:09.117399: step 5279, loss 0.0496028, acc 0.98
2016-09-07T18:47:09.798671: step 5280, loss 0.0456539, acc 0.96
2016-09-07T18:47:10.455945: step 5281, loss 0.00659031, acc 1
2016-09-07T18:47:11.156368: step 5282, loss 0.000182439, acc 1
2016-09-07T18:47:11.851219: step 5283, loss 0.0364169, acc 0.98
2016-09-07T18:47:12.528045: step 5284, loss 0.0821766, acc 0.96
2016-09-07T18:47:13.205023: step 5285, loss 0.0448741, acc 0.98
2016-09-07T18:47:13.895296: step 5286, loss 0.0253073, acc 0.98
2016-09-07T18:47:14.561132: step 5287, loss 0.0133941, acc 1
2016-09-07T18:47:15.235956: step 5288, loss 0.0292216, acc 0.98
2016-09-07T18:47:15.896927: step 5289, loss 0.0255293, acc 0.98
2016-09-07T18:47:16.547285: step 5290, loss 0.0627337, acc 0.96
2016-09-07T18:47:17.221084: step 5291, loss 0.0324708, acc 0.98
2016-09-07T18:47:17.894531: step 5292, loss 0.0262432, acc 0.98
2016-09-07T18:47:18.544217: step 5293, loss 0.000731747, acc 1
2016-09-07T18:47:19.224363: step 5294, loss 0.00662617, acc 1
2016-09-07T18:47:19.898212: step 5295, loss 0.0299945, acc 0.98
2016-09-07T18:47:20.572121: step 5296, loss 0.000189399, acc 1
2016-09-07T18:47:21.231210: step 5297, loss 0.0316592, acc 0.98
2016-09-07T18:47:21.894675: step 5298, loss 0.0647815, acc 0.98
2016-09-07T18:47:22.561454: step 5299, loss 0.013619, acc 1
2016-09-07T18:47:23.224269: step 5300, loss 0.0534873, acc 0.98

Evaluation:
2016-09-07T18:47:26.122671: step 5300, loss 2.41474, acc 0.718

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-5300

2016-09-07T18:47:27.746029: step 5301, loss 0.074889, acc 0.96
2016-09-07T18:47:28.403021: step 5302, loss 0.0250845, acc 1
2016-09-07T18:47:29.054563: step 5303, loss 0.0547206, acc 0.98
2016-09-07T18:47:29.725079: step 5304, loss 0.00322858, acc 1
2016-09-07T18:47:30.379405: step 5305, loss 0.00700872, acc 1
2016-09-07T18:47:31.052343: step 5306, loss 0.00485743, acc 1
2016-09-07T18:47:31.720829: step 5307, loss 0.138498, acc 0.98
2016-09-07T18:47:32.374087: step 5308, loss 0.0614196, acc 0.96
2016-09-07T18:47:33.018156: step 5309, loss 0.0245035, acc 0.98
2016-09-07T18:47:33.681061: step 5310, loss 0.0210615, acc 1
2016-09-07T18:47:34.352600: step 5311, loss 0.00359175, acc 1
2016-09-07T18:47:35.010607: step 5312, loss 0.00216699, acc 1
2016-09-07T18:47:35.672893: step 5313, loss 0.0120003, acc 1
2016-09-07T18:47:36.332460: step 5314, loss 0.13227, acc 0.94
2016-09-07T18:47:36.993040: step 5315, loss 0.0199418, acc 1
2016-09-07T18:47:37.668198: step 5316, loss 0.0103128, acc 1
2016-09-07T18:47:38.329588: step 5317, loss 0.00839196, acc 1
2016-09-07T18:47:39.012823: step 5318, loss 0.0800751, acc 0.96
2016-09-07T18:47:39.692764: step 5319, loss 0.0675376, acc 0.94
2016-09-07T18:47:40.366029: step 5320, loss 0.0288634, acc 0.98
2016-09-07T18:47:41.040173: step 5321, loss 0.040441, acc 0.96
2016-09-07T18:47:41.710430: step 5322, loss 0.0230493, acc 0.98
2016-09-07T18:47:42.374797: step 5323, loss 0.00879339, acc 1
2016-09-07T18:47:43.050264: step 5324, loss 0.00845256, acc 1
2016-09-07T18:47:43.714646: step 5325, loss 0.0540008, acc 0.98
2016-09-07T18:47:44.379382: step 5326, loss 0.0330031, acc 0.98
2016-09-07T18:47:45.044769: step 5327, loss 0.00335016, acc 1
2016-09-07T18:47:45.713998: step 5328, loss 0.0243175, acc 1
2016-09-07T18:47:46.387101: step 5329, loss 0.0297932, acc 0.98
2016-09-07T18:47:47.055812: step 5330, loss 0.0152703, acc 1
2016-09-07T18:47:47.735843: step 5331, loss 0.0222916, acc 1
2016-09-07T18:47:48.421463: step 5332, loss 0.0869414, acc 0.98
2016-09-07T18:47:49.094040: step 5333, loss 0.0929538, acc 0.96
2016-09-07T18:47:49.764546: step 5334, loss 0.0351477, acc 1
2016-09-07T18:47:50.428159: step 5335, loss 0.0146023, acc 1
2016-09-07T18:47:51.086077: step 5336, loss 0.0646142, acc 0.96
2016-09-07T18:47:51.761433: step 5337, loss 0.0180827, acc 0.98
2016-09-07T18:47:52.432084: step 5338, loss 0.0408605, acc 0.96
2016-09-07T18:47:53.096917: step 5339, loss 0.0158286, acc 1
2016-09-07T18:47:53.772216: step 5340, loss 0.0391013, acc 0.96
2016-09-07T18:47:54.439085: step 5341, loss 0.0297961, acc 1
2016-09-07T18:47:55.111810: step 5342, loss 0.0190065, acc 1
2016-09-07T18:47:55.786987: step 5343, loss 0.0281122, acc 0.98
2016-09-07T18:47:56.458287: step 5344, loss 0.0163763, acc 1
2016-09-07T18:47:57.138209: step 5345, loss 0.027729, acc 0.98
2016-09-07T18:47:57.802614: step 5346, loss 0.0196004, acc 1
2016-09-07T18:47:58.467124: step 5347, loss 0.0194101, acc 1
2016-09-07T18:47:59.122217: step 5348, loss 0.0575857, acc 0.98
2016-09-07T18:47:59.780683: step 5349, loss 0.0903979, acc 0.96
2016-09-07T18:48:00.485585: step 5350, loss 0.00541093, acc 1
2016-09-07T18:48:01.138519: step 5351, loss 0.0202803, acc 0.98
2016-09-07T18:48:01.799443: step 5352, loss 0.0211192, acc 1
2016-09-07T18:48:02.465792: step 5353, loss 0.0204161, acc 0.98
2016-09-07T18:48:03.132700: step 5354, loss 0.0439108, acc 0.96
2016-09-07T18:48:03.804768: step 5355, loss 0.0342431, acc 0.98
2016-09-07T18:48:04.483170: step 5356, loss 0.0237501, acc 1
2016-09-07T18:48:05.135395: step 5357, loss 0.000821786, acc 1
2016-09-07T18:48:05.801502: step 5358, loss 0.0267834, acc 0.98
2016-09-07T18:48:06.473803: step 5359, loss 0.00484532, acc 1
2016-09-07T18:48:07.137643: step 5360, loss 0.0934647, acc 0.98
2016-09-07T18:48:07.798727: step 5361, loss 0.0633639, acc 0.96
2016-09-07T18:48:08.447118: step 5362, loss 0.00317649, acc 1
2016-09-07T18:48:09.106803: step 5363, loss 0.0205995, acc 1
2016-09-07T18:48:09.767559: step 5364, loss 0.0112082, acc 1
2016-09-07T18:48:10.438719: step 5365, loss 0.0802886, acc 0.96
2016-09-07T18:48:11.102470: step 5366, loss 6.27954e-05, acc 1
2016-09-07T18:48:11.771179: step 5367, loss 0.0176935, acc 0.98
2016-09-07T18:48:12.442235: step 5368, loss 0.00111498, acc 1
2016-09-07T18:48:13.111105: step 5369, loss 0.0271389, acc 1
2016-09-07T18:48:13.778708: step 5370, loss 0.0174991, acc 0.98
2016-09-07T18:48:14.463832: step 5371, loss 0.00167351, acc 1
2016-09-07T18:48:15.121396: step 5372, loss 0.0304961, acc 0.98
2016-09-07T18:48:15.796943: step 5373, loss 0.0311018, acc 1
2016-09-07T18:48:16.469976: step 5374, loss 0.00943592, acc 1
2016-09-07T18:48:17.135456: step 5375, loss 0.00308109, acc 1
2016-09-07T18:48:17.825508: step 5376, loss 0.00108907, acc 1
2016-09-07T18:48:18.499488: step 5377, loss 0.0017382, acc 1
2016-09-07T18:48:19.157580: step 5378, loss 0.0390824, acc 0.96
2016-09-07T18:48:19.815690: step 5379, loss 0.0364266, acc 0.98
2016-09-07T18:48:20.479939: step 5380, loss 0.0209312, acc 1
2016-09-07T18:48:21.140647: step 5381, loss 0.0413616, acc 0.98
2016-09-07T18:48:21.822467: step 5382, loss 0.000614335, acc 1
2016-09-07T18:48:22.475896: step 5383, loss 0.0117888, acc 1
2016-09-07T18:48:23.134814: step 5384, loss 0.00302601, acc 1
2016-09-07T18:48:23.798773: step 5385, loss 0.0480275, acc 0.98
2016-09-07T18:48:24.434928: step 5386, loss 0.046959, acc 0.98
2016-09-07T18:48:25.106049: step 5387, loss 0.0279233, acc 0.98
2016-09-07T18:48:25.772747: step 5388, loss 0.00144, acc 1
2016-09-07T18:48:26.431899: step 5389, loss 0.0212498, acc 1
2016-09-07T18:48:27.107146: step 5390, loss 0.0267247, acc 1
2016-09-07T18:48:27.778106: step 5391, loss 0.0310261, acc 0.98
2016-09-07T18:48:28.469182: step 5392, loss 0.0591003, acc 0.98
2016-09-07T18:48:29.138315: step 5393, loss 0.03908, acc 0.98
2016-09-07T18:48:29.805132: step 5394, loss 0.0529204, acc 0.98
2016-09-07T18:48:30.460666: step 5395, loss 0.000152978, acc 1
2016-09-07T18:48:31.137854: step 5396, loss 0.0300339, acc 0.98
2016-09-07T18:48:31.817111: step 5397, loss 0.000681043, acc 1
2016-09-07T18:48:32.488164: step 5398, loss 0.0300511, acc 0.98
2016-09-07T18:48:33.148809: step 5399, loss 0.0475031, acc 0.98
2016-09-07T18:48:33.798853: step 5400, loss 0.013898, acc 1

Evaluation:
2016-09-07T18:48:36.694863: step 5400, loss 1.96956, acc 0.743

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-5400

2016-09-07T18:48:38.292532: step 5401, loss 0.000116729, acc 1
2016-09-07T18:48:38.963702: step 5402, loss 0.0322, acc 0.96
2016-09-07T18:48:39.634334: step 5403, loss 0.00597955, acc 1
2016-09-07T18:48:40.291313: step 5404, loss 0.0303586, acc 0.98
2016-09-07T18:48:40.928441: step 5405, loss 0.00439096, acc 1
2016-09-07T18:48:41.605446: step 5406, loss 0.0372962, acc 0.98
2016-09-07T18:48:42.299077: step 5407, loss 0.0770811, acc 0.98
2016-09-07T18:48:42.975803: step 5408, loss 0.0509358, acc 0.98
2016-09-07T18:48:43.651533: step 5409, loss 0.0292221, acc 0.98
2016-09-07T18:48:44.313023: step 5410, loss 0.0361175, acc 0.96
2016-09-07T18:48:44.977125: step 5411, loss 0.0351383, acc 0.96
2016-09-07T18:48:45.641573: step 5412, loss 0.000850699, acc 1
2016-09-07T18:48:46.301086: step 5413, loss 0.0441349, acc 0.98
2016-09-07T18:48:46.961075: step 5414, loss 0.0034084, acc 1
2016-09-07T18:48:47.619567: step 5415, loss 0.00479163, acc 1
2016-09-07T18:48:48.281650: step 5416, loss 0.00185773, acc 1
2016-09-07T18:48:48.939282: step 5417, loss 0.004781, acc 1
2016-09-07T18:48:49.612526: step 5418, loss 0.0110806, acc 1
2016-09-07T18:48:50.283076: step 5419, loss 0.0219148, acc 0.98
2016-09-07T18:48:50.949585: step 5420, loss 0.00264126, acc 1
2016-09-07T18:48:51.621770: step 5421, loss 0.000545902, acc 1
2016-09-07T18:48:52.277122: step 5422, loss 0.0161231, acc 1
2016-09-07T18:48:52.940523: step 5423, loss 0.0714022, acc 0.98
2016-09-07T18:48:53.590135: step 5424, loss 0.00881715, acc 1
2016-09-07T18:48:54.243478: step 5425, loss 0.0518125, acc 0.98
2016-09-07T18:48:54.927138: step 5426, loss 0.00125791, acc 1
2016-09-07T18:48:55.597729: step 5427, loss 0.0552614, acc 0.98
2016-09-07T18:48:56.279493: step 5428, loss 0.0518195, acc 0.96
2016-09-07T18:48:56.940367: step 5429, loss 0.110824, acc 0.98
2016-09-07T18:48:57.606838: step 5430, loss 0.0164032, acc 1
2016-09-07T18:48:58.270649: step 5431, loss 0.00165619, acc 1
2016-09-07T18:48:58.639841: step 5432, loss 0.0363968, acc 1
2016-09-07T18:48:59.316813: step 5433, loss 0.01846, acc 1
2016-09-07T18:48:59.981847: step 5434, loss 0.0396325, acc 0.98
2016-09-07T18:49:00.680750: step 5435, loss 0.0284108, acc 1
2016-09-07T18:49:01.364045: step 5436, loss 0.00964294, acc 1
2016-09-07T18:49:02.034133: step 5437, loss 0.034366, acc 0.98
2016-09-07T18:49:02.710720: step 5438, loss 0.0371604, acc 0.98
2016-09-07T18:49:03.370809: step 5439, loss 0.0105763, acc 1
2016-09-07T18:49:04.051614: step 5440, loss 0.000176293, acc 1
2016-09-07T18:49:04.709233: step 5441, loss 7.24201e-05, acc 1
2016-09-07T18:49:05.395652: step 5442, loss 0.0174769, acc 0.98
2016-09-07T18:49:06.071943: step 5443, loss 0.0736825, acc 0.96
2016-09-07T18:49:06.726252: step 5444, loss 0.00347897, acc 1
2016-09-07T18:49:07.376950: step 5445, loss 0.0481248, acc 0.98
2016-09-07T18:49:08.072097: step 5446, loss 0.0508786, acc 0.98
2016-09-07T18:49:08.746523: step 5447, loss 0.00420481, acc 1
2016-09-07T18:49:09.400304: step 5448, loss 0.129419, acc 0.96
2016-09-07T18:49:10.065026: step 5449, loss 0.0246274, acc 0.98
2016-09-07T18:49:10.731585: step 5450, loss 0.0404138, acc 0.98
2016-09-07T18:49:11.383846: step 5451, loss 0.0572051, acc 0.96
2016-09-07T18:49:12.038758: step 5452, loss 0.013938, acc 1
2016-09-07T18:49:12.709991: step 5453, loss 0.0674106, acc 0.96
2016-09-07T18:49:13.372544: step 5454, loss 0.0183167, acc 0.98
2016-09-07T18:49:14.048430: step 5455, loss 0.0317213, acc 0.98
2016-09-07T18:49:14.714093: step 5456, loss 0.000409189, acc 1
2016-09-07T18:49:15.365228: step 5457, loss 0.0110672, acc 1
2016-09-07T18:49:16.020401: step 5458, loss 0.0250939, acc 1
2016-09-07T18:49:16.690791: step 5459, loss 0.00242952, acc 1
2016-09-07T18:49:17.385559: step 5460, loss 0.0176474, acc 1
2016-09-07T18:49:18.044067: step 5461, loss 0.0399049, acc 0.98
2016-09-07T18:49:18.686138: step 5462, loss 0.0268635, acc 0.98
2016-09-07T18:49:19.360429: step 5463, loss 0.0350604, acc 0.98
2016-09-07T18:49:20.045532: step 5464, loss 0.0513542, acc 0.96
2016-09-07T18:49:20.707128: step 5465, loss 0.0478574, acc 0.98
2016-09-07T18:49:21.369387: step 5466, loss 0.0010504, acc 1
2016-09-07T18:49:22.047057: step 5467, loss 0.0053108, acc 1
2016-09-07T18:49:22.718143: step 5468, loss 0.0233829, acc 1
2016-09-07T18:49:23.384935: step 5469, loss 0.0201615, acc 0.98
2016-09-07T18:49:24.032931: step 5470, loss 0.00902513, acc 1
2016-09-07T18:49:24.697782: step 5471, loss 0.0222482, acc 1
2016-09-07T18:49:25.376178: step 5472, loss 0.000980407, acc 1
2016-09-07T18:49:26.086941: step 5473, loss 0.0389131, acc 0.98
2016-09-07T18:49:26.737423: step 5474, loss 0.0024637, acc 1
2016-09-07T18:49:27.413834: step 5475, loss 0.0329935, acc 0.96
2016-09-07T18:49:28.088140: step 5476, loss 0.0275158, acc 0.98
2016-09-07T18:49:28.752518: step 5477, loss 0.029892, acc 0.98
2016-09-07T18:49:29.427861: step 5478, loss 0.0377008, acc 0.96
2016-09-07T18:49:30.088050: step 5479, loss 0.0353489, acc 0.98
2016-09-07T18:49:30.741859: step 5480, loss 0.0198111, acc 0.98
2016-09-07T18:49:31.394119: step 5481, loss 0.00630915, acc 1
2016-09-07T18:49:32.042184: step 5482, loss 0.0300008, acc 0.98
2016-09-07T18:49:32.713664: step 5483, loss 0.0134119, acc 1
2016-09-07T18:49:33.385431: step 5484, loss 0.0448657, acc 0.98
2016-09-07T18:49:34.057872: step 5485, loss 0.00159108, acc 1
2016-09-07T18:49:34.728659: step 5486, loss 0.0973733, acc 0.98
2016-09-07T18:49:35.395729: step 5487, loss 0.000969615, acc 1
2016-09-07T18:49:36.093004: step 5488, loss 0.0109568, acc 1
2016-09-07T18:49:36.753701: step 5489, loss 0.0096103, acc 1
2016-09-07T18:49:37.447595: step 5490, loss 0.000224366, acc 1
2016-09-07T18:49:38.103000: step 5491, loss 0.0107131, acc 1
2016-09-07T18:49:38.771739: step 5492, loss 0.00583231, acc 1
2016-09-07T18:49:39.446079: step 5493, loss 0.0439588, acc 0.98
2016-09-07T18:49:40.119382: step 5494, loss 0.000628613, acc 1
2016-09-07T18:49:40.785874: step 5495, loss 0.00737514, acc 1
2016-09-07T18:49:41.452530: step 5496, loss 0.0275451, acc 1
2016-09-07T18:49:42.128565: step 5497, loss 0.0393094, acc 0.98
2016-09-07T18:49:42.805669: step 5498, loss 0.0325115, acc 1
2016-09-07T18:49:43.474401: step 5499, loss 0.000374645, acc 1
2016-09-07T18:49:44.146443: step 5500, loss 0.0191905, acc 0.98

Evaluation:
2016-09-07T18:49:47.011758: step 5500, loss 2.08369, acc 0.754

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-5500

2016-09-07T18:49:48.763222: step 5501, loss 0.0229243, acc 0.98
2016-09-07T18:49:49.425294: step 5502, loss 0.0516153, acc 0.98
2016-09-07T18:49:50.096134: step 5503, loss 0.110368, acc 0.98
2016-09-07T18:49:50.790332: step 5504, loss 0.0232959, acc 1
2016-09-07T18:49:51.477127: step 5505, loss 0.00751343, acc 1
2016-09-07T18:49:52.140654: step 5506, loss 0.0128558, acc 1
2016-09-07T18:49:52.811460: step 5507, loss 0.0226494, acc 0.98
2016-09-07T18:49:53.470391: step 5508, loss 0.0214698, acc 0.98
2016-09-07T18:49:54.131003: step 5509, loss 0.0408216, acc 0.98
2016-09-07T18:49:54.784446: step 5510, loss 1.11728e-05, acc 1
2016-09-07T18:49:55.442983: step 5511, loss 0.00563829, acc 1
2016-09-07T18:49:56.115126: step 5512, loss 0.0100845, acc 1
2016-09-07T18:49:56.785362: step 5513, loss 0.0186271, acc 0.98
2016-09-07T18:49:57.462991: step 5514, loss 0.0324641, acc 0.98
2016-09-07T18:49:58.120990: step 5515, loss 0.107622, acc 0.96
2016-09-07T18:49:58.790720: step 5516, loss 0.00408322, acc 1
2016-09-07T18:49:59.455825: step 5517, loss 0.0131382, acc 1
2016-09-07T18:50:00.126771: step 5518, loss 0.00581573, acc 1
2016-09-07T18:50:00.829528: step 5519, loss 0.000265417, acc 1
2016-09-07T18:50:01.491286: step 5520, loss 0.0398503, acc 0.98
2016-09-07T18:50:02.157181: step 5521, loss 0.000991344, acc 1
2016-09-07T18:50:02.810149: step 5522, loss 0.0113049, acc 1
2016-09-07T18:50:03.487589: step 5523, loss 0.0247503, acc 1
2016-09-07T18:50:04.159559: step 5524, loss 0.00573278, acc 1
2016-09-07T18:50:04.811509: step 5525, loss 0.00207049, acc 1
2016-09-07T18:50:05.476739: step 5526, loss 0.0398422, acc 0.98
2016-09-07T18:50:06.136480: step 5527, loss 0.0240076, acc 1
2016-09-07T18:50:06.806577: step 5528, loss 0.0163925, acc 1
2016-09-07T18:50:07.467570: step 5529, loss 9.56662e-05, acc 1
2016-09-07T18:50:08.139892: step 5530, loss 0.0955684, acc 0.94
2016-09-07T18:50:08.820964: step 5531, loss 0.0216496, acc 0.98
2016-09-07T18:50:09.498991: step 5532, loss 0.0550663, acc 0.96
2016-09-07T18:50:10.166555: step 5533, loss 0.0411985, acc 0.98
2016-09-07T18:50:10.828013: step 5534, loss 0.00995222, acc 1
2016-09-07T18:50:11.520104: step 5535, loss 0.00933005, acc 1
2016-09-07T18:50:12.179275: step 5536, loss 0.0373572, acc 0.98
2016-09-07T18:50:12.822396: step 5537, loss 0.000547696, acc 1
2016-09-07T18:50:13.479765: step 5538, loss 0.00837719, acc 1
2016-09-07T18:50:14.152685: step 5539, loss 0.0156031, acc 1
2016-09-07T18:50:14.822623: step 5540, loss 0.031245, acc 0.98
2016-09-07T18:50:15.506721: step 5541, loss 0.0245032, acc 1
2016-09-07T18:50:16.174514: step 5542, loss 0.045689, acc 0.98
2016-09-07T18:50:16.839590: step 5543, loss 0.0366188, acc 0.96
2016-09-07T18:50:17.526464: step 5544, loss 0.0110974, acc 1
2016-09-07T18:50:18.192188: step 5545, loss 0.0110937, acc 1
2016-09-07T18:50:18.847675: step 5546, loss 0.0318433, acc 1
2016-09-07T18:50:19.522842: step 5547, loss 0.0437419, acc 0.96
2016-09-07T18:50:20.165010: step 5548, loss 0.0153123, acc 1
2016-09-07T18:50:20.831801: step 5549, loss 0.0455224, acc 0.98
2016-09-07T18:50:21.494489: step 5550, loss 0.0370882, acc 0.96
2016-09-07T18:50:22.152877: step 5551, loss 4.29727e-05, acc 1
2016-09-07T18:50:22.807212: step 5552, loss 0.0516806, acc 0.98
2016-09-07T18:50:23.481068: step 5553, loss 0.0239983, acc 0.98
2016-09-07T18:50:24.133708: step 5554, loss 0.017526, acc 1
2016-09-07T18:50:24.819975: step 5555, loss 0.000429908, acc 1
2016-09-07T18:50:25.516216: step 5556, loss 0.000639099, acc 1
2016-09-07T18:50:26.190376: step 5557, loss 0.0220302, acc 0.98
2016-09-07T18:50:26.866153: step 5558, loss 0.0236268, acc 0.98
2016-09-07T18:50:27.537550: step 5559, loss 0.0191889, acc 1
2016-09-07T18:50:28.198139: step 5560, loss 0.0312169, acc 0.98
2016-09-07T18:50:28.862714: step 5561, loss 0.141234, acc 0.98
2016-09-07T18:50:29.520501: step 5562, loss 0.00624637, acc 1
2016-09-07T18:50:30.192890: step 5563, loss 0.0302237, acc 0.98
2016-09-07T18:50:30.865860: step 5564, loss 0.072417, acc 0.98
2016-09-07T18:50:31.537189: step 5565, loss 5.99889e-05, acc 1
2016-09-07T18:50:32.205907: step 5566, loss 0.0112263, acc 1
2016-09-07T18:50:32.876394: step 5567, loss 0.148856, acc 0.96
2016-09-07T18:50:33.562199: step 5568, loss 0.01672, acc 0.98
2016-09-07T18:50:34.240278: step 5569, loss 0.0122933, acc 1
2016-09-07T18:50:34.915170: step 5570, loss 0.00129751, acc 1
2016-09-07T18:50:35.592058: step 5571, loss 0.00263121, acc 1
2016-09-07T18:50:36.254558: step 5572, loss 0.0768808, acc 0.94
2016-09-07T18:50:36.926990: step 5573, loss 0.00299568, acc 1
2016-09-07T18:50:37.595193: step 5574, loss 0.0116589, acc 1
2016-09-07T18:50:38.265549: step 5575, loss 0.0212149, acc 1
2016-09-07T18:50:38.936067: step 5576, loss 0.018783, acc 1
2016-09-07T18:50:39.585445: step 5577, loss 0.0167837, acc 1
2016-09-07T18:50:40.238125: step 5578, loss 0.00897547, acc 1
2016-09-07T18:50:40.907309: step 5579, loss 0.00848228, acc 1
2016-09-07T18:50:41.581625: step 5580, loss 0.0186402, acc 0.98
2016-09-07T18:50:42.233193: step 5581, loss 0.0679702, acc 0.98
2016-09-07T18:50:42.896516: step 5582, loss 0.0171161, acc 1
2016-09-07T18:50:43.554274: step 5583, loss 0.0330507, acc 1
2016-09-07T18:50:44.241308: step 5584, loss 0.0223917, acc 0.98
2016-09-07T18:50:44.905357: step 5585, loss 0.049885, acc 0.96
2016-09-07T18:50:45.588408: step 5586, loss 0.0716862, acc 0.96
2016-09-07T18:50:46.266576: step 5587, loss 0.0204067, acc 0.98
2016-09-07T18:50:46.930738: step 5588, loss 0.00549687, acc 1
2016-09-07T18:50:47.630549: step 5589, loss 0.0109489, acc 1
2016-09-07T18:50:48.295931: step 5590, loss 0.00395724, acc 1
2016-09-07T18:50:48.982306: step 5591, loss 0.00623341, acc 1
2016-09-07T18:50:49.642950: step 5592, loss 0.038523, acc 0.98
2016-09-07T18:50:50.317302: step 5593, loss 0.0196592, acc 0.98
2016-09-07T18:50:50.987494: step 5594, loss 0.0419533, acc 0.98
2016-09-07T18:50:51.688810: step 5595, loss 0.00165423, acc 1
2016-09-07T18:50:52.349376: step 5596, loss 0.0362022, acc 0.98
2016-09-07T18:50:53.024769: step 5597, loss 0.0450945, acc 0.98
2016-09-07T18:50:53.690314: step 5598, loss 0.0285871, acc 0.98
2016-09-07T18:50:54.348058: step 5599, loss 0.000764734, acc 1
2016-09-07T18:50:55.022519: step 5600, loss 0.0298218, acc 0.98

Evaluation:
2016-09-07T18:50:57.902264: step 5600, loss 2.12809, acc 0.736

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-5600

2016-09-07T18:50:59.510272: step 5601, loss 0.0354959, acc 0.98
2016-09-07T18:51:00.184763: step 5602, loss 0.0329775, acc 1
2016-09-07T18:51:00.902098: step 5603, loss 0.0540398, acc 0.96
2016-09-07T18:51:01.586826: step 5604, loss 0.0118865, acc 1
2016-09-07T18:51:02.263310: step 5605, loss 0.102334, acc 0.98
2016-09-07T18:51:02.941154: step 5606, loss 0.0165647, acc 1
2016-09-07T18:51:03.623305: step 5607, loss 0.0133516, acc 1
2016-09-07T18:51:04.303436: step 5608, loss 0.0435281, acc 0.98
2016-09-07T18:51:04.971207: step 5609, loss 0.00399319, acc 1
2016-09-07T18:51:05.645357: step 5610, loss 0.0069917, acc 1
2016-09-07T18:51:06.320570: step 5611, loss 0.0152216, acc 1
2016-09-07T18:51:06.962287: step 5612, loss 0.0153988, acc 0.98
2016-09-07T18:51:07.623055: step 5613, loss 0.00397688, acc 1
2016-09-07T18:51:08.292232: step 5614, loss 0.0513291, acc 0.98
2016-09-07T18:51:08.973744: step 5615, loss 0.0189009, acc 0.98
2016-09-07T18:51:09.655556: step 5616, loss 0.00149534, acc 1
2016-09-07T18:51:10.305223: step 5617, loss 0.0133602, acc 1
2016-09-07T18:51:10.978724: step 5618, loss 0.00115773, acc 1
2016-09-07T18:51:11.622668: step 5619, loss 0.00543058, acc 1
2016-09-07T18:51:12.281295: step 5620, loss 9.47828e-05, acc 1
2016-09-07T18:51:12.959995: step 5621, loss 0.0448461, acc 0.98
2016-09-07T18:51:13.646098: step 5622, loss 0.0245669, acc 1
2016-09-07T18:51:14.330936: step 5623, loss 0.0284886, acc 0.98
2016-09-07T18:51:14.998932: step 5624, loss 0.00965305, acc 1
2016-09-07T18:51:15.662818: step 5625, loss 0.0230278, acc 0.98
2016-09-07T18:51:16.027611: step 5626, loss 0.0204148, acc 1
2016-09-07T18:51:16.712501: step 5627, loss 0.0388467, acc 0.98
2016-09-07T18:51:17.377309: step 5628, loss 0.0180826, acc 0.98
2016-09-07T18:51:18.037317: step 5629, loss 0.0290296, acc 0.98
2016-09-07T18:51:18.694693: step 5630, loss 0.0230681, acc 0.98
2016-09-07T18:51:19.365856: step 5631, loss 0.0505125, acc 0.98
2016-09-07T18:51:20.054377: step 5632, loss 0.0113773, acc 1
2016-09-07T18:51:20.717147: step 5633, loss 0.0196845, acc 0.98
2016-09-07T18:51:21.382096: step 5634, loss 0.00589502, acc 1
2016-09-07T18:51:22.048774: step 5635, loss 0.00474602, acc 1
2016-09-07T18:51:22.713148: step 5636, loss 0.00136031, acc 1
2016-09-07T18:51:23.388772: step 5637, loss 0.0402637, acc 0.98
2016-09-07T18:51:24.060031: step 5638, loss 0.0255012, acc 0.98
2016-09-07T18:51:24.731557: step 5639, loss 0.0298206, acc 0.98
2016-09-07T18:51:25.413227: step 5640, loss 0.0300006, acc 0.98
2016-09-07T18:51:26.065085: step 5641, loss 0.000274226, acc 1
2016-09-07T18:51:26.725511: step 5642, loss 0.000339815, acc 1
2016-09-07T18:51:27.402234: step 5643, loss 0.0579142, acc 0.96
2016-09-07T18:51:28.089023: step 5644, loss 0.00631739, acc 1
2016-09-07T18:51:28.765936: step 5645, loss 0.056373, acc 0.96
2016-09-07T18:51:29.428338: step 5646, loss 4.82671e-05, acc 1
2016-09-07T18:51:30.106121: step 5647, loss 0.0920287, acc 0.94
2016-09-07T18:51:30.789763: step 5648, loss 0.000103305, acc 1
2016-09-07T18:51:31.449032: step 5649, loss 0.0338189, acc 0.98
2016-09-07T18:51:32.103630: step 5650, loss 0.0041377, acc 1
2016-09-07T18:51:32.773549: step 5651, loss 0.018051, acc 0.98
2016-09-07T18:51:33.434755: step 5652, loss 0.000186909, acc 1
2016-09-07T18:51:34.094887: step 5653, loss 0.00185266, acc 1
2016-09-07T18:51:34.736425: step 5654, loss 0.0163063, acc 0.98
2016-09-07T18:51:35.416328: step 5655, loss 0.0106024, acc 1
2016-09-07T18:51:36.078417: step 5656, loss 0.0133441, acc 1
2016-09-07T18:51:36.743495: step 5657, loss 0.0108313, acc 1
2016-09-07T18:51:37.420045: step 5658, loss 0.0616052, acc 0.98
2016-09-07T18:51:38.092422: step 5659, loss 0.0107727, acc 1
2016-09-07T18:51:38.757339: step 5660, loss 0.0532398, acc 0.98
2016-09-07T18:51:39.417021: step 5661, loss 0.000209036, acc 1
2016-09-07T18:51:40.082894: step 5662, loss 0.0127769, acc 1
2016-09-07T18:51:40.800737: step 5663, loss 0.029923, acc 0.98
2016-09-07T18:51:41.452451: step 5664, loss 0.0411863, acc 0.98
2016-09-07T18:51:42.114506: step 5665, loss 0.0275703, acc 0.98
2016-09-07T18:51:42.769662: step 5666, loss 0.00129048, acc 1
2016-09-07T18:51:43.425855: step 5667, loss 0.00641864, acc 1
2016-09-07T18:51:44.092104: step 5668, loss 0.0111656, acc 1
2016-09-07T18:51:44.749647: step 5669, loss 0.0116895, acc 1
2016-09-07T18:51:45.402785: step 5670, loss 0.00607255, acc 1
2016-09-07T18:51:46.064400: step 5671, loss 0.0410536, acc 0.98
2016-09-07T18:51:46.747447: step 5672, loss 0.0171955, acc 0.98
2016-09-07T18:51:47.397414: step 5673, loss 0.0553129, acc 0.96
2016-09-07T18:51:48.082211: step 5674, loss 0.0451664, acc 0.98
2016-09-07T18:51:48.752801: step 5675, loss 0.00903956, acc 1
2016-09-07T18:51:49.421346: step 5676, loss 0.0212765, acc 1
2016-09-07T18:51:50.097135: step 5677, loss 0.0108905, acc 1
2016-09-07T18:51:50.774620: step 5678, loss 0.00381169, acc 1
2016-09-07T18:51:51.438203: step 5679, loss 0.0444417, acc 0.98
2016-09-07T18:51:52.094987: step 5680, loss 0.00880165, acc 1
2016-09-07T18:51:52.761839: step 5681, loss 0.00756023, acc 1
2016-09-07T18:51:53.420574: step 5682, loss 0.0109143, acc 1
2016-09-07T18:51:54.101895: step 5683, loss 0.00367431, acc 1
2016-09-07T18:51:54.760105: step 5684, loss 0.000123186, acc 1
2016-09-07T18:51:55.410315: step 5685, loss 0.00449798, acc 1
2016-09-07T18:51:56.077277: step 5686, loss 9.21422e-05, acc 1
2016-09-07T18:51:56.719979: step 5687, loss 0.021455, acc 0.98
2016-09-07T18:51:57.371414: step 5688, loss 0.0157229, acc 1
2016-09-07T18:51:58.033550: step 5689, loss 0.00377575, acc 1
2016-09-07T18:51:58.712840: step 5690, loss 0.0214531, acc 1
2016-09-07T18:51:59.380884: step 5691, loss 0.00535231, acc 1
2016-09-07T18:52:00.044954: step 5692, loss 0.0324023, acc 0.98
2016-09-07T18:52:00.756798: step 5693, loss 0.00622588, acc 1
2016-09-07T18:52:01.419825: step 5694, loss 0.00384405, acc 1
2016-09-07T18:52:02.111405: step 5695, loss 0.0426979, acc 0.98
2016-09-07T18:52:02.773544: step 5696, loss 0.0195435, acc 1
2016-09-07T18:52:03.422681: step 5697, loss 0.00455656, acc 1
2016-09-07T18:52:04.096615: step 5698, loss 0.00370063, acc 1
2016-09-07T18:52:04.747507: step 5699, loss 0.028003, acc 0.98
2016-09-07T18:52:05.398446: step 5700, loss 0.0110529, acc 1

Evaluation:
2016-09-07T18:52:08.276850: step 5700, loss 2.67069, acc 0.73

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-5700

2016-09-07T18:52:09.833206: step 5701, loss 0.0143269, acc 0.98
2016-09-07T18:52:10.492556: step 5702, loss 0.0903884, acc 0.94
2016-09-07T18:52:11.160420: step 5703, loss 0.00800169, acc 1
2016-09-07T18:52:11.813612: step 5704, loss 8.70955e-05, acc 1
2016-09-07T18:52:12.490884: step 5705, loss 0.0148005, acc 1
2016-09-07T18:52:13.172908: step 5706, loss 0.0360179, acc 0.98
2016-09-07T18:52:13.827734: step 5707, loss 0.000635414, acc 1
2016-09-07T18:52:14.482101: step 5708, loss 0.153362, acc 0.96
2016-09-07T18:52:15.149314: step 5709, loss 0.00754922, acc 1
2016-09-07T18:52:15.813528: step 5710, loss 2.68379e-05, acc 1
2016-09-07T18:52:16.496692: step 5711, loss 0.00613373, acc 1
2016-09-07T18:52:17.159082: step 5712, loss 0.0242402, acc 0.98
2016-09-07T18:52:17.801706: step 5713, loss 0.000912969, acc 1
2016-09-07T18:52:18.476069: step 5714, loss 0.0137574, acc 1
2016-09-07T18:52:19.144435: step 5715, loss 0.00828344, acc 1
2016-09-07T18:52:19.809997: step 5716, loss 0.0532041, acc 0.98
2016-09-07T18:52:20.484549: step 5717, loss 0.000494138, acc 1
2016-09-07T18:52:21.153051: step 5718, loss 0.0209503, acc 0.98
2016-09-07T18:52:21.812389: step 5719, loss 0.00369541, acc 1
2016-09-07T18:52:22.486226: step 5720, loss 0.089964, acc 0.96
2016-09-07T18:52:23.151559: step 5721, loss 0.157372, acc 0.94
2016-09-07T18:52:23.813527: step 5722, loss 0.00528341, acc 1
2016-09-07T18:52:24.500473: step 5723, loss 0.00400388, acc 1
2016-09-07T18:52:25.175885: step 5724, loss 0.0398519, acc 0.98
2016-09-07T18:52:25.850594: step 5725, loss 0.0379639, acc 0.96
2016-09-07T18:52:26.549616: step 5726, loss 0.00217244, acc 1
2016-09-07T18:52:27.218148: step 5727, loss 0.00694774, acc 1
2016-09-07T18:52:27.887957: step 5728, loss 0.0803979, acc 0.96
2016-09-07T18:52:28.556173: step 5729, loss 0.00844322, acc 1
2016-09-07T18:52:29.209666: step 5730, loss 0.0065589, acc 1
2016-09-07T18:52:29.884683: step 5731, loss 0.00637864, acc 1
2016-09-07T18:52:30.542533: step 5732, loss 0.00242196, acc 1
2016-09-07T18:52:31.219200: step 5733, loss 0.0321744, acc 0.98
2016-09-07T18:52:31.868743: step 5734, loss 0.00354026, acc 1
2016-09-07T18:52:32.550336: step 5735, loss 0.0184532, acc 1
2016-09-07T18:52:33.188259: step 5736, loss 0.0141657, acc 1
2016-09-07T18:52:33.850061: step 5737, loss 0.0222793, acc 1
2016-09-07T18:52:34.527618: step 5738, loss 0.00693497, acc 1
2016-09-07T18:52:35.205119: step 5739, loss 0.00622979, acc 1
2016-09-07T18:52:35.915414: step 5740, loss 0.0142441, acc 1
2016-09-07T18:52:36.593328: step 5741, loss 0.029574, acc 0.98
2016-09-07T18:52:37.271015: step 5742, loss 0.0441134, acc 0.98
2016-09-07T18:52:37.924732: step 5743, loss 0.0113653, acc 1
2016-09-07T18:52:38.593155: step 5744, loss 0.0527204, acc 0.96
2016-09-07T18:52:39.260766: step 5745, loss 0.00903371, acc 1
2016-09-07T18:52:39.910694: step 5746, loss 0.00612131, acc 1
2016-09-07T18:52:40.576039: step 5747, loss 0.0128401, acc 1
2016-09-07T18:52:41.246562: step 5748, loss 0.0403452, acc 0.96
2016-09-07T18:52:41.900799: step 5749, loss 0.000112332, acc 1
2016-09-07T18:52:42.555772: step 5750, loss 0.0067726, acc 1
2016-09-07T18:52:43.201800: step 5751, loss 0.144198, acc 0.94
2016-09-07T18:52:43.862379: step 5752, loss 0.119825, acc 0.98
2016-09-07T18:52:44.508155: step 5753, loss 0.0582133, acc 0.98
2016-09-07T18:52:45.157338: step 5754, loss 0.0108526, acc 1
2016-09-07T18:52:45.818990: step 5755, loss 0.0268227, acc 0.98
2016-09-07T18:52:46.487315: step 5756, loss 0.0580312, acc 0.98
2016-09-07T18:52:47.161569: step 5757, loss 0.0362078, acc 0.96
2016-09-07T18:52:47.824159: step 5758, loss 0.0156013, acc 1
2016-09-07T18:52:48.494971: step 5759, loss 0.0246763, acc 0.98
2016-09-07T18:52:49.162998: step 5760, loss 0.0143013, acc 1
2016-09-07T18:52:49.813764: step 5761, loss 0.00978772, acc 1
2016-09-07T18:52:50.457724: step 5762, loss 0.00109712, acc 1
2016-09-07T18:52:51.120500: step 5763, loss 0.0204241, acc 1
2016-09-07T18:52:51.775603: step 5764, loss 0.00674492, acc 1
2016-09-07T18:52:52.417300: step 5765, loss 0.0384336, acc 0.98
2016-09-07T18:52:53.070523: step 5766, loss 0.00372982, acc 1
2016-09-07T18:52:53.737132: step 5767, loss 0.000987463, acc 1
2016-09-07T18:52:54.404868: step 5768, loss 0.0137439, acc 1
2016-09-07T18:52:55.056373: step 5769, loss 0.0221554, acc 1
2016-09-07T18:52:55.738782: step 5770, loss 0.0362972, acc 0.98
2016-09-07T18:52:56.408804: step 5771, loss 0.00695637, acc 1
2016-09-07T18:52:57.065724: step 5772, loss 0.163289, acc 0.96
2016-09-07T18:52:57.723757: step 5773, loss 0.016441, acc 0.98
2016-09-07T18:52:58.404873: step 5774, loss 0.0295434, acc 0.98
2016-09-07T18:52:59.073343: step 5775, loss 0.000810399, acc 1
2016-09-07T18:52:59.728515: step 5776, loss 0.00612955, acc 1
2016-09-07T18:53:00.433802: step 5777, loss 0.011238, acc 1
2016-09-07T18:53:01.096796: step 5778, loss 0.0364898, acc 0.96
2016-09-07T18:53:01.758248: step 5779, loss 0.0530772, acc 0.96
2016-09-07T18:53:02.421126: step 5780, loss 0.0405817, acc 0.98
2016-09-07T18:53:03.086307: step 5781, loss 0.0160521, acc 0.98
2016-09-07T18:53:03.757360: step 5782, loss 0.00109415, acc 1
2016-09-07T18:53:04.430197: step 5783, loss 0.00795773, acc 1
2016-09-07T18:53:05.099674: step 5784, loss 0.098371, acc 0.94
2016-09-07T18:53:05.772017: step 5785, loss 0.118491, acc 0.96
2016-09-07T18:53:06.435695: step 5786, loss 0.0155153, acc 1
2016-09-07T18:53:07.080627: step 5787, loss 0.0898227, acc 0.98
2016-09-07T18:53:07.756696: step 5788, loss 0.00762042, acc 1
2016-09-07T18:53:08.424337: step 5789, loss 0.00466695, acc 1
2016-09-07T18:53:09.099575: step 5790, loss 0.0205727, acc 0.98
2016-09-07T18:53:09.791932: step 5791, loss 0.00697901, acc 1
2016-09-07T18:53:10.457333: step 5792, loss 0.0342363, acc 0.98
2016-09-07T18:53:11.119575: step 5793, loss 0.00787037, acc 1
2016-09-07T18:53:11.796093: step 5794, loss 0.0363763, acc 0.98
2016-09-07T18:53:12.468759: step 5795, loss 0.0341487, acc 0.98
2016-09-07T18:53:13.136106: step 5796, loss 0.0416095, acc 0.98
2016-09-07T18:53:13.806540: step 5797, loss 0.0228988, acc 0.98
2016-09-07T18:53:14.480454: step 5798, loss 0.0363331, acc 0.98
2016-09-07T18:53:15.147600: step 5799, loss 0.0333884, acc 0.98
2016-09-07T18:53:15.807204: step 5800, loss 0.028099, acc 0.98

Evaluation:
2016-09-07T18:53:18.666700: step 5800, loss 1.96712, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473241413/checkpoints/model-5800

2016-09-07T18:53:20.396636: step 5801, loss 0.0257846, acc 0.98
2016-09-07T18:53:21.047383: step 5802, loss 0.0220583, acc 1
2016-09-07T18:53:21.699190: step 5803, loss 0.00165571, acc 1
2016-09-07T18:53:22.358417: step 5804, loss 0.0134236, acc 1
2016-09-07T18:53:23.017945: step 5805, loss 0.0172347, acc 0.98
2016-09-07T18:53:23.674806: step 5806, loss 0.0144436, acc 0.98
2016-09-07T18:53:24.339911: step 5807, loss 0.0175241, acc 0.98
2016-09-07T18:53:25.001845: step 5808, loss 0.0230333, acc 0.98
2016-09-07T18:53:25.673961: step 5809, loss 0.0192592, acc 1
2016-09-07T18:53:26.331163: step 5810, loss 0.0161431, acc 0.98
2016-09-07T18:53:26.996725: step 5811, loss 0.0291744, acc 0.98
2016-09-07T18:53:27.675980: step 5812, loss 0.0521936, acc 0.96
2016-09-07T18:53:28.329400: step 5813, loss 0.0696193, acc 0.94
2016-09-07T18:53:28.987280: step 5814, loss 0.000265198, acc 1
2016-09-07T18:53:29.669289: step 5815, loss 0.0340978, acc 0.98
2016-09-07T18:53:30.316827: step 5816, loss 0.0401627, acc 0.98
2016-09-07T18:53:30.990774: step 5817, loss 0.0160475, acc 1
2016-09-07T18:53:31.649762: step 5818, loss 0.0068749, acc 1
2016-09-07T18:53:32.311137: step 5819, loss 0.000133085, acc 1
2016-09-07T18:53:32.681160: step 5820, loss 0.0187395, acc 1

WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fd457fede90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fd457fede50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
CV_INDEX=4
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=30
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9594/1066
Writing to /home/cil/lstm-context-embeddings/runs/1473173739

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-06T22:55:58.568439: step 1, loss 0.693147, acc 0.46
2016-09-06T22:55:59.257684: step 2, loss 0.688708, acc 0.6
2016-09-06T22:55:59.916988: step 3, loss 0.743953, acc 0.38
2016-09-06T22:56:00.643470: step 4, loss 0.69992, acc 0.42
2016-09-06T22:56:01.330059: step 5, loss 0.696261, acc 0.5
2016-09-06T22:56:01.998970: step 6, loss 0.686579, acc 0.54
2016-09-06T22:56:02.676844: step 7, loss 0.692544, acc 0.52
2016-09-06T22:56:03.370916: step 8, loss 0.701751, acc 0.46
2016-09-06T22:56:04.085255: step 9, loss 0.69816, acc 0.46
2016-09-06T22:56:04.753550: step 10, loss 0.687544, acc 0.5
2016-09-06T22:56:05.468774: step 11, loss 0.70695, acc 0.36
2016-09-06T22:56:06.152078: step 12, loss 0.682796, acc 0.58
2016-09-06T22:56:06.837858: step 13, loss 0.687389, acc 0.58
2016-09-06T22:56:07.531509: step 14, loss 0.682492, acc 0.62
2016-09-06T22:56:08.234016: step 15, loss 0.659617, acc 0.68
2016-09-06T22:56:08.938531: step 16, loss 0.723032, acc 0.5
2016-09-06T22:56:09.609194: step 17, loss 0.652052, acc 0.72
2016-09-06T22:56:10.317999: step 18, loss 0.718723, acc 0.48
2016-09-06T22:56:11.003319: step 19, loss 0.703323, acc 0.48
2016-09-06T22:56:11.701566: step 20, loss 0.676378, acc 0.54
2016-09-06T22:56:12.379019: step 21, loss 0.645721, acc 0.62
2016-09-06T22:56:13.047358: step 22, loss 0.68473, acc 0.54
2016-09-06T22:56:13.741234: step 23, loss 0.63669, acc 0.66
2016-09-06T22:56:14.390614: step 24, loss 0.664671, acc 0.62
2016-09-06T22:56:15.093564: step 25, loss 0.63463, acc 0.66
2016-09-06T22:56:15.762695: step 26, loss 0.625852, acc 0.64
2016-09-06T22:56:16.453562: step 27, loss 0.667165, acc 0.68
2016-09-06T22:56:17.119841: step 28, loss 0.543416, acc 0.74
2016-09-06T22:56:17.797426: step 29, loss 0.614694, acc 0.68
2016-09-06T22:56:18.506968: step 30, loss 0.712793, acc 0.6
2016-09-06T22:56:19.188337: step 31, loss 0.714006, acc 0.66
2016-09-06T22:56:19.896796: step 32, loss 0.643347, acc 0.64
2016-09-06T22:56:20.581183: step 33, loss 0.570335, acc 0.68
2016-09-06T22:56:21.263392: step 34, loss 0.635508, acc 0.6
2016-09-06T22:56:21.945247: step 35, loss 0.614429, acc 0.62
2016-09-06T22:56:22.612040: step 36, loss 0.649089, acc 0.64
2016-09-06T22:56:23.293126: step 37, loss 0.633114, acc 0.58
2016-09-06T22:56:23.988658: step 38, loss 0.61655, acc 0.66
2016-09-06T22:56:24.691156: step 39, loss 0.608133, acc 0.66
2016-09-06T22:56:25.355565: step 40, loss 0.49396, acc 0.8
2016-09-06T22:56:26.036963: step 41, loss 0.551957, acc 0.62
2016-09-06T22:56:26.707873: step 42, loss 0.560407, acc 0.86
2016-09-06T22:56:27.388611: step 43, loss 0.697297, acc 0.7
2016-09-06T22:56:28.074668: step 44, loss 0.615894, acc 0.72
2016-09-06T22:56:28.755036: step 45, loss 0.598148, acc 0.68
2016-09-06T22:56:29.455352: step 46, loss 0.628263, acc 0.64
2016-09-06T22:56:30.124713: step 47, loss 0.640059, acc 0.72
2016-09-06T22:56:30.822886: step 48, loss 0.672478, acc 0.68
2016-09-06T22:56:31.523264: step 49, loss 0.488697, acc 0.84
2016-09-06T22:56:32.224072: step 50, loss 0.724576, acc 0.58
2016-09-06T22:56:32.904486: step 51, loss 0.661411, acc 0.6
2016-09-06T22:56:33.561423: step 52, loss 0.649857, acc 0.7
2016-09-06T22:56:34.249700: step 53, loss 0.556189, acc 0.68
2016-09-06T22:56:34.934529: step 54, loss 0.554519, acc 0.7
2016-09-06T22:56:35.633436: step 55, loss 0.662596, acc 0.56
2016-09-06T22:56:36.331615: step 56, loss 0.573997, acc 0.66
2016-09-06T22:56:37.031765: step 57, loss 0.610431, acc 0.7
2016-09-06T22:56:37.710141: step 58, loss 0.589642, acc 0.7
2016-09-06T22:56:38.400836: step 59, loss 0.606594, acc 0.56
2016-09-06T22:56:39.097405: step 60, loss 0.602428, acc 0.7
2016-09-06T22:56:39.807254: step 61, loss 0.592831, acc 0.68
2016-09-06T22:56:40.497653: step 62, loss 0.563952, acc 0.74
2016-09-06T22:56:41.180800: step 63, loss 0.631241, acc 0.66
2016-09-06T22:56:41.872644: step 64, loss 0.59564, acc 0.7
2016-09-06T22:56:42.560739: step 65, loss 0.438995, acc 0.82
2016-09-06T22:56:43.226187: step 66, loss 0.552692, acc 0.7
2016-09-06T22:56:43.942270: step 67, loss 0.473448, acc 0.76
2016-09-06T22:56:44.627455: step 68, loss 0.623605, acc 0.68
2016-09-06T22:56:45.313166: step 69, loss 0.663849, acc 0.56
2016-09-06T22:56:45.987421: step 70, loss 0.536601, acc 0.74
2016-09-06T22:56:46.672808: step 71, loss 0.411399, acc 0.86
2016-09-06T22:56:47.375501: step 72, loss 0.486109, acc 0.88
2016-09-06T22:56:48.051129: step 73, loss 0.526405, acc 0.7
2016-09-06T22:56:48.776966: step 74, loss 0.529042, acc 0.74
2016-09-06T22:56:49.477695: step 75, loss 0.59068, acc 0.64
2016-09-06T22:56:50.157556: step 76, loss 0.533898, acc 0.72
2016-09-06T22:56:50.826749: step 77, loss 0.486488, acc 0.72
2016-09-06T22:56:51.516940: step 78, loss 0.693834, acc 0.68
2016-09-06T22:56:52.206564: step 79, loss 0.678201, acc 0.72
2016-09-06T22:56:52.871529: step 80, loss 0.504297, acc 0.72
2016-09-06T22:56:53.576533: step 81, loss 0.545136, acc 0.74
2016-09-06T22:56:54.240474: step 82, loss 0.665982, acc 0.6
2016-09-06T22:56:54.918863: step 83, loss 0.680493, acc 0.62
2016-09-06T22:56:55.609774: step 84, loss 0.702016, acc 0.62
2016-09-06T22:56:56.304630: step 85, loss 0.620259, acc 0.7
2016-09-06T22:56:57.010446: step 86, loss 0.608657, acc 0.68
2016-09-06T22:56:57.683089: step 87, loss 0.593534, acc 0.72
2016-09-06T22:56:58.385723: step 88, loss 0.56974, acc 0.72
2016-09-06T22:56:59.052767: step 89, loss 0.673491, acc 0.6
2016-09-06T22:56:59.739129: step 90, loss 0.660247, acc 0.6
2016-09-06T22:57:00.478505: step 91, loss 0.635566, acc 0.68
2016-09-06T22:57:01.180092: step 92, loss 0.590827, acc 0.74
2016-09-06T22:57:01.892221: step 93, loss 0.596266, acc 0.66
2016-09-06T22:57:02.544394: step 94, loss 0.626035, acc 0.76
2016-09-06T22:57:03.233194: step 95, loss 0.588339, acc 0.7
2016-09-06T22:57:03.916625: step 96, loss 0.531448, acc 0.72
2016-09-06T22:57:04.597841: step 97, loss 0.550529, acc 0.7
2016-09-06T22:57:05.281867: step 98, loss 0.505669, acc 0.78
2016-09-06T22:57:05.966521: step 99, loss 0.517009, acc 0.76
2016-09-06T22:57:06.697999: step 100, loss 0.553502, acc 0.8

Evaluation:
2016-09-06T22:57:09.850692: step 100, loss 0.55488, acc 0.725141

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-100

2016-09-06T22:57:11.554756: step 101, loss 0.577806, acc 0.7
2016-09-06T22:57:12.226162: step 102, loss 0.48272, acc 0.78
2016-09-06T22:57:12.894289: step 103, loss 0.475027, acc 0.8
2016-09-06T22:57:13.585197: step 104, loss 0.430493, acc 0.84
2016-09-06T22:57:14.276866: step 105, loss 0.592237, acc 0.68
2016-09-06T22:57:14.959037: step 106, loss 0.743292, acc 0.54
2016-09-06T22:57:15.627745: step 107, loss 0.478095, acc 0.7
2016-09-06T22:57:16.329994: step 108, loss 0.584684, acc 0.68
2016-09-06T22:57:16.978982: step 109, loss 0.67127, acc 0.68
2016-09-06T22:57:17.682131: step 110, loss 0.544316, acc 0.72
2016-09-06T22:57:18.374621: step 111, loss 0.451602, acc 0.84
2016-09-06T22:57:19.071518: step 112, loss 0.559683, acc 0.7
2016-09-06T22:57:19.741464: step 113, loss 0.534332, acc 0.7
2016-09-06T22:57:20.414718: step 114, loss 0.494814, acc 0.76
2016-09-06T22:57:21.113378: step 115, loss 0.579148, acc 0.66
2016-09-06T22:57:21.792229: step 116, loss 0.476255, acc 0.72
2016-09-06T22:57:22.481806: step 117, loss 0.435395, acc 0.82
2016-09-06T22:57:23.160792: step 118, loss 0.518357, acc 0.8
2016-09-06T22:57:23.847450: step 119, loss 0.615399, acc 0.64
2016-09-06T22:57:24.540712: step 120, loss 0.498593, acc 0.76
2016-09-06T22:57:25.245052: step 121, loss 0.388555, acc 0.84
2016-09-06T22:57:25.950609: step 122, loss 0.675525, acc 0.66
2016-09-06T22:57:26.620583: step 123, loss 0.454756, acc 0.76
2016-09-06T22:57:27.315450: step 124, loss 0.491303, acc 0.78
2016-09-06T22:57:28.035600: step 125, loss 0.482557, acc 0.82
2016-09-06T22:57:28.716191: step 126, loss 0.556807, acc 0.7
2016-09-06T22:57:29.383970: step 127, loss 0.576594, acc 0.68
2016-09-06T22:57:30.058635: step 128, loss 0.636334, acc 0.66
2016-09-06T22:57:30.760655: step 129, loss 0.488129, acc 0.74
2016-09-06T22:57:31.414281: step 130, loss 0.464167, acc 0.82
2016-09-06T22:57:32.117382: step 131, loss 0.657783, acc 0.64
2016-09-06T22:57:32.807838: step 132, loss 0.477268, acc 0.72
2016-09-06T22:57:33.480445: step 133, loss 0.593313, acc 0.7
2016-09-06T22:57:34.163672: step 134, loss 0.450018, acc 0.78
2016-09-06T22:57:34.836303: step 135, loss 0.466112, acc 0.78
2016-09-06T22:57:35.545358: step 136, loss 0.577099, acc 0.66
2016-09-06T22:57:36.205249: step 137, loss 0.505225, acc 0.72
2016-09-06T22:57:36.900627: step 138, loss 0.465728, acc 0.82
2016-09-06T22:57:37.583839: step 139, loss 0.433999, acc 0.8
2016-09-06T22:57:38.264886: step 140, loss 0.578645, acc 0.68
2016-09-06T22:57:38.956706: step 141, loss 0.447794, acc 0.78
2016-09-06T22:57:39.639004: step 142, loss 0.405957, acc 0.82
2016-09-06T22:57:40.344588: step 143, loss 0.431453, acc 0.74
2016-09-06T22:57:41.012384: step 144, loss 0.497385, acc 0.72
2016-09-06T22:57:41.711872: step 145, loss 0.48919, acc 0.74
2016-09-06T22:57:42.404637: step 146, loss 0.393466, acc 0.8
2016-09-06T22:57:43.087745: step 147, loss 0.592083, acc 0.72
2016-09-06T22:57:43.765382: step 148, loss 0.511159, acc 0.72
2016-09-06T22:57:44.454020: step 149, loss 0.51614, acc 0.78
2016-09-06T22:57:45.180801: step 150, loss 0.525681, acc 0.74
2016-09-06T22:57:45.846485: step 151, loss 0.488682, acc 0.7
2016-09-06T22:57:46.546864: step 152, loss 0.455431, acc 0.72
2016-09-06T22:57:47.230477: step 153, loss 0.546836, acc 0.72
2016-09-06T22:57:47.907316: step 154, loss 0.552819, acc 0.68
2016-09-06T22:57:48.575555: step 155, loss 0.485332, acc 0.74
2016-09-06T22:57:49.237421: step 156, loss 0.64189, acc 0.66
2016-09-06T22:57:49.935579: step 157, loss 0.59601, acc 0.64
2016-09-06T22:57:50.597621: step 158, loss 0.665088, acc 0.66
2016-09-06T22:57:51.290624: step 159, loss 0.386128, acc 0.84
2016-09-06T22:57:51.954668: step 160, loss 0.540341, acc 0.72
2016-09-06T22:57:52.629002: step 161, loss 0.44216, acc 0.84
2016-09-06T22:57:53.311625: step 162, loss 0.492839, acc 0.76
2016-09-06T22:57:54.000663: step 163, loss 0.646461, acc 0.64
2016-09-06T22:57:54.693133: step 164, loss 0.461725, acc 0.76
2016-09-06T22:57:55.347344: step 165, loss 0.486948, acc 0.78
2016-09-06T22:57:56.073973: step 166, loss 0.527848, acc 0.7
2016-09-06T22:57:56.777230: step 167, loss 0.461367, acc 0.76
2016-09-06T22:57:57.455276: step 168, loss 0.475833, acc 0.76
2016-09-06T22:57:58.128788: step 169, loss 0.572808, acc 0.68
2016-09-06T22:57:58.812724: step 170, loss 0.488631, acc 0.82
2016-09-06T22:57:59.500242: step 171, loss 0.534915, acc 0.72
2016-09-06T22:58:00.188022: step 172, loss 0.524363, acc 0.84
2016-09-06T22:58:00.907656: step 173, loss 0.508346, acc 0.74
2016-09-06T22:58:01.604295: step 174, loss 0.51875, acc 0.76
2016-09-06T22:58:02.288481: step 175, loss 0.472196, acc 0.78
2016-09-06T22:58:02.990366: step 176, loss 0.435254, acc 0.82
2016-09-06T22:58:03.673160: step 177, loss 0.43444, acc 0.8
2016-09-06T22:58:04.372114: step 178, loss 0.37746, acc 0.82
2016-09-06T22:58:05.038802: step 179, loss 0.413271, acc 0.8
2016-09-06T22:58:05.730933: step 180, loss 0.487525, acc 0.8
2016-09-06T22:58:06.417326: step 181, loss 0.358495, acc 0.84
2016-09-06T22:58:07.093891: step 182, loss 0.451963, acc 0.78
2016-09-06T22:58:07.796280: step 183, loss 0.483209, acc 0.82
2016-09-06T22:58:08.497951: step 184, loss 0.579805, acc 0.66
2016-09-06T22:58:09.187833: step 185, loss 0.521451, acc 0.82
2016-09-06T22:58:09.859691: step 186, loss 0.523384, acc 0.74
2016-09-06T22:58:10.567109: step 187, loss 0.426279, acc 0.72
2016-09-06T22:58:11.258876: step 188, loss 0.303497, acc 0.86
2016-09-06T22:58:11.962724: step 189, loss 0.597654, acc 0.74
2016-09-06T22:58:12.644596: step 190, loss 0.567821, acc 0.72
2016-09-06T22:58:13.314322: step 191, loss 0.458669, acc 0.8
2016-09-06T22:58:13.996453: step 192, loss 0.557484, acc 0.704545
2016-09-06T22:58:14.661274: step 193, loss 0.389534, acc 0.86
2016-09-06T22:58:15.369453: step 194, loss 0.311822, acc 0.86
2016-09-06T22:58:16.061124: step 195, loss 0.515708, acc 0.72
2016-09-06T22:58:16.735605: step 196, loss 0.486414, acc 0.76
2016-09-06T22:58:17.417541: step 197, loss 0.372073, acc 0.82
2016-09-06T22:58:18.093554: step 198, loss 0.403015, acc 0.82
2016-09-06T22:58:18.781870: step 199, loss 0.292411, acc 0.88
2016-09-06T22:58:19.449252: step 200, loss 0.357483, acc 0.9

Evaluation:
2016-09-06T22:58:22.623635: step 200, loss 0.46621, acc 0.773921

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-200

2016-09-06T22:58:24.250770: step 201, loss 0.23727, acc 0.98
2016-09-06T22:58:24.952792: step 202, loss 0.35291, acc 0.88
2016-09-06T22:58:25.648130: step 203, loss 0.307669, acc 0.84
2016-09-06T22:58:26.311280: step 204, loss 0.291532, acc 0.86
2016-09-06T22:58:27.007571: step 205, loss 0.415873, acc 0.78
2016-09-06T22:58:27.694972: step 206, loss 0.381785, acc 0.82
2016-09-06T22:58:28.378763: step 207, loss 0.441698, acc 0.76
2016-09-06T22:58:29.039130: step 208, loss 0.428737, acc 0.82
2016-09-06T22:58:29.751416: step 209, loss 0.429659, acc 0.84
2016-09-06T22:58:30.409681: step 210, loss 0.284494, acc 0.84
2016-09-06T22:58:31.093461: step 211, loss 0.38991, acc 0.82
2016-09-06T22:58:31.772732: step 212, loss 0.368229, acc 0.84
2016-09-06T22:58:32.458379: step 213, loss 0.381246, acc 0.84
2016-09-06T22:58:33.170585: step 214, loss 0.401542, acc 0.82
2016-09-06T22:58:33.838542: step 215, loss 0.296847, acc 0.86
2016-09-06T22:58:34.543186: step 216, loss 0.475149, acc 0.78
2016-09-06T22:58:35.204296: step 217, loss 0.456394, acc 0.74
2016-09-06T22:58:35.896517: step 218, loss 0.38824, acc 0.82
2016-09-06T22:58:36.584717: step 219, loss 0.268795, acc 0.86
2016-09-06T22:58:37.267646: step 220, loss 0.373716, acc 0.86
2016-09-06T22:58:37.932803: step 221, loss 0.463204, acc 0.72
2016-09-06T22:58:38.621341: step 222, loss 0.407301, acc 0.88
2016-09-06T22:58:39.327865: step 223, loss 0.36218, acc 0.88
2016-09-06T22:58:40.011028: step 224, loss 0.410486, acc 0.76
2016-09-06T22:58:40.690842: step 225, loss 0.438435, acc 0.8
2016-09-06T22:58:41.373300: step 226, loss 0.443025, acc 0.8
2016-09-06T22:58:42.074496: step 227, loss 0.37928, acc 0.88
2016-09-06T22:58:42.759990: step 228, loss 0.508626, acc 0.8
2016-09-06T22:58:43.451682: step 229, loss 0.302475, acc 0.9
2016-09-06T22:58:44.144838: step 230, loss 0.323105, acc 0.86
2016-09-06T22:58:44.820952: step 231, loss 0.313159, acc 0.84
2016-09-06T22:58:45.496520: step 232, loss 0.369439, acc 0.88
2016-09-06T22:58:46.186434: step 233, loss 0.315849, acc 0.82
2016-09-06T22:58:46.853792: step 234, loss 0.363709, acc 0.76
2016-09-06T22:58:47.546774: step 235, loss 0.377876, acc 0.84
2016-09-06T22:58:48.224853: step 236, loss 0.422567, acc 0.78
2016-09-06T22:58:48.935948: step 237, loss 0.62343, acc 0.74
2016-09-06T22:58:49.630918: step 238, loss 0.338599, acc 0.84
2016-09-06T22:58:50.321547: step 239, loss 0.333477, acc 0.84
2016-09-06T22:58:51.006989: step 240, loss 0.321083, acc 0.84
2016-09-06T22:58:51.691053: step 241, loss 0.358048, acc 0.86
2016-09-06T22:58:52.375571: step 242, loss 0.490389, acc 0.78
2016-09-06T22:58:53.054085: step 243, loss 0.314569, acc 0.84
2016-09-06T22:58:53.755426: step 244, loss 0.457724, acc 0.84
2016-09-06T22:58:54.432778: step 245, loss 0.489575, acc 0.78
2016-09-06T22:58:55.111032: step 246, loss 0.429395, acc 0.78
2016-09-06T22:58:55.807075: step 247, loss 0.221397, acc 0.92
2016-09-06T22:58:56.508498: step 248, loss 0.337505, acc 0.86
2016-09-06T22:58:57.193568: step 249, loss 0.371043, acc 0.86
2016-09-06T22:58:57.867200: step 250, loss 0.446213, acc 0.76
2016-09-06T22:58:58.576216: step 251, loss 0.507045, acc 0.76
2016-09-06T22:58:59.243019: step 252, loss 0.361498, acc 0.82
2016-09-06T22:58:59.910494: step 253, loss 0.35878, acc 0.78
2016-09-06T22:59:00.622278: step 254, loss 0.374142, acc 0.86
2016-09-06T22:59:01.333829: step 255, loss 0.597359, acc 0.72
2016-09-06T22:59:02.015257: step 256, loss 0.452886, acc 0.82
2016-09-06T22:59:02.677658: step 257, loss 0.220684, acc 0.96
2016-09-06T22:59:03.356942: step 258, loss 0.377217, acc 0.8
2016-09-06T22:59:04.039229: step 259, loss 0.369674, acc 0.86
2016-09-06T22:59:04.732966: step 260, loss 0.374866, acc 0.84
2016-09-06T22:59:05.416838: step 261, loss 0.373484, acc 0.82
2016-09-06T22:59:06.100602: step 262, loss 0.430298, acc 0.8
2016-09-06T22:59:06.808597: step 263, loss 0.392098, acc 0.92
2016-09-06T22:59:07.498408: step 264, loss 0.397034, acc 0.84
2016-09-06T22:59:08.199592: step 265, loss 0.30738, acc 0.86
2016-09-06T22:59:08.870960: step 266, loss 0.560481, acc 0.7
2016-09-06T22:59:09.540395: step 267, loss 0.35641, acc 0.84
2016-09-06T22:59:10.223004: step 268, loss 0.362189, acc 0.84
2016-09-06T22:59:10.894482: step 269, loss 0.336594, acc 0.8
2016-09-06T22:59:11.558487: step 270, loss 0.312967, acc 0.88
2016-09-06T22:59:12.223901: step 271, loss 0.316948, acc 0.82
2016-09-06T22:59:12.923015: step 272, loss 0.435408, acc 0.76
2016-09-06T22:59:13.583422: step 273, loss 0.245877, acc 0.88
2016-09-06T22:59:14.273086: step 274, loss 0.520984, acc 0.76
2016-09-06T22:59:14.949979: step 275, loss 0.410221, acc 0.82
2016-09-06T22:59:15.629658: step 276, loss 0.436014, acc 0.8
2016-09-06T22:59:16.301562: step 277, loss 0.327332, acc 0.84
2016-09-06T22:59:16.999900: step 278, loss 0.450932, acc 0.8
2016-09-06T22:59:17.683427: step 279, loss 0.373588, acc 0.78
2016-09-06T22:59:18.348400: step 280, loss 0.333637, acc 0.84
2016-09-06T22:59:19.041191: step 281, loss 0.338294, acc 0.84
2016-09-06T22:59:19.733826: step 282, loss 0.344673, acc 0.88
2016-09-06T22:59:20.407922: step 283, loss 0.393722, acc 0.82
2016-09-06T22:59:21.098975: step 284, loss 0.514946, acc 0.8
2016-09-06T22:59:21.785932: step 285, loss 0.334013, acc 0.8
2016-09-06T22:59:22.479828: step 286, loss 0.350682, acc 0.8
2016-09-06T22:59:23.146577: step 287, loss 0.341688, acc 0.82
2016-09-06T22:59:23.844571: step 288, loss 0.288819, acc 0.92
2016-09-06T22:59:24.513326: step 289, loss 0.425711, acc 0.72
2016-09-06T22:59:25.199530: step 290, loss 0.275754, acc 0.9
2016-09-06T22:59:25.879675: step 291, loss 0.451602, acc 0.78
2016-09-06T22:59:26.557899: step 292, loss 0.318875, acc 0.9
2016-09-06T22:59:27.243401: step 293, loss 0.399434, acc 0.82
2016-09-06T22:59:27.938833: step 294, loss 0.382581, acc 0.82
2016-09-06T22:59:28.669904: step 295, loss 0.323731, acc 0.82
2016-09-06T22:59:29.346846: step 296, loss 0.291641, acc 0.88
2016-09-06T22:59:30.024610: step 297, loss 0.437585, acc 0.78
2016-09-06T22:59:30.736239: step 298, loss 0.34502, acc 0.82
2016-09-06T22:59:31.427978: step 299, loss 0.267698, acc 0.88
2016-09-06T22:59:32.103739: step 300, loss 0.322483, acc 0.88

Evaluation:
2016-09-06T22:59:35.260686: step 300, loss 0.478353, acc 0.78424

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-300

2016-09-06T22:59:36.940285: step 301, loss 0.395819, acc 0.82
2016-09-06T22:59:37.612187: step 302, loss 0.379536, acc 0.82
2016-09-06T22:59:38.304854: step 303, loss 0.417935, acc 0.86
2016-09-06T22:59:38.990631: step 304, loss 0.251327, acc 0.92
2016-09-06T22:59:39.689424: step 305, loss 0.335401, acc 0.88
2016-09-06T22:59:40.352880: step 306, loss 0.334416, acc 0.84
2016-09-06T22:59:41.035289: step 307, loss 0.412324, acc 0.78
2016-09-06T22:59:41.709618: step 308, loss 0.348363, acc 0.82
2016-09-06T22:59:42.385225: step 309, loss 0.349449, acc 0.82
2016-09-06T22:59:43.097416: step 310, loss 0.297354, acc 0.82
2016-09-06T22:59:43.767299: step 311, loss 0.327235, acc 0.86
2016-09-06T22:59:44.451261: step 312, loss 0.387119, acc 0.86
2016-09-06T22:59:45.127174: step 313, loss 0.441303, acc 0.82
2016-09-06T22:59:45.813957: step 314, loss 0.483786, acc 0.78
2016-09-06T22:59:46.501463: step 315, loss 0.272869, acc 0.88
2016-09-06T22:59:47.183816: step 316, loss 0.642459, acc 0.74
2016-09-06T22:59:47.892987: step 317, loss 0.273343, acc 0.84
2016-09-06T22:59:48.571678: step 318, loss 0.443757, acc 0.78
2016-09-06T22:59:49.277937: step 319, loss 0.276853, acc 0.88
2016-09-06T22:59:49.976127: step 320, loss 0.350012, acc 0.86
2016-09-06T22:59:50.663998: step 321, loss 0.381206, acc 0.88
2016-09-06T22:59:51.339783: step 322, loss 0.342264, acc 0.88
2016-09-06T22:59:52.014885: step 323, loss 0.421518, acc 0.82
2016-09-06T22:59:52.690599: step 324, loss 0.414961, acc 0.78
2016-09-06T22:59:53.359726: step 325, loss 0.421907, acc 0.78
2016-09-06T22:59:54.042482: step 326, loss 0.263576, acc 0.9
2016-09-06T22:59:54.737753: step 327, loss 0.235456, acc 0.9
2016-09-06T22:59:55.420222: step 328, loss 0.468564, acc 0.8
2016-09-06T22:59:56.105563: step 329, loss 0.333544, acc 0.9
2016-09-06T22:59:56.793868: step 330, loss 0.382103, acc 0.8
2016-09-06T22:59:57.489549: step 331, loss 0.344145, acc 0.88
2016-09-06T22:59:58.165978: step 332, loss 0.347071, acc 0.88
2016-09-06T22:59:58.845089: step 333, loss 0.344227, acc 0.86
2016-09-06T22:59:59.533213: step 334, loss 0.322098, acc 0.86
2016-09-06T23:00:00.238668: step 335, loss 0.475582, acc 0.82
2016-09-06T23:00:00.907823: step 336, loss 0.305657, acc 0.84
2016-09-06T23:00:01.571456: step 337, loss 0.401627, acc 0.82
2016-09-06T23:00:02.279345: step 338, loss 0.50201, acc 0.78
2016-09-06T23:00:02.923273: step 339, loss 0.618152, acc 0.7
2016-09-06T23:00:03.616678: step 340, loss 0.240005, acc 0.9
2016-09-06T23:00:04.308988: step 341, loss 0.41554, acc 0.82
2016-09-06T23:00:04.990964: step 342, loss 0.469514, acc 0.8
2016-09-06T23:00:05.668055: step 343, loss 0.440713, acc 0.8
2016-09-06T23:00:06.341165: step 344, loss 0.37044, acc 0.84
2016-09-06T23:00:07.053340: step 345, loss 0.284479, acc 0.88
2016-09-06T23:00:07.729401: step 346, loss 0.478702, acc 0.74
2016-09-06T23:00:08.410754: step 347, loss 0.432537, acc 0.8
2016-09-06T23:00:09.089148: step 348, loss 0.374421, acc 0.86
2016-09-06T23:00:09.783449: step 349, loss 0.301556, acc 0.86
2016-09-06T23:00:10.478442: step 350, loss 0.268347, acc 0.92
2016-09-06T23:00:11.197567: step 351, loss 0.418139, acc 0.82
2016-09-06T23:00:11.904208: step 352, loss 0.392616, acc 0.76
2016-09-06T23:00:12.582560: step 353, loss 0.392356, acc 0.84
2016-09-06T23:00:13.258880: step 354, loss 0.485438, acc 0.78
2016-09-06T23:00:13.950355: step 355, loss 0.351586, acc 0.84
2016-09-06T23:00:14.645975: step 356, loss 0.255626, acc 0.94
2016-09-06T23:00:15.355449: step 357, loss 0.294753, acc 0.84
2016-09-06T23:00:16.025578: step 358, loss 0.428798, acc 0.8
2016-09-06T23:00:16.718834: step 359, loss 0.334898, acc 0.88
2016-09-06T23:00:17.406471: step 360, loss 0.433058, acc 0.82
2016-09-06T23:00:18.073477: step 361, loss 0.537484, acc 0.78
2016-09-06T23:00:18.774047: step 362, loss 0.367778, acc 0.8
2016-09-06T23:00:19.445195: step 363, loss 0.33213, acc 0.82
2016-09-06T23:00:20.117745: step 364, loss 0.32969, acc 0.84
2016-09-06T23:00:20.790922: step 365, loss 0.352471, acc 0.9
2016-09-06T23:00:21.463413: step 366, loss 0.431266, acc 0.76
2016-09-06T23:00:22.122712: step 367, loss 0.375277, acc 0.88
2016-09-06T23:00:22.828891: step 368, loss 0.386745, acc 0.82
2016-09-06T23:00:23.512162: step 369, loss 0.191516, acc 0.96
2016-09-06T23:00:24.194393: step 370, loss 0.400427, acc 0.84
2016-09-06T23:00:24.877866: step 371, loss 0.390366, acc 0.82
2016-09-06T23:00:25.565236: step 372, loss 0.324736, acc 0.82
2016-09-06T23:00:26.281451: step 373, loss 0.284992, acc 0.9
2016-09-06T23:00:26.942011: step 374, loss 0.374397, acc 0.88
2016-09-06T23:00:27.645785: step 375, loss 0.299758, acc 0.84
2016-09-06T23:00:28.313596: step 376, loss 0.439989, acc 0.84
2016-09-06T23:00:28.991381: step 377, loss 0.345479, acc 0.9
2016-09-06T23:00:29.679600: step 378, loss 0.432707, acc 0.78
2016-09-06T23:00:30.356978: step 379, loss 0.321167, acc 0.88
2016-09-06T23:00:31.072023: step 380, loss 0.291874, acc 0.88
2016-09-06T23:00:31.752399: step 381, loss 0.594868, acc 0.68
2016-09-06T23:00:32.474625: step 382, loss 0.279919, acc 0.9
2016-09-06T23:00:33.169141: step 383, loss 0.502634, acc 0.8
2016-09-06T23:00:33.806108: step 384, loss 0.375278, acc 0.795455
2016-09-06T23:00:34.542260: step 385, loss 0.320414, acc 0.88
2016-09-06T23:00:35.235408: step 386, loss 0.336472, acc 0.84
2016-09-06T23:00:35.942829: step 387, loss 0.359435, acc 0.86
2016-09-06T23:00:36.622688: step 388, loss 0.292843, acc 0.9
2016-09-06T23:00:37.285742: step 389, loss 0.279873, acc 0.86
2016-09-06T23:00:38.002287: step 390, loss 0.173615, acc 0.96
2016-09-06T23:00:38.686886: step 391, loss 0.267467, acc 0.86
2016-09-06T23:00:39.373400: step 392, loss 0.183621, acc 0.98
2016-09-06T23:00:40.056904: step 393, loss 0.207977, acc 0.96
2016-09-06T23:00:40.751192: step 394, loss 0.210668, acc 0.92
2016-09-06T23:00:41.459417: step 395, loss 0.186142, acc 0.9
2016-09-06T23:00:42.133928: step 396, loss 0.276954, acc 0.92
2016-09-06T23:00:42.807550: step 397, loss 0.317738, acc 0.88
2016-09-06T23:00:43.494135: step 398, loss 0.187459, acc 0.94
2016-09-06T23:00:44.209166: step 399, loss 0.187674, acc 0.9
2016-09-06T23:00:44.899582: step 400, loss 0.154405, acc 0.92

Evaluation:
2016-09-06T23:00:48.027575: step 400, loss 0.602178, acc 0.77955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-400

2016-09-06T23:00:49.658296: step 401, loss 0.307345, acc 0.92
2016-09-06T23:00:50.336997: step 402, loss 0.247709, acc 0.88
2016-09-06T23:00:51.010747: step 403, loss 0.21543, acc 0.9
2016-09-06T23:00:51.705345: step 404, loss 0.0933788, acc 0.96
2016-09-06T23:00:52.386944: step 405, loss 0.370649, acc 0.82
2016-09-06T23:00:53.052302: step 406, loss 0.0996094, acc 0.98
2016-09-06T23:00:53.740202: step 407, loss 0.193379, acc 0.9
2016-09-06T23:00:54.434305: step 408, loss 0.1417, acc 0.94
2016-09-06T23:00:55.132933: step 409, loss 0.182456, acc 0.94
2016-09-06T23:00:55.790220: step 410, loss 0.191254, acc 0.88
2016-09-06T23:00:56.496174: step 411, loss 0.164876, acc 0.94
2016-09-06T23:00:57.180203: step 412, loss 0.239631, acc 0.92
2016-09-06T23:00:57.863359: step 413, loss 0.269279, acc 0.88
2016-09-06T23:00:58.557514: step 414, loss 0.20967, acc 0.94
2016-09-06T23:00:59.253864: step 415, loss 0.237606, acc 0.94
2016-09-06T23:00:59.961437: step 416, loss 0.327923, acc 0.88
2016-09-06T23:01:00.658432: step 417, loss 0.300373, acc 0.92
2016-09-06T23:01:01.368456: step 418, loss 0.36423, acc 0.9
2016-09-06T23:01:02.047151: step 419, loss 0.235396, acc 0.88
2016-09-06T23:01:02.729678: step 420, loss 0.145165, acc 0.92
2016-09-06T23:01:03.441213: step 421, loss 0.320049, acc 0.84
2016-09-06T23:01:04.123525: step 422, loss 0.275429, acc 0.88
2016-09-06T23:01:04.828390: step 423, loss 0.13424, acc 0.98
2016-09-06T23:01:05.501359: step 424, loss 0.142475, acc 0.96
2016-09-06T23:01:06.196205: step 425, loss 0.250932, acc 0.9
2016-09-06T23:01:06.887392: step 426, loss 0.161869, acc 0.92
2016-09-06T23:01:07.567918: step 427, loss 0.294957, acc 0.86
2016-09-06T23:01:08.259697: step 428, loss 0.211119, acc 0.88
2016-09-06T23:01:08.936175: step 429, loss 0.295064, acc 0.84
2016-09-06T23:01:09.639893: step 430, loss 0.23887, acc 0.88
2016-09-06T23:01:10.309417: step 431, loss 0.217513, acc 0.92
2016-09-06T23:01:10.992612: step 432, loss 0.243384, acc 0.9
2016-09-06T23:01:11.668902: step 433, loss 0.195722, acc 0.92
2016-09-06T23:01:12.340135: step 434, loss 0.251308, acc 0.84
2016-09-06T23:01:13.027205: step 435, loss 0.223122, acc 0.86
2016-09-06T23:01:13.726637: step 436, loss 0.287841, acc 0.88
2016-09-06T23:01:14.421318: step 437, loss 0.26794, acc 0.84
2016-09-06T23:01:15.085058: step 438, loss 0.276105, acc 0.86
2016-09-06T23:01:15.778426: step 439, loss 0.257465, acc 0.9
2016-09-06T23:01:16.465203: step 440, loss 0.241543, acc 0.86
2016-09-06T23:01:17.151779: step 441, loss 0.207857, acc 0.94
2016-09-06T23:01:17.832385: step 442, loss 0.14449, acc 0.9
2016-09-06T23:01:18.534004: step 443, loss 0.314394, acc 0.82
2016-09-06T23:01:19.250460: step 444, loss 0.290505, acc 0.86
2016-09-06T23:01:19.917556: step 445, loss 0.118344, acc 0.98
2016-09-06T23:01:20.611832: step 446, loss 0.306403, acc 0.86
2016-09-06T23:01:21.282654: step 447, loss 0.153927, acc 0.98
2016-09-06T23:01:21.994023: step 448, loss 0.386035, acc 0.84
2016-09-06T23:01:22.688404: step 449, loss 0.346011, acc 0.88
2016-09-06T23:01:23.351062: step 450, loss 0.262875, acc 0.88
2016-09-06T23:01:24.046373: step 451, loss 0.227661, acc 0.9
2016-09-06T23:01:24.713636: step 452, loss 0.126915, acc 0.94
2016-09-06T23:01:25.404974: step 453, loss 0.335947, acc 0.86
2016-09-06T23:01:26.069011: step 454, loss 0.373574, acc 0.78
2016-09-06T23:01:26.744395: step 455, loss 0.387446, acc 0.9
2016-09-06T23:01:27.434465: step 456, loss 0.519362, acc 0.8
2016-09-06T23:01:28.109016: step 457, loss 0.185931, acc 0.94
2016-09-06T23:01:28.789706: step 458, loss 0.359151, acc 0.88
2016-09-06T23:01:29.443564: step 459, loss 0.309681, acc 0.84
2016-09-06T23:01:30.146850: step 460, loss 0.427342, acc 0.84
2016-09-06T23:01:30.825421: step 461, loss 0.217783, acc 0.88
2016-09-06T23:01:31.517385: step 462, loss 0.194585, acc 0.9
2016-09-06T23:01:32.198652: step 463, loss 0.309219, acc 0.8
2016-09-06T23:01:32.868292: step 464, loss 0.217343, acc 0.94
2016-09-06T23:01:33.578064: step 465, loss 0.240056, acc 0.9
2016-09-06T23:01:34.268057: step 466, loss 0.206683, acc 0.92
2016-09-06T23:01:34.966775: step 467, loss 0.262224, acc 0.84
2016-09-06T23:01:35.620062: step 468, loss 0.263508, acc 0.88
2016-09-06T23:01:36.309100: step 469, loss 0.222288, acc 0.92
2016-09-06T23:01:36.980583: step 470, loss 0.323852, acc 0.9
2016-09-06T23:01:37.634111: step 471, loss 0.291901, acc 0.82
2016-09-06T23:01:38.312487: step 472, loss 0.261952, acc 0.88
2016-09-06T23:01:38.981113: step 473, loss 0.183969, acc 0.94
2016-09-06T23:01:39.667207: step 474, loss 0.277079, acc 0.84
2016-09-06T23:01:40.348798: step 475, loss 0.273113, acc 0.88
2016-09-06T23:01:41.044005: step 476, loss 0.246366, acc 0.92
2016-09-06T23:01:41.735735: step 477, loss 0.302357, acc 0.9
2016-09-06T23:01:42.423597: step 478, loss 0.232796, acc 0.88
2016-09-06T23:01:43.112344: step 479, loss 0.216596, acc 0.88
2016-09-06T23:01:43.794053: step 480, loss 0.286356, acc 0.86
2016-09-06T23:01:44.493273: step 481, loss 0.275621, acc 0.86
2016-09-06T23:01:45.174698: step 482, loss 0.179179, acc 0.94
2016-09-06T23:01:45.864193: step 483, loss 0.359753, acc 0.84
2016-09-06T23:01:46.550097: step 484, loss 0.189884, acc 0.92
2016-09-06T23:01:47.230044: step 485, loss 0.251453, acc 0.92
2016-09-06T23:01:47.906223: step 486, loss 0.289416, acc 0.86
2016-09-06T23:01:48.605402: step 487, loss 0.141733, acc 0.94
2016-09-06T23:01:49.292257: step 488, loss 0.172409, acc 0.92
2016-09-06T23:01:49.957758: step 489, loss 0.251023, acc 0.86
2016-09-06T23:01:50.662595: step 490, loss 0.177139, acc 0.92
2016-09-06T23:01:51.340980: step 491, loss 0.277014, acc 0.9
2016-09-06T23:01:52.016821: step 492, loss 0.135246, acc 0.94
2016-09-06T23:01:52.686077: step 493, loss 0.310353, acc 0.92
2016-09-06T23:01:53.371607: step 494, loss 0.178651, acc 0.9
2016-09-06T23:01:54.050927: step 495, loss 0.2055, acc 0.9
2016-09-06T23:01:54.719358: step 496, loss 0.179572, acc 0.94
2016-09-06T23:01:55.436692: step 497, loss 0.346298, acc 0.84
2016-09-06T23:01:56.120545: step 498, loss 0.30997, acc 0.88
2016-09-06T23:01:56.810648: step 499, loss 0.468243, acc 0.84
2016-09-06T23:01:57.491099: step 500, loss 0.307459, acc 0.82

Evaluation:
2016-09-06T23:02:00.671140: step 500, loss 0.516557, acc 0.787992

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-500

2016-09-06T23:02:02.436416: step 501, loss 0.323356, acc 0.92
2016-09-06T23:02:03.108192: step 502, loss 0.315939, acc 0.84
2016-09-06T23:02:03.820450: step 503, loss 0.196253, acc 0.96
2016-09-06T23:02:04.496030: step 504, loss 0.325267, acc 0.86
2016-09-06T23:02:05.166479: step 505, loss 0.252305, acc 0.9
2016-09-06T23:02:05.859380: step 506, loss 0.309298, acc 0.86
2016-09-06T23:02:06.530802: step 507, loss 0.231261, acc 0.88
2016-09-06T23:02:07.223541: step 508, loss 0.408811, acc 0.8
2016-09-06T23:02:07.901130: step 509, loss 0.256866, acc 0.88
2016-09-06T23:02:08.608606: step 510, loss 0.239329, acc 0.94
2016-09-06T23:02:09.280349: step 511, loss 0.468216, acc 0.76
2016-09-06T23:02:09.989459: step 512, loss 0.309896, acc 0.84
2016-09-06T23:02:10.684974: step 513, loss 0.311953, acc 0.88
2016-09-06T23:02:11.361217: step 514, loss 0.368993, acc 0.86
2016-09-06T23:02:12.050757: step 515, loss 0.239995, acc 0.9
2016-09-06T23:02:12.727214: step 516, loss 0.217374, acc 0.94
2016-09-06T23:02:13.429824: step 517, loss 0.250755, acc 0.9
2016-09-06T23:02:14.086749: step 518, loss 0.261144, acc 0.88
2016-09-06T23:02:14.748510: step 519, loss 0.273452, acc 0.88
2016-09-06T23:02:15.435066: step 520, loss 0.264849, acc 0.92
2016-09-06T23:02:16.099499: step 521, loss 0.228412, acc 0.94
2016-09-06T23:02:16.781544: step 522, loss 0.41402, acc 0.88
2016-09-06T23:02:17.464068: step 523, loss 0.203104, acc 0.88
2016-09-06T23:02:18.140399: step 524, loss 0.411734, acc 0.8
2016-09-06T23:02:18.799738: step 525, loss 0.123397, acc 0.96
2016-09-06T23:02:19.499160: step 526, loss 0.279572, acc 0.86
2016-09-06T23:02:20.187907: step 527, loss 0.182856, acc 0.92
2016-09-06T23:02:20.894543: step 528, loss 0.333771, acc 0.82
2016-09-06T23:02:21.581022: step 529, loss 0.123199, acc 0.92
2016-09-06T23:02:22.256481: step 530, loss 0.25656, acc 0.88
2016-09-06T23:02:22.952137: step 531, loss 0.3218, acc 0.9
2016-09-06T23:02:23.612201: step 532, loss 0.270752, acc 0.88
2016-09-06T23:02:24.304822: step 533, loss 0.209582, acc 0.9
2016-09-06T23:02:24.987690: step 534, loss 0.300579, acc 0.88
2016-09-06T23:02:25.656088: step 535, loss 0.303038, acc 0.84
2016-09-06T23:02:26.313186: step 536, loss 0.221945, acc 0.92
2016-09-06T23:02:26.997913: step 537, loss 0.248363, acc 0.92
2016-09-06T23:02:27.673891: step 538, loss 0.28766, acc 0.88
2016-09-06T23:02:28.359716: step 539, loss 0.424535, acc 0.86
2016-09-06T23:02:29.048579: step 540, loss 0.18847, acc 0.92
2016-09-06T23:02:29.734503: step 541, loss 0.190148, acc 0.92
2016-09-06T23:02:30.417412: step 542, loss 0.16285, acc 0.96
2016-09-06T23:02:31.088360: step 543, loss 0.216112, acc 0.9
2016-09-06T23:02:31.766409: step 544, loss 0.41453, acc 0.82
2016-09-06T23:02:32.478810: step 545, loss 0.366587, acc 0.82
2016-09-06T23:02:33.152251: step 546, loss 0.216987, acc 0.9
2016-09-06T23:02:33.871790: step 547, loss 0.141972, acc 0.96
2016-09-06T23:02:34.562998: step 548, loss 0.220596, acc 0.84
2016-09-06T23:02:35.259796: step 549, loss 0.236739, acc 0.92
2016-09-06T23:02:35.948941: step 550, loss 0.173745, acc 0.94
2016-09-06T23:02:36.624180: step 551, loss 0.246612, acc 0.9
2016-09-06T23:02:37.298919: step 552, loss 0.169999, acc 0.92
2016-09-06T23:02:37.987917: step 553, loss 0.313764, acc 0.9
2016-09-06T23:02:38.691855: step 554, loss 0.355974, acc 0.86
2016-09-06T23:02:39.375427: step 555, loss 0.198363, acc 0.92
2016-09-06T23:02:40.054901: step 556, loss 0.0754903, acc 0.98
2016-09-06T23:02:40.748797: step 557, loss 0.131713, acc 0.96
2016-09-06T23:02:41.420579: step 558, loss 0.278684, acc 0.86
2016-09-06T23:02:42.100347: step 559, loss 0.231478, acc 0.9
2016-09-06T23:02:42.792891: step 560, loss 0.275081, acc 0.88
2016-09-06T23:02:43.494332: step 561, loss 0.250198, acc 0.86
2016-09-06T23:02:44.180396: step 562, loss 0.23477, acc 0.9
2016-09-06T23:02:44.870158: step 563, loss 0.105309, acc 0.98
2016-09-06T23:02:45.562731: step 564, loss 0.263363, acc 0.86
2016-09-06T23:02:46.258886: step 565, loss 0.255803, acc 0.9
2016-09-06T23:02:46.943170: step 566, loss 0.322503, acc 0.86
2016-09-06T23:02:47.620763: step 567, loss 0.3182, acc 0.88
2016-09-06T23:02:48.318760: step 568, loss 0.252817, acc 0.82
2016-09-06T23:02:48.978944: step 569, loss 0.219137, acc 0.92
2016-09-06T23:02:49.655188: step 570, loss 0.257814, acc 0.9
2016-09-06T23:02:50.348530: step 571, loss 0.35143, acc 0.84
2016-09-06T23:02:51.030211: step 572, loss 0.337463, acc 0.9
2016-09-06T23:02:51.693983: step 573, loss 0.431933, acc 0.78
2016-09-06T23:02:52.369536: step 574, loss 0.280692, acc 0.82
2016-09-06T23:02:53.090045: step 575, loss 0.222615, acc 0.88
2016-09-06T23:02:53.701044: step 576, loss 0.2477, acc 0.886364
2016-09-06T23:02:54.396483: step 577, loss 0.19675, acc 0.9
2016-09-06T23:02:55.117659: step 578, loss 0.20328, acc 0.9
2016-09-06T23:02:55.800395: step 579, loss 0.169543, acc 0.92
2016-09-06T23:02:56.485700: step 580, loss 0.158478, acc 0.96
2016-09-06T23:02:57.181256: step 581, loss 0.180469, acc 0.92
2016-09-06T23:02:57.884660: step 582, loss 0.124844, acc 0.98
2016-09-06T23:02:58.539096: step 583, loss 0.116523, acc 0.98
2016-09-06T23:02:59.245183: step 584, loss 0.148968, acc 0.92
2016-09-06T23:02:59.930307: step 585, loss 0.17818, acc 0.92
2016-09-06T23:03:00.646811: step 586, loss 0.0939473, acc 0.98
2016-09-06T23:03:01.324382: step 587, loss 0.128133, acc 0.94
2016-09-06T23:03:02.013775: step 588, loss 0.177021, acc 0.94
2016-09-06T23:03:02.696936: step 589, loss 0.182032, acc 0.9
2016-09-06T23:03:03.364449: step 590, loss 0.0859137, acc 0.98
2016-09-06T23:03:04.067352: step 591, loss 0.156606, acc 0.94
2016-09-06T23:03:04.749608: step 592, loss 0.107787, acc 0.96
2016-09-06T23:03:05.444225: step 593, loss 0.0979848, acc 0.96
2016-09-06T23:03:06.130393: step 594, loss 0.0578124, acc 0.98
2016-09-06T23:03:06.822176: step 595, loss 0.214745, acc 0.92
2016-09-06T23:03:07.540537: step 596, loss 0.165714, acc 0.92
2016-09-06T23:03:08.235490: step 597, loss 0.154622, acc 0.92
2016-09-06T23:03:08.922869: step 598, loss 0.0493534, acc 1
2016-09-06T23:03:09.588944: step 599, loss 0.0807888, acc 0.96
2016-09-06T23:03:10.280110: step 600, loss 0.0974954, acc 0.98

Evaluation:
2016-09-06T23:03:13.446215: step 600, loss 0.762873, acc 0.792683

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-600

2016-09-06T23:03:15.169270: step 601, loss 0.141064, acc 0.96
2016-09-06T23:03:15.868223: step 602, loss 0.223353, acc 0.92
2016-09-06T23:03:16.535580: step 603, loss 0.197822, acc 0.92
2016-09-06T23:03:17.246609: step 604, loss 0.204713, acc 0.92
2016-09-06T23:03:17.937762: step 605, loss 0.275266, acc 0.94
2016-09-06T23:03:18.631322: step 606, loss 0.0900161, acc 0.96
2016-09-06T23:03:19.323116: step 607, loss 0.180341, acc 0.94
2016-09-06T23:03:20.000585: step 608, loss 0.173603, acc 0.92
2016-09-06T23:03:20.690793: step 609, loss 0.216171, acc 0.92
2016-09-06T23:03:21.347931: step 610, loss 0.312504, acc 0.9
2016-09-06T23:03:22.055991: step 611, loss 0.275434, acc 0.88
2016-09-06T23:03:22.761901: step 612, loss 0.182584, acc 0.92
2016-09-06T23:03:23.463187: step 613, loss 0.117811, acc 0.96
2016-09-06T23:03:24.156682: step 614, loss 0.100092, acc 0.96
2016-09-06T23:03:24.847843: step 615, loss 0.205713, acc 0.9
2016-09-06T23:03:25.558264: step 616, loss 0.164264, acc 0.92
2016-09-06T23:03:26.218358: step 617, loss 0.174914, acc 0.92
2016-09-06T23:03:26.906349: step 618, loss 0.192742, acc 0.9
2016-09-06T23:03:27.609929: step 619, loss 0.314305, acc 0.92
2016-09-06T23:03:28.306703: step 620, loss 0.135076, acc 0.96
2016-09-06T23:03:29.005039: step 621, loss 0.213555, acc 0.9
2016-09-06T23:03:29.702401: step 622, loss 0.209917, acc 0.92
2016-09-06T23:03:30.390461: step 623, loss 0.201419, acc 0.94
2016-09-06T23:03:31.060030: step 624, loss 0.326701, acc 0.86
2016-09-06T23:03:31.753449: step 625, loss 0.179636, acc 0.94
2016-09-06T23:03:32.437002: step 626, loss 0.238293, acc 0.88
2016-09-06T23:03:33.109841: step 627, loss 0.168352, acc 0.94
2016-09-06T23:03:33.790027: step 628, loss 0.0553057, acc 0.98
2016-09-06T23:03:34.469194: step 629, loss 0.087148, acc 0.98
2016-09-06T23:03:35.170505: step 630, loss 0.0963537, acc 0.98
2016-09-06T23:03:35.830201: step 631, loss 0.247662, acc 0.88
2016-09-06T23:03:36.527377: step 632, loss 0.197825, acc 0.92
2016-09-06T23:03:37.220574: step 633, loss 0.189329, acc 0.92
2016-09-06T23:03:37.919105: step 634, loss 0.122915, acc 0.94
2016-09-06T23:03:38.601927: step 635, loss 0.0847817, acc 0.98
2016-09-06T23:03:39.279626: step 636, loss 0.14539, acc 0.98
2016-09-06T23:03:39.951340: step 637, loss 0.227024, acc 0.9
2016-09-06T23:03:40.634071: step 638, loss 0.0893517, acc 0.96
2016-09-06T23:03:41.337069: step 639, loss 0.102433, acc 0.98
2016-09-06T23:03:42.038992: step 640, loss 0.226799, acc 0.92
2016-09-06T23:03:42.736240: step 641, loss 0.0562987, acc 0.98
2016-09-06T23:03:43.428239: step 642, loss 0.196844, acc 0.86
2016-09-06T23:03:44.101367: step 643, loss 0.261152, acc 0.92
2016-09-06T23:03:44.811207: step 644, loss 0.190732, acc 0.92
2016-09-06T23:03:45.501670: step 645, loss 0.171689, acc 0.96
2016-09-06T23:03:46.211662: step 646, loss 0.224322, acc 0.86
2016-09-06T23:03:46.900131: step 647, loss 0.131014, acc 0.94
2016-09-06T23:03:47.591396: step 648, loss 0.130432, acc 0.92
2016-09-06T23:03:48.299368: step 649, loss 0.218637, acc 0.9
2016-09-06T23:03:48.957345: step 650, loss 0.140192, acc 0.94
2016-09-06T23:03:49.667047: step 651, loss 0.0680143, acc 0.96
2016-09-06T23:03:50.350646: step 652, loss 0.110396, acc 0.96
2016-09-06T23:03:51.027787: step 653, loss 0.168038, acc 0.92
2016-09-06T23:03:51.715539: step 654, loss 0.130462, acc 0.96
2016-09-06T23:03:52.414194: step 655, loss 0.212567, acc 0.92
2016-09-06T23:03:53.114298: step 656, loss 0.173842, acc 0.92
2016-09-06T23:03:53.784211: step 657, loss 0.0654959, acc 0.98
2016-09-06T23:03:54.484140: step 658, loss 0.228055, acc 0.88
2016-09-06T23:03:55.165659: step 659, loss 0.180958, acc 0.9
2016-09-06T23:03:55.856119: step 660, loss 0.103533, acc 0.94
2016-09-06T23:03:56.541279: step 661, loss 0.156449, acc 0.88
2016-09-06T23:03:57.212766: step 662, loss 0.210527, acc 0.88
2016-09-06T23:03:57.911557: step 663, loss 0.147608, acc 0.92
2016-09-06T23:03:58.591939: step 664, loss 0.218604, acc 0.96
2016-09-06T23:03:59.285606: step 665, loss 0.435044, acc 0.82
2016-09-06T23:03:59.952131: step 666, loss 0.108358, acc 0.92
2016-09-06T23:04:00.669678: step 667, loss 0.152348, acc 0.96
2016-09-06T23:04:01.357793: step 668, loss 0.181565, acc 0.92
2016-09-06T23:04:02.034711: step 669, loss 0.10827, acc 0.96
2016-09-06T23:04:02.739815: step 670, loss 0.203816, acc 0.9
2016-09-06T23:04:03.421347: step 671, loss 0.137975, acc 0.94
2016-09-06T23:04:04.101632: step 672, loss 0.140775, acc 0.94
2016-09-06T23:04:04.776816: step 673, loss 0.169875, acc 0.9
2016-09-06T23:04:05.454208: step 674, loss 0.132051, acc 0.92
2016-09-06T23:04:06.126992: step 675, loss 0.130556, acc 0.94
2016-09-06T23:04:06.823289: step 676, loss 0.101284, acc 0.96
2016-09-06T23:04:07.503871: step 677, loss 0.165877, acc 0.94
2016-09-06T23:04:08.212667: step 678, loss 0.225236, acc 0.92
2016-09-06T23:04:08.906014: step 679, loss 0.329177, acc 0.92
2016-09-06T23:04:09.586043: step 680, loss 0.0706733, acc 1
2016-09-06T23:04:10.273782: step 681, loss 0.316884, acc 0.92
2016-09-06T23:04:10.957270: step 682, loss 0.137747, acc 0.94
2016-09-06T23:04:11.658883: step 683, loss 0.105534, acc 0.96
2016-09-06T23:04:12.344545: step 684, loss 0.181285, acc 0.88
2016-09-06T23:04:13.015287: step 685, loss 0.190347, acc 0.92
2016-09-06T23:04:13.710895: step 686, loss 0.0918115, acc 0.94
2016-09-06T23:04:14.372190: step 687, loss 0.0899234, acc 1
2016-09-06T23:04:15.038825: step 688, loss 0.213133, acc 0.94
2016-09-06T23:04:15.742427: step 689, loss 0.171359, acc 0.98
2016-09-06T23:04:16.439992: step 690, loss 0.102662, acc 0.94
2016-09-06T23:04:17.129125: step 691, loss 0.180859, acc 0.92
2016-09-06T23:04:17.815800: step 692, loss 0.191037, acc 0.92
2016-09-06T23:04:18.507051: step 693, loss 0.137365, acc 0.94
2016-09-06T23:04:19.187584: step 694, loss 0.151746, acc 0.96
2016-09-06T23:04:19.881960: step 695, loss 0.143789, acc 0.94
2016-09-06T23:04:20.589336: step 696, loss 0.164771, acc 0.92
2016-09-06T23:04:21.241340: step 697, loss 0.244751, acc 0.9
2016-09-06T23:04:21.928241: step 698, loss 0.318273, acc 0.86
2016-09-06T23:04:22.616602: step 699, loss 0.0755153, acc 0.98
2016-09-06T23:04:23.300708: step 700, loss 0.134201, acc 0.92

Evaluation:
2016-09-06T23:04:26.428231: step 700, loss 0.615414, acc 0.785178

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-700

2016-09-06T23:04:28.124890: step 701, loss 0.0732083, acc 0.98
2016-09-06T23:04:28.811521: step 702, loss 0.174703, acc 0.92
2016-09-06T23:04:29.484859: step 703, loss 0.0941229, acc 0.96
2016-09-06T23:04:30.170059: step 704, loss 0.167063, acc 0.88
2016-09-06T23:04:30.880307: step 705, loss 0.332671, acc 0.88
2016-09-06T23:04:31.576142: step 706, loss 0.265618, acc 0.88
2016-09-06T23:04:32.231035: step 707, loss 0.221656, acc 0.88
2016-09-06T23:04:32.925765: step 708, loss 0.207857, acc 0.88
2016-09-06T23:04:33.596104: step 709, loss 0.268967, acc 0.92
2016-09-06T23:04:34.280969: step 710, loss 0.183069, acc 0.92
2016-09-06T23:04:34.968699: step 711, loss 0.102317, acc 0.94
2016-09-06T23:04:35.646467: step 712, loss 0.179945, acc 0.9
2016-09-06T23:04:36.344593: step 713, loss 0.104647, acc 0.96
2016-09-06T23:04:37.044436: step 714, loss 0.122615, acc 0.96
2016-09-06T23:04:37.770958: step 715, loss 0.0971698, acc 0.94
2016-09-06T23:04:38.466500: step 716, loss 0.118871, acc 0.98
2016-09-06T23:04:39.159084: step 717, loss 0.162978, acc 0.92
2016-09-06T23:04:39.835923: step 718, loss 0.0740922, acc 0.98
2016-09-06T23:04:40.545198: step 719, loss 0.179973, acc 0.94
2016-09-06T23:04:41.222703: step 720, loss 0.147408, acc 0.92
2016-09-06T23:04:41.885863: step 721, loss 0.0785871, acc 0.98
2016-09-06T23:04:42.599724: step 722, loss 0.108791, acc 0.96
2016-09-06T23:04:43.267663: step 723, loss 0.143299, acc 0.94
2016-09-06T23:04:43.950822: step 724, loss 0.217966, acc 0.94
2016-09-06T23:04:44.634086: step 725, loss 0.173209, acc 0.92
2016-09-06T23:04:45.320687: step 726, loss 0.211969, acc 0.92
2016-09-06T23:04:46.020579: step 727, loss 0.276626, acc 0.9
2016-09-06T23:04:46.706959: step 728, loss 0.331081, acc 0.9
2016-09-06T23:04:47.397817: step 729, loss 0.235073, acc 0.9
2016-09-06T23:04:48.055158: step 730, loss 0.231135, acc 0.88
2016-09-06T23:04:48.741579: step 731, loss 0.204536, acc 0.92
2016-09-06T23:04:49.433266: step 732, loss 0.137571, acc 0.92
2016-09-06T23:04:50.134922: step 733, loss 0.216695, acc 0.94
2016-09-06T23:04:50.825103: step 734, loss 0.190603, acc 0.9
2016-09-06T23:04:51.486492: step 735, loss 0.0934982, acc 0.94
2016-09-06T23:04:52.196156: step 736, loss 0.356869, acc 0.82
2016-09-06T23:04:52.864065: step 737, loss 0.113161, acc 0.92
2016-09-06T23:04:53.536971: step 738, loss 0.438485, acc 0.8
2016-09-06T23:04:54.246570: step 739, loss 0.280754, acc 0.84
2016-09-06T23:04:54.940458: step 740, loss 0.100903, acc 0.96
2016-09-06T23:04:55.631158: step 741, loss 0.174933, acc 0.9
2016-09-06T23:04:56.301619: step 742, loss 0.265105, acc 0.9
2016-09-06T23:04:57.001325: step 743, loss 0.124697, acc 0.94
2016-09-06T23:04:57.666279: step 744, loss 0.170685, acc 0.9
2016-09-06T23:04:58.328754: step 745, loss 0.113463, acc 0.96
2016-09-06T23:04:59.011627: step 746, loss 0.264074, acc 0.88
2016-09-06T23:04:59.688397: step 747, loss 0.265095, acc 0.9
2016-09-06T23:05:00.413836: step 748, loss 0.200434, acc 0.9
2016-09-06T23:05:01.091638: step 749, loss 0.198026, acc 0.86
2016-09-06T23:05:01.795921: step 750, loss 0.220145, acc 0.94
2016-09-06T23:05:02.470824: step 751, loss 0.174238, acc 0.98
2016-09-06T23:05:03.137153: step 752, loss 0.132275, acc 0.92
2016-09-06T23:05:03.816343: step 753, loss 0.194677, acc 0.94
2016-09-06T23:05:04.498123: step 754, loss 0.140605, acc 0.94
2016-09-06T23:05:05.191055: step 755, loss 0.283183, acc 0.84
2016-09-06T23:05:05.858957: step 756, loss 0.117827, acc 0.96
2016-09-06T23:05:06.540691: step 757, loss 0.18516, acc 0.94
2016-09-06T23:05:07.198573: step 758, loss 0.15758, acc 0.94
2016-09-06T23:05:07.907685: step 759, loss 0.105565, acc 0.96
2016-09-06T23:05:08.589335: step 760, loss 0.198487, acc 0.92
2016-09-06T23:05:09.280001: step 761, loss 0.211308, acc 0.88
2016-09-06T23:05:09.951607: step 762, loss 0.0836212, acc 1
2016-09-06T23:05:10.638859: step 763, loss 0.147299, acc 0.94
2016-09-06T23:05:11.364308: step 764, loss 0.0842876, acc 0.98
2016-09-06T23:05:12.037771: step 765, loss 0.0951788, acc 0.94
2016-09-06T23:05:12.734865: step 766, loss 0.254476, acc 0.92
2016-09-06T23:05:13.412444: step 767, loss 0.162316, acc 0.94
2016-09-06T23:05:14.070040: step 768, loss 0.228303, acc 0.886364
2016-09-06T23:05:14.766338: step 769, loss 0.0966042, acc 0.96
2016-09-06T23:05:15.444379: step 770, loss 0.132461, acc 0.96
2016-09-06T23:05:16.174355: step 771, loss 0.196003, acc 0.88
2016-09-06T23:05:16.849081: step 772, loss 0.121117, acc 0.98
2016-09-06T23:05:17.533786: step 773, loss 0.111877, acc 0.94
2016-09-06T23:05:18.218224: step 774, loss 0.324886, acc 0.86
2016-09-06T23:05:18.896773: step 775, loss 0.276641, acc 0.86
2016-09-06T23:05:19.593530: step 776, loss 0.0773402, acc 0.96
2016-09-06T23:05:20.266762: step 777, loss 0.308131, acc 0.86
2016-09-06T23:05:20.978279: step 778, loss 0.233542, acc 0.9
2016-09-06T23:05:21.631863: step 779, loss 0.0496194, acc 1
2016-09-06T23:05:22.340151: step 780, loss 0.157424, acc 0.92
2016-09-06T23:05:23.029061: step 781, loss 0.19415, acc 0.92
2016-09-06T23:05:23.712280: step 782, loss 0.14244, acc 0.92
2016-09-06T23:05:24.387993: step 783, loss 0.0603602, acc 1
2016-09-06T23:05:25.083300: step 784, loss 0.147936, acc 0.96
2016-09-06T23:05:25.782552: step 785, loss 0.235499, acc 0.92
2016-09-06T23:05:26.433630: step 786, loss 0.149061, acc 0.94
2016-09-06T23:05:27.137437: step 787, loss 0.122911, acc 0.96
2016-09-06T23:05:27.811957: step 788, loss 0.15811, acc 0.96
2016-09-06T23:05:28.515852: step 789, loss 0.0931203, acc 0.98
2016-09-06T23:05:29.213732: step 790, loss 0.113689, acc 0.96
2016-09-06T23:05:29.904860: step 791, loss 0.330679, acc 0.88
2016-09-06T23:05:30.600698: step 792, loss 0.0947833, acc 0.96
2016-09-06T23:05:31.254492: step 793, loss 0.0870073, acc 0.98
2016-09-06T23:05:31.958785: step 794, loss 0.0788542, acc 0.96
2016-09-06T23:05:32.656044: step 795, loss 0.138627, acc 0.94
2016-09-06T23:05:33.335116: step 796, loss 0.0742209, acc 1
2016-09-06T23:05:34.019409: step 797, loss 0.0584037, acc 1
2016-09-06T23:05:34.678446: step 798, loss 0.135133, acc 0.94
2016-09-06T23:05:35.347942: step 799, loss 0.0966522, acc 0.94
2016-09-06T23:05:36.017399: step 800, loss 0.266932, acc 0.82

Evaluation:
2016-09-06T23:05:39.174645: step 800, loss 0.727725, acc 0.77955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-800

2016-09-06T23:05:40.908451: step 801, loss 0.0648578, acc 0.96
2016-09-06T23:05:41.593833: step 802, loss 0.0515556, acc 0.98
2016-09-06T23:05:42.291718: step 803, loss 0.078822, acc 0.94
2016-09-06T23:05:42.979404: step 804, loss 0.0742765, acc 0.96
2016-09-06T23:05:43.678654: step 805, loss 0.0204287, acc 1
2016-09-06T23:05:44.386105: step 806, loss 0.15648, acc 0.98
2016-09-06T23:05:45.103154: step 807, loss 0.0855196, acc 0.96
2016-09-06T23:05:45.777530: step 808, loss 0.0739462, acc 0.98
2016-09-06T23:05:46.443720: step 809, loss 0.0796826, acc 0.94
2016-09-06T23:05:47.115247: step 810, loss 0.0561975, acc 0.98
2016-09-06T23:05:47.798607: step 811, loss 0.212051, acc 0.94
2016-09-06T23:05:48.501307: step 812, loss 0.192246, acc 0.92
2016-09-06T23:05:49.189923: step 813, loss 0.0684626, acc 0.98
2016-09-06T23:05:49.884013: step 814, loss 0.117047, acc 0.94
2016-09-06T23:05:50.551810: step 815, loss 0.0689566, acc 0.96
2016-09-06T23:05:51.229103: step 816, loss 0.0636932, acc 0.96
2016-09-06T23:05:51.911840: step 817, loss 0.0645312, acc 0.98
2016-09-06T23:05:52.589705: step 818, loss 0.221839, acc 0.92
2016-09-06T23:05:53.254449: step 819, loss 0.0931963, acc 0.98
2016-09-06T23:05:53.955905: step 820, loss 0.0636746, acc 0.96
2016-09-06T23:05:54.646356: step 821, loss 0.211106, acc 0.9
2016-09-06T23:05:55.310804: step 822, loss 0.0580502, acc 0.98
2016-09-06T23:05:56.004461: step 823, loss 0.242887, acc 0.92
2016-09-06T23:05:56.697875: step 824, loss 0.25489, acc 0.88
2016-09-06T23:05:57.382282: step 825, loss 0.067863, acc 0.96
2016-09-06T23:05:58.068375: step 826, loss 0.190832, acc 0.9
2016-09-06T23:05:58.746827: step 827, loss 0.143319, acc 0.94
2016-09-06T23:05:59.432666: step 828, loss 0.159621, acc 0.94
2016-09-06T23:06:00.102757: step 829, loss 0.26134, acc 0.92
2016-09-06T23:06:00.820599: step 830, loss 0.0839235, acc 0.96
2016-09-06T23:06:01.513957: step 831, loss 0.189367, acc 0.88
2016-09-06T23:06:02.193879: step 832, loss 0.180878, acc 0.92
2016-09-06T23:06:02.881354: step 833, loss 0.0954737, acc 0.98
2016-09-06T23:06:03.579860: step 834, loss 0.185511, acc 0.88
2016-09-06T23:06:04.291309: step 835, loss 0.0723983, acc 0.98
2016-09-06T23:06:04.958219: step 836, loss 0.137096, acc 0.92
2016-09-06T23:06:05.643207: step 837, loss 0.171614, acc 0.92
2016-09-06T23:06:06.329562: step 838, loss 0.294524, acc 0.9
2016-09-06T23:06:07.011046: step 839, loss 0.0942247, acc 0.96
2016-09-06T23:06:07.694071: step 840, loss 0.0652395, acc 0.98
2016-09-06T23:06:08.382729: step 841, loss 0.16202, acc 0.96
2016-09-06T23:06:09.100771: step 842, loss 0.0949843, acc 0.94
2016-09-06T23:06:09.755000: step 843, loss 0.144692, acc 0.94
2016-09-06T23:06:10.471361: step 844, loss 0.091984, acc 0.94
2016-09-06T23:06:11.166812: step 845, loss 0.131184, acc 0.96
2016-09-06T23:06:11.856098: step 846, loss 0.0723904, acc 0.94
2016-09-06T23:06:12.543381: step 847, loss 0.0508902, acc 0.98
2016-09-06T23:06:13.231505: step 848, loss 0.0708529, acc 0.98
2016-09-06T23:06:13.928308: step 849, loss 0.115523, acc 0.94
2016-09-06T23:06:14.589686: step 850, loss 0.133064, acc 0.96
2016-09-06T23:06:15.277287: step 851, loss 0.0893058, acc 0.98
2016-09-06T23:06:15.959933: step 852, loss 0.0875826, acc 0.94
2016-09-06T23:06:16.629504: step 853, loss 0.21545, acc 0.92
2016-09-06T23:06:17.306598: step 854, loss 0.160961, acc 0.96
2016-09-06T23:06:18.004972: step 855, loss 0.119701, acc 0.96
2016-09-06T23:06:18.719310: step 856, loss 0.0355861, acc 1
2016-09-06T23:06:19.393859: step 857, loss 0.219488, acc 0.92
2016-09-06T23:06:20.075206: step 858, loss 0.217777, acc 0.88
2016-09-06T23:06:20.769763: step 859, loss 0.0326437, acc 1
2016-09-06T23:06:21.453333: step 860, loss 0.204051, acc 0.92
2016-09-06T23:06:22.171712: step 861, loss 0.0854648, acc 0.96
2016-09-06T23:06:22.883099: step 862, loss 0.157686, acc 0.94
2016-09-06T23:06:23.579465: step 863, loss 0.076734, acc 0.94
2016-09-06T23:06:24.262859: step 864, loss 0.107822, acc 0.94
2016-09-06T23:06:24.947335: step 865, loss 0.150517, acc 0.94
2016-09-06T23:06:25.637855: step 866, loss 0.0973183, acc 0.94
2016-09-06T23:06:26.322994: step 867, loss 0.198214, acc 0.92
2016-09-06T23:06:27.012210: step 868, loss 0.154818, acc 0.94
2016-09-06T23:06:27.684504: step 869, loss 0.161254, acc 0.94
2016-09-06T23:06:28.362239: step 870, loss 0.0438065, acc 1
2016-09-06T23:06:29.025341: step 871, loss 0.112225, acc 0.96
2016-09-06T23:06:29.708101: step 872, loss 0.105954, acc 0.96
2016-09-06T23:06:30.402218: step 873, loss 0.145026, acc 0.94
2016-09-06T23:06:31.067129: step 874, loss 0.211233, acc 0.9
2016-09-06T23:06:31.775972: step 875, loss 0.124558, acc 0.92
2016-09-06T23:06:32.449537: step 876, loss 0.110463, acc 0.94
2016-09-06T23:06:33.136502: step 877, loss 0.0805307, acc 0.96
2016-09-06T23:06:33.794467: step 878, loss 0.186862, acc 0.96
2016-09-06T23:06:34.495008: step 879, loss 0.120608, acc 0.94
2016-09-06T23:06:35.181548: step 880, loss 0.0972013, acc 0.94
2016-09-06T23:06:35.874556: step 881, loss 0.0385733, acc 1
2016-09-06T23:06:36.548935: step 882, loss 0.178051, acc 0.96
2016-09-06T23:06:37.235516: step 883, loss 0.140499, acc 0.92
2016-09-06T23:06:37.939745: step 884, loss 0.181894, acc 0.92
2016-09-06T23:06:38.623802: step 885, loss 0.0893745, acc 0.94
2016-09-06T23:06:39.305569: step 886, loss 0.10094, acc 0.96
2016-09-06T23:06:40.005901: step 887, loss 0.191704, acc 0.86
2016-09-06T23:06:40.674961: step 888, loss 0.091028, acc 0.94
2016-09-06T23:06:41.369980: step 889, loss 0.0563384, acc 0.98
2016-09-06T23:06:42.061950: step 890, loss 0.105051, acc 0.96
2016-09-06T23:06:42.784160: step 891, loss 0.18672, acc 0.9
2016-09-06T23:06:43.448295: step 892, loss 0.205137, acc 0.88
2016-09-06T23:06:44.120766: step 893, loss 0.138825, acc 0.94
2016-09-06T23:06:44.806369: step 894, loss 0.0838577, acc 0.94
2016-09-06T23:06:45.484656: step 895, loss 0.0412348, acc 1
2016-09-06T23:06:46.180352: step 896, loss 0.203816, acc 0.92
2016-09-06T23:06:46.870828: step 897, loss 0.0716347, acc 0.98
2016-09-06T23:06:47.574575: step 898, loss 0.107091, acc 0.96
2016-09-06T23:06:48.227370: step 899, loss 0.0974753, acc 0.98
2016-09-06T23:06:48.936253: step 900, loss 0.100648, acc 0.94

Evaluation:
2016-09-06T23:06:52.079913: step 900, loss 0.832341, acc 0.776735

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-900

2016-09-06T23:06:53.722523: step 901, loss 0.15976, acc 0.9
2016-09-06T23:06:54.401227: step 902, loss 0.0741475, acc 0.98
2016-09-06T23:06:55.077913: step 903, loss 0.0670866, acc 0.98
2016-09-06T23:06:55.767711: step 904, loss 0.232691, acc 0.9
2016-09-06T23:06:56.470920: step 905, loss 0.128447, acc 0.94
2016-09-06T23:06:57.167755: step 906, loss 0.100288, acc 0.96
2016-09-06T23:06:57.837119: step 907, loss 0.0938178, acc 0.94
2016-09-06T23:06:58.531500: step 908, loss 0.140859, acc 0.92
2016-09-06T23:06:59.202344: step 909, loss 0.165249, acc 0.96
2016-09-06T23:06:59.908987: step 910, loss 0.215737, acc 0.9
2016-09-06T23:07:00.639552: step 911, loss 0.29663, acc 0.9
2016-09-06T23:07:01.368381: step 912, loss 0.243226, acc 0.94
2016-09-06T23:07:02.074646: step 913, loss 0.198145, acc 0.9
2016-09-06T23:07:02.723872: step 914, loss 0.101606, acc 0.96
2016-09-06T23:07:03.396128: step 915, loss 0.0641813, acc 1
2016-09-06T23:07:04.084420: step 916, loss 0.086554, acc 0.98
2016-09-06T23:07:04.772433: step 917, loss 0.115696, acc 0.94
2016-09-06T23:07:05.454108: step 918, loss 0.159471, acc 0.92
2016-09-06T23:07:06.130673: step 919, loss 0.391292, acc 0.84
2016-09-06T23:07:06.838486: step 920, loss 0.227253, acc 0.92
2016-09-06T23:07:07.515041: step 921, loss 0.0428844, acc 1
2016-09-06T23:07:08.192361: step 922, loss 0.0787278, acc 0.98
2016-09-06T23:07:08.873126: step 923, loss 0.128532, acc 0.96
2016-09-06T23:07:09.566890: step 924, loss 0.230551, acc 0.86
2016-09-06T23:07:10.252632: step 925, loss 0.109878, acc 0.94
2016-09-06T23:07:10.950054: step 926, loss 0.150007, acc 0.9
2016-09-06T23:07:11.661882: step 927, loss 0.153252, acc 0.9
2016-09-06T23:07:12.329071: step 928, loss 0.116127, acc 0.92
2016-09-06T23:07:13.014396: step 929, loss 0.0415611, acc 1
2016-09-06T23:07:13.691832: step 930, loss 0.0902034, acc 0.98
2016-09-06T23:07:14.375550: step 931, loss 0.140414, acc 0.96
2016-09-06T23:07:15.068367: step 932, loss 0.118845, acc 0.94
2016-09-06T23:07:15.772235: step 933, loss 0.0973752, acc 0.94
2016-09-06T23:07:16.469662: step 934, loss 0.258311, acc 0.92
2016-09-06T23:07:17.209430: step 935, loss 0.328364, acc 0.86
2016-09-06T23:07:17.894977: step 936, loss 0.155019, acc 0.92
2016-09-06T23:07:18.575547: step 937, loss 0.118816, acc 0.98
2016-09-06T23:07:19.261271: step 938, loss 0.0753234, acc 0.98
2016-09-06T23:07:19.948348: step 939, loss 0.133061, acc 0.92
2016-09-06T23:07:20.616240: step 940, loss 0.083347, acc 0.96
2016-09-06T23:07:21.314281: step 941, loss 0.313934, acc 0.9
2016-09-06T23:07:22.006959: step 942, loss 0.156586, acc 0.9
2016-09-06T23:07:22.676516: step 943, loss 0.34108, acc 0.86
2016-09-06T23:07:23.346237: step 944, loss 0.144181, acc 0.96
2016-09-06T23:07:24.029912: step 945, loss 0.0618011, acc 0.98
2016-09-06T23:07:24.721125: step 946, loss 0.139985, acc 0.92
2016-09-06T23:07:25.402025: step 947, loss 0.140278, acc 0.94
2016-09-06T23:07:26.092716: step 948, loss 0.060664, acc 1
2016-09-06T23:07:26.778396: step 949, loss 0.246896, acc 0.88
2016-09-06T23:07:27.459712: step 950, loss 0.150906, acc 0.98
2016-09-06T23:07:28.134914: step 951, loss 0.137127, acc 0.94
2016-09-06T23:07:28.799418: step 952, loss 0.151088, acc 0.94
2016-09-06T23:07:29.467460: step 953, loss 0.180503, acc 0.92
2016-09-06T23:07:30.141132: step 954, loss 0.0837759, acc 0.94
2016-09-06T23:07:30.846663: step 955, loss 0.139079, acc 0.92
2016-09-06T23:07:31.535990: step 956, loss 0.101374, acc 0.96
2016-09-06T23:07:32.231676: step 957, loss 0.0834191, acc 0.96
2016-09-06T23:07:32.934043: step 958, loss 0.075777, acc 0.98
2016-09-06T23:07:33.620901: step 959, loss 0.151725, acc 0.92
2016-09-06T23:07:34.274647: step 960, loss 0.212606, acc 0.931818
2016-09-06T23:07:34.984938: step 961, loss 0.0876934, acc 0.96
2016-09-06T23:07:35.694287: step 962, loss 0.100721, acc 0.96
2016-09-06T23:07:36.390713: step 963, loss 0.137567, acc 0.94
2016-09-06T23:07:37.061781: step 964, loss 0.0531345, acc 0.98
2016-09-06T23:07:37.753517: step 965, loss 0.059841, acc 1
2016-09-06T23:07:38.452509: step 966, loss 0.109481, acc 0.92
2016-09-06T23:07:39.145415: step 967, loss 0.0536308, acc 0.98
2016-09-06T23:07:39.818868: step 968, loss 0.0515913, acc 0.98
2016-09-06T23:07:40.530260: step 969, loss 0.0491545, acc 0.98
2016-09-06T23:07:41.207150: step 970, loss 0.0688425, acc 1
2016-09-06T23:07:41.894733: step 971, loss 0.0189347, acc 1
2016-09-06T23:07:42.589351: step 972, loss 0.0826084, acc 0.98
2016-09-06T23:07:43.275532: step 973, loss 0.120971, acc 0.92
2016-09-06T23:07:43.963832: step 974, loss 0.0956356, acc 0.94
2016-09-06T23:07:44.628812: step 975, loss 0.0562388, acc 0.98
2016-09-06T23:07:45.336822: step 976, loss 0.0556097, acc 0.98
2016-09-06T23:07:46.015978: step 977, loss 0.0422541, acc 1
2016-09-06T23:07:46.690479: step 978, loss 0.123162, acc 0.94
2016-09-06T23:07:47.355111: step 979, loss 0.0728821, acc 0.94
2016-09-06T23:07:48.044226: step 980, loss 0.0927994, acc 0.94
2016-09-06T23:07:48.735127: step 981, loss 0.076802, acc 0.96
2016-09-06T23:07:49.413697: step 982, loss 0.0544913, acc 1
2016-09-06T23:07:50.120181: step 983, loss 0.171025, acc 0.92
2016-09-06T23:07:50.791491: step 984, loss 0.101125, acc 0.96
2016-09-06T23:07:51.483600: step 985, loss 0.0502218, acc 0.98
2016-09-06T23:07:52.181429: step 986, loss 0.122894, acc 0.94
2016-09-06T23:07:52.875730: step 987, loss 0.0303399, acc 0.98
2016-09-06T23:07:53.558396: step 988, loss 0.0598941, acc 0.98
2016-09-06T23:07:54.244530: step 989, loss 0.141798, acc 0.96
2016-09-06T23:07:54.933009: step 990, loss 0.0699156, acc 0.96
2016-09-06T23:07:55.596118: step 991, loss 0.061384, acc 0.96
2016-09-06T23:07:56.298073: step 992, loss 0.0860819, acc 0.96
2016-09-06T23:07:56.985818: step 993, loss 0.110187, acc 0.96
2016-09-06T23:07:57.674649: step 994, loss 0.0187197, acc 1
2016-09-06T23:07:58.350509: step 995, loss 0.118961, acc 0.92
2016-09-06T23:07:59.030378: step 996, loss 0.230669, acc 0.88
2016-09-06T23:07:59.727787: step 997, loss 0.0601873, acc 0.98
2016-09-06T23:08:00.417792: step 998, loss 0.28399, acc 0.94
2016-09-06T23:08:01.092745: step 999, loss 0.0435484, acc 0.98
2016-09-06T23:08:01.768523: step 1000, loss 0.164456, acc 0.98

Evaluation:
2016-09-06T23:08:04.907296: step 1000, loss 0.912078, acc 0.778612

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-1000

2016-09-06T23:08:06.638211: step 1001, loss 0.0147425, acc 1
2016-09-06T23:08:07.311742: step 1002, loss 0.106336, acc 0.9
2016-09-06T23:08:07.990411: step 1003, loss 0.0628922, acc 0.96
2016-09-06T23:08:08.671849: step 1004, loss 0.147821, acc 0.94
2016-09-06T23:08:09.387721: step 1005, loss 0.0525554, acc 0.98
2016-09-06T23:08:10.049136: step 1006, loss 0.126741, acc 0.92
2016-09-06T23:08:10.723890: step 1007, loss 0.0979374, acc 0.94
2016-09-06T23:08:11.432409: step 1008, loss 0.0821497, acc 0.96
2016-09-06T23:08:12.111045: step 1009, loss 0.0515572, acc 0.98
2016-09-06T23:08:12.799236: step 1010, loss 0.0978932, acc 0.96
2016-09-06T23:08:13.469813: step 1011, loss 0.225959, acc 0.88
2016-09-06T23:08:14.161812: step 1012, loss 0.220771, acc 0.92
2016-09-06T23:08:14.834045: step 1013, loss 0.136106, acc 0.96
2016-09-06T23:08:15.513900: step 1014, loss 0.0578309, acc 0.96
2016-09-06T23:08:16.203558: step 1015, loss 0.0501826, acc 0.98
2016-09-06T23:08:16.893951: step 1016, loss 0.0239032, acc 1
2016-09-06T23:08:17.591631: step 1017, loss 0.11947, acc 0.92
2016-09-06T23:08:18.284453: step 1018, loss 0.0393985, acc 0.98
2016-09-06T23:08:18.997520: step 1019, loss 0.115502, acc 0.94
2016-09-06T23:08:19.672627: step 1020, loss 0.0793305, acc 0.96
2016-09-06T23:08:20.358393: step 1021, loss 0.133114, acc 0.9
2016-09-06T23:08:21.052119: step 1022, loss 0.107926, acc 0.94
2016-09-06T23:08:21.728025: step 1023, loss 0.0849324, acc 0.96
2016-09-06T23:08:22.407771: step 1024, loss 0.171595, acc 0.92
2016-09-06T23:08:23.097279: step 1025, loss 0.0700401, acc 0.98
2016-09-06T23:08:23.832847: step 1026, loss 0.181232, acc 0.88
2016-09-06T23:08:24.523271: step 1027, loss 0.166894, acc 0.92
2016-09-06T23:08:25.213267: step 1028, loss 0.0348572, acc 1
2016-09-06T23:08:25.909666: step 1029, loss 0.0889734, acc 0.92
2016-09-06T23:08:26.594503: step 1030, loss 0.0837421, acc 0.96
2016-09-06T23:08:27.293743: step 1031, loss 0.0723754, acc 0.96
2016-09-06T23:08:27.965462: step 1032, loss 0.0679767, acc 0.96
2016-09-06T23:08:28.647748: step 1033, loss 0.0374554, acc 0.98
2016-09-06T23:08:29.338310: step 1034, loss 0.0841496, acc 0.98
2016-09-06T23:08:30.019462: step 1035, loss 0.156004, acc 0.92
2016-09-06T23:08:30.691356: step 1036, loss 0.114161, acc 0.96
2016-09-06T23:08:31.378210: step 1037, loss 0.0644626, acc 0.98
2016-09-06T23:08:32.069397: step 1038, loss 0.0745314, acc 0.96
2016-09-06T23:08:32.719855: step 1039, loss 0.0989627, acc 0.94
2016-09-06T23:08:33.427971: step 1040, loss 0.0506251, acc 1
2016-09-06T23:08:34.113528: step 1041, loss 0.11639, acc 0.96
2016-09-06T23:08:34.783226: step 1042, loss 0.0406132, acc 1
2016-09-06T23:08:35.472941: step 1043, loss 0.138846, acc 0.96
2016-09-06T23:08:36.154022: step 1044, loss 0.237669, acc 0.94
2016-09-06T23:08:36.818731: step 1045, loss 0.194182, acc 0.9
2016-09-06T23:08:37.504180: step 1046, loss 0.0569883, acc 0.98
2016-09-06T23:08:38.223454: step 1047, loss 0.0342122, acc 1
2016-09-06T23:08:38.906154: step 1048, loss 0.19107, acc 0.96
2016-09-06T23:08:39.586913: step 1049, loss 0.055028, acc 0.98
2016-09-06T23:08:40.281243: step 1050, loss 0.104636, acc 0.96
2016-09-06T23:08:40.980103: step 1051, loss 0.0884116, acc 0.98
2016-09-06T23:08:41.662414: step 1052, loss 0.121201, acc 0.9
2016-09-06T23:08:42.347571: step 1053, loss 0.0720096, acc 0.98
2016-09-06T23:08:43.071220: step 1054, loss 0.0306661, acc 0.98
2016-09-06T23:08:43.750732: step 1055, loss 0.0888819, acc 0.96
2016-09-06T23:08:44.423119: step 1056, loss 0.264857, acc 0.88
2016-09-06T23:08:45.095396: step 1057, loss 0.0888356, acc 0.96
2016-09-06T23:08:45.780725: step 1058, loss 0.312525, acc 0.92
2016-09-06T23:08:46.451693: step 1059, loss 0.0569244, acc 0.98
2016-09-06T23:08:47.135909: step 1060, loss 0.0917799, acc 0.94
2016-09-06T23:08:47.827056: step 1061, loss 0.15647, acc 0.94
2016-09-06T23:08:48.511375: step 1062, loss 0.178317, acc 0.94
2016-09-06T23:08:49.199173: step 1063, loss 0.263833, acc 0.98
2016-09-06T23:08:49.876920: step 1064, loss 0.159486, acc 0.92
2016-09-06T23:08:50.574571: step 1065, loss 0.138941, acc 0.98
2016-09-06T23:08:51.261677: step 1066, loss 0.0742744, acc 0.96
2016-09-06T23:08:51.950704: step 1067, loss 0.217502, acc 0.9
2016-09-06T23:08:52.639294: step 1068, loss 0.0882029, acc 0.96
2016-09-06T23:08:53.322171: step 1069, loss 0.108982, acc 0.96
2016-09-06T23:08:54.041409: step 1070, loss 0.0875651, acc 0.96
2016-09-06T23:08:54.727548: step 1071, loss 0.149732, acc 0.98
2016-09-06T23:08:55.407618: step 1072, loss 0.115377, acc 0.92
2016-09-06T23:08:56.090804: step 1073, loss 0.12206, acc 0.96
2016-09-06T23:08:56.753771: step 1074, loss 0.0792325, acc 0.98
2016-09-06T23:08:57.467339: step 1075, loss 0.0743906, acc 0.96
2016-09-06T23:08:58.138289: step 1076, loss 0.0667574, acc 0.96
2016-09-06T23:08:58.846197: step 1077, loss 0.040657, acc 1
2016-09-06T23:08:59.540965: step 1078, loss 0.0805712, acc 0.96
2016-09-06T23:09:00.234572: step 1079, loss 0.0396722, acc 1
2016-09-06T23:09:00.910614: step 1080, loss 0.12697, acc 0.96
2016-09-06T23:09:01.580702: step 1081, loss 0.127337, acc 0.94
2016-09-06T23:09:02.277296: step 1082, loss 0.165621, acc 0.92
2016-09-06T23:09:02.958857: step 1083, loss 0.156457, acc 0.94
2016-09-06T23:09:03.637075: step 1084, loss 0.20831, acc 0.94
2016-09-06T23:09:04.317724: step 1085, loss 0.168794, acc 0.92
2016-09-06T23:09:05.021153: step 1086, loss 0.0690728, acc 0.98
2016-09-06T23:09:05.693913: step 1087, loss 0.164811, acc 0.92
2016-09-06T23:09:06.383959: step 1088, loss 0.097625, acc 0.94
2016-09-06T23:09:07.082639: step 1089, loss 0.115841, acc 0.98
2016-09-06T23:09:07.758823: step 1090, loss 0.102357, acc 0.96
2016-09-06T23:09:08.460895: step 1091, loss 0.106971, acc 0.92
2016-09-06T23:09:09.149073: step 1092, loss 0.101413, acc 0.96
2016-09-06T23:09:09.834829: step 1093, loss 0.0857795, acc 0.98
2016-09-06T23:09:10.512327: step 1094, loss 0.122344, acc 0.96
2016-09-06T23:09:11.194619: step 1095, loss 0.0765973, acc 0.96
2016-09-06T23:09:11.891091: step 1096, loss 0.115051, acc 0.94
2016-09-06T23:09:12.566056: step 1097, loss 0.192289, acc 0.92
2016-09-06T23:09:13.257365: step 1098, loss 0.209003, acc 0.94
2016-09-06T23:09:13.942631: step 1099, loss 0.177086, acc 0.9
2016-09-06T23:09:14.630495: step 1100, loss 0.195444, acc 0.94

Evaluation:
2016-09-06T23:09:17.787031: step 1100, loss 0.733063, acc 0.786116

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-1100

2016-09-06T23:09:19.502002: step 1101, loss 0.115522, acc 0.98
2016-09-06T23:09:20.175644: step 1102, loss 0.0534807, acc 0.98
2016-09-06T23:09:20.842568: step 1103, loss 0.138596, acc 0.98
2016-09-06T23:09:21.539003: step 1104, loss 0.0935172, acc 0.94
2016-09-06T23:09:22.208261: step 1105, loss 0.155209, acc 0.94
2016-09-06T23:09:22.884792: step 1106, loss 0.0998791, acc 0.94
2016-09-06T23:09:23.563837: step 1107, loss 0.0970933, acc 0.94
2016-09-06T23:09:24.234804: step 1108, loss 0.0854317, acc 0.98
2016-09-06T23:09:24.922677: step 1109, loss 0.190614, acc 0.94
2016-09-06T23:09:25.602126: step 1110, loss 0.0866573, acc 0.96
2016-09-06T23:09:26.306404: step 1111, loss 0.102858, acc 0.96
2016-09-06T23:09:26.971875: step 1112, loss 0.113124, acc 0.94
2016-09-06T23:09:27.649425: step 1113, loss 0.107364, acc 0.96
2016-09-06T23:09:28.316812: step 1114, loss 0.206154, acc 0.92
2016-09-06T23:09:28.995609: step 1115, loss 0.168347, acc 0.92
2016-09-06T23:09:29.671459: step 1116, loss 0.127254, acc 0.96
2016-09-06T23:09:30.359998: step 1117, loss 0.0639923, acc 0.98
2016-09-06T23:09:31.064004: step 1118, loss 0.101767, acc 0.96
2016-09-06T23:09:31.758371: step 1119, loss 0.0631827, acc 0.96
2016-09-06T23:09:32.447206: step 1120, loss 0.19266, acc 0.92
2016-09-06T23:09:33.162491: step 1121, loss 0.186807, acc 0.9
2016-09-06T23:09:33.861806: step 1122, loss 0.0807151, acc 0.96
2016-09-06T23:09:34.551558: step 1123, loss 0.0535537, acc 1
2016-09-06T23:09:35.230051: step 1124, loss 0.105615, acc 0.96
2016-09-06T23:09:35.919458: step 1125, loss 0.123958, acc 0.96
2016-09-06T23:09:36.590197: step 1126, loss 0.188972, acc 0.9
2016-09-06T23:09:37.285266: step 1127, loss 0.0275926, acc 1
2016-09-06T23:09:37.974815: step 1128, loss 0.0587778, acc 0.98
2016-09-06T23:09:38.653233: step 1129, loss 0.161643, acc 0.9
2016-09-06T23:09:39.345206: step 1130, loss 0.0741629, acc 0.98
2016-09-06T23:09:40.034728: step 1131, loss 0.0845182, acc 0.94
2016-09-06T23:09:40.726913: step 1132, loss 0.0604491, acc 1
2016-09-06T23:09:41.394823: step 1133, loss 0.0415918, acc 1
2016-09-06T23:09:42.060903: step 1134, loss 0.125257, acc 0.96
2016-09-06T23:09:42.745832: step 1135, loss 0.074097, acc 0.98
2016-09-06T23:09:43.432076: step 1136, loss 0.203292, acc 0.92
2016-09-06T23:09:44.122101: step 1137, loss 0.0783878, acc 0.96
2016-09-06T23:09:44.810865: step 1138, loss 0.270586, acc 0.86
2016-09-06T23:09:45.509722: step 1139, loss 0.122175, acc 0.94
2016-09-06T23:09:46.197052: step 1140, loss 0.104441, acc 0.94
2016-09-06T23:09:46.887238: step 1141, loss 0.0864237, acc 0.94
2016-09-06T23:09:47.582043: step 1142, loss 0.0657396, acc 0.96
2016-09-06T23:09:48.276876: step 1143, loss 0.0158344, acc 1
2016-09-06T23:09:48.966716: step 1144, loss 0.129211, acc 0.94
2016-09-06T23:09:49.632010: step 1145, loss 0.0658252, acc 0.98
2016-09-06T23:09:50.338484: step 1146, loss 0.0943345, acc 0.94
2016-09-06T23:09:51.028260: step 1147, loss 0.209828, acc 0.88
2016-09-06T23:09:51.719862: step 1148, loss 0.165386, acc 0.96
2016-09-06T23:09:52.411597: step 1149, loss 0.175558, acc 0.92
2016-09-06T23:09:53.102064: step 1150, loss 0.158977, acc 0.9
2016-09-06T23:09:53.821510: step 1151, loss 0.117121, acc 0.98
2016-09-06T23:09:54.458120: step 1152, loss 0.0459006, acc 1
2016-09-06T23:09:55.194241: step 1153, loss 0.0626208, acc 0.98
2016-09-06T23:09:55.867490: step 1154, loss 0.0859632, acc 0.94
2016-09-06T23:09:56.548452: step 1155, loss 0.127942, acc 0.92
2016-09-06T23:09:57.245522: step 1156, loss 0.0325796, acc 1
2016-09-06T23:09:57.924913: step 1157, loss 0.0425201, acc 0.98
2016-09-06T23:09:58.600768: step 1158, loss 0.0824325, acc 0.94
2016-09-06T23:09:59.273808: step 1159, loss 0.0471473, acc 0.98
2016-09-06T23:09:59.968262: step 1160, loss 0.0603121, acc 0.96
2016-09-06T23:10:00.671458: step 1161, loss 0.0393503, acc 0.98
2016-09-06T23:10:01.358172: step 1162, loss 0.180023, acc 0.92
2016-09-06T23:10:02.041477: step 1163, loss 0.0226276, acc 1
2016-09-06T23:10:02.727629: step 1164, loss 0.0892292, acc 0.96
2016-09-06T23:10:03.409424: step 1165, loss 0.0690589, acc 0.96
2016-09-06T23:10:04.092266: step 1166, loss 0.0521077, acc 0.98
2016-09-06T23:10:04.773776: step 1167, loss 0.0377201, acc 0.98
2016-09-06T23:10:05.456645: step 1168, loss 0.0947715, acc 0.96
2016-09-06T23:10:06.170278: step 1169, loss 0.0920778, acc 0.96
2016-09-06T23:10:06.854395: step 1170, loss 0.0573969, acc 0.98
2016-09-06T23:10:07.550946: step 1171, loss 0.0695504, acc 0.98
2016-09-06T23:10:08.223138: step 1172, loss 0.0577466, acc 0.98
2016-09-06T23:10:08.928178: step 1173, loss 0.0272855, acc 1
2016-09-06T23:10:09.635567: step 1174, loss 0.0563642, acc 0.98
2016-09-06T23:10:10.329130: step 1175, loss 0.133635, acc 0.94
2016-09-06T23:10:11.017376: step 1176, loss 0.189599, acc 0.98
2016-09-06T23:10:11.693990: step 1177, loss 0.0576773, acc 0.98
2016-09-06T23:10:12.392792: step 1178, loss 0.241228, acc 0.92
2016-09-06T23:10:13.099213: step 1179, loss 0.0515344, acc 0.96
2016-09-06T23:10:13.766586: step 1180, loss 0.0687664, acc 0.98
2016-09-06T23:10:14.469530: step 1181, loss 0.14725, acc 0.92
2016-09-06T23:10:15.141675: step 1182, loss 0.0664848, acc 0.98
2016-09-06T23:10:15.816664: step 1183, loss 0.0596329, acc 0.96
2016-09-06T23:10:16.485939: step 1184, loss 0.100892, acc 0.96
2016-09-06T23:10:17.178615: step 1185, loss 0.0322162, acc 1
2016-09-06T23:10:17.860854: step 1186, loss 0.0779785, acc 0.98
2016-09-06T23:10:18.557737: step 1187, loss 0.0420251, acc 0.98
2016-09-06T23:10:19.242055: step 1188, loss 0.0435887, acc 1
2016-09-06T23:10:19.901956: step 1189, loss 0.0609396, acc 0.98
2016-09-06T23:10:20.610766: step 1190, loss 0.0480765, acc 0.98
2016-09-06T23:10:21.301858: step 1191, loss 0.00883456, acc 1
2016-09-06T23:10:21.998247: step 1192, loss 0.0542579, acc 0.98
2016-09-06T23:10:22.677155: step 1193, loss 0.0315234, acc 0.98
2016-09-06T23:10:23.364997: step 1194, loss 0.0790263, acc 0.96
2016-09-06T23:10:24.064299: step 1195, loss 0.0219673, acc 1
2016-09-06T23:10:24.726446: step 1196, loss 0.0664676, acc 0.96
2016-09-06T23:10:25.422623: step 1197, loss 0.158276, acc 0.94
2016-09-06T23:10:26.103615: step 1198, loss 0.0676257, acc 0.96
2016-09-06T23:10:26.774719: step 1199, loss 0.0645911, acc 0.96
2016-09-06T23:10:27.475264: step 1200, loss 0.0765626, acc 0.94

Evaluation:
2016-09-06T23:10:30.604101: step 1200, loss 1.02341, acc 0.77955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-1200

2016-09-06T23:10:32.335038: step 1201, loss 0.0131825, acc 1
2016-09-06T23:10:33.024606: step 1202, loss 0.0638907, acc 0.98
2016-09-06T23:10:33.717285: step 1203, loss 0.0522024, acc 1
2016-09-06T23:10:34.386600: step 1204, loss 0.0367957, acc 0.98
2016-09-06T23:10:35.096960: step 1205, loss 0.127834, acc 0.96
2016-09-06T23:10:35.776188: step 1206, loss 0.0185684, acc 1
2016-09-06T23:10:36.465286: step 1207, loss 0.0780047, acc 0.98
2016-09-06T23:10:37.165270: step 1208, loss 0.0990334, acc 0.94
2016-09-06T23:10:37.836249: step 1209, loss 0.063094, acc 0.98
2016-09-06T23:10:38.526309: step 1210, loss 0.174438, acc 0.96
2016-09-06T23:10:39.207149: step 1211, loss 0.215108, acc 0.94
2016-09-06T23:10:39.894459: step 1212, loss 0.0159819, acc 1
2016-09-06T23:10:40.584339: step 1213, loss 0.0929998, acc 0.96
2016-09-06T23:10:41.274746: step 1214, loss 0.0204904, acc 1
2016-09-06T23:10:41.996151: step 1215, loss 0.059501, acc 0.98
2016-09-06T23:10:42.672712: step 1216, loss 0.0639532, acc 0.94
2016-09-06T23:10:43.370167: step 1217, loss 0.0638927, acc 0.96
2016-09-06T23:10:44.054324: step 1218, loss 0.0898047, acc 0.96
2016-09-06T23:10:44.737107: step 1219, loss 0.122278, acc 0.92
2016-09-06T23:10:45.422759: step 1220, loss 0.0184368, acc 1
2016-09-06T23:10:46.123527: step 1221, loss 0.106326, acc 0.94
2016-09-06T23:10:46.832752: step 1222, loss 0.067891, acc 0.96
2016-09-06T23:10:47.490908: step 1223, loss 0.0300733, acc 1
2016-09-06T23:10:48.200239: step 1224, loss 0.059488, acc 0.98
2016-09-06T23:10:48.893470: step 1225, loss 0.0247603, acc 1
2016-09-06T23:10:49.572622: step 1226, loss 0.154566, acc 0.92
2016-09-06T23:10:50.279639: step 1227, loss 0.0380211, acc 1
2016-09-06T23:10:50.966197: step 1228, loss 0.0476743, acc 0.98
2016-09-06T23:10:51.670486: step 1229, loss 0.0575344, acc 0.98
2016-09-06T23:10:52.326241: step 1230, loss 0.0398991, acc 0.98
2016-09-06T23:10:53.031442: step 1231, loss 0.0383437, acc 1
2016-09-06T23:10:53.711105: step 1232, loss 0.0336433, acc 0.98
2016-09-06T23:10:54.397362: step 1233, loss 0.0452356, acc 0.98
2016-09-06T23:10:55.070646: step 1234, loss 0.0219311, acc 1
2016-09-06T23:10:55.752911: step 1235, loss 0.0382189, acc 0.98
2016-09-06T23:10:56.425562: step 1236, loss 0.0687473, acc 0.98
2016-09-06T23:10:57.080792: step 1237, loss 0.139132, acc 0.94
2016-09-06T23:10:57.780273: step 1238, loss 0.0125913, acc 1
2016-09-06T23:10:58.455761: step 1239, loss 0.0526339, acc 0.98
2016-09-06T23:10:59.151668: step 1240, loss 0.0664999, acc 0.96
2016-09-06T23:10:59.859846: step 1241, loss 0.105276, acc 0.94
2016-09-06T23:11:00.567796: step 1242, loss 0.143709, acc 0.94
2016-09-06T23:11:01.246932: step 1243, loss 0.18681, acc 0.96
2016-09-06T23:11:01.953759: step 1244, loss 0.0624943, acc 0.98
2016-09-06T23:11:02.668704: step 1245, loss 0.0299996, acc 0.98
2016-09-06T23:11:03.334989: step 1246, loss 0.0363351, acc 0.98
2016-09-06T23:11:04.018252: step 1247, loss 0.17643, acc 0.94
2016-09-06T23:11:04.688229: step 1248, loss 0.287231, acc 0.92
2016-09-06T23:11:05.359268: step 1249, loss 0.103307, acc 0.94
2016-09-06T23:11:06.062274: step 1250, loss 0.0561461, acc 0.94
2016-09-06T23:11:06.722493: step 1251, loss 0.25698, acc 0.9
2016-09-06T23:11:07.440407: step 1252, loss 0.0646555, acc 0.96
2016-09-06T23:11:08.097377: step 1253, loss 0.159864, acc 0.9
2016-09-06T23:11:08.773651: step 1254, loss 0.0906492, acc 0.94
2016-09-06T23:11:09.481841: step 1255, loss 0.0279182, acc 1
2016-09-06T23:11:10.171757: step 1256, loss 0.185431, acc 0.9
2016-09-06T23:11:10.863932: step 1257, loss 0.15679, acc 0.92
2016-09-06T23:11:11.532688: step 1258, loss 0.131399, acc 0.92
2016-09-06T23:11:12.227289: step 1259, loss 0.0854169, acc 0.96
2016-09-06T23:11:12.907860: step 1260, loss 0.0865512, acc 0.98
2016-09-06T23:11:13.588114: step 1261, loss 0.0815812, acc 0.98
2016-09-06T23:11:14.273816: step 1262, loss 0.0655047, acc 0.98
2016-09-06T23:11:14.960232: step 1263, loss 0.115576, acc 0.96
2016-09-06T23:11:15.648621: step 1264, loss 0.058953, acc 0.98
2016-09-06T23:11:16.307569: step 1265, loss 0.0987492, acc 0.96
2016-09-06T23:11:17.031199: step 1266, loss 0.0715406, acc 0.96
2016-09-06T23:11:17.698599: step 1267, loss 0.0367888, acc 1
2016-09-06T23:11:18.380847: step 1268, loss 0.0583378, acc 0.98
2016-09-06T23:11:19.064279: step 1269, loss 0.116491, acc 0.96
2016-09-06T23:11:19.749456: step 1270, loss 0.105418, acc 0.96
2016-09-06T23:11:20.436054: step 1271, loss 0.161125, acc 0.9
2016-09-06T23:11:21.127081: step 1272, loss 0.10283, acc 0.92
2016-09-06T23:11:21.847376: step 1273, loss 0.0485988, acc 0.98
2016-09-06T23:11:22.541519: step 1274, loss 0.0757344, acc 0.96
2016-09-06T23:11:23.244702: step 1275, loss 0.140895, acc 0.96
2016-09-06T23:11:23.925793: step 1276, loss 0.113677, acc 0.94
2016-09-06T23:11:24.604881: step 1277, loss 0.105725, acc 0.96
2016-09-06T23:11:25.290539: step 1278, loss 0.0153953, acc 1
2016-09-06T23:11:25.961142: step 1279, loss 0.0562822, acc 0.98
2016-09-06T23:11:26.695569: step 1280, loss 0.0767803, acc 0.96
2016-09-06T23:11:27.387336: step 1281, loss 0.0186469, acc 1
2016-09-06T23:11:28.085998: step 1282, loss 0.0699558, acc 0.96
2016-09-06T23:11:28.764373: step 1283, loss 0.19102, acc 0.92
2016-09-06T23:11:29.438369: step 1284, loss 0.0780911, acc 0.98
2016-09-06T23:11:30.146545: step 1285, loss 0.190826, acc 0.96
2016-09-06T23:11:30.806851: step 1286, loss 0.0983113, acc 0.98
2016-09-06T23:11:31.505382: step 1287, loss 0.0702697, acc 0.98
2016-09-06T23:11:32.206934: step 1288, loss 0.00487217, acc 1
2016-09-06T23:11:32.902957: step 1289, loss 0.145109, acc 0.94
2016-09-06T23:11:33.592901: step 1290, loss 0.0799007, acc 0.96
2016-09-06T23:11:34.289528: step 1291, loss 0.0349642, acc 0.98
2016-09-06T23:11:34.967555: step 1292, loss 0.0777657, acc 0.96
2016-09-06T23:11:35.638164: step 1293, loss 0.161634, acc 0.98
2016-09-06T23:11:36.347639: step 1294, loss 0.0454789, acc 0.98
2016-09-06T23:11:37.033417: step 1295, loss 0.159093, acc 0.94
2016-09-06T23:11:37.711396: step 1296, loss 0.0218896, acc 0.98
2016-09-06T23:11:38.407111: step 1297, loss 0.174617, acc 0.9
2016-09-06T23:11:39.090811: step 1298, loss 0.115314, acc 0.96
2016-09-06T23:11:39.830020: step 1299, loss 0.085886, acc 0.96
2016-09-06T23:11:40.521611: step 1300, loss 0.117719, acc 0.92

Evaluation:
2016-09-06T23:11:43.657021: step 1300, loss 1.07299, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-1300

2016-09-06T23:11:45.293866: step 1301, loss 0.0975938, acc 0.98
2016-09-06T23:11:45.993351: step 1302, loss 0.0816562, acc 0.98
2016-09-06T23:11:46.673998: step 1303, loss 0.0803658, acc 0.96
2016-09-06T23:11:47.358899: step 1304, loss 0.0705809, acc 0.98
2016-09-06T23:11:48.037115: step 1305, loss 0.119455, acc 0.94
2016-09-06T23:11:48.723582: step 1306, loss 0.0708173, acc 0.96
2016-09-06T23:11:49.409995: step 1307, loss 0.0746548, acc 0.96
2016-09-06T23:11:50.082198: step 1308, loss 0.12135, acc 0.94
2016-09-06T23:11:50.787795: step 1309, loss 0.0509909, acc 0.98
2016-09-06T23:11:51.492941: step 1310, loss 0.0525484, acc 0.98
2016-09-06T23:11:52.187830: step 1311, loss 0.0268792, acc 0.98
2016-09-06T23:11:52.890673: step 1312, loss 0.0760049, acc 1
2016-09-06T23:11:53.575170: step 1313, loss 0.203558, acc 0.92
2016-09-06T23:11:54.287163: step 1314, loss 0.191925, acc 0.96
2016-09-06T23:11:54.966755: step 1315, loss 0.0514896, acc 0.98
2016-09-06T23:11:55.683054: step 1316, loss 0.0818394, acc 0.98
2016-09-06T23:11:56.354954: step 1317, loss 0.0576031, acc 0.98
2016-09-06T23:11:57.036380: step 1318, loss 0.0450941, acc 0.96
2016-09-06T23:11:57.724396: step 1319, loss 0.0821368, acc 0.98
2016-09-06T23:11:58.418856: step 1320, loss 0.208947, acc 0.88
2016-09-06T23:11:59.127812: step 1321, loss 0.0863783, acc 0.96
2016-09-06T23:11:59.826250: step 1322, loss 0.0541386, acc 0.96
2016-09-06T23:12:00.538610: step 1323, loss 0.0480481, acc 1
2016-09-06T23:12:01.225234: step 1324, loss 0.147563, acc 0.92
2016-09-06T23:12:01.891854: step 1325, loss 0.0741509, acc 0.96
2016-09-06T23:12:02.580259: step 1326, loss 0.109192, acc 0.94
2016-09-06T23:12:03.237522: step 1327, loss 0.0871297, acc 0.94
2016-09-06T23:12:03.955223: step 1328, loss 0.136596, acc 0.96
2016-09-06T23:12:04.619593: step 1329, loss 0.0659417, acc 0.94
2016-09-06T23:12:05.303447: step 1330, loss 0.0708489, acc 0.98
2016-09-06T23:12:05.983021: step 1331, loss 0.031612, acc 1
2016-09-06T23:12:06.666417: step 1332, loss 0.0384921, acc 0.98
2016-09-06T23:12:07.376284: step 1333, loss 0.21671, acc 0.92
2016-09-06T23:12:08.056512: step 1334, loss 0.0564606, acc 0.98
2016-09-06T23:12:08.773594: step 1335, loss 0.0749472, acc 0.98
2016-09-06T23:12:09.458363: step 1336, loss 0.235612, acc 0.9
2016-09-06T23:12:10.138803: step 1337, loss 0.0755022, acc 0.94
2016-09-06T23:12:10.832551: step 1338, loss 0.0898834, acc 0.94
2016-09-06T23:12:11.510839: step 1339, loss 0.0618874, acc 0.96
2016-09-06T23:12:12.214293: step 1340, loss 0.0874194, acc 0.98
2016-09-06T23:12:12.874536: step 1341, loss 0.0580002, acc 0.96
2016-09-06T23:12:13.594037: step 1342, loss 0.108276, acc 0.96
2016-09-06T23:12:14.258991: step 1343, loss 0.164097, acc 0.94
2016-09-06T23:12:14.884048: step 1344, loss 0.0140535, acc 1
2016-09-06T23:12:15.588223: step 1345, loss 0.0766722, acc 0.96
2016-09-06T23:12:16.272134: step 1346, loss 0.210837, acc 0.88
2016-09-06T23:12:16.960847: step 1347, loss 0.0261619, acc 1
2016-09-06T23:12:17.637287: step 1348, loss 0.0710021, acc 0.98
2016-09-06T23:12:18.338396: step 1349, loss 0.0641464, acc 0.98
2016-09-06T23:12:19.013356: step 1350, loss 0.140005, acc 0.98
2016-09-06T23:12:19.682898: step 1351, loss 0.0177052, acc 1
2016-09-06T23:12:20.364720: step 1352, loss 0.113676, acc 0.96
2016-09-06T23:12:21.065927: step 1353, loss 0.162533, acc 0.96
2016-09-06T23:12:21.759881: step 1354, loss 0.0454309, acc 0.98
2016-09-06T23:12:22.450630: step 1355, loss 0.0429608, acc 1
2016-09-06T23:12:23.163147: step 1356, loss 0.0858664, acc 0.96
2016-09-06T23:12:23.836576: step 1357, loss 0.0894869, acc 0.94
2016-09-06T23:12:24.527578: step 1358, loss 0.0318976, acc 1
2016-09-06T23:12:25.234240: step 1359, loss 0.0311443, acc 0.98
2016-09-06T23:12:25.940822: step 1360, loss 0.0143361, acc 1
2016-09-06T23:12:26.621742: step 1361, loss 0.0763446, acc 0.96
2016-09-06T23:12:27.289595: step 1362, loss 0.0365012, acc 0.98
2016-09-06T23:12:27.988767: step 1363, loss 0.0701481, acc 0.96
2016-09-06T23:12:28.663015: step 1364, loss 0.128466, acc 0.94
2016-09-06T23:12:29.323765: step 1365, loss 0.0739205, acc 0.98
2016-09-06T23:12:30.011405: step 1366, loss 0.148347, acc 0.96
2016-09-06T23:12:30.683384: step 1367, loss 0.0406291, acc 1
2016-09-06T23:12:31.368546: step 1368, loss 0.0287979, acc 0.98
2016-09-06T23:12:32.059899: step 1369, loss 0.0903838, acc 0.94
2016-09-06T23:12:32.800619: step 1370, loss 0.201888, acc 0.96
2016-09-06T23:12:33.474927: step 1371, loss 0.0999212, acc 0.96
2016-09-06T23:12:34.144118: step 1372, loss 0.0420177, acc 0.98
2016-09-06T23:12:34.838248: step 1373, loss 0.0291774, acc 1
2016-09-06T23:12:35.528568: step 1374, loss 0.064833, acc 0.98
2016-09-06T23:12:36.222449: step 1375, loss 0.00645308, acc 1
2016-09-06T23:12:36.912368: step 1376, loss 0.0566556, acc 0.98
2016-09-06T23:12:37.612156: step 1377, loss 0.0812967, acc 0.96
2016-09-06T23:12:38.282603: step 1378, loss 0.0500093, acc 1
2016-09-06T23:12:38.949517: step 1379, loss 0.0283997, acc 0.98
2016-09-06T23:12:39.642706: step 1380, loss 0.0523687, acc 0.98
2016-09-06T23:12:40.317374: step 1381, loss 0.0461352, acc 0.98
2016-09-06T23:12:41.029363: step 1382, loss 0.041761, acc 1
2016-09-06T23:12:41.683641: step 1383, loss 0.076994, acc 0.96
2016-09-06T23:12:42.406872: step 1384, loss 0.054196, acc 0.98
2016-09-06T23:12:43.085873: step 1385, loss 0.0729572, acc 0.94
2016-09-06T23:12:43.770029: step 1386, loss 0.0878549, acc 0.98
2016-09-06T23:12:44.429449: step 1387, loss 0.0145458, acc 1
2016-09-06T23:12:45.097932: step 1388, loss 0.0467721, acc 0.98
2016-09-06T23:12:45.779855: step 1389, loss 0.210272, acc 0.92
2016-09-06T23:12:46.465203: step 1390, loss 0.128649, acc 0.94
2016-09-06T23:12:47.163953: step 1391, loss 0.0855742, acc 0.94
2016-09-06T23:12:47.836296: step 1392, loss 0.107819, acc 0.96
2016-09-06T23:12:48.523482: step 1393, loss 0.0668715, acc 0.96
2016-09-06T23:12:49.220466: step 1394, loss 0.0140382, acc 1
2016-09-06T23:12:49.892722: step 1395, loss 0.104559, acc 0.96
2016-09-06T23:12:50.589413: step 1396, loss 0.0363542, acc 0.98
2016-09-06T23:12:51.293391: step 1397, loss 0.0526415, acc 0.96
2016-09-06T23:12:52.015521: step 1398, loss 0.0478534, acc 1
2016-09-06T23:12:52.693621: step 1399, loss 0.139315, acc 0.96
2016-09-06T23:12:53.378594: step 1400, loss 0.0453554, acc 1

Evaluation:
2016-09-06T23:12:56.531453: step 1400, loss 1.09992, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-1400

2016-09-06T23:12:58.204644: step 1401, loss 0.084755, acc 0.96
2016-09-06T23:12:58.894641: step 1402, loss 0.0525911, acc 0.98
2016-09-06T23:12:59.574215: step 1403, loss 0.094301, acc 0.96
2016-09-06T23:13:00.336790: step 1404, loss 0.0613003, acc 0.94
2016-09-06T23:13:01.006893: step 1405, loss 0.0854061, acc 0.94
2016-09-06T23:13:01.705073: step 1406, loss 0.0510175, acc 0.98
2016-09-06T23:13:02.384345: step 1407, loss 0.0468347, acc 0.98
2016-09-06T23:13:03.079326: step 1408, loss 0.174485, acc 0.92
2016-09-06T23:13:03.759516: step 1409, loss 0.0497424, acc 0.98
2016-09-06T23:13:04.432029: step 1410, loss 0.0583275, acc 0.96
2016-09-06T23:13:05.131231: step 1411, loss 0.0824318, acc 0.92
2016-09-06T23:13:05.829591: step 1412, loss 0.0806221, acc 0.98
2016-09-06T23:13:06.541419: step 1413, loss 0.0471009, acc 0.98
2016-09-06T23:13:07.232237: step 1414, loss 0.103426, acc 0.96
2016-09-06T23:13:07.929731: step 1415, loss 0.0847941, acc 0.94
2016-09-06T23:13:08.632244: step 1416, loss 0.0699855, acc 0.94
2016-09-06T23:13:09.340657: step 1417, loss 0.0302023, acc 0.98
2016-09-06T23:13:10.059224: step 1418, loss 0.106146, acc 0.94
2016-09-06T23:13:10.705136: step 1419, loss 0.218745, acc 0.94
2016-09-06T23:13:11.390916: step 1420, loss 0.0856889, acc 0.96
2016-09-06T23:13:12.080471: step 1421, loss 0.0756798, acc 0.98
2016-09-06T23:13:12.766121: step 1422, loss 0.0116162, acc 1
2016-09-06T23:13:13.468426: step 1423, loss 0.100436, acc 0.92
2016-09-06T23:13:14.153435: step 1424, loss 0.0284656, acc 1
2016-09-06T23:13:14.862458: step 1425, loss 0.0817157, acc 0.96
2016-09-06T23:13:15.529977: step 1426, loss 0.0917365, acc 0.94
2016-09-06T23:13:16.227513: step 1427, loss 0.0694111, acc 0.96
2016-09-06T23:13:16.924943: step 1428, loss 0.12566, acc 0.94
2016-09-06T23:13:17.610842: step 1429, loss 0.0727491, acc 0.94
2016-09-06T23:13:18.294870: step 1430, loss 0.0825828, acc 0.96
2016-09-06T23:13:18.981053: step 1431, loss 0.0876664, acc 0.96
2016-09-06T23:13:19.690885: step 1432, loss 0.028349, acc 0.98
2016-09-06T23:13:20.372019: step 1433, loss 0.011357, acc 1
2016-09-06T23:13:21.070209: step 1434, loss 0.0352523, acc 0.98
2016-09-06T23:13:21.755582: step 1435, loss 0.0174386, acc 1
2016-09-06T23:13:22.448390: step 1436, loss 0.230464, acc 0.94
2016-09-06T23:13:23.150851: step 1437, loss 0.0314545, acc 0.98
2016-09-06T23:13:23.834220: step 1438, loss 0.0162373, acc 1
2016-09-06T23:13:24.545403: step 1439, loss 0.0376437, acc 0.98
2016-09-06T23:13:25.245223: step 1440, loss 0.0627725, acc 0.98
2016-09-06T23:13:25.933994: step 1441, loss 0.0762514, acc 0.94
2016-09-06T23:13:26.617948: step 1442, loss 0.108477, acc 0.92
2016-09-06T23:13:27.316768: step 1443, loss 0.062345, acc 0.96
2016-09-06T23:13:27.996554: step 1444, loss 0.122591, acc 0.9
2016-09-06T23:13:28.682795: step 1445, loss 0.242605, acc 0.96
2016-09-06T23:13:29.376640: step 1446, loss 0.0660585, acc 0.96
2016-09-06T23:13:30.033791: step 1447, loss 0.0412158, acc 0.98
2016-09-06T23:13:30.737446: step 1448, loss 0.101725, acc 0.94
2016-09-06T23:13:31.444709: step 1449, loss 0.0265691, acc 1
2016-09-06T23:13:32.135288: step 1450, loss 0.0269011, acc 1
2016-09-06T23:13:32.803796: step 1451, loss 0.0407918, acc 0.96
2016-09-06T23:13:33.451589: step 1452, loss 0.0507994, acc 0.98
2016-09-06T23:13:34.149286: step 1453, loss 0.134222, acc 0.96
2016-09-06T23:13:34.833961: step 1454, loss 0.0999346, acc 0.94
2016-09-06T23:13:35.518801: step 1455, loss 0.0269688, acc 1
2016-09-06T23:13:36.212849: step 1456, loss 0.0323629, acc 0.98
2016-09-06T23:13:36.898615: step 1457, loss 0.0401795, acc 0.98
2016-09-06T23:13:37.588959: step 1458, loss 0.11275, acc 0.96
2016-09-06T23:13:38.263395: step 1459, loss 0.0157033, acc 1
2016-09-06T23:13:38.960268: step 1460, loss 0.0292459, acc 0.98
2016-09-06T23:13:39.643027: step 1461, loss 0.0251924, acc 0.98
2016-09-06T23:13:40.325853: step 1462, loss 0.106001, acc 0.98
2016-09-06T23:13:41.033859: step 1463, loss 0.0761538, acc 0.96
2016-09-06T23:13:41.722303: step 1464, loss 0.042562, acc 0.98
2016-09-06T23:13:42.399409: step 1465, loss 0.0444722, acc 0.98
2016-09-06T23:13:43.054805: step 1466, loss 0.10008, acc 0.92
2016-09-06T23:13:43.761876: step 1467, loss 0.12942, acc 0.96
2016-09-06T23:13:44.424982: step 1468, loss 0.183461, acc 0.94
2016-09-06T23:13:45.141789: step 1469, loss 0.0621393, acc 0.98
2016-09-06T23:13:45.836453: step 1470, loss 0.0330492, acc 0.98
2016-09-06T23:13:46.533461: step 1471, loss 0.0394235, acc 0.98
2016-09-06T23:13:47.223968: step 1472, loss 0.0734142, acc 0.96
2016-09-06T23:13:47.896102: step 1473, loss 0.0212676, acc 1
2016-09-06T23:13:48.590080: step 1474, loss 0.0909934, acc 0.98
2016-09-06T23:13:49.263784: step 1475, loss 0.153859, acc 0.9
2016-09-06T23:13:49.930028: step 1476, loss 0.112266, acc 0.96
2016-09-06T23:13:50.610614: step 1477, loss 0.228554, acc 0.94
2016-09-06T23:13:51.302660: step 1478, loss 0.0621433, acc 0.96
2016-09-06T23:13:52.000687: step 1479, loss 0.0801942, acc 0.98
2016-09-06T23:13:52.662545: step 1480, loss 0.146949, acc 0.94
2016-09-06T23:13:53.353779: step 1481, loss 0.139037, acc 0.92
2016-09-06T23:13:54.022516: step 1482, loss 0.0476879, acc 0.98
2016-09-06T23:13:54.727069: step 1483, loss 0.0680123, acc 0.94
2016-09-06T23:13:55.420478: step 1484, loss 0.0830526, acc 0.98
2016-09-06T23:13:56.105863: step 1485, loss 0.0745302, acc 0.98
2016-09-06T23:13:56.777060: step 1486, loss 0.0436927, acc 0.98
2016-09-06T23:13:57.481043: step 1487, loss 0.0761179, acc 0.98
2016-09-06T23:13:58.206718: step 1488, loss 0.106177, acc 0.96
2016-09-06T23:13:58.883025: step 1489, loss 0.130376, acc 0.92
2016-09-06T23:13:59.589945: step 1490, loss 0.103938, acc 0.96
2016-09-06T23:14:00.302139: step 1491, loss 0.0867977, acc 0.94
2016-09-06T23:14:00.983546: step 1492, loss 0.202127, acc 0.94
2016-09-06T23:14:01.724284: step 1493, loss 0.169141, acc 0.96
2016-09-06T23:14:02.395084: step 1494, loss 0.0691058, acc 0.96
2016-09-06T23:14:03.088438: step 1495, loss 0.0444433, acc 0.98
2016-09-06T23:14:03.791929: step 1496, loss 0.0263319, acc 1
2016-09-06T23:14:04.485932: step 1497, loss 0.0346003, acc 1
2016-09-06T23:14:05.177488: step 1498, loss 0.176636, acc 0.94
2016-09-06T23:14:05.853664: step 1499, loss 0.161741, acc 0.94
2016-09-06T23:14:06.553396: step 1500, loss 0.106318, acc 0.96

Evaluation:
2016-09-06T23:14:09.712318: step 1500, loss 1.03236, acc 0.768293

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-1500

2016-09-06T23:14:11.493117: step 1501, loss 0.0652805, acc 0.96
2016-09-06T23:14:12.178726: step 1502, loss 0.120316, acc 0.94
2016-09-06T23:14:12.851333: step 1503, loss 0.0630026, acc 0.98
2016-09-06T23:14:13.533830: step 1504, loss 0.0934548, acc 0.96
2016-09-06T23:14:14.225658: step 1505, loss 0.0780214, acc 0.98
2016-09-06T23:14:14.903095: step 1506, loss 0.0591742, acc 0.96
2016-09-06T23:14:15.578477: step 1507, loss 0.0188744, acc 1
2016-09-06T23:14:16.288902: step 1508, loss 0.0475383, acc 1
2016-09-06T23:14:16.980196: step 1509, loss 0.082688, acc 0.94
2016-09-06T23:14:17.668507: step 1510, loss 0.102002, acc 0.98
2016-09-06T23:14:18.347143: step 1511, loss 0.154412, acc 0.92
2016-09-06T23:14:19.030296: step 1512, loss 0.0490349, acc 0.98
2016-09-06T23:14:19.733099: step 1513, loss 0.0883152, acc 0.98
2016-09-06T23:14:20.395497: step 1514, loss 0.0972152, acc 0.96
2016-09-06T23:14:21.092335: step 1515, loss 0.17714, acc 0.94
2016-09-06T23:14:21.775773: step 1516, loss 0.0266952, acc 0.98
2016-09-06T23:14:22.469473: step 1517, loss 0.0839511, acc 0.96
2016-09-06T23:14:23.145240: step 1518, loss 0.0432329, acc 0.98
2016-09-06T23:14:23.829034: step 1519, loss 0.0509037, acc 0.98
2016-09-06T23:14:24.512403: step 1520, loss 0.0792874, acc 0.98
2016-09-06T23:14:25.178322: step 1521, loss 0.105706, acc 0.96
2016-09-06T23:14:25.880281: step 1522, loss 0.135851, acc 0.92
2016-09-06T23:14:26.571515: step 1523, loss 0.0147156, acc 1
2016-09-06T23:14:27.265875: step 1524, loss 0.0559014, acc 0.98
2016-09-06T23:14:27.949319: step 1525, loss 0.0631507, acc 0.98
2016-09-06T23:14:28.614625: step 1526, loss 0.0636387, acc 0.98
2016-09-06T23:14:29.296593: step 1527, loss 0.106713, acc 0.98
2016-09-06T23:14:29.990343: step 1528, loss 0.0570706, acc 0.98
2016-09-06T23:14:30.675959: step 1529, loss 0.147809, acc 0.94
2016-09-06T23:14:31.334978: step 1530, loss 0.0398203, acc 0.98
2016-09-06T23:14:32.050228: step 1531, loss 0.0831493, acc 0.94
2016-09-06T23:14:32.730017: step 1532, loss 0.0585974, acc 0.98
2016-09-06T23:14:33.418480: step 1533, loss 0.180003, acc 0.94
2016-09-06T23:14:34.088472: step 1534, loss 0.095646, acc 0.96
2016-09-06T23:14:34.753483: step 1535, loss 0.048676, acc 0.98
2016-09-06T23:14:35.411229: step 1536, loss 0.0902058, acc 0.954545
2016-09-06T23:14:36.079342: step 1537, loss 0.0324086, acc 1
2016-09-06T23:14:36.781054: step 1538, loss 0.0218228, acc 1
2016-09-06T23:14:37.473126: step 1539, loss 0.0437952, acc 1
2016-09-06T23:14:38.170951: step 1540, loss 0.137696, acc 0.94
2016-09-06T23:14:38.875502: step 1541, loss 0.0525118, acc 0.98
2016-09-06T23:14:39.546195: step 1542, loss 0.0322059, acc 0.98
2016-09-06T23:14:40.262238: step 1543, loss 0.0302565, acc 0.98
2016-09-06T23:14:40.925434: step 1544, loss 0.052214, acc 0.98
2016-09-06T23:14:41.599046: step 1545, loss 0.00996557, acc 1
2016-09-06T23:14:42.267185: step 1546, loss 0.0800617, acc 0.94
2016-09-06T23:14:42.986043: step 1547, loss 0.107845, acc 0.94
2016-09-06T23:14:43.686069: step 1548, loss 0.0405144, acc 0.98
2016-09-06T23:14:44.347557: step 1549, loss 0.0800981, acc 0.96
2016-09-06T23:14:45.043280: step 1550, loss 0.0530389, acc 0.94
2016-09-06T23:14:45.735300: step 1551, loss 0.059246, acc 0.96
2016-09-06T23:14:46.420176: step 1552, loss 0.0218735, acc 1
2016-09-06T23:14:47.097730: step 1553, loss 0.0491702, acc 0.96
2016-09-06T23:14:47.795423: step 1554, loss 0.00264793, acc 1
2016-09-06T23:14:48.465889: step 1555, loss 0.0884186, acc 0.94
2016-09-06T23:14:49.130209: step 1556, loss 0.0730999, acc 0.94
2016-09-06T23:14:49.829161: step 1557, loss 0.0917807, acc 0.98
2016-09-06T23:14:50.511012: step 1558, loss 0.0350957, acc 0.98
2016-09-06T23:14:51.198533: step 1559, loss 0.0176055, acc 1
2016-09-06T23:14:51.878727: step 1560, loss 0.0425044, acc 0.98
2016-09-06T23:14:52.562678: step 1561, loss 0.119286, acc 0.96
2016-09-06T23:14:53.235300: step 1562, loss 0.0405524, acc 0.98
2016-09-06T23:14:53.937508: step 1563, loss 0.00495609, acc 1
2016-09-06T23:14:54.649844: step 1564, loss 0.0519383, acc 0.98
2016-09-06T23:14:55.340394: step 1565, loss 0.026522, acc 0.98
2016-09-06T23:14:56.016938: step 1566, loss 0.0709797, acc 0.96
2016-09-06T23:14:56.700885: step 1567, loss 0.0926463, acc 0.98
2016-09-06T23:14:57.387597: step 1568, loss 0.0729168, acc 0.96
2016-09-06T23:14:58.084719: step 1569, loss 0.0337466, acc 0.98
2016-09-06T23:14:58.763864: step 1570, loss 0.134661, acc 0.96
2016-09-06T23:14:59.468371: step 1571, loss 0.0909493, acc 0.96
2016-09-06T23:15:00.171205: step 1572, loss 0.0412802, acc 0.98
2016-09-06T23:15:00.924073: step 1573, loss 0.0825294, acc 0.96
2016-09-06T23:15:01.594880: step 1574, loss 0.0474516, acc 0.98
2016-09-06T23:15:02.298228: step 1575, loss 0.0215714, acc 1
2016-09-06T23:15:02.982249: step 1576, loss 0.0171709, acc 1
2016-09-06T23:15:03.637853: step 1577, loss 0.0400599, acc 0.98
2016-09-06T23:15:04.334527: step 1578, loss 0.0155114, acc 1
2016-09-06T23:15:05.005620: step 1579, loss 0.0096271, acc 1
2016-09-06T23:15:05.688233: step 1580, loss 0.0759378, acc 0.98
2016-09-06T23:15:06.361516: step 1581, loss 0.10352, acc 0.96
2016-09-06T23:15:07.037703: step 1582, loss 0.021642, acc 1
2016-09-06T23:15:07.731637: step 1583, loss 0.0232549, acc 1
2016-09-06T23:15:08.414774: step 1584, loss 0.0585388, acc 0.96
2016-09-06T23:15:09.121301: step 1585, loss 0.132254, acc 0.96
2016-09-06T23:15:09.783772: step 1586, loss 0.296264, acc 0.94
2016-09-06T23:15:10.453190: step 1587, loss 0.116144, acc 0.98
2016-09-06T23:15:11.148101: step 1588, loss 0.068404, acc 0.96
2016-09-06T23:15:11.868193: step 1589, loss 0.0831489, acc 0.96
2016-09-06T23:15:12.593718: step 1590, loss 0.032995, acc 0.98
2016-09-06T23:15:13.274866: step 1591, loss 0.0553714, acc 0.98
2016-09-06T23:15:13.958639: step 1592, loss 0.0237548, acc 1
2016-09-06T23:15:14.636197: step 1593, loss 0.115189, acc 0.98
2016-09-06T23:15:15.328367: step 1594, loss 0.055963, acc 0.98
2016-09-06T23:15:15.991683: step 1595, loss 0.117875, acc 0.96
2016-09-06T23:15:16.711858: step 1596, loss 0.037402, acc 1
2016-09-06T23:15:17.406372: step 1597, loss 0.062982, acc 0.96
2016-09-06T23:15:18.067354: step 1598, loss 0.081371, acc 0.94
2016-09-06T23:15:18.776398: step 1599, loss 0.133272, acc 0.92
2016-09-06T23:15:19.460383: step 1600, loss 0.283511, acc 0.92

Evaluation:
2016-09-06T23:15:22.591816: step 1600, loss 1.26263, acc 0.770169

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-1600

2016-09-06T23:15:24.298526: step 1601, loss 0.0356152, acc 1
2016-09-06T23:15:24.980897: step 1602, loss 0.058956, acc 0.96
2016-09-06T23:15:25.655786: step 1603, loss 0.065054, acc 0.96
2016-09-06T23:15:26.334046: step 1604, loss 0.134583, acc 0.94
2016-09-06T23:15:27.044783: step 1605, loss 0.047098, acc 0.98
2016-09-06T23:15:27.709926: step 1606, loss 0.0332908, acc 1
2016-09-06T23:15:28.396696: step 1607, loss 0.0920561, acc 0.94
2016-09-06T23:15:29.077487: step 1608, loss 0.0497483, acc 0.98
2016-09-06T23:15:29.766848: step 1609, loss 0.0571448, acc 0.98
2016-09-06T23:15:30.442490: step 1610, loss 0.0549929, acc 0.98
2016-09-06T23:15:31.130023: step 1611, loss 0.0698785, acc 0.96
2016-09-06T23:15:31.812396: step 1612, loss 0.0711541, acc 0.94
2016-09-06T23:15:32.474395: step 1613, loss 0.0269345, acc 1
2016-09-06T23:15:33.184331: step 1614, loss 0.13275, acc 0.94
2016-09-06T23:15:33.877800: step 1615, loss 0.213131, acc 0.92
2016-09-06T23:15:34.562453: step 1616, loss 0.15503, acc 0.96
2016-09-06T23:15:35.240925: step 1617, loss 0.0484639, acc 0.98
2016-09-06T23:15:35.914605: step 1618, loss 0.070568, acc 0.98
2016-09-06T23:15:36.601159: step 1619, loss 0.0825289, acc 0.98
2016-09-06T23:15:37.258798: step 1620, loss 0.0922251, acc 0.94
2016-09-06T23:15:37.964690: step 1621, loss 0.030748, acc 1
2016-09-06T23:15:38.646410: step 1622, loss 0.0695341, acc 0.96
2016-09-06T23:15:39.345780: step 1623, loss 0.140609, acc 0.94
2016-09-06T23:15:40.029713: step 1624, loss 0.0451768, acc 1
2016-09-06T23:15:40.729871: step 1625, loss 0.113756, acc 0.96
2016-09-06T23:15:41.436615: step 1626, loss 0.0668598, acc 0.98
2016-09-06T23:15:42.120468: step 1627, loss 0.0947912, acc 0.96
2016-09-06T23:15:42.834065: step 1628, loss 0.0966076, acc 0.96
2016-09-06T23:15:43.514333: step 1629, loss 0.0951857, acc 0.98
2016-09-06T23:15:44.217904: step 1630, loss 0.0663091, acc 0.96
2016-09-06T23:15:44.924900: step 1631, loss 0.0640892, acc 0.98
2016-09-06T23:15:45.618770: step 1632, loss 0.0293231, acc 0.98
2016-09-06T23:15:46.309879: step 1633, loss 0.0539978, acc 0.98
2016-09-06T23:15:46.975067: step 1634, loss 0.0610517, acc 0.98
2016-09-06T23:15:47.667983: step 1635, loss 0.0654946, acc 1
2016-09-06T23:15:48.359790: step 1636, loss 0.0377516, acc 0.98
2016-09-06T23:15:49.055598: step 1637, loss 0.0775957, acc 0.96
2016-09-06T23:15:49.727128: step 1638, loss 0.0246401, acc 1
2016-09-06T23:15:50.401829: step 1639, loss 0.01767, acc 1
2016-09-06T23:15:51.122140: step 1640, loss 0.0346817, acc 1
2016-09-06T23:15:51.798103: step 1641, loss 0.0216908, acc 1
2016-09-06T23:15:52.512669: step 1642, loss 0.0014679, acc 1
2016-09-06T23:15:53.199037: step 1643, loss 0.0986829, acc 0.98
2016-09-06T23:15:53.910686: step 1644, loss 0.0352671, acc 0.98
2016-09-06T23:15:54.602593: step 1645, loss 0.0470357, acc 0.96
2016-09-06T23:15:55.289009: step 1646, loss 0.0714449, acc 0.98
2016-09-06T23:15:55.971151: step 1647, loss 0.0306994, acc 0.98
2016-09-06T23:15:56.656774: step 1648, loss 0.0069072, acc 1
2016-09-06T23:15:57.338482: step 1649, loss 0.0937941, acc 0.98
2016-09-06T23:15:58.024966: step 1650, loss 0.022018, acc 1
2016-09-06T23:15:58.700265: step 1651, loss 0.131327, acc 0.94
2016-09-06T23:15:59.373159: step 1652, loss 0.174315, acc 0.92
2016-09-06T23:16:00.051554: step 1653, loss 0.037291, acc 0.98
2016-09-06T23:16:00.779137: step 1654, loss 0.0833356, acc 0.96
2016-09-06T23:16:01.470182: step 1655, loss 0.0389688, acc 0.98
2016-09-06T23:16:02.161361: step 1656, loss 0.0739815, acc 0.96
2016-09-06T23:16:02.839775: step 1657, loss 0.0389695, acc 0.96
2016-09-06T23:16:03.538811: step 1658, loss 0.0165265, acc 1
2016-09-06T23:16:04.234464: step 1659, loss 0.0672143, acc 0.98
2016-09-06T23:16:04.936124: step 1660, loss 0.290938, acc 0.9
2016-09-06T23:16:05.651206: step 1661, loss 0.0912646, acc 0.96
2016-09-06T23:16:06.312718: step 1662, loss 0.0266914, acc 1
2016-09-06T23:16:07.005441: step 1663, loss 0.0229945, acc 1
2016-09-06T23:16:07.739148: step 1664, loss 0.184406, acc 0.94
2016-09-06T23:16:08.442915: step 1665, loss 0.0618828, acc 0.96
2016-09-06T23:16:09.129066: step 1666, loss 0.0889814, acc 0.94
2016-09-06T23:16:09.804136: step 1667, loss 0.124942, acc 0.94
2016-09-06T23:16:10.507825: step 1668, loss 0.0897667, acc 0.98
2016-09-06T23:16:11.175165: step 1669, loss 0.0453009, acc 1
2016-09-06T23:16:11.856501: step 1670, loss 0.0394408, acc 0.98
2016-09-06T23:16:12.539192: step 1671, loss 0.0202138, acc 1
2016-09-06T23:16:13.214455: step 1672, loss 0.0297425, acc 0.98
2016-09-06T23:16:13.908032: step 1673, loss 0.106038, acc 0.96
2016-09-06T23:16:14.563584: step 1674, loss 0.0275587, acc 0.98
2016-09-06T23:16:15.285919: step 1675, loss 0.0302514, acc 0.98
2016-09-06T23:16:15.999472: step 1676, loss 0.085763, acc 0.98
2016-09-06T23:16:16.695655: step 1677, loss 0.0447552, acc 1
2016-09-06T23:16:17.363457: step 1678, loss 0.034818, acc 1
2016-09-06T23:16:18.044436: step 1679, loss 0.0376128, acc 0.98
2016-09-06T23:16:18.727242: step 1680, loss 0.0568415, acc 0.98
2016-09-06T23:16:19.395824: step 1681, loss 0.0741871, acc 0.98
2016-09-06T23:16:20.107193: step 1682, loss 0.0307757, acc 0.98
2016-09-06T23:16:20.791980: step 1683, loss 0.0211726, acc 1
2016-09-06T23:16:21.480649: step 1684, loss 0.0588895, acc 0.98
2016-09-06T23:16:22.167651: step 1685, loss 0.0529018, acc 0.98
2016-09-06T23:16:22.880796: step 1686, loss 0.0967048, acc 0.96
2016-09-06T23:16:23.565008: step 1687, loss 0.0200929, acc 1
2016-09-06T23:16:24.213355: step 1688, loss 0.0676078, acc 0.96
2016-09-06T23:16:24.904052: step 1689, loss 0.01368, acc 1
2016-09-06T23:16:25.574397: step 1690, loss 0.0482915, acc 0.98
2016-09-06T23:16:26.256797: step 1691, loss 0.0124119, acc 1
2016-09-06T23:16:26.941862: step 1692, loss 0.0132951, acc 1
2016-09-06T23:16:27.640462: step 1693, loss 0.0316134, acc 0.98
2016-09-06T23:16:28.330810: step 1694, loss 0.0360479, acc 0.98
2016-09-06T23:16:28.992146: step 1695, loss 0.0334147, acc 1
2016-09-06T23:16:29.672979: step 1696, loss 0.0809063, acc 0.96
2016-09-06T23:16:30.336811: step 1697, loss 0.0214661, acc 0.98
2016-09-06T23:16:31.031852: step 1698, loss 0.0675103, acc 0.98
2016-09-06T23:16:31.714542: step 1699, loss 0.045568, acc 0.96
2016-09-06T23:16:32.415263: step 1700, loss 0.107831, acc 0.98

Evaluation:
2016-09-06T23:16:35.569856: step 1700, loss 1.28629, acc 0.769231

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-1700

2016-09-06T23:16:37.254499: step 1701, loss 0.0340726, acc 1
2016-09-06T23:16:37.941483: step 1702, loss 0.0335824, acc 1
2016-09-06T23:16:38.608552: step 1703, loss 0.110626, acc 0.98
2016-09-06T23:16:39.315301: step 1704, loss 0.101259, acc 0.96
2016-09-06T23:16:40.013850: step 1705, loss 0.0848464, acc 0.98
2016-09-06T23:16:40.697787: step 1706, loss 0.032083, acc 1
2016-09-06T23:16:41.395333: step 1707, loss 0.0507187, acc 0.98
2016-09-06T23:16:42.093802: step 1708, loss 0.0369682, acc 0.98
2016-09-06T23:16:42.773691: step 1709, loss 0.0522408, acc 0.98
2016-09-06T23:16:43.429875: step 1710, loss 0.0316511, acc 0.98
2016-09-06T23:16:44.134823: step 1711, loss 0.0730009, acc 0.94
2016-09-06T23:16:44.820785: step 1712, loss 0.204541, acc 0.92
2016-09-06T23:16:45.501105: step 1713, loss 0.0634903, acc 0.98
2016-09-06T23:16:46.175948: step 1714, loss 0.0764992, acc 0.96
2016-09-06T23:16:46.867321: step 1715, loss 0.0587084, acc 0.96
2016-09-06T23:16:47.558537: step 1716, loss 0.00853673, acc 1
2016-09-06T23:16:48.225456: step 1717, loss 0.0259015, acc 0.98
2016-09-06T23:16:48.924482: step 1718, loss 0.0597312, acc 0.96
2016-09-06T23:16:49.583407: step 1719, loss 0.0201271, acc 1
2016-09-06T23:16:50.293945: step 1720, loss 0.1326, acc 0.96
2016-09-06T23:16:50.987563: step 1721, loss 0.0693404, acc 0.96
2016-09-06T23:16:51.686334: step 1722, loss 0.0546976, acc 0.98
2016-09-06T23:16:52.356324: step 1723, loss 0.0472884, acc 0.98
2016-09-06T23:16:53.026712: step 1724, loss 0.0803267, acc 0.96
2016-09-06T23:16:53.718196: step 1725, loss 0.159379, acc 0.96
2016-09-06T23:16:54.387410: step 1726, loss 0.101436, acc 0.94
2016-09-06T23:16:55.073406: step 1727, loss 0.117479, acc 0.98
2016-09-06T23:16:55.727030: step 1728, loss 0.167778, acc 0.931818
2016-09-06T23:16:56.400003: step 1729, loss 0.103136, acc 0.98
2016-09-06T23:16:57.087861: step 1730, loss 0.123128, acc 0.94
2016-09-06T23:16:57.775304: step 1731, loss 0.0928312, acc 0.96
2016-09-06T23:16:58.494486: step 1732, loss 0.0340116, acc 1
2016-09-06T23:16:59.179394: step 1733, loss 0.0288469, acc 1
2016-09-06T23:16:59.857404: step 1734, loss 0.0524081, acc 0.96
2016-09-06T23:17:00.588758: step 1735, loss 0.0435998, acc 1
2016-09-06T23:17:01.279434: step 1736, loss 0.0609182, acc 0.98
2016-09-06T23:17:01.970575: step 1737, loss 0.0319777, acc 1
2016-09-06T23:17:02.625689: step 1738, loss 0.0532405, acc 0.98
2016-09-06T23:17:03.321286: step 1739, loss 0.0231619, acc 1
2016-09-06T23:17:04.011027: step 1740, loss 0.065139, acc 0.94
2016-09-06T23:17:04.694409: step 1741, loss 0.0605727, acc 0.98
2016-09-06T23:17:05.432235: step 1742, loss 0.0604153, acc 0.98
2016-09-06T23:17:06.121534: step 1743, loss 0.0637235, acc 0.98
2016-09-06T23:17:06.806051: step 1744, loss 0.0283054, acc 0.98
2016-09-06T23:17:07.482575: step 1745, loss 0.00754014, acc 1
2016-09-06T23:17:08.200090: step 1746, loss 0.102072, acc 0.96
2016-09-06T23:17:08.911236: step 1747, loss 0.0787506, acc 0.94
2016-09-06T23:17:09.601046: step 1748, loss 0.0354409, acc 0.98
2016-09-06T23:17:10.290194: step 1749, loss 0.0278076, acc 1
2016-09-06T23:17:10.986171: step 1750, loss 0.140071, acc 0.96
2016-09-06T23:17:11.685249: step 1751, loss 0.0236754, acc 1
2016-09-06T23:17:12.347100: step 1752, loss 0.0687487, acc 0.96
2016-09-06T23:17:13.057660: step 1753, loss 0.106426, acc 0.94
2016-09-06T23:17:13.766086: step 1754, loss 0.04363, acc 0.98
2016-09-06T23:17:14.448814: step 1755, loss 0.142882, acc 0.98
2016-09-06T23:17:15.136716: step 1756, loss 0.00935115, acc 1
2016-09-06T23:17:15.810000: step 1757, loss 0.0614177, acc 0.98
2016-09-06T23:17:16.509571: step 1758, loss 0.0343534, acc 1
2016-09-06T23:17:17.186081: step 1759, loss 0.00779997, acc 1
2016-09-06T23:17:17.874265: step 1760, loss 0.233941, acc 0.94
2016-09-06T23:17:18.569734: step 1761, loss 0.0389775, acc 0.98
2016-09-06T23:17:19.269328: step 1762, loss 0.103822, acc 0.98
2016-09-06T23:17:19.950607: step 1763, loss 0.0359497, acc 0.98
2016-09-06T23:17:20.648107: step 1764, loss 0.0920706, acc 0.96
2016-09-06T23:17:21.337762: step 1765, loss 0.118058, acc 0.98
2016-09-06T23:17:22.033259: step 1766, loss 0.073199, acc 0.94
2016-09-06T23:17:22.715060: step 1767, loss 0.0114179, acc 1
2016-09-06T23:17:23.387042: step 1768, loss 0.0251501, acc 1
2016-09-06T23:17:24.088353: step 1769, loss 0.101274, acc 0.98
2016-09-06T23:17:24.808806: step 1770, loss 0.0192228, acc 1
2016-09-06T23:17:25.479416: step 1771, loss 0.128778, acc 0.96
2016-09-06T23:17:26.200081: step 1772, loss 0.0330787, acc 0.98
2016-09-06T23:17:26.867424: step 1773, loss 0.0226277, acc 0.98
2016-09-06T23:17:27.540263: step 1774, loss 0.0110149, acc 1
2016-09-06T23:17:28.219521: step 1775, loss 0.0479979, acc 0.98
2016-09-06T23:17:28.915956: step 1776, loss 0.143185, acc 0.96
2016-09-06T23:17:29.611913: step 1777, loss 0.0645246, acc 0.96
2016-09-06T23:17:30.298871: step 1778, loss 0.0286511, acc 0.98
2016-09-06T23:17:30.982858: step 1779, loss 0.0181054, acc 1
2016-09-06T23:17:31.665774: step 1780, loss 0.0121692, acc 1
2016-09-06T23:17:32.352923: step 1781, loss 0.124708, acc 0.94
2016-09-06T23:17:33.048943: step 1782, loss 0.0424862, acc 0.98
2016-09-06T23:17:33.733471: step 1783, loss 0.11061, acc 0.96
2016-09-06T23:17:34.417012: step 1784, loss 0.170147, acc 0.94
2016-09-06T23:17:35.122686: step 1785, loss 0.0782135, acc 0.96
2016-09-06T23:17:35.823776: step 1786, loss 0.107886, acc 0.96
2016-09-06T23:17:36.507358: step 1787, loss 0.0496453, acc 0.98
2016-09-06T23:17:37.173407: step 1788, loss 0.0692986, acc 0.98
2016-09-06T23:17:37.872542: step 1789, loss 0.18972, acc 0.92
2016-09-06T23:17:38.567398: step 1790, loss 0.10372, acc 0.96
2016-09-06T23:17:39.257257: step 1791, loss 0.106689, acc 0.98
2016-09-06T23:17:39.920187: step 1792, loss 0.024023, acc 1
2016-09-06T23:17:40.635027: step 1793, loss 0.0587284, acc 0.98
2016-09-06T23:17:41.306056: step 1794, loss 0.0945457, acc 0.98
2016-09-06T23:17:41.998424: step 1795, loss 0.0830272, acc 0.94
2016-09-06T23:17:42.673319: step 1796, loss 0.0750237, acc 0.96
2016-09-06T23:17:43.358704: step 1797, loss 0.0436393, acc 1
2016-09-06T23:17:44.063901: step 1798, loss 0.142115, acc 0.96
2016-09-06T23:17:44.721885: step 1799, loss 0.174544, acc 0.96
2016-09-06T23:17:45.432657: step 1800, loss 0.0951812, acc 0.96

Evaluation:
2016-09-06T23:17:48.577233: step 1800, loss 0.995193, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-1800

2016-09-06T23:17:50.264851: step 1801, loss 0.0498703, acc 0.98
2016-09-06T23:17:50.938371: step 1802, loss 0.0597083, acc 0.98
2016-09-06T23:17:51.639476: step 1803, loss 0.0460481, acc 0.98
2016-09-06T23:17:52.341484: step 1804, loss 0.125556, acc 0.92
2016-09-06T23:17:53.023700: step 1805, loss 0.0522814, acc 1
2016-09-06T23:17:53.712078: step 1806, loss 0.0213846, acc 0.98
2016-09-06T23:17:54.388195: step 1807, loss 0.0542252, acc 0.96
2016-09-06T23:17:55.108146: step 1808, loss 0.0797912, acc 0.96
2016-09-06T23:17:55.790161: step 1809, loss 0.086003, acc 0.98
2016-09-06T23:17:56.490857: step 1810, loss 0.0890575, acc 0.96
2016-09-06T23:17:57.200155: step 1811, loss 0.0693868, acc 0.98
2016-09-06T23:17:57.906902: step 1812, loss 0.0720643, acc 0.96
2016-09-06T23:17:58.595510: step 1813, loss 0.0322964, acc 1
2016-09-06T23:17:59.260785: step 1814, loss 0.0787912, acc 0.96
2016-09-06T23:17:59.977120: step 1815, loss 0.126118, acc 0.94
2016-09-06T23:18:00.697364: step 1816, loss 0.023582, acc 0.98
2016-09-06T23:18:01.394531: step 1817, loss 0.0829559, acc 0.98
2016-09-06T23:18:02.063056: step 1818, loss 0.106105, acc 0.92
2016-09-06T23:18:02.733294: step 1819, loss 0.0387821, acc 0.98
2016-09-06T23:18:03.433811: step 1820, loss 0.0792222, acc 0.94
2016-09-06T23:18:04.113957: step 1821, loss 0.0551942, acc 0.96
2016-09-06T23:18:04.793160: step 1822, loss 0.0158753, acc 1
2016-09-06T23:18:05.481430: step 1823, loss 0.0635991, acc 0.96
2016-09-06T23:18:06.169391: step 1824, loss 0.0292484, acc 0.98
2016-09-06T23:18:06.856540: step 1825, loss 0.0509955, acc 0.96
2016-09-06T23:18:07.540517: step 1826, loss 0.0381617, acc 0.98
2016-09-06T23:18:08.254137: step 1827, loss 0.0502512, acc 0.96
2016-09-06T23:18:08.934208: step 1828, loss 0.0135206, acc 1
2016-09-06T23:18:09.607521: step 1829, loss 0.0204946, acc 0.98
2016-09-06T23:18:10.297021: step 1830, loss 0.0132607, acc 1
2016-09-06T23:18:11.004517: step 1831, loss 0.037749, acc 0.98
2016-09-06T23:18:11.685837: step 1832, loss 0.0338325, acc 0.98
2016-09-06T23:18:12.373496: step 1833, loss 0.103199, acc 0.92
2016-09-06T23:18:13.083321: step 1834, loss 0.0556689, acc 0.98
2016-09-06T23:18:13.753604: step 1835, loss 0.0205143, acc 0.98
2016-09-06T23:18:14.431493: step 1836, loss 0.0924748, acc 0.96
2016-09-06T23:18:15.137195: step 1837, loss 0.10486, acc 0.98
2016-09-06T23:18:15.845991: step 1838, loss 0.0390214, acc 0.98
2016-09-06T23:18:16.539083: step 1839, loss 0.00570186, acc 1
2016-09-06T23:18:17.214097: step 1840, loss 0.0100067, acc 1
2016-09-06T23:18:17.921311: step 1841, loss 0.167713, acc 0.92
2016-09-06T23:18:18.599403: step 1842, loss 0.0269027, acc 1
2016-09-06T23:18:19.294136: step 1843, loss 0.0627894, acc 0.96
2016-09-06T23:18:19.976340: step 1844, loss 0.0549761, acc 0.98
2016-09-06T23:18:20.654927: step 1845, loss 0.0292779, acc 0.98
2016-09-06T23:18:21.359269: step 1846, loss 0.128159, acc 0.94
2016-09-06T23:18:22.052293: step 1847, loss 0.00937795, acc 1
2016-09-06T23:18:22.737630: step 1848, loss 0.00354196, acc 1
2016-09-06T23:18:23.404302: step 1849, loss 0.0502889, acc 0.98
2016-09-06T23:18:24.073225: step 1850, loss 0.085002, acc 0.94
2016-09-06T23:18:24.765656: step 1851, loss 0.00685342, acc 1
2016-09-06T23:18:25.433619: step 1852, loss 0.160269, acc 0.94
2016-09-06T23:18:26.107571: step 1853, loss 0.033211, acc 0.98
2016-09-06T23:18:26.802038: step 1854, loss 0.0545379, acc 0.96
2016-09-06T23:18:27.511665: step 1855, loss 0.0223582, acc 0.98
2016-09-06T23:18:28.187413: step 1856, loss 0.111175, acc 0.96
2016-09-06T23:18:28.873901: step 1857, loss 0.0263729, acc 1
2016-09-06T23:18:29.551873: step 1858, loss 0.0478254, acc 1
2016-09-06T23:18:30.223691: step 1859, loss 0.0983394, acc 0.96
2016-09-06T23:18:30.909461: step 1860, loss 0.082882, acc 0.98
2016-09-06T23:18:31.596102: step 1861, loss 0.034994, acc 0.98
2016-09-06T23:18:32.288304: step 1862, loss 0.0247611, acc 1
2016-09-06T23:18:32.978165: step 1863, loss 0.0480575, acc 0.96
2016-09-06T23:18:33.683563: step 1864, loss 0.0318642, acc 0.98
2016-09-06T23:18:34.398162: step 1865, loss 0.0317355, acc 1
2016-09-06T23:18:35.084614: step 1866, loss 0.00793411, acc 1
2016-09-06T23:18:35.782239: step 1867, loss 0.141909, acc 0.94
2016-09-06T23:18:36.460759: step 1868, loss 0.0240901, acc 1
2016-09-06T23:18:37.168914: step 1869, loss 0.00651753, acc 1
2016-09-06T23:18:37.854798: step 1870, loss 0.00514838, acc 1
2016-09-06T23:18:38.522558: step 1871, loss 0.028536, acc 0.98
2016-09-06T23:18:39.218370: step 1872, loss 0.0394996, acc 0.98
2016-09-06T23:18:39.892020: step 1873, loss 0.0765529, acc 0.96
2016-09-06T23:18:40.574044: step 1874, loss 0.0134762, acc 1
2016-09-06T23:18:41.228852: step 1875, loss 0.11388, acc 0.98
2016-09-06T23:18:41.928914: step 1876, loss 0.0434932, acc 0.96
2016-09-06T23:18:42.590102: step 1877, loss 0.0751392, acc 0.94
2016-09-06T23:18:43.253572: step 1878, loss 0.0325654, acc 0.98
2016-09-06T23:18:43.929618: step 1879, loss 0.0496103, acc 0.96
2016-09-06T23:18:44.626358: step 1880, loss 0.0103435, acc 1
2016-09-06T23:18:45.315526: step 1881, loss 0.0318462, acc 0.98
2016-09-06T23:18:45.994608: step 1882, loss 0.0402637, acc 1
2016-09-06T23:18:46.709263: step 1883, loss 0.0496039, acc 0.96
2016-09-06T23:18:47.385297: step 1884, loss 0.0107003, acc 1
2016-09-06T23:18:48.071597: step 1885, loss 0.106214, acc 0.98
2016-09-06T23:18:48.754163: step 1886, loss 0.0910039, acc 0.96
2016-09-06T23:18:49.468436: step 1887, loss 0.0421371, acc 0.98
2016-09-06T23:18:50.147546: step 1888, loss 0.216695, acc 0.94
2016-09-06T23:18:50.831320: step 1889, loss 0.0282908, acc 0.98
2016-09-06T23:18:51.531150: step 1890, loss 0.0220393, acc 0.98
2016-09-06T23:18:52.203804: step 1891, loss 0.0165239, acc 1
2016-09-06T23:18:52.908474: step 1892, loss 0.051711, acc 0.98
2016-09-06T23:18:53.600262: step 1893, loss 0.0255607, acc 0.98
2016-09-06T23:18:54.298601: step 1894, loss 0.0483389, acc 0.96
2016-09-06T23:18:54.998514: step 1895, loss 0.0227028, acc 0.98
2016-09-06T23:18:55.652439: step 1896, loss 0.0530815, acc 0.96
2016-09-06T23:18:56.355282: step 1897, loss 0.0658684, acc 0.96
2016-09-06T23:18:57.037853: step 1898, loss 0.0825609, acc 0.98
2016-09-06T23:18:57.709945: step 1899, loss 0.0689887, acc 0.96
2016-09-06T23:18:58.396949: step 1900, loss 0.0271042, acc 0.98

Evaluation:
2016-09-06T23:19:01.562465: step 1900, loss 1.37191, acc 0.772045

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-1900

2016-09-06T23:19:03.298982: step 1901, loss 0.0102939, acc 1
2016-09-06T23:19:03.992484: step 1902, loss 0.115454, acc 0.96
2016-09-06T23:19:04.673475: step 1903, loss 0.0244678, acc 0.98
2016-09-06T23:19:05.329850: step 1904, loss 0.066387, acc 0.98
2016-09-06T23:19:06.040173: step 1905, loss 0.0120004, acc 1
2016-09-06T23:19:06.737423: step 1906, loss 0.1376, acc 0.96
2016-09-06T23:19:07.427903: step 1907, loss 0.0612456, acc 0.96
2016-09-06T23:19:08.133055: step 1908, loss 0.0856273, acc 0.96
2016-09-06T23:19:08.853872: step 1909, loss 0.023306, acc 1
2016-09-06T23:19:09.538800: step 1910, loss 0.00915496, acc 1
2016-09-06T23:19:10.206651: step 1911, loss 0.0606571, acc 0.98
2016-09-06T23:19:10.902900: step 1912, loss 0.0453193, acc 0.98
2016-09-06T23:19:11.582859: step 1913, loss 0.0298949, acc 1
2016-09-06T23:19:12.267310: step 1914, loss 0.0570503, acc 0.98
2016-09-06T23:19:12.977227: step 1915, loss 0.0218507, acc 1
2016-09-06T23:19:13.680381: step 1916, loss 0.0667618, acc 0.96
2016-09-06T23:19:14.397232: step 1917, loss 0.0660363, acc 0.98
2016-09-06T23:19:15.077427: step 1918, loss 0.205569, acc 0.94
2016-09-06T23:19:15.788510: step 1919, loss 0.0280684, acc 1
2016-09-06T23:19:16.415980: step 1920, loss 0.0235913, acc 1
2016-09-06T23:19:17.099678: step 1921, loss 0.0424965, acc 0.98
2016-09-06T23:19:17.774362: step 1922, loss 0.113398, acc 0.94
2016-09-06T23:19:18.470826: step 1923, loss 0.171596, acc 0.96
2016-09-06T23:19:19.200953: step 1924, loss 0.082697, acc 0.96
2016-09-06T23:19:19.890149: step 1925, loss 0.0642602, acc 0.96
2016-09-06T23:19:20.599231: step 1926, loss 0.0214985, acc 1
2016-09-06T23:19:21.301465: step 1927, loss 0.128308, acc 0.94
2016-09-06T23:19:21.992210: step 1928, loss 0.0359767, acc 1
2016-09-06T23:19:22.684627: step 1929, loss 0.019852, acc 1
2016-09-06T23:19:23.339366: step 1930, loss 0.0778817, acc 0.96
2016-09-06T23:19:24.015730: step 1931, loss 0.00487959, acc 1
2016-09-06T23:19:24.703139: step 1932, loss 0.0392645, acc 0.96
2016-09-06T23:19:25.417405: step 1933, loss 0.0612337, acc 0.98
2016-09-06T23:19:26.115630: step 1934, loss 0.172661, acc 0.92
2016-09-06T23:19:26.810751: step 1935, loss 0.23909, acc 0.86
2016-09-06T23:19:27.505278: step 1936, loss 0.0676984, acc 0.96
2016-09-06T23:19:28.186228: step 1937, loss 0.0840019, acc 0.98
2016-09-06T23:19:28.898275: step 1938, loss 0.0645244, acc 0.98
2016-09-06T23:19:29.582766: step 1939, loss 0.0682059, acc 0.98
2016-09-06T23:19:30.268032: step 1940, loss 0.00700401, acc 1
2016-09-06T23:19:30.966024: step 1941, loss 0.0166982, acc 1
2016-09-06T23:19:31.645553: step 1942, loss 0.0482818, acc 0.98
2016-09-06T23:19:32.334140: step 1943, loss 0.0675619, acc 0.98
2016-09-06T23:19:33.015991: step 1944, loss 0.0309512, acc 1
2016-09-06T23:19:33.714701: step 1945, loss 0.117877, acc 0.96
2016-09-06T23:19:34.429008: step 1946, loss 0.00794646, acc 1
2016-09-06T23:19:35.114368: step 1947, loss 0.0317422, acc 0.98
2016-09-06T23:19:35.794557: step 1948, loss 0.121598, acc 0.96
2016-09-06T23:19:36.468353: step 1949, loss 0.0569573, acc 0.96
2016-09-06T23:19:37.137745: step 1950, loss 0.087622, acc 0.96
2016-09-06T23:19:37.791245: step 1951, loss 0.0344834, acc 0.98
2016-09-06T23:19:38.482636: step 1952, loss 0.0607164, acc 0.98
2016-09-06T23:19:39.168366: step 1953, loss 0.0389955, acc 0.98
2016-09-06T23:19:39.853591: step 1954, loss 0.152471, acc 0.94
2016-09-06T23:19:40.535105: step 1955, loss 0.0308686, acc 1
2016-09-06T23:19:41.236084: step 1956, loss 0.0412823, acc 0.98
2016-09-06T23:19:41.920569: step 1957, loss 0.215305, acc 0.92
2016-09-06T23:19:42.573842: step 1958, loss 0.058293, acc 0.96
2016-09-06T23:19:43.283487: step 1959, loss 0.0351462, acc 1
2016-09-06T23:19:43.969803: step 1960, loss 0.00455957, acc 1
2016-09-06T23:19:44.650359: step 1961, loss 0.0749198, acc 0.94
2016-09-06T23:19:45.335654: step 1962, loss 0.0500996, acc 0.98
2016-09-06T23:19:46.013392: step 1963, loss 0.126255, acc 0.96
2016-09-06T23:19:46.722007: step 1964, loss 0.0270103, acc 1
2016-09-06T23:19:47.392690: step 1965, loss 0.0348819, acc 0.98
2016-09-06T23:19:48.110537: step 1966, loss 0.0363764, acc 0.98
2016-09-06T23:19:48.809552: step 1967, loss 0.00993033, acc 1
2016-09-06T23:19:49.522811: step 1968, loss 0.0388812, acc 1
2016-09-06T23:19:50.208080: step 1969, loss 0.0536502, acc 0.98
2016-09-06T23:19:50.878510: step 1970, loss 0.0471828, acc 0.98
2016-09-06T23:19:51.566290: step 1971, loss 0.0351691, acc 0.96
2016-09-06T23:19:52.236194: step 1972, loss 0.0487287, acc 0.96
2016-09-06T23:19:52.929918: step 1973, loss 0.0120708, acc 1
2016-09-06T23:19:53.634489: step 1974, loss 0.207086, acc 0.98
2016-09-06T23:19:54.331070: step 1975, loss 0.0888182, acc 0.98
2016-09-06T23:19:55.018360: step 1976, loss 0.0443623, acc 0.96
2016-09-06T23:19:55.694642: step 1977, loss 0.0799132, acc 0.96
2016-09-06T23:19:56.391168: step 1978, loss 0.0701917, acc 0.98
2016-09-06T23:19:57.072517: step 1979, loss 0.109591, acc 0.96
2016-09-06T23:19:57.784473: step 1980, loss 0.0266957, acc 0.98
2016-09-06T23:19:58.482718: step 1981, loss 0.0620217, acc 0.96
2016-09-06T23:19:59.188599: step 1982, loss 0.0210379, acc 1
2016-09-06T23:19:59.902533: step 1983, loss 0.0321494, acc 1
2016-09-06T23:20:00.634736: step 1984, loss 0.0412224, acc 1
2016-09-06T23:20:01.335004: step 1985, loss 0.0991975, acc 0.94
2016-09-06T23:20:02.035942: step 1986, loss 0.0530147, acc 0.98
2016-09-06T23:20:02.701808: step 1987, loss 0.0880778, acc 0.96
2016-09-06T23:20:03.383171: step 1988, loss 0.0441418, acc 0.98
2016-09-06T23:20:04.059810: step 1989, loss 0.0675949, acc 0.96
2016-09-06T23:20:04.746057: step 1990, loss 0.0547969, acc 0.96
2016-09-06T23:20:05.430316: step 1991, loss 0.0637784, acc 0.98
2016-09-06T23:20:06.126938: step 1992, loss 0.0233408, acc 1
2016-09-06T23:20:06.794973: step 1993, loss 0.0221865, acc 0.98
2016-09-06T23:20:07.458472: step 1994, loss 0.0698021, acc 0.96
2016-09-06T23:20:08.133429: step 1995, loss 0.0707111, acc 0.96
2016-09-06T23:20:08.816845: step 1996, loss 0.0554524, acc 0.94
2016-09-06T23:20:09.510687: step 1997, loss 0.0617395, acc 0.98
2016-09-06T23:20:10.206382: step 1998, loss 0.0136605, acc 1
2016-09-06T23:20:10.918745: step 1999, loss 0.0597219, acc 0.96
2016-09-06T23:20:11.603199: step 2000, loss 0.0833958, acc 0.94

Evaluation:
2016-09-06T23:20:14.715540: step 2000, loss 1.37409, acc 0.767355

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-2000

2016-09-06T23:20:16.336156: step 2001, loss 0.172368, acc 0.94
2016-09-06T23:20:17.059206: step 2002, loss 0.0702841, acc 0.98
2016-09-06T23:20:17.782791: step 2003, loss 0.0374536, acc 0.98
2016-09-06T23:20:18.486118: step 2004, loss 0.0328701, acc 1
2016-09-06T23:20:19.184619: step 2005, loss 0.030654, acc 0.98
2016-09-06T23:20:19.871965: step 2006, loss 0.0299579, acc 0.98
2016-09-06T23:20:20.590424: step 2007, loss 0.0738856, acc 0.96
2016-09-06T23:20:21.272684: step 2008, loss 0.010944, acc 1
2016-09-06T23:20:21.974384: step 2009, loss 0.0954001, acc 0.92
2016-09-06T23:20:22.663855: step 2010, loss 0.10067, acc 0.96
2016-09-06T23:20:23.369645: step 2011, loss 0.118826, acc 0.94
2016-09-06T23:20:24.076449: step 2012, loss 0.113742, acc 0.94
2016-09-06T23:20:24.748746: step 2013, loss 0.0284647, acc 0.98
2016-09-06T23:20:25.434634: step 2014, loss 0.0344892, acc 1
2016-09-06T23:20:26.126943: step 2015, loss 0.0652907, acc 0.98
2016-09-06T23:20:26.802172: step 2016, loss 0.114098, acc 0.96
2016-09-06T23:20:27.473070: step 2017, loss 0.0387578, acc 0.98
2016-09-06T23:20:28.146159: step 2018, loss 0.0911378, acc 0.92
2016-09-06T23:20:28.839450: step 2019, loss 0.0271784, acc 1
2016-09-06T23:20:29.514980: step 2020, loss 0.070941, acc 0.94
2016-09-06T23:20:30.215595: step 2021, loss 0.108665, acc 0.98
2016-09-06T23:20:30.890676: step 2022, loss 0.00592635, acc 1
2016-09-06T23:20:31.561369: step 2023, loss 0.0523801, acc 0.98
2016-09-06T23:20:32.246141: step 2024, loss 0.0270557, acc 0.98
2016-09-06T23:20:32.954130: step 2025, loss 0.0488325, acc 0.96
2016-09-06T23:20:33.667304: step 2026, loss 0.13947, acc 0.92
2016-09-06T23:20:34.341114: step 2027, loss 0.0276568, acc 1
2016-09-06T23:20:35.065698: step 2028, loss 0.0857813, acc 0.96
2016-09-06T23:20:35.765162: step 2029, loss 0.129935, acc 0.94
2016-09-06T23:20:36.449022: step 2030, loss 0.0518649, acc 0.98
2016-09-06T23:20:37.151810: step 2031, loss 0.0750429, acc 0.94
2016-09-06T23:20:37.852893: step 2032, loss 0.0265382, acc 1
2016-09-06T23:20:38.592805: step 2033, loss 0.0350482, acc 0.98
2016-09-06T23:20:39.279598: step 2034, loss 0.0551308, acc 0.98
2016-09-06T23:20:39.992711: step 2035, loss 0.0749605, acc 0.96
2016-09-06T23:20:40.710094: step 2036, loss 0.0434288, acc 0.96
2016-09-06T23:20:41.402724: step 2037, loss 0.0367767, acc 0.98
2016-09-06T23:20:42.080821: step 2038, loss 0.043664, acc 0.98
2016-09-06T23:20:42.742694: step 2039, loss 0.0380135, acc 0.98
2016-09-06T23:20:43.428153: step 2040, loss 0.0221017, acc 1
2016-09-06T23:20:44.093040: step 2041, loss 0.168852, acc 0.9
2016-09-06T23:20:44.761036: step 2042, loss 0.0272646, acc 0.98
2016-09-06T23:20:45.453256: step 2043, loss 0.0617089, acc 0.96
2016-09-06T23:20:46.160532: step 2044, loss 0.134647, acc 0.96
2016-09-06T23:20:46.841191: step 2045, loss 0.0204607, acc 1
2016-09-06T23:20:47.531659: step 2046, loss 0.0974922, acc 0.96
2016-09-06T23:20:48.241946: step 2047, loss 0.0273908, acc 0.98
2016-09-06T23:20:48.953562: step 2048, loss 0.116667, acc 0.94
2016-09-06T23:20:49.632080: step 2049, loss 0.0220455, acc 1
2016-09-06T23:20:50.320506: step 2050, loss 0.0414236, acc 0.98
2016-09-06T23:20:51.001108: step 2051, loss 0.0955081, acc 0.92
2016-09-06T23:20:51.694302: step 2052, loss 0.0346027, acc 0.98
2016-09-06T23:20:52.353413: step 2053, loss 0.0685302, acc 0.94
2016-09-06T23:20:53.059071: step 2054, loss 0.0587949, acc 0.98
2016-09-06T23:20:53.743437: step 2055, loss 0.0146775, acc 1
2016-09-06T23:20:54.440916: step 2056, loss 0.0959359, acc 0.96
2016-09-06T23:20:55.130513: step 2057, loss 0.0597907, acc 0.98
2016-09-06T23:20:55.811385: step 2058, loss 0.00378836, acc 1
2016-09-06T23:20:56.478934: step 2059, loss 0.051922, acc 0.98
2016-09-06T23:20:57.139502: step 2060, loss 0.0357622, acc 1
2016-09-06T23:20:57.858219: step 2061, loss 0.0677041, acc 0.98
2016-09-06T23:20:58.549910: step 2062, loss 0.0159187, acc 1
2016-09-06T23:20:59.233942: step 2063, loss 0.0027172, acc 1
2016-09-06T23:20:59.920854: step 2064, loss 0.088792, acc 0.96
2016-09-06T23:21:00.625949: step 2065, loss 0.124437, acc 0.94
2016-09-06T23:21:01.315434: step 2066, loss 0.00389017, acc 1
2016-09-06T23:21:01.981360: step 2067, loss 0.0445778, acc 0.98
2016-09-06T23:21:02.664662: step 2068, loss 0.0474019, acc 0.98
2016-09-06T23:21:03.336540: step 2069, loss 0.048065, acc 0.98
2016-09-06T23:21:04.020295: step 2070, loss 0.0336707, acc 0.98
2016-09-06T23:21:04.698893: step 2071, loss 0.021798, acc 1
2016-09-06T23:21:05.378876: step 2072, loss 0.0246912, acc 1
2016-09-06T23:21:06.093214: step 2073, loss 0.00356066, acc 1
2016-09-06T23:21:06.780575: step 2074, loss 0.146892, acc 0.96
2016-09-06T23:21:07.493280: step 2075, loss 0.0105408, acc 1
2016-09-06T23:21:08.184579: step 2076, loss 0.0497106, acc 0.98
2016-09-06T23:21:08.875808: step 2077, loss 0.0113457, acc 1
2016-09-06T23:21:09.543928: step 2078, loss 0.0401211, acc 1
2016-09-06T23:21:10.240572: step 2079, loss 0.0122831, acc 1
2016-09-06T23:21:10.939875: step 2080, loss 0.0909718, acc 0.94
2016-09-06T23:21:11.602337: step 2081, loss 0.0584837, acc 0.96
2016-09-06T23:21:12.305307: step 2082, loss 0.0328829, acc 0.98
2016-09-06T23:21:12.993375: step 2083, loss 0.175355, acc 0.96
2016-09-06T23:21:13.675486: step 2084, loss 0.16169, acc 0.98
2016-09-06T23:21:14.367880: step 2085, loss 0.0873797, acc 0.94
2016-09-06T23:21:15.062897: step 2086, loss 0.0859221, acc 0.94
2016-09-06T23:21:15.775523: step 2087, loss 0.106308, acc 0.96
2016-09-06T23:21:16.458557: step 2088, loss 0.0190429, acc 1
2016-09-06T23:21:17.168718: step 2089, loss 0.0278453, acc 0.98
2016-09-06T23:21:17.863215: step 2090, loss 0.101646, acc 0.94
2016-09-06T23:21:18.557193: step 2091, loss 0.0164506, acc 0.98
2016-09-06T23:21:19.280948: step 2092, loss 0.0420696, acc 0.96
2016-09-06T23:21:19.956268: step 2093, loss 0.0423684, acc 0.96
2016-09-06T23:21:20.665543: step 2094, loss 0.124313, acc 0.92
2016-09-06T23:21:21.337946: step 2095, loss 0.036868, acc 0.98
2016-09-06T23:21:22.028732: step 2096, loss 0.0435652, acc 0.98
2016-09-06T23:21:22.709677: step 2097, loss 0.0489566, acc 0.98
2016-09-06T23:21:23.390417: step 2098, loss 0.156648, acc 0.9
2016-09-06T23:21:24.091438: step 2099, loss 0.0279081, acc 1
2016-09-06T23:21:24.750317: step 2100, loss 0.0419646, acc 0.96

Evaluation:
2016-09-06T23:21:27.945910: step 2100, loss 1.31312, acc 0.75985

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-2100

2016-09-06T23:21:29.643157: step 2101, loss 0.0433897, acc 0.96
2016-09-06T23:21:30.336935: step 2102, loss 0.0510941, acc 0.96
2016-09-06T23:21:31.010664: step 2103, loss 0.00986895, acc 1
2016-09-06T23:21:31.686670: step 2104, loss 0.102701, acc 0.98
2016-09-06T23:21:32.385241: step 2105, loss 0.138036, acc 0.94
2016-09-06T23:21:33.075096: step 2106, loss 0.0526717, acc 0.96
2016-09-06T23:21:33.764721: step 2107, loss 0.0745908, acc 0.98
2016-09-06T23:21:34.427149: step 2108, loss 0.0455721, acc 0.98
2016-09-06T23:21:35.124266: step 2109, loss 0.0873316, acc 0.96
2016-09-06T23:21:35.788093: step 2110, loss 0.100626, acc 0.98
2016-09-06T23:21:36.473248: step 2111, loss 0.062036, acc 0.96
2016-09-06T23:21:37.089723: step 2112, loss 0.00458895, acc 1
2016-09-06T23:21:37.782565: step 2113, loss 0.0630088, acc 0.98
2016-09-06T23:21:38.466633: step 2114, loss 0.141555, acc 0.96
2016-09-06T23:21:39.137589: step 2115, loss 0.0522421, acc 0.96
2016-09-06T23:21:39.835653: step 2116, loss 0.0559288, acc 0.96
2016-09-06T23:21:40.499260: step 2117, loss 0.00797749, acc 1
2016-09-06T23:21:41.199496: step 2118, loss 0.0430861, acc 1
2016-09-06T23:21:41.883120: step 2119, loss 0.179014, acc 0.94
2016-09-06T23:21:42.574165: step 2120, loss 0.0443723, acc 1
2016-09-06T23:21:43.268324: step 2121, loss 0.0704693, acc 0.96
2016-09-06T23:21:43.960107: step 2122, loss 0.026459, acc 1
2016-09-06T23:21:44.636966: step 2123, loss 0.0253749, acc 0.98
2016-09-06T23:21:45.306511: step 2124, loss 0.0230796, acc 0.98
2016-09-06T23:21:46.012290: step 2125, loss 0.102476, acc 0.94
2016-09-06T23:21:46.682010: step 2126, loss 0.0574152, acc 0.96
2016-09-06T23:21:47.382308: step 2127, loss 0.0169818, acc 1
2016-09-06T23:21:48.091927: step 2128, loss 0.0360316, acc 1
2016-09-06T23:21:48.786175: step 2129, loss 0.0492027, acc 0.98
2016-09-06T23:21:49.504303: step 2130, loss 0.0756834, acc 0.94
2016-09-06T23:21:50.177481: step 2131, loss 0.1154, acc 0.94
2016-09-06T23:21:50.860384: step 2132, loss 0.0405442, acc 0.98
2016-09-06T23:21:51.553813: step 2133, loss 0.138902, acc 0.98
2016-09-06T23:21:52.239052: step 2134, loss 0.120531, acc 0.96
2016-09-06T23:21:52.920392: step 2135, loss 0.0921194, acc 0.94
2016-09-06T23:21:53.623151: step 2136, loss 0.00947868, acc 1
2016-09-06T23:21:54.324411: step 2137, loss 0.107572, acc 0.96
2016-09-06T23:21:54.989249: step 2138, loss 0.113899, acc 0.94
2016-09-06T23:21:55.694478: step 2139, loss 0.00900817, acc 1
2016-09-06T23:21:56.382704: step 2140, loss 0.0890083, acc 0.96
2016-09-06T23:21:57.079613: step 2141, loss 0.111339, acc 0.94
2016-09-06T23:21:57.771612: step 2142, loss 0.0384012, acc 1
2016-09-06T23:21:58.455897: step 2143, loss 0.0115192, acc 1
2016-09-06T23:21:59.163120: step 2144, loss 0.131487, acc 0.92
2016-09-06T23:21:59.851312: step 2145, loss 0.0196001, acc 1
2016-09-06T23:22:00.563347: step 2146, loss 0.0358138, acc 1
2016-09-06T23:22:01.234851: step 2147, loss 0.105434, acc 0.96
2016-09-06T23:22:01.920616: step 2148, loss 0.0357583, acc 1
2016-09-06T23:22:02.609516: step 2149, loss 0.0496733, acc 0.96
2016-09-06T23:22:03.276637: step 2150, loss 0.0486975, acc 0.98
2016-09-06T23:22:03.954447: step 2151, loss 0.0124041, acc 1
2016-09-06T23:22:04.633641: step 2152, loss 0.0208627, acc 1
2016-09-06T23:22:05.317359: step 2153, loss 0.0542183, acc 1
2016-09-06T23:22:06.037791: step 2154, loss 0.0838519, acc 0.96
2016-09-06T23:22:06.719494: step 2155, loss 0.0260049, acc 1
2016-09-06T23:22:07.397626: step 2156, loss 0.00685811, acc 1
2016-09-06T23:22:08.083760: step 2157, loss 0.154012, acc 0.94
2016-09-06T23:22:08.792250: step 2158, loss 0.0698871, acc 0.98
2016-09-06T23:22:09.485545: step 2159, loss 0.0494189, acc 0.98
2016-09-06T23:22:10.185085: step 2160, loss 0.0953234, acc 0.96
2016-09-06T23:22:10.870827: step 2161, loss 0.00344299, acc 1
2016-09-06T23:22:11.562685: step 2162, loss 0.0781596, acc 0.96
2016-09-06T23:22:12.272710: step 2163, loss 0.0127962, acc 1
2016-09-06T23:22:12.975104: step 2164, loss 0.00918571, acc 1
2016-09-06T23:22:13.679656: step 2165, loss 0.0772536, acc 0.96
2016-09-06T23:22:14.373892: step 2166, loss 0.0270421, acc 1
2016-09-06T23:22:15.042256: step 2167, loss 0.0103852, acc 1
2016-09-06T23:22:15.714601: step 2168, loss 0.0577528, acc 0.98
2016-09-06T23:22:16.398850: step 2169, loss 0.0310326, acc 1
2016-09-06T23:22:17.075925: step 2170, loss 0.0155608, acc 1
2016-09-06T23:22:17.741999: step 2171, loss 0.0156773, acc 0.98
2016-09-06T23:22:18.445034: step 2172, loss 0.00773473, acc 1
2016-09-06T23:22:19.138899: step 2173, loss 0.022884, acc 1
2016-09-06T23:22:19.822608: step 2174, loss 0.0152695, acc 1
2016-09-06T23:22:20.499049: step 2175, loss 0.0431557, acc 1
2016-09-06T23:22:21.186380: step 2176, loss 0.065559, acc 0.94
2016-09-06T23:22:21.877440: step 2177, loss 0.130851, acc 0.94
2016-09-06T23:22:22.534925: step 2178, loss 0.0583422, acc 0.96
2016-09-06T23:22:23.234191: step 2179, loss 0.0511239, acc 0.96
2016-09-06T23:22:23.905912: step 2180, loss 0.0310074, acc 0.98
2016-09-06T23:22:24.596707: step 2181, loss 0.00717562, acc 1
2016-09-06T23:22:25.280194: step 2182, loss 0.0437964, acc 0.96
2016-09-06T23:22:25.954446: step 2183, loss 0.0618856, acc 0.98
2016-09-06T23:22:26.642126: step 2184, loss 0.0842167, acc 0.92
2016-09-06T23:22:27.309536: step 2185, loss 0.0156814, acc 1
2016-09-06T23:22:28.012944: step 2186, loss 0.00252607, acc 1
2016-09-06T23:22:28.695087: step 2187, loss 0.0560657, acc 0.96
2016-09-06T23:22:29.375104: step 2188, loss 0.0718538, acc 0.98
2016-09-06T23:22:30.068984: step 2189, loss 0.013505, acc 1
2016-09-06T23:22:30.768969: step 2190, loss 0.0118691, acc 1
2016-09-06T23:22:31.437189: step 2191, loss 0.0989863, acc 0.96
2016-09-06T23:22:32.130032: step 2192, loss 0.048851, acc 0.96
2016-09-06T23:22:32.852374: step 2193, loss 0.00584255, acc 1
2016-09-06T23:22:33.515262: step 2194, loss 0.0133424, acc 1
2016-09-06T23:22:34.206418: step 2195, loss 0.00477038, acc 1
2016-09-06T23:22:34.892037: step 2196, loss 0.0582653, acc 0.98
2016-09-06T23:22:35.590943: step 2197, loss 0.0420975, acc 0.98
2016-09-06T23:22:36.283346: step 2198, loss 0.0332267, acc 0.98
2016-09-06T23:22:36.943079: step 2199, loss 0.0276918, acc 1
2016-09-06T23:22:37.651182: step 2200, loss 0.0124287, acc 1

Evaluation:
2016-09-06T23:22:40.784302: step 2200, loss 1.61368, acc 0.766416

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-2200

2016-09-06T23:22:42.507575: step 2201, loss 0.0579284, acc 0.96
2016-09-06T23:22:43.210103: step 2202, loss 0.0674509, acc 0.98
2016-09-06T23:22:43.907427: step 2203, loss 0.0777004, acc 0.98
2016-09-06T23:22:44.597985: step 2204, loss 0.0314036, acc 0.98
2016-09-06T23:22:45.276277: step 2205, loss 0.0137331, acc 1
2016-09-06T23:22:45.984780: step 2206, loss 0.0889545, acc 0.94
2016-09-06T23:22:46.672052: step 2207, loss 0.0285548, acc 1
2016-09-06T23:22:47.377098: step 2208, loss 0.0642961, acc 0.98
2016-09-06T23:22:48.073624: step 2209, loss 0.0593812, acc 0.96
2016-09-06T23:22:48.768188: step 2210, loss 0.0356205, acc 1
2016-09-06T23:22:49.463971: step 2211, loss 0.032716, acc 0.98
2016-09-06T23:22:50.143867: step 2212, loss 0.111907, acc 0.98
2016-09-06T23:22:50.827874: step 2213, loss 0.117549, acc 0.98
2016-09-06T23:22:51.512833: step 2214, loss 0.00289203, acc 1
2016-09-06T23:22:52.203488: step 2215, loss 0.189816, acc 0.94
2016-09-06T23:22:52.907262: step 2216, loss 0.112724, acc 0.96
2016-09-06T23:22:53.600946: step 2217, loss 0.0566138, acc 0.94
2016-09-06T23:22:54.297362: step 2218, loss 0.0698594, acc 0.96
2016-09-06T23:22:54.974298: step 2219, loss 0.0131575, acc 1
2016-09-06T23:22:55.692160: step 2220, loss 0.0363807, acc 0.98
2016-09-06T23:22:56.359163: step 2221, loss 0.0548229, acc 0.96
2016-09-06T23:22:57.026132: step 2222, loss 0.110582, acc 0.96
2016-09-06T23:22:57.730948: step 2223, loss 0.096254, acc 0.94
2016-09-06T23:22:58.426125: step 2224, loss 0.016582, acc 1
2016-09-06T23:22:59.126382: step 2225, loss 0.0229127, acc 1
2016-09-06T23:22:59.829042: step 2226, loss 0.0741033, acc 0.96
2016-09-06T23:23:00.573613: step 2227, loss 0.0475762, acc 0.98
2016-09-06T23:23:01.248875: step 2228, loss 0.0291752, acc 0.98
2016-09-06T23:23:01.936090: step 2229, loss 0.04271, acc 0.98
2016-09-06T23:23:02.626754: step 2230, loss 0.0544185, acc 0.98
2016-09-06T23:23:03.306348: step 2231, loss 0.0297804, acc 1
2016-09-06T23:23:03.989302: step 2232, loss 0.0278836, acc 1
2016-09-06T23:23:04.651497: step 2233, loss 0.036857, acc 0.98
2016-09-06T23:23:05.349356: step 2234, loss 0.0365638, acc 0.98
2016-09-06T23:23:06.028835: step 2235, loss 0.053165, acc 0.98
2016-09-06T23:23:06.719700: step 2236, loss 0.0529596, acc 0.96
2016-09-06T23:23:07.390996: step 2237, loss 0.01888, acc 1
2016-09-06T23:23:08.065801: step 2238, loss 0.0511142, acc 0.94
2016-09-06T23:23:08.748733: step 2239, loss 0.0713784, acc 0.98
2016-09-06T23:23:09.402914: step 2240, loss 0.0260678, acc 1
2016-09-06T23:23:10.093466: step 2241, loss 0.0148209, acc 1
2016-09-06T23:23:10.782304: step 2242, loss 0.0370912, acc 0.98
2016-09-06T23:23:11.463023: step 2243, loss 0.0962571, acc 0.96
2016-09-06T23:23:12.150901: step 2244, loss 0.00443107, acc 1
2016-09-06T23:23:12.839258: step 2245, loss 0.167662, acc 0.94
2016-09-06T23:23:13.515764: step 2246, loss 0.123931, acc 0.92
2016-09-06T23:23:14.212868: step 2247, loss 0.0707905, acc 0.94
2016-09-06T23:23:14.903373: step 2248, loss 0.0791026, acc 0.96
2016-09-06T23:23:15.591972: step 2249, loss 0.052005, acc 0.98
2016-09-06T23:23:16.269555: step 2250, loss 0.0728991, acc 0.98
2016-09-06T23:23:16.965255: step 2251, loss 0.103563, acc 0.92
2016-09-06T23:23:17.658853: step 2252, loss 0.0543949, acc 0.94
2016-09-06T23:23:18.344050: step 2253, loss 0.02453, acc 1
2016-09-06T23:23:19.025931: step 2254, loss 0.130779, acc 0.94
2016-09-06T23:23:19.718891: step 2255, loss 0.0388288, acc 0.98
2016-09-06T23:23:20.386039: step 2256, loss 0.0391194, acc 0.98
2016-09-06T23:23:21.084015: step 2257, loss 0.0877882, acc 0.94
2016-09-06T23:23:21.775293: step 2258, loss 0.0698301, acc 0.96
2016-09-06T23:23:22.451529: step 2259, loss 0.0223127, acc 1
2016-09-06T23:23:23.129003: step 2260, loss 0.0320349, acc 1
2016-09-06T23:23:23.813332: step 2261, loss 0.0581585, acc 0.98
2016-09-06T23:23:24.517508: step 2262, loss 0.110259, acc 0.96
2016-09-06T23:23:25.206291: step 2263, loss 0.0548112, acc 0.96
2016-09-06T23:23:25.873409: step 2264, loss 0.0311486, acc 1
2016-09-06T23:23:26.573187: step 2265, loss 0.0100707, acc 1
2016-09-06T23:23:27.251139: step 2266, loss 0.0426753, acc 1
2016-09-06T23:23:27.920265: step 2267, loss 0.104354, acc 0.94
2016-09-06T23:23:28.603494: step 2268, loss 0.251249, acc 0.94
2016-09-06T23:23:29.309142: step 2269, loss 0.105326, acc 0.94
2016-09-06T23:23:30.024068: step 2270, loss 0.0367455, acc 1
2016-09-06T23:23:30.740826: step 2271, loss 0.0848762, acc 0.98
2016-09-06T23:23:31.421911: step 2272, loss 0.0653967, acc 0.96
2016-09-06T23:23:32.094865: step 2273, loss 0.0207063, acc 1
2016-09-06T23:23:32.796591: step 2274, loss 0.0112308, acc 1
2016-09-06T23:23:33.482174: step 2275, loss 0.0394538, acc 1
2016-09-06T23:23:34.184050: step 2276, loss 0.140975, acc 0.96
2016-09-06T23:23:34.862686: step 2277, loss 0.08625, acc 0.98
2016-09-06T23:23:35.539388: step 2278, loss 0.0135579, acc 1
2016-09-06T23:23:36.244246: step 2279, loss 0.102346, acc 0.94
2016-09-06T23:23:36.923412: step 2280, loss 0.067695, acc 0.98
2016-09-06T23:23:37.625837: step 2281, loss 0.097679, acc 0.94
2016-09-06T23:23:38.309356: step 2282, loss 0.0413231, acc 0.98
2016-09-06T23:23:39.010741: step 2283, loss 0.0430604, acc 0.98
2016-09-06T23:23:39.703924: step 2284, loss 0.0235265, acc 1
2016-09-06T23:23:40.387270: step 2285, loss 0.0403558, acc 0.98
2016-09-06T23:23:41.062257: step 2286, loss 0.108451, acc 0.98
2016-09-06T23:23:41.742093: step 2287, loss 0.0585895, acc 0.96
2016-09-06T23:23:42.448936: step 2288, loss 0.0337457, acc 0.98
2016-09-06T23:23:43.133897: step 2289, loss 0.0168973, acc 1
2016-09-06T23:23:43.846212: step 2290, loss 0.140851, acc 0.96
2016-09-06T23:23:44.521227: step 2291, loss 0.0310834, acc 1
2016-09-06T23:23:45.239053: step 2292, loss 0.0376843, acc 0.98
2016-09-06T23:23:45.922111: step 2293, loss 0.0273124, acc 1
2016-09-06T23:23:46.600721: step 2294, loss 0.00584975, acc 1
2016-09-06T23:23:47.297561: step 2295, loss 0.0417557, acc 0.96
2016-09-06T23:23:47.965691: step 2296, loss 0.0708306, acc 0.98
2016-09-06T23:23:48.650744: step 2297, loss 0.0577844, acc 0.98
2016-09-06T23:23:49.328090: step 2298, loss 0.0264089, acc 0.98
2016-09-06T23:23:50.019372: step 2299, loss 0.0856404, acc 0.94
2016-09-06T23:23:50.720288: step 2300, loss 0.0416275, acc 0.98

Evaluation:
2016-09-06T23:23:53.858831: step 2300, loss 1.51886, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-2300

2016-09-06T23:23:55.533659: step 2301, loss 0.0661429, acc 0.96
2016-09-06T23:23:56.233189: step 2302, loss 0.103027, acc 0.98
2016-09-06T23:23:56.932994: step 2303, loss 0.0620958, acc 0.98
2016-09-06T23:23:57.565925: step 2304, loss 0.132177, acc 0.954545
2016-09-06T23:23:58.272180: step 2305, loss 0.0746915, acc 0.94
2016-09-06T23:23:58.956934: step 2306, loss 0.0583639, acc 0.98
2016-09-06T23:23:59.644798: step 2307, loss 0.0489253, acc 0.96
2016-09-06T23:24:00.378812: step 2308, loss 0.0195008, acc 1
2016-09-06T23:24:01.058364: step 2309, loss 0.00196226, acc 1
2016-09-06T23:24:01.780771: step 2310, loss 0.0426885, acc 0.98
2016-09-06T23:24:02.447397: step 2311, loss 0.0700001, acc 0.96
2016-09-06T23:24:03.141896: step 2312, loss 0.0310894, acc 0.98
2016-09-06T23:24:03.833645: step 2313, loss 0.0657155, acc 0.96
2016-09-06T23:24:04.526207: step 2314, loss 0.0395463, acc 0.98
2016-09-06T23:24:05.222650: step 2315, loss 0.0197233, acc 1
2016-09-06T23:24:05.907473: step 2316, loss 0.0407838, acc 0.98
2016-09-06T23:24:06.608278: step 2317, loss 0.0391153, acc 0.98
2016-09-06T23:24:07.292649: step 2318, loss 0.108254, acc 0.96
2016-09-06T23:24:07.961279: step 2319, loss 0.0046047, acc 1
2016-09-06T23:24:08.653751: step 2320, loss 0.00227252, acc 1
2016-09-06T23:24:09.343950: step 2321, loss 0.0997194, acc 0.9
2016-09-06T23:24:10.023401: step 2322, loss 0.0128173, acc 1
2016-09-06T23:24:10.706587: step 2323, loss 0.070952, acc 0.96
2016-09-06T23:24:11.403195: step 2324, loss 0.0394837, acc 0.98
2016-09-06T23:24:12.092139: step 2325, loss 0.00931696, acc 1
2016-09-06T23:24:12.770844: step 2326, loss 0.0773107, acc 0.96
2016-09-06T23:24:13.457556: step 2327, loss 0.0294099, acc 0.98
2016-09-06T23:24:14.137591: step 2328, loss 0.0504073, acc 0.98
2016-09-06T23:24:14.826090: step 2329, loss 0.242969, acc 0.96
2016-09-06T23:24:15.502688: step 2330, loss 0.0198929, acc 1
2016-09-06T23:24:16.225149: step 2331, loss 0.0249703, acc 0.98
2016-09-06T23:24:16.903732: step 2332, loss 0.0707041, acc 0.98
2016-09-06T23:24:17.593231: step 2333, loss 0.0557445, acc 0.98
2016-09-06T23:24:18.274843: step 2334, loss 0.0117667, acc 1
2016-09-06T23:24:18.954068: step 2335, loss 0.062662, acc 0.94
2016-09-06T23:24:19.636235: step 2336, loss 0.100051, acc 0.96
2016-09-06T23:24:20.315190: step 2337, loss 0.0517663, acc 0.98
2016-09-06T23:24:21.024403: step 2338, loss 0.0176251, acc 0.98
2016-09-06T23:24:21.715041: step 2339, loss 0.00479767, acc 1
2016-09-06T23:24:22.398660: step 2340, loss 0.022014, acc 0.98
2016-09-06T23:24:23.103735: step 2341, loss 0.0611762, acc 0.96
2016-09-06T23:24:23.804973: step 2342, loss 0.0239536, acc 1
2016-09-06T23:24:24.473646: step 2343, loss 0.0167117, acc 1
2016-09-06T23:24:25.149330: step 2344, loss 0.00872676, acc 1
2016-09-06T23:24:25.855104: step 2345, loss 0.0476357, acc 1
2016-09-06T23:24:26.509931: step 2346, loss 0.0132371, acc 1
2016-09-06T23:24:27.210648: step 2347, loss 0.0195225, acc 0.98
2016-09-06T23:24:27.898062: step 2348, loss 0.0601179, acc 0.96
2016-09-06T23:24:28.598315: step 2349, loss 0.0456487, acc 0.96
2016-09-06T23:24:29.297471: step 2350, loss 0.0369209, acc 1
2016-09-06T23:24:29.970743: step 2351, loss 0.0498053, acc 0.98
2016-09-06T23:24:30.661378: step 2352, loss 0.0708987, acc 0.96
2016-09-06T23:24:31.340184: step 2353, loss 0.0023816, acc 1
2016-09-06T23:24:32.060716: step 2354, loss 0.0332987, acc 0.98
2016-09-06T23:24:32.745106: step 2355, loss 0.0317994, acc 0.98
2016-09-06T23:24:33.442582: step 2356, loss 0.0271387, acc 0.98
2016-09-06T23:24:34.136505: step 2357, loss 0.0253855, acc 0.98
2016-09-06T23:24:34.805239: step 2358, loss 0.0465882, acc 0.96
2016-09-06T23:24:35.536856: step 2359, loss 0.0047725, acc 1
2016-09-06T23:24:36.220424: step 2360, loss 0.089609, acc 0.96
2016-09-06T23:24:36.887676: step 2361, loss 0.0391248, acc 0.98
2016-09-06T23:24:37.558259: step 2362, loss 0.0382793, acc 0.98
2016-09-06T23:24:38.235421: step 2363, loss 0.0156453, acc 1
2016-09-06T23:24:38.939127: step 2364, loss 0.0641566, acc 0.96
2016-09-06T23:24:39.615496: step 2365, loss 0.0877235, acc 0.94
2016-09-06T23:24:40.332545: step 2366, loss 0.0059483, acc 1
2016-09-06T23:24:41.026705: step 2367, loss 0.0891039, acc 0.96
2016-09-06T23:24:41.730389: step 2368, loss 0.125372, acc 0.96
2016-09-06T23:24:42.419342: step 2369, loss 0.0165918, acc 1
2016-09-06T23:24:43.100574: step 2370, loss 0.0546062, acc 0.96
2016-09-06T23:24:43.784235: step 2371, loss 0.0268662, acc 0.98
2016-09-06T23:24:44.458534: step 2372, loss 0.0273339, acc 0.98
2016-09-06T23:24:45.159899: step 2373, loss 0.114447, acc 0.98
2016-09-06T23:24:45.849533: step 2374, loss 0.0369424, acc 1
2016-09-06T23:24:46.546871: step 2375, loss 0.0331302, acc 0.98
2016-09-06T23:24:47.239014: step 2376, loss 0.0597211, acc 0.98
2016-09-06T23:24:47.936416: step 2377, loss 0.0240011, acc 1
2016-09-06T23:24:48.635820: step 2378, loss 0.083766, acc 0.96
2016-09-06T23:24:49.301038: step 2379, loss 0.0165449, acc 1
2016-09-06T23:24:50.003910: step 2380, loss 0.0509894, acc 0.94
2016-09-06T23:24:50.681013: step 2381, loss 0.00649514, acc 1
2016-09-06T23:24:51.363183: step 2382, loss 0.014574, acc 1
2016-09-06T23:24:52.056114: step 2383, loss 0.0454337, acc 0.98
2016-09-06T23:24:52.737779: step 2384, loss 0.0313523, acc 0.98
2016-09-06T23:24:53.438432: step 2385, loss 0.00955659, acc 1
2016-09-06T23:24:54.096667: step 2386, loss 0.0886167, acc 0.96
2016-09-06T23:24:54.799568: step 2387, loss 0.10116, acc 0.98
2016-09-06T23:24:55.481099: step 2388, loss 0.0198765, acc 1
2016-09-06T23:24:56.171271: step 2389, loss 0.0613128, acc 0.98
2016-09-06T23:24:56.874120: step 2390, loss 0.0329052, acc 0.98
2016-09-06T23:24:57.575607: step 2391, loss 0.0617339, acc 0.96
2016-09-06T23:24:58.295602: step 2392, loss 0.0473472, acc 0.98
2016-09-06T23:24:58.980391: step 2393, loss 0.0441395, acc 0.96
2016-09-06T23:24:59.653423: step 2394, loss 0.057451, acc 0.96
2016-09-06T23:25:00.357133: step 2395, loss 0.220877, acc 0.96
2016-09-06T23:25:01.038555: step 2396, loss 0.0255041, acc 1
2016-09-06T23:25:01.722025: step 2397, loss 0.00518659, acc 1
2016-09-06T23:25:02.422401: step 2398, loss 0.0144968, acc 1
2016-09-06T23:25:03.130494: step 2399, loss 0.0467728, acc 0.98
2016-09-06T23:25:03.814646: step 2400, loss 0.0160713, acc 1

Evaluation:
2016-09-06T23:25:06.944590: step 2400, loss 1.60785, acc 0.768293

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-2400

2016-09-06T23:25:08.700179: step 2401, loss 0.0148721, acc 1
2016-09-06T23:25:09.374001: step 2402, loss 0.0493928, acc 0.94
2016-09-06T23:25:10.042560: step 2403, loss 0.0629521, acc 0.96
2016-09-06T23:25:10.731790: step 2404, loss 0.123401, acc 0.98
2016-09-06T23:25:11.433406: step 2405, loss 0.0739664, acc 0.94
2016-09-06T23:25:12.103512: step 2406, loss 0.00489341, acc 1
2016-09-06T23:25:12.805294: step 2407, loss 0.0998981, acc 0.98
2016-09-06T23:25:13.499404: step 2408, loss 0.0569023, acc 0.98
2016-09-06T23:25:14.188174: step 2409, loss 0.0301431, acc 1
2016-09-06T23:25:14.913730: step 2410, loss 0.0506971, acc 1
2016-09-06T23:25:15.602185: step 2411, loss 0.0348422, acc 0.98
2016-09-06T23:25:16.331927: step 2412, loss 0.0300168, acc 0.98
2016-09-06T23:25:17.018832: step 2413, loss 0.0340329, acc 0.98
2016-09-06T23:25:17.699990: step 2414, loss 0.162341, acc 0.96
2016-09-06T23:25:18.390035: step 2415, loss 0.111179, acc 0.96
2016-09-06T23:25:19.106219: step 2416, loss 0.0263981, acc 0.98
2016-09-06T23:25:19.799049: step 2417, loss 0.0954481, acc 0.98
2016-09-06T23:25:20.451119: step 2418, loss 0.0367286, acc 0.98
2016-09-06T23:25:21.163460: step 2419, loss 0.0454131, acc 0.98
2016-09-06T23:25:21.838733: step 2420, loss 0.0434076, acc 1
2016-09-06T23:25:22.518415: step 2421, loss 0.0707478, acc 0.98
2016-09-06T23:25:23.218751: step 2422, loss 0.0453152, acc 0.98
2016-09-06T23:25:23.892077: step 2423, loss 0.0117163, acc 1
2016-09-06T23:25:24.609206: step 2424, loss 0.0836686, acc 0.96
2016-09-06T23:25:25.282800: step 2425, loss 0.106869, acc 0.96
2016-09-06T23:25:26.013517: step 2426, loss 0.0513197, acc 0.98
2016-09-06T23:25:26.689575: step 2427, loss 0.00118821, acc 1
2016-09-06T23:25:27.375900: step 2428, loss 0.0328753, acc 0.98
2016-09-06T23:25:28.050072: step 2429, loss 0.00527148, acc 1
2016-09-06T23:25:28.725460: step 2430, loss 0.041246, acc 0.98
2016-09-06T23:25:29.431303: step 2431, loss 0.0259541, acc 0.98
2016-09-06T23:25:30.111998: step 2432, loss 0.0259332, acc 0.98
2016-09-06T23:25:30.824513: step 2433, loss 0.00958436, acc 1
2016-09-06T23:25:31.520983: step 2434, loss 0.0451465, acc 0.98
2016-09-06T23:25:32.195897: step 2435, loss 0.0859631, acc 0.96
2016-09-06T23:25:32.892189: step 2436, loss 0.019313, acc 1
2016-09-06T23:25:33.579192: step 2437, loss 0.110806, acc 0.98
2016-09-06T23:25:34.302196: step 2438, loss 0.0790378, acc 0.94
2016-09-06T23:25:34.981265: step 2439, loss 0.00648251, acc 1
2016-09-06T23:25:35.681491: step 2440, loss 0.0729764, acc 0.98
2016-09-06T23:25:36.352824: step 2441, loss 0.0254946, acc 1
2016-09-06T23:25:37.036469: step 2442, loss 0.0432594, acc 0.96
2016-09-06T23:25:37.735647: step 2443, loss 0.0802898, acc 0.96
2016-09-06T23:25:38.421382: step 2444, loss 0.0115659, acc 1
2016-09-06T23:25:39.137872: step 2445, loss 0.0253058, acc 0.98
2016-09-06T23:25:39.825071: step 2446, loss 0.0534005, acc 0.96
2016-09-06T23:25:40.515884: step 2447, loss 0.0541639, acc 0.98
2016-09-06T23:25:41.213411: step 2448, loss 0.0148548, acc 1
2016-09-06T23:25:41.900430: step 2449, loss 0.0991697, acc 0.96
2016-09-06T23:25:42.574832: step 2450, loss 0.0326465, acc 0.96
2016-09-06T23:25:43.278013: step 2451, loss 0.0316316, acc 0.98
2016-09-06T23:25:43.973373: step 2452, loss 0.0172766, acc 0.98
2016-09-06T23:25:44.653238: step 2453, loss 0.124018, acc 0.96
2016-09-06T23:25:45.346039: step 2454, loss 0.00287484, acc 1
2016-09-06T23:25:46.044508: step 2455, loss 0.0181874, acc 1
2016-09-06T23:25:46.737285: step 2456, loss 0.0331851, acc 0.98
2016-09-06T23:25:47.433103: step 2457, loss 0.0330538, acc 1
2016-09-06T23:25:48.119805: step 2458, loss 0.0382496, acc 1
2016-09-06T23:25:48.821909: step 2459, loss 0.0770306, acc 0.98
2016-09-06T23:25:49.491709: step 2460, loss 0.0187181, acc 0.98
2016-09-06T23:25:50.168076: step 2461, loss 0.00617945, acc 1
2016-09-06T23:25:50.852746: step 2462, loss 0.0471745, acc 0.98
2016-09-06T23:25:51.523914: step 2463, loss 0.0306781, acc 0.98
2016-09-06T23:25:52.207880: step 2464, loss 0.0859585, acc 0.94
2016-09-06T23:25:52.904858: step 2465, loss 0.00172699, acc 1
2016-09-06T23:25:53.606327: step 2466, loss 0.0318407, acc 0.98
2016-09-06T23:25:54.267801: step 2467, loss 0.0597439, acc 0.96
2016-09-06T23:25:54.937342: step 2468, loss 0.0220308, acc 1
2016-09-06T23:25:55.613898: step 2469, loss 0.00814301, acc 1
2016-09-06T23:25:56.307525: step 2470, loss 0.0635733, acc 0.98
2016-09-06T23:25:56.998934: step 2471, loss 0.0312777, acc 0.98
2016-09-06T23:25:57.679799: step 2472, loss 0.0460627, acc 0.98
2016-09-06T23:25:58.375410: step 2473, loss 0.0246197, acc 1
2016-09-06T23:25:59.037723: step 2474, loss 0.0441507, acc 0.98
2016-09-06T23:25:59.748922: step 2475, loss 0.0134593, acc 1
2016-09-06T23:26:00.480283: step 2476, loss 0.0276276, acc 0.98
2016-09-06T23:26:01.161362: step 2477, loss 0.0280519, acc 0.98
2016-09-06T23:26:01.852798: step 2478, loss 0.0793526, acc 0.98
2016-09-06T23:26:02.533331: step 2479, loss 0.0483754, acc 0.98
2016-09-06T23:26:03.242500: step 2480, loss 0.0322905, acc 1
2016-09-06T23:26:03.952522: step 2481, loss 0.237209, acc 0.96
2016-09-06T23:26:04.635696: step 2482, loss 0.0218008, acc 1
2016-09-06T23:26:05.333981: step 2483, loss 0.111875, acc 0.96
2016-09-06T23:26:06.023941: step 2484, loss 0.139796, acc 0.94
2016-09-06T23:26:06.702184: step 2485, loss 0.0173053, acc 1
2016-09-06T23:26:07.371094: step 2486, loss 0.021967, acc 0.98
2016-09-06T23:26:08.097151: step 2487, loss 0.0496424, acc 0.98
2016-09-06T23:26:08.775784: step 2488, loss 0.107111, acc 0.94
2016-09-06T23:26:09.443962: step 2489, loss 0.0221768, acc 1
2016-09-06T23:26:10.125440: step 2490, loss 0.0114685, acc 1
2016-09-06T23:26:10.810736: step 2491, loss 0.0360436, acc 0.96
2016-09-06T23:26:11.485783: step 2492, loss 0.0172197, acc 1
2016-09-06T23:26:12.165507: step 2493, loss 0.0599993, acc 0.96
2016-09-06T23:26:12.879515: step 2494, loss 0.0998152, acc 0.92
2016-09-06T23:26:13.575398: step 2495, loss 0.0870502, acc 0.98
2016-09-06T23:26:14.214343: step 2496, loss 0.0235978, acc 1
2016-09-06T23:26:14.899646: step 2497, loss 0.0891274, acc 0.96
2016-09-06T23:26:15.581997: step 2498, loss 0.0306923, acc 1
2016-09-06T23:26:16.264297: step 2499, loss 0.0527638, acc 0.98
2016-09-06T23:26:16.944415: step 2500, loss 0.0679565, acc 0.98

Evaluation:
2016-09-06T23:26:20.095256: step 2500, loss 1.36796, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-2500

2016-09-06T23:26:21.809464: step 2501, loss 0.0353309, acc 0.98
2016-09-06T23:26:22.528496: step 2502, loss 0.10367, acc 0.98
2016-09-06T23:26:23.199575: step 2503, loss 0.0427644, acc 0.98
2016-09-06T23:26:23.886877: step 2504, loss 0.0162201, acc 1
2016-09-06T23:26:24.586382: step 2505, loss 0.0240978, acc 1
2016-09-06T23:26:25.260847: step 2506, loss 0.0273986, acc 1
2016-09-06T23:26:25.962260: step 2507, loss 0.0719148, acc 0.98
2016-09-06T23:26:26.630048: step 2508, loss 0.0491893, acc 0.98
2016-09-06T23:26:27.339455: step 2509, loss 0.0387392, acc 0.98
2016-09-06T23:26:28.022506: step 2510, loss 0.0479896, acc 0.96
2016-09-06T23:26:28.700235: step 2511, loss 0.0807216, acc 0.96
2016-09-06T23:26:29.400126: step 2512, loss 0.0319297, acc 1
2016-09-06T23:26:30.067959: step 2513, loss 0.0104361, acc 1
2016-09-06T23:26:30.746415: step 2514, loss 0.0260677, acc 0.98
2016-09-06T23:26:31.400221: step 2515, loss 0.0186444, acc 0.98
2016-09-06T23:26:32.107192: step 2516, loss 0.0495686, acc 0.98
2016-09-06T23:26:32.769877: step 2517, loss 0.0499742, acc 0.98
2016-09-06T23:26:33.447546: step 2518, loss 0.125177, acc 0.96
2016-09-06T23:26:34.143237: step 2519, loss 0.0616644, acc 0.98
2016-09-06T23:26:34.833084: step 2520, loss 0.0368843, acc 0.96
2016-09-06T23:26:35.541321: step 2521, loss 0.0665443, acc 0.98
2016-09-06T23:26:36.197662: step 2522, loss 0.0259882, acc 0.98
2016-09-06T23:26:36.902356: step 2523, loss 0.0492339, acc 0.98
2016-09-06T23:26:37.562796: step 2524, loss 0.0249454, acc 0.98
2016-09-06T23:26:38.260683: step 2525, loss 0.0184008, acc 1
2016-09-06T23:26:38.951378: step 2526, loss 0.0289678, acc 0.98
2016-09-06T23:26:39.641496: step 2527, loss 0.0345279, acc 1
2016-09-06T23:26:40.329354: step 2528, loss 0.0963192, acc 0.94
2016-09-06T23:26:41.010563: step 2529, loss 0.0748955, acc 0.98
2016-09-06T23:26:41.681923: step 2530, loss 0.092292, acc 0.96
2016-09-06T23:26:42.357247: step 2531, loss 0.0390144, acc 0.98
2016-09-06T23:26:43.038374: step 2532, loss 0.0404476, acc 0.96
2016-09-06T23:26:43.723415: step 2533, loss 0.0295635, acc 0.98
2016-09-06T23:26:44.400967: step 2534, loss 0.015746, acc 1
2016-09-06T23:26:45.086451: step 2535, loss 0.00708645, acc 1
2016-09-06T23:26:45.817734: step 2536, loss 0.0219641, acc 1
2016-09-06T23:26:46.529470: step 2537, loss 0.0583476, acc 0.98
2016-09-06T23:26:47.200470: step 2538, loss 0.0319065, acc 1
2016-09-06T23:26:47.867980: step 2539, loss 0.05885, acc 0.96
2016-09-06T23:26:48.545306: step 2540, loss 0.0243259, acc 0.98
2016-09-06T23:26:49.225611: step 2541, loss 0.0664798, acc 0.98
2016-09-06T23:26:49.913812: step 2542, loss 0.00338437, acc 1
2016-09-06T23:26:50.624550: step 2543, loss 0.0104314, acc 1
2016-09-06T23:26:51.331214: step 2544, loss 0.0242248, acc 0.98
2016-09-06T23:26:51.997779: step 2545, loss 0.0133804, acc 1
2016-09-06T23:26:52.692843: step 2546, loss 0.162492, acc 0.92
2016-09-06T23:26:53.377275: step 2547, loss 0.0603436, acc 0.98
2016-09-06T23:26:54.070533: step 2548, loss 0.0137646, acc 1
2016-09-06T23:26:54.773659: step 2549, loss 0.0395528, acc 0.98
2016-09-06T23:26:55.463447: step 2550, loss 0.00778742, acc 1
2016-09-06T23:26:56.171073: step 2551, loss 0.0203789, acc 0.98
2016-09-06T23:26:56.851621: step 2552, loss 0.0458194, acc 0.96
2016-09-06T23:26:57.527107: step 2553, loss 0.0460662, acc 0.96
2016-09-06T23:26:58.231528: step 2554, loss 0.0426352, acc 0.96
2016-09-06T23:26:58.921741: step 2555, loss 0.0169553, acc 0.98
2016-09-06T23:26:59.637450: step 2556, loss 0.00649041, acc 1
2016-09-06T23:27:00.350558: step 2557, loss 0.0412369, acc 0.98
2016-09-06T23:27:01.068925: step 2558, loss 0.0068085, acc 1
2016-09-06T23:27:01.756069: step 2559, loss 0.0740508, acc 0.94
2016-09-06T23:27:02.438323: step 2560, loss 0.0768957, acc 0.96
2016-09-06T23:27:03.121459: step 2561, loss 0.0180471, acc 1
2016-09-06T23:27:03.807521: step 2562, loss 0.0615904, acc 0.96
2016-09-06T23:27:04.528394: step 2563, loss 0.00292644, acc 1
2016-09-06T23:27:05.214864: step 2564, loss 0.0114697, acc 1
2016-09-06T23:27:05.911065: step 2565, loss 0.0201428, acc 1
2016-09-06T23:27:06.605979: step 2566, loss 0.0575828, acc 0.98
2016-09-06T23:27:07.292372: step 2567, loss 0.0755354, acc 0.96
2016-09-06T23:27:07.983455: step 2568, loss 0.0159636, acc 1
2016-09-06T23:27:08.658521: step 2569, loss 0.00194408, acc 1
2016-09-06T23:27:09.333781: step 2570, loss 0.0994588, acc 0.98
2016-09-06T23:27:09.996647: step 2571, loss 0.00564318, acc 1
2016-09-06T23:27:10.697787: step 2572, loss 0.0108186, acc 1
2016-09-06T23:27:11.395005: step 2573, loss 0.0258161, acc 1
2016-09-06T23:27:12.063236: step 2574, loss 0.0718438, acc 0.96
2016-09-06T23:27:12.746027: step 2575, loss 0.0426278, acc 0.98
2016-09-06T23:27:13.445907: step 2576, loss 0.0499565, acc 0.98
2016-09-06T23:27:14.163151: step 2577, loss 0.0590262, acc 0.96
2016-09-06T23:27:14.850167: step 2578, loss 0.0339082, acc 0.98
2016-09-06T23:27:15.542708: step 2579, loss 0.0316538, acc 1
2016-09-06T23:27:16.239414: step 2580, loss 0.0135361, acc 1
2016-09-06T23:27:16.930302: step 2581, loss 0.0150406, acc 1
2016-09-06T23:27:17.646164: step 2582, loss 0.0324569, acc 1
2016-09-06T23:27:18.331891: step 2583, loss 0.174675, acc 0.96
2016-09-06T23:27:19.030588: step 2584, loss 0.0337853, acc 1
2016-09-06T23:27:19.692460: step 2585, loss 0.105468, acc 0.98
2016-09-06T23:27:20.382499: step 2586, loss 0.0311511, acc 0.98
2016-09-06T23:27:21.077671: step 2587, loss 0.00821969, acc 1
2016-09-06T23:27:21.757292: step 2588, loss 0.0850874, acc 0.96
2016-09-06T23:27:22.457706: step 2589, loss 0.205055, acc 0.94
2016-09-06T23:27:23.144666: step 2590, loss 0.0190001, acc 0.98
2016-09-06T23:27:23.831343: step 2591, loss 0.0236952, acc 0.98
2016-09-06T23:27:24.503614: step 2592, loss 0.0613527, acc 0.96
2016-09-06T23:27:25.191507: step 2593, loss 0.0424952, acc 0.96
2016-09-06T23:27:25.873280: step 2594, loss 0.0155552, acc 1
2016-09-06T23:27:26.559406: step 2595, loss 0.0748555, acc 0.94
2016-09-06T23:27:27.278431: step 2596, loss 0.0843014, acc 0.96
2016-09-06T23:27:27.950382: step 2597, loss 0.0481653, acc 0.98
2016-09-06T23:27:28.664717: step 2598, loss 0.0435975, acc 0.96
2016-09-06T23:27:29.325692: step 2599, loss 0.0252941, acc 1
2016-09-06T23:27:30.021942: step 2600, loss 0.0710308, acc 0.94

Evaluation:
2016-09-06T23:27:33.173609: step 2600, loss 1.58605, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-2600

2016-09-06T23:27:34.900159: step 2601, loss 0.0506674, acc 0.98
2016-09-06T23:27:35.579857: step 2602, loss 0.0427897, acc 0.98
2016-09-06T23:27:36.280540: step 2603, loss 0.040958, acc 0.98
2016-09-06T23:27:37.000144: step 2604, loss 0.0215355, acc 1
2016-09-06T23:27:37.680885: step 2605, loss 0.105612, acc 0.96
2016-09-06T23:27:38.404900: step 2606, loss 0.0112264, acc 1
2016-09-06T23:27:39.118474: step 2607, loss 0.0393623, acc 0.98
2016-09-06T23:27:39.807788: step 2608, loss 0.0232057, acc 0.98
2016-09-06T23:27:40.495990: step 2609, loss 0.0452588, acc 0.98
2016-09-06T23:27:41.168351: step 2610, loss 0.0103591, acc 1
2016-09-06T23:27:41.866128: step 2611, loss 0.036983, acc 0.98
2016-09-06T23:27:42.536665: step 2612, loss 0.0213087, acc 1
2016-09-06T23:27:43.226170: step 2613, loss 0.00433523, acc 1
2016-09-06T23:27:43.913492: step 2614, loss 0.00229762, acc 1
2016-09-06T23:27:44.589138: step 2615, loss 0.0476519, acc 0.98
2016-09-06T23:27:45.296019: step 2616, loss 0.0176533, acc 1
2016-09-06T23:27:45.978203: step 2617, loss 0.0453186, acc 0.96
2016-09-06T23:27:46.690242: step 2618, loss 0.0525604, acc 0.98
2016-09-06T23:27:47.374337: step 2619, loss 0.00327105, acc 1
2016-09-06T23:27:48.040790: step 2620, loss 0.0350783, acc 0.98
2016-09-06T23:27:48.717274: step 2621, loss 0.0475707, acc 0.98
2016-09-06T23:27:49.384602: step 2622, loss 0.0938706, acc 0.98
2016-09-06T23:27:50.077566: step 2623, loss 0.0580198, acc 0.96
2016-09-06T23:27:50.749661: step 2624, loss 0.0994405, acc 0.96
2016-09-06T23:27:51.462935: step 2625, loss 0.0448754, acc 0.98
2016-09-06T23:27:52.134797: step 2626, loss 0.0298368, acc 1
2016-09-06T23:27:52.820829: step 2627, loss 0.0715584, acc 0.98
2016-09-06T23:27:53.503737: step 2628, loss 0.0521566, acc 0.98
2016-09-06T23:27:54.193223: step 2629, loss 0.0412923, acc 0.96
2016-09-06T23:27:54.893380: step 2630, loss 0.105048, acc 0.98
2016-09-06T23:27:55.569425: step 2631, loss 0.00867519, acc 1
2016-09-06T23:27:56.270532: step 2632, loss 0.00854591, acc 1
2016-09-06T23:27:56.947394: step 2633, loss 0.0017646, acc 1
2016-09-06T23:27:57.640800: step 2634, loss 0.00719902, acc 1
2016-09-06T23:27:58.321188: step 2635, loss 0.0172685, acc 1
2016-09-06T23:27:58.994179: step 2636, loss 0.0352017, acc 0.96
2016-09-06T23:27:59.702352: step 2637, loss 0.0977123, acc 0.98
2016-09-06T23:28:00.417002: step 2638, loss 0.00768148, acc 1
2016-09-06T23:28:01.155992: step 2639, loss 0.0529972, acc 0.98
2016-09-06T23:28:01.844357: step 2640, loss 0.0362986, acc 1
2016-09-06T23:28:02.539123: step 2641, loss 0.0170956, acc 1
2016-09-06T23:28:03.220433: step 2642, loss 0.013369, acc 1
2016-09-06T23:28:03.918598: step 2643, loss 0.148554, acc 0.94
2016-09-06T23:28:04.611429: step 2644, loss 0.0794419, acc 0.94
2016-09-06T23:28:05.271182: step 2645, loss 0.0159295, acc 1
2016-09-06T23:28:05.977367: step 2646, loss 0.024332, acc 0.98
2016-09-06T23:28:06.692971: step 2647, loss 0.0109853, acc 1
2016-09-06T23:28:07.355492: step 2648, loss 0.0441557, acc 1
2016-09-06T23:28:08.034743: step 2649, loss 0.0140657, acc 1
2016-09-06T23:28:08.700636: step 2650, loss 0.109733, acc 0.94
2016-09-06T23:28:09.397871: step 2651, loss 0.031642, acc 0.98
2016-09-06T23:28:10.068423: step 2652, loss 0.0128979, acc 1
2016-09-06T23:28:10.791100: step 2653, loss 0.166953, acc 0.94
2016-09-06T23:28:11.482268: step 2654, loss 0.0213747, acc 0.98
2016-09-06T23:28:12.169866: step 2655, loss 0.000435414, acc 1
2016-09-06T23:28:12.871104: step 2656, loss 0.0417459, acc 0.96
2016-09-06T23:28:13.563066: step 2657, loss 0.0290035, acc 0.98
2016-09-06T23:28:14.271058: step 2658, loss 0.0275238, acc 0.98
2016-09-06T23:28:14.953840: step 2659, loss 0.134341, acc 0.96
2016-09-06T23:28:15.648918: step 2660, loss 0.0274455, acc 0.98
2016-09-06T23:28:16.336265: step 2661, loss 0.0427568, acc 0.98
2016-09-06T23:28:17.031361: step 2662, loss 0.033884, acc 0.98
2016-09-06T23:28:17.712664: step 2663, loss 0.0174962, acc 1
2016-09-06T23:28:18.387473: step 2664, loss 0.0180006, acc 0.98
2016-09-06T23:28:19.090086: step 2665, loss 0.022207, acc 1
2016-09-06T23:28:19.771971: step 2666, loss 0.0165219, acc 1
2016-09-06T23:28:20.472500: step 2667, loss 0.0419366, acc 0.98
2016-09-06T23:28:21.154046: step 2668, loss 0.201299, acc 0.98
2016-09-06T23:28:21.854368: step 2669, loss 0.114204, acc 0.98
2016-09-06T23:28:22.564029: step 2670, loss 0.159001, acc 0.94
2016-09-06T23:28:23.253066: step 2671, loss 0.0165885, acc 1
2016-09-06T23:28:23.964696: step 2672, loss 0.0399193, acc 0.98
2016-09-06T23:28:24.640881: step 2673, loss 0.0181216, acc 1
2016-09-06T23:28:25.314036: step 2674, loss 0.06009, acc 0.96
2016-09-06T23:28:26.016225: step 2675, loss 0.0288571, acc 1
2016-09-06T23:28:26.716338: step 2676, loss 0.0483139, acc 0.96
2016-09-06T23:28:27.406709: step 2677, loss 0.0323401, acc 0.98
2016-09-06T23:28:28.058033: step 2678, loss 0.0225559, acc 0.98
2016-09-06T23:28:28.759748: step 2679, loss 0.0594018, acc 0.96
2016-09-06T23:28:29.441205: step 2680, loss 0.0794266, acc 0.98
2016-09-06T23:28:30.115729: step 2681, loss 0.0308087, acc 0.98
2016-09-06T23:28:30.799991: step 2682, loss 0.0229873, acc 1
2016-09-06T23:28:31.480361: step 2683, loss 0.0233141, acc 0.98
2016-09-06T23:28:32.156033: step 2684, loss 0.013593, acc 1
2016-09-06T23:28:32.826236: step 2685, loss 0.0323296, acc 0.98
2016-09-06T23:28:33.524711: step 2686, loss 0.0587282, acc 0.96
2016-09-06T23:28:34.191704: step 2687, loss 0.0416089, acc 0.98
2016-09-06T23:28:34.830997: step 2688, loss 0.048369, acc 0.977273
2016-09-06T23:28:35.531594: step 2689, loss 0.077112, acc 0.96
2016-09-06T23:28:36.225774: step 2690, loss 0.0714496, acc 0.96
2016-09-06T23:28:36.914869: step 2691, loss 0.0799787, acc 0.96
2016-09-06T23:28:37.578344: step 2692, loss 0.0213615, acc 1
2016-09-06T23:28:38.251418: step 2693, loss 0.020807, acc 1
2016-09-06T23:28:38.914397: step 2694, loss 0.0578231, acc 0.96
2016-09-06T23:28:39.616601: step 2695, loss 0.0203911, acc 0.98
2016-09-06T23:28:40.301732: step 2696, loss 0.00945187, acc 1
2016-09-06T23:28:40.998469: step 2697, loss 0.00286798, acc 1
2016-09-06T23:28:41.695845: step 2698, loss 0.0952672, acc 0.96
2016-09-06T23:28:42.385282: step 2699, loss 0.0057231, acc 1
2016-09-06T23:28:43.060655: step 2700, loss 0.00750562, acc 1

Evaluation:
2016-09-06T23:28:46.195995: step 2700, loss 1.61787, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-2700

2016-09-06T23:28:47.828444: step 2701, loss 0.0765906, acc 0.98
2016-09-06T23:28:48.487619: step 2702, loss 0.00723112, acc 1
2016-09-06T23:28:49.197644: step 2703, loss 0.0461859, acc 0.98
2016-09-06T23:28:49.855472: step 2704, loss 0.0230421, acc 1
2016-09-06T23:28:50.519847: step 2705, loss 0.0272063, acc 1
2016-09-06T23:28:51.200489: step 2706, loss 0.0263578, acc 0.98
2016-09-06T23:28:51.889026: step 2707, loss 0.121084, acc 0.94
2016-09-06T23:28:52.584230: step 2708, loss 0.0391519, acc 0.98
2016-09-06T23:28:53.282492: step 2709, loss 0.0116433, acc 1
2016-09-06T23:28:53.981043: step 2710, loss 0.0742518, acc 0.96
2016-09-06T23:28:54.642986: step 2711, loss 0.0710686, acc 0.98
2016-09-06T23:28:55.345783: step 2712, loss 0.00784013, acc 1
2016-09-06T23:28:56.029271: step 2713, loss 0.0402831, acc 0.98
2016-09-06T23:28:56.717023: step 2714, loss 0.00140404, acc 1
2016-09-06T23:28:57.392528: step 2715, loss 0.01829, acc 1
2016-09-06T23:28:58.079053: step 2716, loss 0.00476325, acc 1
2016-09-06T23:28:58.791044: step 2717, loss 0.0298088, acc 1
2016-09-06T23:28:59.476876: step 2718, loss 0.105577, acc 0.98
2016-09-06T23:29:00.172465: step 2719, loss 0.0285247, acc 0.98
2016-09-06T23:29:00.894066: step 2720, loss 0.0432561, acc 0.98
2016-09-06T23:29:01.588888: step 2721, loss 0.0107506, acc 1
2016-09-06T23:29:02.277625: step 2722, loss 0.0333866, acc 0.98
2016-09-06T23:29:02.939880: step 2723, loss 0.00270731, acc 1
2016-09-06T23:29:03.625585: step 2724, loss 0.0155077, acc 1
2016-09-06T23:29:04.305239: step 2725, loss 0.00231687, acc 1
2016-09-06T23:29:04.980510: step 2726, loss 0.0308265, acc 0.98
2016-09-06T23:29:05.660234: step 2727, loss 0.0109407, acc 1
2016-09-06T23:29:06.349938: step 2728, loss 0.00127603, acc 1
2016-09-06T23:29:07.029699: step 2729, loss 0.0286094, acc 1
2016-09-06T23:29:07.737237: step 2730, loss 0.0123905, acc 1
2016-09-06T23:29:08.437809: step 2731, loss 0.00156684, acc 1
2016-09-06T23:29:09.122531: step 2732, loss 0.0957803, acc 0.98
2016-09-06T23:29:09.831027: step 2733, loss 0.109973, acc 0.98
2016-09-06T23:29:10.523510: step 2734, loss 0.055882, acc 0.96
2016-09-06T23:29:11.225497: step 2735, loss 0.00656029, acc 1
2016-09-06T23:29:11.904571: step 2736, loss 0.0156716, acc 1
2016-09-06T23:29:12.611285: step 2737, loss 0.015013, acc 1
2016-09-06T23:29:13.300602: step 2738, loss 0.050419, acc 0.96
2016-09-06T23:29:13.964738: step 2739, loss 0.0638621, acc 0.98
2016-09-06T23:29:14.655901: step 2740, loss 0.0310776, acc 0.98
2016-09-06T23:29:15.337985: step 2741, loss 0.0219826, acc 0.98
2016-09-06T23:29:16.030746: step 2742, loss 0.0283574, acc 1
2016-09-06T23:29:16.727060: step 2743, loss 0.0992801, acc 0.96
2016-09-06T23:29:17.394944: step 2744, loss 0.0059282, acc 1
2016-09-06T23:29:18.101510: step 2745, loss 0.0707161, acc 0.94
2016-09-06T23:29:18.774420: step 2746, loss 0.0265261, acc 0.98
2016-09-06T23:29:19.457343: step 2747, loss 0.00970714, acc 1
2016-09-06T23:29:20.125368: step 2748, loss 0.0211421, acc 1
2016-09-06T23:29:20.821029: step 2749, loss 0.0870196, acc 0.96
2016-09-06T23:29:21.520680: step 2750, loss 0.00594897, acc 1
2016-09-06T23:29:22.187642: step 2751, loss 0.0371543, acc 0.98
2016-09-06T23:29:22.909509: step 2752, loss 0.0616656, acc 0.96
2016-09-06T23:29:23.603385: step 2753, loss 0.0866798, acc 0.96
2016-09-06T23:29:24.293033: step 2754, loss 0.0839952, acc 0.98
2016-09-06T23:29:24.975194: step 2755, loss 0.0234613, acc 1
2016-09-06T23:29:25.672194: step 2756, loss 0.00651771, acc 1
2016-09-06T23:29:26.349568: step 2757, loss 0.125015, acc 0.96
2016-09-06T23:29:27.015329: step 2758, loss 0.0332656, acc 0.98
2016-09-06T23:29:27.736671: step 2759, loss 0.0856258, acc 0.94
2016-09-06T23:29:28.445094: step 2760, loss 0.0523179, acc 0.96
2016-09-06T23:29:29.141412: step 2761, loss 0.0848963, acc 0.98
2016-09-06T23:29:29.842376: step 2762, loss 0.0126328, acc 1
2016-09-06T23:29:30.545844: step 2763, loss 0.00496603, acc 1
2016-09-06T23:29:31.256319: step 2764, loss 0.118107, acc 0.96
2016-09-06T23:29:31.940837: step 2765, loss 0.0365402, acc 0.98
2016-09-06T23:29:32.617848: step 2766, loss 0.00691473, acc 1
2016-09-06T23:29:33.290969: step 2767, loss 0.0377484, acc 0.96
2016-09-06T23:29:33.967143: step 2768, loss 0.0230917, acc 1
2016-09-06T23:29:34.648118: step 2769, loss 0.0206783, acc 1
2016-09-06T23:29:35.322498: step 2770, loss 0.035382, acc 1
2016-09-06T23:29:36.019087: step 2771, loss 0.0577649, acc 0.98
2016-09-06T23:29:36.695772: step 2772, loss 0.0672157, acc 0.98
2016-09-06T23:29:37.405003: step 2773, loss 0.0373488, acc 0.98
2016-09-06T23:29:38.090935: step 2774, loss 0.0356849, acc 1
2016-09-06T23:29:38.775378: step 2775, loss 0.0703408, acc 0.94
2016-09-06T23:29:39.464991: step 2776, loss 0.0188636, acc 1
2016-09-06T23:29:40.168353: step 2777, loss 0.0287876, acc 0.98
2016-09-06T23:29:40.856302: step 2778, loss 0.0399807, acc 1
2016-09-06T23:29:41.522884: step 2779, loss 0.0024705, acc 1
2016-09-06T23:29:42.218115: step 2780, loss 0.0623467, acc 0.98
2016-09-06T23:29:42.911328: step 2781, loss 0.00306404, acc 1
2016-09-06T23:29:43.585603: step 2782, loss 0.0326987, acc 1
2016-09-06T23:29:44.274010: step 2783, loss 0.0366514, acc 0.98
2016-09-06T23:29:44.972045: step 2784, loss 0.0490575, acc 0.96
2016-09-06T23:29:45.663841: step 2785, loss 0.0228961, acc 1
2016-09-06T23:29:46.322091: step 2786, loss 0.0753468, acc 0.96
2016-09-06T23:29:47.016156: step 2787, loss 0.0779944, acc 0.96
2016-09-06T23:29:47.707427: step 2788, loss 0.0430307, acc 0.96
2016-09-06T23:29:48.409697: step 2789, loss 0.0266299, acc 1
2016-09-06T23:29:49.120784: step 2790, loss 0.00206426, acc 1
2016-09-06T23:29:49.801524: step 2791, loss 0.0297945, acc 0.98
2016-09-06T23:29:50.514371: step 2792, loss 0.145034, acc 0.98
2016-09-06T23:29:51.179139: step 2793, loss 0.083436, acc 0.96
2016-09-06T23:29:51.870656: step 2794, loss 0.0248621, acc 1
2016-09-06T23:29:52.556430: step 2795, loss 0.0250028, acc 0.98
2016-09-06T23:29:53.226782: step 2796, loss 0.0253883, acc 1
2016-09-06T23:29:53.902633: step 2797, loss 0.147432, acc 0.96
2016-09-06T23:29:54.586570: step 2798, loss 0.0992886, acc 0.94
2016-09-06T23:29:55.308043: step 2799, loss 0.214752, acc 0.94
2016-09-06T23:29:56.010308: step 2800, loss 0.0538924, acc 0.96

Evaluation:
2016-09-06T23:29:59.167879: step 2800, loss 1.62256, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-2800

2016-09-06T23:30:00.824187: step 2801, loss 0.0505835, acc 0.96
2016-09-06T23:30:01.497621: step 2802, loss 0.0220546, acc 0.98
2016-09-06T23:30:02.197634: step 2803, loss 0.143922, acc 0.96
2016-09-06T23:30:02.888219: step 2804, loss 0.00233929, acc 1
2016-09-06T23:30:03.567143: step 2805, loss 0.0186711, acc 1
2016-09-06T23:30:04.257360: step 2806, loss 0.0730108, acc 0.96
2016-09-06T23:30:04.957803: step 2807, loss 0.0471573, acc 0.96
2016-09-06T23:30:05.627077: step 2808, loss 0.0275861, acc 1
2016-09-06T23:30:06.305645: step 2809, loss 0.0244838, acc 1
2016-09-06T23:30:07.010896: step 2810, loss 0.0307622, acc 0.98
2016-09-06T23:30:07.696090: step 2811, loss 0.10219, acc 0.98
2016-09-06T23:30:08.376526: step 2812, loss 0.0532152, acc 0.96
2016-09-06T23:30:09.054678: step 2813, loss 0.135285, acc 0.92
2016-09-06T23:30:09.740367: step 2814, loss 0.0318976, acc 1
2016-09-06T23:30:10.402114: step 2815, loss 0.0466336, acc 1
2016-09-06T23:30:11.076223: step 2816, loss 0.0252378, acc 0.98
2016-09-06T23:30:11.793708: step 2817, loss 0.0498044, acc 0.98
2016-09-06T23:30:12.502336: step 2818, loss 0.068109, acc 0.96
2016-09-06T23:30:13.181683: step 2819, loss 0.112025, acc 0.96
2016-09-06T23:30:13.879434: step 2820, loss 0.0187581, acc 1
2016-09-06T23:30:14.575139: step 2821, loss 0.105484, acc 0.96
2016-09-06T23:30:15.249924: step 2822, loss 0.00828108, acc 1
2016-09-06T23:30:15.930971: step 2823, loss 0.0691226, acc 0.98
2016-09-06T23:30:16.622330: step 2824, loss 0.0102937, acc 1
2016-09-06T23:30:17.299812: step 2825, loss 0.0435539, acc 0.98
2016-09-06T23:30:17.992222: step 2826, loss 0.0576055, acc 0.98
2016-09-06T23:30:18.682150: step 2827, loss 0.295937, acc 0.94
2016-09-06T23:30:19.372794: step 2828, loss 0.0678883, acc 0.98
2016-09-06T23:30:20.052485: step 2829, loss 0.0464908, acc 1
2016-09-06T23:30:20.739171: step 2830, loss 0.00771272, acc 1
2016-09-06T23:30:21.422473: step 2831, loss 0.015807, acc 1
2016-09-06T23:30:22.116830: step 2832, loss 0.0463614, acc 0.96
2016-09-06T23:30:22.817601: step 2833, loss 0.0634288, acc 0.98
2016-09-06T23:30:23.507286: step 2834, loss 0.030005, acc 0.98
2016-09-06T23:30:24.216602: step 2835, loss 0.0586367, acc 0.96
2016-09-06T23:30:24.911284: step 2836, loss 0.138828, acc 0.9
2016-09-06T23:30:25.619696: step 2837, loss 0.0228834, acc 1
2016-09-06T23:30:26.305778: step 2838, loss 0.0427212, acc 0.98
2016-09-06T23:30:26.989027: step 2839, loss 0.0236631, acc 1
2016-09-06T23:30:27.682191: step 2840, loss 0.0457626, acc 0.98
2016-09-06T23:30:28.391714: step 2841, loss 0.00434337, acc 1
2016-09-06T23:30:29.108122: step 2842, loss 0.0263186, acc 1
2016-09-06T23:30:29.796325: step 2843, loss 0.0641835, acc 0.98
2016-09-06T23:30:30.492510: step 2844, loss 0.0145049, acc 1
2016-09-06T23:30:31.192555: step 2845, loss 0.0426376, acc 0.96
2016-09-06T23:30:31.895010: step 2846, loss 0.0810175, acc 0.96
2016-09-06T23:30:32.597510: step 2847, loss 0.0453931, acc 0.98
2016-09-06T23:30:33.275246: step 2848, loss 0.0169155, acc 1
2016-09-06T23:30:33.945548: step 2849, loss 0.0243383, acc 0.98
2016-09-06T23:30:34.638910: step 2850, loss 0.0065981, acc 1
2016-09-06T23:30:35.327365: step 2851, loss 0.00419936, acc 1
2016-09-06T23:30:35.998561: step 2852, loss 0.0759608, acc 0.96
2016-09-06T23:30:36.684401: step 2853, loss 0.034527, acc 0.98
2016-09-06T23:30:37.378644: step 2854, loss 0.0882248, acc 0.98
2016-09-06T23:30:38.060784: step 2855, loss 0.102199, acc 0.98
2016-09-06T23:30:38.742023: step 2856, loss 0.0674765, acc 0.96
2016-09-06T23:30:39.466563: step 2857, loss 0.0311175, acc 0.98
2016-09-06T23:30:40.156459: step 2858, loss 0.034988, acc 1
2016-09-06T23:30:40.855511: step 2859, loss 0.0571527, acc 0.96
2016-09-06T23:30:41.543043: step 2860, loss 0.0296992, acc 0.98
2016-09-06T23:30:42.281292: step 2861, loss 0.0272701, acc 0.98
2016-09-06T23:30:42.960960: step 2862, loss 0.0517004, acc 0.98
2016-09-06T23:30:43.639131: step 2863, loss 0.0554685, acc 0.96
2016-09-06T23:30:44.330930: step 2864, loss 0.0321019, acc 0.98
2016-09-06T23:30:45.011063: step 2865, loss 0.0262713, acc 1
2016-09-06T23:30:45.704693: step 2866, loss 0.0105344, acc 1
2016-09-06T23:30:46.369639: step 2867, loss 0.061182, acc 0.98
2016-09-06T23:30:47.093917: step 2868, loss 0.111841, acc 0.96
2016-09-06T23:30:47.783652: step 2869, loss 0.0622887, acc 0.94
2016-09-06T23:30:48.481378: step 2870, loss 0.0688152, acc 0.98
2016-09-06T23:30:49.171624: step 2871, loss 0.024869, acc 0.98
2016-09-06T23:30:49.866560: step 2872, loss 0.0289094, acc 0.98
2016-09-06T23:30:50.543976: step 2873, loss 0.038919, acc 0.96
2016-09-06T23:30:51.208460: step 2874, loss 0.0185208, acc 0.98
2016-09-06T23:30:51.899679: step 2875, loss 0.203957, acc 0.92
2016-09-06T23:30:52.568765: step 2876, loss 0.0120127, acc 1
2016-09-06T23:30:53.253652: step 2877, loss 0.0199061, acc 1
2016-09-06T23:30:53.944103: step 2878, loss 0.0821573, acc 0.98
2016-09-06T23:30:54.633302: step 2879, loss 0.0277575, acc 0.98
2016-09-06T23:30:55.281338: step 2880, loss 0.0141509, acc 1
2016-09-06T23:30:55.978038: step 2881, loss 0.039172, acc 1
2016-09-06T23:30:56.661866: step 2882, loss 0.0258911, acc 0.98
2016-09-06T23:30:57.339642: step 2883, loss 0.00342615, acc 1
2016-09-06T23:30:58.025184: step 2884, loss 0.0287095, acc 0.98
2016-09-06T23:30:58.720511: step 2885, loss 0.0629904, acc 0.98
2016-09-06T23:30:59.403135: step 2886, loss 0.0378573, acc 0.98
2016-09-06T23:31:00.097569: step 2887, loss 0.00886455, acc 1
2016-09-06T23:31:00.818886: step 2888, loss 0.023512, acc 0.98
2016-09-06T23:31:01.511731: step 2889, loss 0.0314639, acc 0.98
2016-09-06T23:31:02.210849: step 2890, loss 0.0693803, acc 0.96
2016-09-06T23:31:02.896852: step 2891, loss 0.0076735, acc 1
2016-09-06T23:31:03.595230: step 2892, loss 0.0275805, acc 0.98
2016-09-06T23:31:04.273041: step 2893, loss 0.14734, acc 0.96
2016-09-06T23:31:04.960042: step 2894, loss 0.0348728, acc 0.98
2016-09-06T23:31:05.624002: step 2895, loss 0.00389125, acc 1
2016-09-06T23:31:06.329470: step 2896, loss 0.0201653, acc 0.98
2016-09-06T23:31:06.980364: step 2897, loss 0.0212196, acc 1
2016-09-06T23:31:07.649114: step 2898, loss 0.0382782, acc 1
2016-09-06T23:31:08.356247: step 2899, loss 0.0361262, acc 0.96
2016-09-06T23:31:09.047108: step 2900, loss 0.0260906, acc 1

Evaluation:
2016-09-06T23:31:12.198231: step 2900, loss 1.66172, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-2900

2016-09-06T23:31:14.001981: step 2901, loss 0.0290438, acc 0.98
2016-09-06T23:31:14.699533: step 2902, loss 0.0371028, acc 1
2016-09-06T23:31:15.397627: step 2903, loss 0.0543584, acc 0.98
2016-09-06T23:31:16.076037: step 2904, loss 0.0392162, acc 0.98
2016-09-06T23:31:16.762175: step 2905, loss 0.0137625, acc 1
2016-09-06T23:31:17.439582: step 2906, loss 0.0128956, acc 1
2016-09-06T23:31:18.120557: step 2907, loss 0.00410343, acc 1
2016-09-06T23:31:18.800337: step 2908, loss 0.0263864, acc 1
2016-09-06T23:31:19.521592: step 2909, loss 0.0990786, acc 0.94
2016-09-06T23:31:20.211771: step 2910, loss 0.0573341, acc 0.96
2016-09-06T23:31:20.904500: step 2911, loss 0.0220616, acc 0.98
2016-09-06T23:31:21.581335: step 2912, loss 0.0592166, acc 0.98
2016-09-06T23:31:22.273995: step 2913, loss 0.0203306, acc 0.98
2016-09-06T23:31:22.969944: step 2914, loss 0.0172915, acc 1
2016-09-06T23:31:23.643378: step 2915, loss 0.0021733, acc 1
2016-09-06T23:31:24.372215: step 2916, loss 0.0184733, acc 1
2016-09-06T23:31:25.047604: step 2917, loss 0.00339415, acc 1
2016-09-06T23:31:25.715948: step 2918, loss 0.0600572, acc 0.98
2016-09-06T23:31:26.398622: step 2919, loss 0.00245098, acc 1
2016-09-06T23:31:27.082556: step 2920, loss 0.00738961, acc 1
2016-09-06T23:31:27.757563: step 2921, loss 0.0127318, acc 1
2016-09-06T23:31:28.449950: step 2922, loss 0.0544194, acc 0.96
2016-09-06T23:31:29.200417: step 2923, loss 0.00264333, acc 1
2016-09-06T23:31:29.904418: step 2924, loss 0.00597975, acc 1
2016-09-06T23:31:30.572366: step 2925, loss 0.0616295, acc 0.96
2016-09-06T23:31:31.280445: step 2926, loss 0.059047, acc 0.98
2016-09-06T23:31:31.961477: step 2927, loss 0.00335098, acc 1
2016-09-06T23:31:32.661373: step 2928, loss 0.0367547, acc 0.98
2016-09-06T23:31:33.321544: step 2929, loss 0.00375662, acc 1
2016-09-06T23:31:34.026907: step 2930, loss 0.0685541, acc 0.98
2016-09-06T23:31:34.693337: step 2931, loss 0.0759806, acc 0.98
2016-09-06T23:31:35.383981: step 2932, loss 0.00365104, acc 1
2016-09-06T23:31:36.075841: step 2933, loss 0.00545792, acc 1
2016-09-06T23:31:36.763553: step 2934, loss 0.0127001, acc 1
2016-09-06T23:31:37.450808: step 2935, loss 0.0176219, acc 0.98
2016-09-06T23:31:38.107626: step 2936, loss 0.0291783, acc 0.98
2016-09-06T23:31:38.810749: step 2937, loss 0.0533075, acc 0.96
2016-09-06T23:31:39.489766: step 2938, loss 0.0026923, acc 1
2016-09-06T23:31:40.174881: step 2939, loss 0.00278159, acc 1
2016-09-06T23:31:40.860688: step 2940, loss 0.00854025, acc 1
2016-09-06T23:31:41.545435: step 2941, loss 0.0218102, acc 1
2016-09-06T23:31:42.243728: step 2942, loss 0.0478525, acc 0.96
2016-09-06T23:31:42.909774: step 2943, loss 0.113175, acc 0.96
2016-09-06T23:31:43.604001: step 2944, loss 0.0959528, acc 0.98
2016-09-06T23:31:44.293193: step 2945, loss 0.157206, acc 0.96
2016-09-06T23:31:45.010811: step 2946, loss 0.0559998, acc 0.96
2016-09-06T23:31:45.689705: step 2947, loss 0.0273451, acc 1
2016-09-06T23:31:46.393752: step 2948, loss 0.0684502, acc 0.96
2016-09-06T23:31:47.092906: step 2949, loss 0.0320729, acc 0.98
2016-09-06T23:31:47.762422: step 2950, loss 0.0393512, acc 0.98
2016-09-06T23:31:48.457493: step 2951, loss 0.0452421, acc 0.98
2016-09-06T23:31:49.144137: step 2952, loss 0.0822746, acc 0.96
2016-09-06T23:31:49.826246: step 2953, loss 0.0322753, acc 0.98
2016-09-06T23:31:50.511549: step 2954, loss 0.0234875, acc 0.98
2016-09-06T23:31:51.180268: step 2955, loss 0.0698847, acc 0.96
2016-09-06T23:31:51.873919: step 2956, loss 0.0531789, acc 0.98
2016-09-06T23:31:52.545894: step 2957, loss 0.0393142, acc 0.98
2016-09-06T23:31:53.267268: step 2958, loss 0.025019, acc 0.98
2016-09-06T23:31:53.962275: step 2959, loss 0.0428129, acc 0.98
2016-09-06T23:31:54.631294: step 2960, loss 0.0514984, acc 0.98
2016-09-06T23:31:55.312980: step 2961, loss 0.0200259, acc 1
2016-09-06T23:31:55.985057: step 2962, loss 0.017167, acc 0.98
2016-09-06T23:31:56.686910: step 2963, loss 0.0382836, acc 0.96
2016-09-06T23:31:57.346968: step 2964, loss 0.0593466, acc 0.96
2016-09-06T23:31:58.049235: step 2965, loss 0.0595856, acc 0.98
2016-09-06T23:31:58.744888: step 2966, loss 0.00632674, acc 1
2016-09-06T23:31:59.445394: step 2967, loss 0.0849176, acc 0.96
2016-09-06T23:32:00.123094: step 2968, loss 0.0201881, acc 1
2016-09-06T23:32:00.855968: step 2969, loss 0.0382184, acc 0.98
2016-09-06T23:32:01.542071: step 2970, loss 0.0186895, acc 1
2016-09-06T23:32:02.202712: step 2971, loss 0.0575898, acc 0.98
2016-09-06T23:32:02.919327: step 2972, loss 0.0857309, acc 0.94
2016-09-06T23:32:03.605139: step 2973, loss 0.0341051, acc 0.98
2016-09-06T23:32:04.285310: step 2974, loss 0.126717, acc 0.98
2016-09-06T23:32:04.981889: step 2975, loss 0.0285869, acc 0.98
2016-09-06T23:32:05.658749: step 2976, loss 0.0450159, acc 0.98
2016-09-06T23:32:06.347893: step 2977, loss 0.0106186, acc 1
2016-09-06T23:32:07.012469: step 2978, loss 0.029927, acc 1
2016-09-06T23:32:07.697846: step 2979, loss 0.132089, acc 0.92
2016-09-06T23:32:08.383477: step 2980, loss 0.0346612, acc 0.98
2016-09-06T23:32:09.065087: step 2981, loss 0.0526698, acc 0.98
2016-09-06T23:32:09.763136: step 2982, loss 0.0113033, acc 1
2016-09-06T23:32:10.446203: step 2983, loss 0.0875866, acc 0.96
2016-09-06T23:32:11.116848: step 2984, loss 0.0246526, acc 0.98
2016-09-06T23:32:11.778330: step 2985, loss 0.123278, acc 0.96
2016-09-06T23:32:12.458559: step 2986, loss 0.226612, acc 0.94
2016-09-06T23:32:13.127040: step 2987, loss 0.00304228, acc 1
2016-09-06T23:32:13.824204: step 2988, loss 0.0644114, acc 0.94
2016-09-06T23:32:14.524659: step 2989, loss 0.0167275, acc 1
2016-09-06T23:32:15.212343: step 2990, loss 0.0227691, acc 0.98
2016-09-06T23:32:15.910879: step 2991, loss 0.0549177, acc 0.98
2016-09-06T23:32:16.601273: step 2992, loss 0.0368265, acc 0.98
2016-09-06T23:32:17.328913: step 2993, loss 0.0259935, acc 0.98
2016-09-06T23:32:18.043190: step 2994, loss 0.0207324, acc 0.98
2016-09-06T23:32:18.729414: step 2995, loss 0.0454411, acc 0.98
2016-09-06T23:32:19.405029: step 2996, loss 0.0539016, acc 0.96
2016-09-06T23:32:20.106898: step 2997, loss 0.0108512, acc 1
2016-09-06T23:32:20.818983: step 2998, loss 0.0295662, acc 1
2016-09-06T23:32:21.485513: step 2999, loss 0.0362413, acc 0.98
2016-09-06T23:32:22.186754: step 3000, loss 0.136079, acc 0.96

Evaluation:
2016-09-06T23:32:25.345749: step 3000, loss 1.48446, acc 0.743902

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-3000

2016-09-06T23:32:27.009686: step 3001, loss 0.00877563, acc 1
2016-09-06T23:32:27.703261: step 3002, loss 0.0203493, acc 0.98
2016-09-06T23:32:28.389335: step 3003, loss 0.0288358, acc 0.98
2016-09-06T23:32:29.060541: step 3004, loss 0.0320986, acc 0.98
2016-09-06T23:32:29.736578: step 3005, loss 0.0137614, acc 1
2016-09-06T23:32:30.427122: step 3006, loss 0.0237199, acc 0.98
2016-09-06T23:32:31.099586: step 3007, loss 0.0715739, acc 0.96
2016-09-06T23:32:31.801999: step 3008, loss 0.0217214, acc 1
2016-09-06T23:32:32.474764: step 3009, loss 0.00591556, acc 1
2016-09-06T23:32:33.159518: step 3010, loss 0.0366759, acc 0.98
2016-09-06T23:32:33.843765: step 3011, loss 0.0953638, acc 0.92
2016-09-06T23:32:34.514017: step 3012, loss 0.00681791, acc 1
2016-09-06T23:32:35.218952: step 3013, loss 0.0092077, acc 1
2016-09-06T23:32:35.885639: step 3014, loss 0.0447266, acc 0.98
2016-09-06T23:32:36.592260: step 3015, loss 0.000238569, acc 1
2016-09-06T23:32:37.277145: step 3016, loss 0.0210482, acc 0.98
2016-09-06T23:32:37.977461: step 3017, loss 0.0501299, acc 0.98
2016-09-06T23:32:38.630618: step 3018, loss 0.00820223, acc 1
2016-09-06T23:32:39.307293: step 3019, loss 0.0560025, acc 0.98
2016-09-06T23:32:40.007968: step 3020, loss 0.0323829, acc 0.98
2016-09-06T23:32:40.677541: step 3021, loss 0.0213441, acc 1
2016-09-06T23:32:41.385891: step 3022, loss 0.012284, acc 1
2016-09-06T23:32:42.061069: step 3023, loss 0.0226615, acc 1
2016-09-06T23:32:42.745792: step 3024, loss 0.0260013, acc 1
2016-09-06T23:32:43.421446: step 3025, loss 0.0620577, acc 0.98
2016-09-06T23:32:44.120938: step 3026, loss 0.0535973, acc 0.98
2016-09-06T23:32:44.812795: step 3027, loss 0.0110223, acc 1
2016-09-06T23:32:45.458305: step 3028, loss 0.0907799, acc 0.94
2016-09-06T23:32:46.156727: step 3029, loss 0.0283063, acc 0.98
2016-09-06T23:32:46.849836: step 3030, loss 0.00222767, acc 1
2016-09-06T23:32:47.549136: step 3031, loss 0.0461978, acc 0.98
2016-09-06T23:32:48.231790: step 3032, loss 0.000721255, acc 1
2016-09-06T23:32:48.932817: step 3033, loss 0.100353, acc 0.96
2016-09-06T23:32:49.642290: step 3034, loss 0.0286067, acc 0.98
2016-09-06T23:32:50.311977: step 3035, loss 0.104855, acc 0.98
2016-09-06T23:32:51.005846: step 3036, loss 0.00144988, acc 1
2016-09-06T23:32:51.687846: step 3037, loss 0.037706, acc 0.98
2016-09-06T23:32:52.364331: step 3038, loss 0.0302931, acc 0.98
2016-09-06T23:32:53.044387: step 3039, loss 0.0596049, acc 0.96
2016-09-06T23:32:53.731463: step 3040, loss 0.0732413, acc 0.96
2016-09-06T23:32:54.417681: step 3041, loss 0.00287398, acc 1
2016-09-06T23:32:55.086349: step 3042, loss 0.0131772, acc 1
2016-09-06T23:32:55.781126: step 3043, loss 0.0410193, acc 0.98
2016-09-06T23:32:56.457591: step 3044, loss 0.131179, acc 0.96
2016-09-06T23:32:57.159035: step 3045, loss 0.0773269, acc 0.96
2016-09-06T23:32:57.847036: step 3046, loss 0.0239801, acc 0.98
2016-09-06T23:32:58.529298: step 3047, loss 0.0326798, acc 0.98
2016-09-06T23:32:59.219504: step 3048, loss 0.0992585, acc 0.94
2016-09-06T23:32:59.883050: step 3049, loss 0.0760333, acc 0.96
2016-09-06T23:33:00.651891: step 3050, loss 0.0847593, acc 0.94
2016-09-06T23:33:01.342830: step 3051, loss 0.0287872, acc 0.98
2016-09-06T23:33:02.031964: step 3052, loss 0.0113405, acc 1
2016-09-06T23:33:02.717916: step 3053, loss 0.0646883, acc 0.96
2016-09-06T23:33:03.409609: step 3054, loss 0.0285967, acc 1
2016-09-06T23:33:04.104002: step 3055, loss 0.00594022, acc 1
2016-09-06T23:33:04.770198: step 3056, loss 0.0248069, acc 1
2016-09-06T23:33:05.475429: step 3057, loss 0.0553721, acc 0.98
2016-09-06T23:33:06.177629: step 3058, loss 0.0715863, acc 0.94
2016-09-06T23:33:06.855420: step 3059, loss 0.010025, acc 1
2016-09-06T23:33:07.551270: step 3060, loss 0.00975369, acc 1
2016-09-06T23:33:08.248587: step 3061, loss 0.0307812, acc 0.98
2016-09-06T23:33:08.948080: step 3062, loss 0.00412765, acc 1
2016-09-06T23:33:09.610106: step 3063, loss 0.0188277, acc 1
2016-09-06T23:33:10.279482: step 3064, loss 0.0672539, acc 0.98
2016-09-06T23:33:10.967198: step 3065, loss 0.0803882, acc 0.98
2016-09-06T23:33:11.653202: step 3066, loss 0.0111419, acc 1
2016-09-06T23:33:12.340476: step 3067, loss 0.0262842, acc 1
2016-09-06T23:33:13.033682: step 3068, loss 0.00852336, acc 1
2016-09-06T23:33:13.726754: step 3069, loss 0.0657713, acc 0.96
2016-09-06T23:33:14.394370: step 3070, loss 0.0319773, acc 0.98
2016-09-06T23:33:15.085079: step 3071, loss 0.030381, acc 1
2016-09-06T23:33:15.710761: step 3072, loss 0.000804791, acc 1
2016-09-06T23:33:16.399446: step 3073, loss 0.0473676, acc 0.96
2016-09-06T23:33:17.083820: step 3074, loss 0.0221103, acc 1
2016-09-06T23:33:17.762327: step 3075, loss 0.0496062, acc 0.96
2016-09-06T23:33:18.465156: step 3076, loss 0.00755057, acc 1
2016-09-06T23:33:19.162079: step 3077, loss 0.0804564, acc 0.98
2016-09-06T23:33:19.873736: step 3078, loss 0.0545781, acc 0.98
2016-09-06T23:33:20.570653: step 3079, loss 0.0154224, acc 0.98
2016-09-06T23:33:21.234365: step 3080, loss 0.028819, acc 0.98
2016-09-06T23:33:21.931196: step 3081, loss 0.0144356, acc 1
2016-09-06T23:33:22.613088: step 3082, loss 0.0347986, acc 0.98
2016-09-06T23:33:23.297415: step 3083, loss 0.0101713, acc 1
2016-09-06T23:33:23.969670: step 3084, loss 0.0302994, acc 1
2016-09-06T23:33:24.684608: step 3085, loss 0.0847529, acc 0.96
2016-09-06T23:33:25.374991: step 3086, loss 0.0381847, acc 0.98
2016-09-06T23:33:26.040422: step 3087, loss 0.00202899, acc 1
2016-09-06T23:33:26.721349: step 3088, loss 0.00166829, acc 1
2016-09-06T23:33:27.394703: step 3089, loss 0.0434793, acc 0.98
2016-09-06T23:33:28.075959: step 3090, loss 0.0802847, acc 0.94
2016-09-06T23:33:28.722212: step 3091, loss 0.110042, acc 0.94
2016-09-06T23:33:29.424286: step 3092, loss 0.105447, acc 0.98
2016-09-06T23:33:30.087075: step 3093, loss 0.00382909, acc 1
2016-09-06T23:33:30.792420: step 3094, loss 0.0676821, acc 0.96
2016-09-06T23:33:31.479569: step 3095, loss 0.0187301, acc 0.98
2016-09-06T23:33:32.176118: step 3096, loss 0.0532339, acc 0.98
2016-09-06T23:33:32.848383: step 3097, loss 0.0316326, acc 0.98
2016-09-06T23:33:33.528492: step 3098, loss 0.0138707, acc 1
2016-09-06T23:33:34.242191: step 3099, loss 0.00394436, acc 1
2016-09-06T23:33:34.938628: step 3100, loss 0.0462909, acc 0.98

Evaluation:
2016-09-06T23:33:38.099313: step 3100, loss 1.59689, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-3100

2016-09-06T23:33:39.731736: step 3101, loss 0.0149543, acc 1
2016-09-06T23:33:40.404272: step 3102, loss 0.0193263, acc 0.98
2016-09-06T23:33:41.111486: step 3103, loss 0.0467706, acc 0.96
2016-09-06T23:33:41.800916: step 3104, loss 0.0389413, acc 1
2016-09-06T23:33:42.500886: step 3105, loss 0.00376571, acc 1
2016-09-06T23:33:43.140619: step 3106, loss 0.0342225, acc 0.98
2016-09-06T23:33:43.830254: step 3107, loss 0.0420248, acc 0.96
2016-09-06T23:33:44.509097: step 3108, loss 0.162324, acc 0.94
2016-09-06T23:33:45.187600: step 3109, loss 0.0221642, acc 1
2016-09-06T23:33:45.867186: step 3110, loss 0.0404036, acc 0.98
2016-09-06T23:33:46.553729: step 3111, loss 0.0117444, acc 1
2016-09-06T23:33:47.242157: step 3112, loss 0.0458933, acc 0.98
2016-09-06T23:33:47.938254: step 3113, loss 0.0414406, acc 0.98
2016-09-06T23:33:48.640133: step 3114, loss 0.0270428, acc 0.98
2016-09-06T23:33:49.321530: step 3115, loss 0.000536121, acc 1
2016-09-06T23:33:49.999702: step 3116, loss 0.0130759, acc 1
2016-09-06T23:33:50.675096: step 3117, loss 0.0270364, acc 0.98
2016-09-06T23:33:51.364851: step 3118, loss 0.0277494, acc 1
2016-09-06T23:33:52.040334: step 3119, loss 0.0211076, acc 1
2016-09-06T23:33:52.729714: step 3120, loss 0.0224143, acc 1
2016-09-06T23:33:53.415586: step 3121, loss 0.0141898, acc 1
2016-09-06T23:33:54.075462: step 3122, loss 0.00194348, acc 1
2016-09-06T23:33:54.753496: step 3123, loss 0.0373219, acc 1
2016-09-06T23:33:55.447733: step 3124, loss 0.00430882, acc 1
2016-09-06T23:33:56.130443: step 3125, loss 0.0452991, acc 0.98
2016-09-06T23:33:56.804187: step 3126, loss 0.00926753, acc 1
2016-09-06T23:33:57.480322: step 3127, loss 0.0156591, acc 1
2016-09-06T23:33:58.175654: step 3128, loss 0.0117276, acc 1
2016-09-06T23:33:58.829133: step 3129, loss 0.045413, acc 0.98
2016-09-06T23:33:59.548762: step 3130, loss 0.00535996, acc 1
2016-09-06T23:34:00.231249: step 3131, loss 0.00680481, acc 1
2016-09-06T23:34:00.903385: step 3132, loss 0.00301702, acc 1
2016-09-06T23:34:01.601566: step 3133, loss 0.0753595, acc 0.98
2016-09-06T23:34:02.300877: step 3134, loss 0.0098639, acc 1
2016-09-06T23:34:02.994976: step 3135, loss 0.0343014, acc 0.98
2016-09-06T23:34:03.657178: step 3136, loss 0.0416728, acc 0.98
2016-09-06T23:34:04.348218: step 3137, loss 0.0897811, acc 0.94
2016-09-06T23:34:05.020538: step 3138, loss 0.0778586, acc 0.96
2016-09-06T23:34:05.709576: step 3139, loss 0.0514316, acc 0.98
2016-09-06T23:34:06.404436: step 3140, loss 0.0434864, acc 0.96
2016-09-06T23:34:07.094206: step 3141, loss 0.0411246, acc 0.98
2016-09-06T23:34:07.778117: step 3142, loss 0.0489944, acc 0.98
2016-09-06T23:34:08.436509: step 3143, loss 0.0849283, acc 0.98
2016-09-06T23:34:09.165310: step 3144, loss 0.0231955, acc 1
2016-09-06T23:34:09.861902: step 3145, loss 0.000208427, acc 1
2016-09-06T23:34:10.539511: step 3146, loss 0.0465745, acc 0.96
2016-09-06T23:34:11.252419: step 3147, loss 0.0175136, acc 0.98
2016-09-06T23:34:11.963539: step 3148, loss 0.0810403, acc 0.96
2016-09-06T23:34:12.664958: step 3149, loss 0.0406954, acc 0.98
2016-09-06T23:34:13.335527: step 3150, loss 0.0130444, acc 1
2016-09-06T23:34:14.064931: step 3151, loss 0.05462, acc 0.98
2016-09-06T23:34:14.757715: step 3152, loss 0.0591356, acc 1
2016-09-06T23:34:15.457892: step 3153, loss 0.0023113, acc 1
2016-09-06T23:34:16.147897: step 3154, loss 0.00639948, acc 1
2016-09-06T23:34:16.840421: step 3155, loss 0.045616, acc 0.96
2016-09-06T23:34:17.563779: step 3156, loss 0.0348287, acc 0.98
2016-09-06T23:34:18.256945: step 3157, loss 0.0394512, acc 0.98
2016-09-06T23:34:18.937072: step 3158, loss 0.0335263, acc 0.98
2016-09-06T23:34:19.618362: step 3159, loss 0.0608639, acc 0.96
2016-09-06T23:34:20.302892: step 3160, loss 0.0456736, acc 0.98
2016-09-06T23:34:21.007297: step 3161, loss 0.033509, acc 1
2016-09-06T23:34:21.663177: step 3162, loss 0.0231778, acc 1
2016-09-06T23:34:22.368347: step 3163, loss 0.0336031, acc 0.98
2016-09-06T23:34:23.034436: step 3164, loss 0.0969035, acc 0.94
2016-09-06T23:34:23.697770: step 3165, loss 0.00837304, acc 1
2016-09-06T23:34:24.367478: step 3166, loss 0.0469844, acc 0.98
2016-09-06T23:34:25.057488: step 3167, loss 0.0409417, acc 0.98
2016-09-06T23:34:25.735033: step 3168, loss 0.00234797, acc 1
2016-09-06T23:34:26.407610: step 3169, loss 0.0699219, acc 0.98
2016-09-06T23:34:27.102371: step 3170, loss 0.0353663, acc 0.98
2016-09-06T23:34:27.784168: step 3171, loss 0.0977584, acc 0.94
2016-09-06T23:34:28.484029: step 3172, loss 0.0163064, acc 1
2016-09-06T23:34:29.185524: step 3173, loss 0.0236736, acc 1
2016-09-06T23:34:29.850266: step 3174, loss 0.00133748, acc 1
2016-09-06T23:34:30.534010: step 3175, loss 0.0400562, acc 0.96
2016-09-06T23:34:31.217634: step 3176, loss 0.0841974, acc 0.94
2016-09-06T23:34:31.925807: step 3177, loss 0.0331573, acc 1
2016-09-06T23:34:32.642092: step 3178, loss 0.0500986, acc 0.98
2016-09-06T23:34:33.334344: step 3179, loss 0.0355773, acc 0.98
2016-09-06T23:34:34.019479: step 3180, loss 0.0348942, acc 0.98
2016-09-06T23:34:34.708855: step 3181, loss 0.0308201, acc 0.98
2016-09-06T23:34:35.379808: step 3182, loss 0.0182378, acc 1
2016-09-06T23:34:36.058682: step 3183, loss 0.00149293, acc 1
2016-09-06T23:34:36.785644: step 3184, loss 0.0207182, acc 0.98
2016-09-06T23:34:37.463814: step 3185, loss 0.0162659, acc 1
2016-09-06T23:34:38.156924: step 3186, loss 0.00806679, acc 1
2016-09-06T23:34:38.844316: step 3187, loss 0.0902595, acc 0.94
2016-09-06T23:34:39.533509: step 3188, loss 0.0236859, acc 1
2016-09-06T23:34:40.234531: step 3189, loss 0.00642679, acc 1
2016-09-06T23:34:40.918221: step 3190, loss 0.000345922, acc 1
2016-09-06T23:34:41.616458: step 3191, loss 0.043535, acc 0.98
2016-09-06T23:34:42.277555: step 3192, loss 0.00925392, acc 1
2016-09-06T23:34:42.957140: step 3193, loss 0.0210012, acc 1
2016-09-06T23:34:43.639871: step 3194, loss 0.168284, acc 0.94
2016-09-06T23:34:44.324031: step 3195, loss 0.0788472, acc 0.96
2016-09-06T23:34:45.025128: step 3196, loss 0.0468494, acc 0.96
2016-09-06T23:34:45.692966: step 3197, loss 0.0264259, acc 0.98
2016-09-06T23:34:46.403000: step 3198, loss 0.0320787, acc 0.98
2016-09-06T23:34:47.085851: step 3199, loss 0.0854468, acc 0.96
2016-09-06T23:34:47.766018: step 3200, loss 0.097171, acc 0.96

Evaluation:
2016-09-06T23:34:50.910520: step 3200, loss 1.91456, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-3200

2016-09-06T23:34:52.576626: step 3201, loss 0.0557062, acc 0.98
2016-09-06T23:34:53.250952: step 3202, loss 0.00748618, acc 1
2016-09-06T23:34:53.946389: step 3203, loss 0.0622341, acc 0.96
2016-09-06T23:34:54.620161: step 3204, loss 0.00157556, acc 1
2016-09-06T23:34:55.299940: step 3205, loss 0.0515078, acc 0.98
2016-09-06T23:34:55.995262: step 3206, loss 0.00852945, acc 1
2016-09-06T23:34:56.665015: step 3207, loss 0.0294949, acc 0.98
2016-09-06T23:34:57.336267: step 3208, loss 0.00448321, acc 1
2016-09-06T23:34:58.053310: step 3209, loss 0.0648109, acc 0.98
2016-09-06T23:34:58.756641: step 3210, loss 0.013372, acc 1
2016-09-06T23:34:59.434380: step 3211, loss 0.063783, acc 0.98
2016-09-06T23:35:00.138459: step 3212, loss 0.0176819, acc 1
2016-09-06T23:35:00.885304: step 3213, loss 0.0242834, acc 1
2016-09-06T23:35:01.579636: step 3214, loss 0.033916, acc 1
2016-09-06T23:35:02.272674: step 3215, loss 0.0226439, acc 1
2016-09-06T23:35:02.965191: step 3216, loss 0.0389409, acc 0.98
2016-09-06T23:35:03.663521: step 3217, loss 0.0238092, acc 1
2016-09-06T23:35:04.371722: step 3218, loss 0.0694279, acc 0.96
2016-09-06T23:35:05.036745: step 3219, loss 0.012128, acc 1
2016-09-06T23:35:05.738635: step 3220, loss 0.13321, acc 0.92
2016-09-06T23:35:06.430992: step 3221, loss 0.0214456, acc 1
2016-09-06T23:35:07.121532: step 3222, loss 0.0104442, acc 1
2016-09-06T23:35:07.807129: step 3223, loss 0.00383302, acc 1
2016-09-06T23:35:08.500616: step 3224, loss 0.0366756, acc 0.98
2016-09-06T23:35:09.209087: step 3225, loss 0.0176849, acc 0.98
2016-09-06T23:35:09.888751: step 3226, loss 0.0668656, acc 0.96
2016-09-06T23:35:10.584133: step 3227, loss 0.083563, acc 0.96
2016-09-06T23:35:11.266457: step 3228, loss 0.00639741, acc 1
2016-09-06T23:35:11.966201: step 3229, loss 0.00355324, acc 1
2016-09-06T23:35:12.652715: step 3230, loss 0.0760894, acc 0.98
2016-09-06T23:35:13.331879: step 3231, loss 0.00048049, acc 1
2016-09-06T23:35:14.029257: step 3232, loss 0.0257392, acc 1
2016-09-06T23:35:14.689188: step 3233, loss 0.0110134, acc 1
2016-09-06T23:35:15.397382: step 3234, loss 0.0272494, acc 1
2016-09-06T23:35:16.067386: step 3235, loss 0.0281722, acc 0.98
2016-09-06T23:35:16.744060: step 3236, loss 0.0062328, acc 1
2016-09-06T23:35:17.426467: step 3237, loss 0.0254146, acc 0.98
2016-09-06T23:35:18.109081: step 3238, loss 0.0266665, acc 1
2016-09-06T23:35:18.819057: step 3239, loss 0.0224732, acc 0.98
2016-09-06T23:35:19.481547: step 3240, loss 0.0565917, acc 0.98
2016-09-06T23:35:20.208105: step 3241, loss 0.0278791, acc 0.98
2016-09-06T23:35:20.905220: step 3242, loss 0.0559635, acc 0.94
2016-09-06T23:35:21.596329: step 3243, loss 0.0337771, acc 0.96
2016-09-06T23:35:22.292043: step 3244, loss 0.112383, acc 0.94
2016-09-06T23:35:22.987579: step 3245, loss 0.0285741, acc 0.98
2016-09-06T23:35:23.699312: step 3246, loss 0.196664, acc 0.9
2016-09-06T23:35:24.378038: step 3247, loss 0.010759, acc 1
2016-09-06T23:35:25.051411: step 3248, loss 0.0724869, acc 0.96
2016-09-06T23:35:25.754739: step 3249, loss 0.0881946, acc 0.94
2016-09-06T23:35:26.442278: step 3250, loss 0.169528, acc 0.96
2016-09-06T23:35:27.128085: step 3251, loss 0.0359183, acc 0.98
2016-09-06T23:35:27.815709: step 3252, loss 0.0210831, acc 1
2016-09-06T23:35:28.545621: step 3253, loss 0.0173327, acc 0.98
2016-09-06T23:35:29.226159: step 3254, loss 0.0439759, acc 0.98
2016-09-06T23:35:29.916365: step 3255, loss 0.057191, acc 0.96
2016-09-06T23:35:30.593848: step 3256, loss 0.0514059, acc 0.96
2016-09-06T23:35:31.286470: step 3257, loss 0.0394314, acc 1
2016-09-06T23:35:31.965985: step 3258, loss 0.0741122, acc 0.94
2016-09-06T23:35:32.638758: step 3259, loss 0.0825075, acc 0.98
2016-09-06T23:35:33.363609: step 3260, loss 0.000572204, acc 1
2016-09-06T23:35:34.061661: step 3261, loss 0.0519589, acc 0.96
2016-09-06T23:35:34.735976: step 3262, loss 0.0558495, acc 0.96
2016-09-06T23:35:35.424977: step 3263, loss 0.0330049, acc 0.98
2016-09-06T23:35:36.045795: step 3264, loss 0.0698849, acc 0.977273
2016-09-06T23:35:36.720492: step 3265, loss 0.0145672, acc 1
2016-09-06T23:35:37.412806: step 3266, loss 0.108299, acc 0.94
2016-09-06T23:35:38.116309: step 3267, loss 0.0931581, acc 0.96
2016-09-06T23:35:38.793036: step 3268, loss 0.0965957, acc 0.96
2016-09-06T23:35:39.478164: step 3269, loss 0.0162978, acc 1
2016-09-06T23:35:40.143727: step 3270, loss 0.0313822, acc 1
2016-09-06T23:35:40.827256: step 3271, loss 0.00571149, acc 1
2016-09-06T23:35:41.502411: step 3272, loss 0.0331054, acc 0.98
2016-09-06T23:35:42.200057: step 3273, loss 0.00949701, acc 1
2016-09-06T23:35:42.913138: step 3274, loss 0.0227592, acc 1
2016-09-06T23:35:43.580882: step 3275, loss 0.0445934, acc 0.98
2016-09-06T23:35:44.284611: step 3276, loss 0.00113667, acc 1
2016-09-06T23:35:44.962069: step 3277, loss 0.0728066, acc 0.94
2016-09-06T23:35:45.642458: step 3278, loss 0.0382608, acc 0.96
2016-09-06T23:35:46.335608: step 3279, loss 0.0202857, acc 1
2016-09-06T23:35:47.019469: step 3280, loss 0.00582395, acc 1
2016-09-06T23:35:47.713579: step 3281, loss 0.0152113, acc 0.98
2016-09-06T23:35:48.396770: step 3282, loss 0.0159011, acc 1
2016-09-06T23:35:49.099329: step 3283, loss 0.0187204, acc 1
2016-09-06T23:35:49.788900: step 3284, loss 0.0184142, acc 1
2016-09-06T23:35:50.497452: step 3285, loss 0.0430975, acc 0.98
2016-09-06T23:35:51.188920: step 3286, loss 0.0724416, acc 0.94
2016-09-06T23:35:51.877963: step 3287, loss 0.0059708, acc 1
2016-09-06T23:35:52.583486: step 3288, loss 0.0004071, acc 1
2016-09-06T23:35:53.253128: step 3289, loss 0.0322452, acc 0.96
2016-09-06T23:35:53.944118: step 3290, loss 0.01693, acc 1
2016-09-06T23:35:54.632268: step 3291, loss 0.000260095, acc 1
2016-09-06T23:35:55.313581: step 3292, loss 0.0273076, acc 0.98
2016-09-06T23:35:55.985753: step 3293, loss 0.0464947, acc 0.96
2016-09-06T23:35:56.666540: step 3294, loss 0.224004, acc 0.96
2016-09-06T23:35:57.354103: step 3295, loss 0.00480173, acc 1
2016-09-06T23:35:58.021449: step 3296, loss 0.0476347, acc 0.98
2016-09-06T23:35:58.728171: step 3297, loss 0.0244725, acc 0.98
2016-09-06T23:35:59.430868: step 3298, loss 0.0408576, acc 0.98
2016-09-06T23:36:00.115717: step 3299, loss 0.0188508, acc 1
2016-09-06T23:36:00.818197: step 3300, loss 0.0464555, acc 0.98

Evaluation:
2016-09-06T23:36:03.945924: step 3300, loss 1.85746, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-3300

2016-09-06T23:36:05.788331: step 3301, loss 0.0110016, acc 1
2016-09-06T23:36:06.468974: step 3302, loss 0.0557534, acc 0.96
2016-09-06T23:36:07.170244: step 3303, loss 0.0760135, acc 0.94
2016-09-06T23:36:07.850360: step 3304, loss 0.00510347, acc 1
2016-09-06T23:36:08.559747: step 3305, loss 0.0367247, acc 1
2016-09-06T23:36:09.259419: step 3306, loss 0.0367652, acc 0.98
2016-09-06T23:36:09.920840: step 3307, loss 0.075677, acc 0.94
2016-09-06T23:36:10.644394: step 3308, loss 0.0566471, acc 0.98
2016-09-06T23:36:11.310389: step 3309, loss 0.0616913, acc 0.96
2016-09-06T23:36:11.971191: step 3310, loss 0.0590403, acc 0.96
2016-09-06T23:36:12.662389: step 3311, loss 0.013668, acc 1
2016-09-06T23:36:13.342617: step 3312, loss 0.0336428, acc 0.98
2016-09-06T23:36:14.053434: step 3313, loss 0.0116354, acc 1
2016-09-06T23:36:14.733126: step 3314, loss 0.0568535, acc 0.96
2016-09-06T23:36:15.439618: step 3315, loss 0.00352139, acc 1
2016-09-06T23:36:16.107908: step 3316, loss 0.0781701, acc 0.96
2016-09-06T23:36:16.779493: step 3317, loss 0.0177589, acc 0.98
2016-09-06T23:36:17.475744: step 3318, loss 0.0853751, acc 0.96
2016-09-06T23:36:18.156958: step 3319, loss 0.035552, acc 1
2016-09-06T23:36:18.838648: step 3320, loss 0.0480778, acc 0.98
2016-09-06T23:36:19.522130: step 3321, loss 0.116437, acc 0.98
2016-09-06T23:36:20.240331: step 3322, loss 0.00693926, acc 1
2016-09-06T23:36:20.921054: step 3323, loss 0.0251823, acc 0.98
2016-09-06T23:36:21.596864: step 3324, loss 0.0435886, acc 0.98
2016-09-06T23:36:22.277879: step 3325, loss 0.0478502, acc 0.96
2016-09-06T23:36:22.962330: step 3326, loss 0.00947629, acc 1
2016-09-06T23:36:23.637695: step 3327, loss 0.018674, acc 0.98
2016-09-06T23:36:24.337864: step 3328, loss 0.0257829, acc 0.98
2016-09-06T23:36:25.051972: step 3329, loss 0.0352645, acc 0.98
2016-09-06T23:36:25.747280: step 3330, loss 0.00301844, acc 1
2016-09-06T23:36:26.446777: step 3331, loss 0.0301373, acc 0.98
2016-09-06T23:36:27.123914: step 3332, loss 0.027699, acc 0.98
2016-09-06T23:36:27.827995: step 3333, loss 0.018425, acc 1
2016-09-06T23:36:28.493445: step 3334, loss 0.0663261, acc 0.96
2016-09-06T23:36:29.160865: step 3335, loss 0.0307239, acc 0.98
2016-09-06T23:36:29.851647: step 3336, loss 0.0338521, acc 0.98
2016-09-06T23:36:30.519224: step 3337, loss 0.0306226, acc 0.98
2016-09-06T23:36:31.204079: step 3338, loss 0.0717088, acc 0.98
2016-09-06T23:36:31.914525: step 3339, loss 0.0274909, acc 1
2016-09-06T23:36:32.590648: step 3340, loss 0.0285241, acc 1
2016-09-06T23:36:33.292345: step 3341, loss 0.0366324, acc 1
2016-09-06T23:36:33.978819: step 3342, loss 0.0161478, acc 1
2016-09-06T23:36:34.681971: step 3343, loss 0.0810946, acc 0.98
2016-09-06T23:36:35.345269: step 3344, loss 0.0614794, acc 0.96
2016-09-06T23:36:36.027503: step 3345, loss 0.0277662, acc 1
2016-09-06T23:36:36.711326: step 3346, loss 0.0363156, acc 0.98
2016-09-06T23:36:37.395669: step 3347, loss 0.0566577, acc 0.98
2016-09-06T23:36:38.097408: step 3348, loss 0.000996585, acc 1
2016-09-06T23:36:38.770581: step 3349, loss 0.0128695, acc 1
2016-09-06T23:36:39.463670: step 3350, loss 0.0875515, acc 0.98
2016-09-06T23:36:40.131074: step 3351, loss 0.0267872, acc 1
2016-09-06T23:36:40.819350: step 3352, loss 0.014301, acc 1
2016-09-06T23:36:41.485446: step 3353, loss 0.031522, acc 0.98
2016-09-06T23:36:42.167975: step 3354, loss 0.0551855, acc 0.96
2016-09-06T23:36:42.846430: step 3355, loss 0.0190396, acc 0.98
2016-09-06T23:36:43.529192: step 3356, loss 0.0240051, acc 0.98
2016-09-06T23:36:44.213246: step 3357, loss 0.0169665, acc 1
2016-09-06T23:36:44.887755: step 3358, loss 0.0150972, acc 1
2016-09-06T23:36:45.595981: step 3359, loss 0.0355418, acc 0.98
2016-09-06T23:36:46.293034: step 3360, loss 0.0281802, acc 0.98
2016-09-06T23:36:46.980479: step 3361, loss 0.034431, acc 0.98
2016-09-06T23:36:47.691420: step 3362, loss 0.0831238, acc 0.98
2016-09-06T23:36:48.368778: step 3363, loss 0.0206188, acc 1
2016-09-06T23:36:49.070764: step 3364, loss 0.0131737, acc 1
2016-09-06T23:36:49.733376: step 3365, loss 0.021088, acc 1
2016-09-06T23:36:50.406069: step 3366, loss 0.00184048, acc 1
2016-09-06T23:36:51.105076: step 3367, loss 0.0484563, acc 0.98
2016-09-06T23:36:51.825093: step 3368, loss 0.000491878, acc 1
2016-09-06T23:36:52.503475: step 3369, loss 0.0441168, acc 0.98
2016-09-06T23:36:53.173928: step 3370, loss 0.0330378, acc 0.98
2016-09-06T23:36:53.887766: step 3371, loss 0.0218818, acc 1
2016-09-06T23:36:54.552552: step 3372, loss 0.0409182, acc 0.96
2016-09-06T23:36:55.219848: step 3373, loss 0.0498161, acc 0.98
2016-09-06T23:36:55.918785: step 3374, loss 0.026615, acc 1
2016-09-06T23:36:56.617909: step 3375, loss 0.00620862, acc 1
2016-09-06T23:36:57.303494: step 3376, loss 0.00964875, acc 1
2016-09-06T23:36:57.993435: step 3377, loss 0.187431, acc 0.98
2016-09-06T23:36:58.700369: step 3378, loss 0.20736, acc 0.94
2016-09-06T23:36:59.374035: step 3379, loss 0.0202689, acc 1
2016-09-06T23:37:00.037532: step 3380, loss 0.0110949, acc 1
2016-09-06T23:37:00.745336: step 3381, loss 0.00549363, acc 1
2016-09-06T23:37:01.425192: step 3382, loss 0.0261456, acc 0.98
2016-09-06T23:37:02.109667: step 3383, loss 0.0117022, acc 1
2016-09-06T23:37:02.790445: step 3384, loss 0.190102, acc 0.94
2016-09-06T23:37:03.480971: step 3385, loss 0.0135302, acc 1
2016-09-06T23:37:04.170993: step 3386, loss 0.0820986, acc 0.96
2016-09-06T23:37:04.857081: step 3387, loss 0.0292211, acc 1
2016-09-06T23:37:05.556624: step 3388, loss 0.0227192, acc 0.98
2016-09-06T23:37:06.258422: step 3389, loss 0.0149001, acc 1
2016-09-06T23:37:06.931631: step 3390, loss 0.0267122, acc 0.98
2016-09-06T23:37:07.636337: step 3391, loss 0.0390993, acc 0.98
2016-09-06T23:37:08.352741: step 3392, loss 0.00858826, acc 1
2016-09-06T23:37:09.040538: step 3393, loss 0.0261482, acc 1
2016-09-06T23:37:09.730373: step 3394, loss 0.037706, acc 1
2016-09-06T23:37:10.436504: step 3395, loss 0.0717065, acc 1
2016-09-06T23:37:11.136943: step 3396, loss 0.00818515, acc 1
2016-09-06T23:37:11.811448: step 3397, loss 0.0788686, acc 0.96
2016-09-06T23:37:12.477908: step 3398, loss 0.00503495, acc 1
2016-09-06T23:37:13.181808: step 3399, loss 0.00188359, acc 1
2016-09-06T23:37:13.840241: step 3400, loss 0.0612283, acc 0.98

Evaluation:
2016-09-06T23:37:16.979177: step 3400, loss 1.79729, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-3400

2016-09-06T23:37:18.656575: step 3401, loss 0.0233548, acc 0.98
2016-09-06T23:37:19.337616: step 3402, loss 0.00105046, acc 1
2016-09-06T23:37:20.033096: step 3403, loss 0.0293751, acc 0.96
2016-09-06T23:37:20.724942: step 3404, loss 0.010486, acc 1
2016-09-06T23:37:21.408227: step 3405, loss 0.0974158, acc 0.98
2016-09-06T23:37:22.072536: step 3406, loss 0.0293371, acc 0.98
2016-09-06T23:37:22.788982: step 3407, loss 0.0017457, acc 1
2016-09-06T23:37:23.443227: step 3408, loss 0.0518769, acc 0.96
2016-09-06T23:37:24.131557: step 3409, loss 0.032972, acc 1
2016-09-06T23:37:24.813456: step 3410, loss 0.0035015, acc 1
2016-09-06T23:37:25.512908: step 3411, loss 0.0312915, acc 0.98
2016-09-06T23:37:26.221945: step 3412, loss 0.101915, acc 0.98
2016-09-06T23:37:26.897474: step 3413, loss 0.0253946, acc 1
2016-09-06T23:37:27.608107: step 3414, loss 0.00208562, acc 1
2016-09-06T23:37:28.271979: step 3415, loss 0.0101191, acc 1
2016-09-06T23:37:28.942605: step 3416, loss 0.0487543, acc 0.96
2016-09-06T23:37:29.612780: step 3417, loss 0.076069, acc 0.98
2016-09-06T23:37:30.287412: step 3418, loss 0.0379986, acc 0.98
2016-09-06T23:37:30.962274: step 3419, loss 0.0820771, acc 0.98
2016-09-06T23:37:31.642982: step 3420, loss 0.00513948, acc 1
2016-09-06T23:37:32.348909: step 3421, loss 0.0187109, acc 0.98
2016-09-06T23:37:33.038785: step 3422, loss 0.0362123, acc 0.98
2016-09-06T23:37:33.734813: step 3423, loss 0.0663312, acc 0.98
2016-09-06T23:37:34.410378: step 3424, loss 0.0421404, acc 0.98
2016-09-06T23:37:35.097519: step 3425, loss 7.94732e-05, acc 1
2016-09-06T23:37:35.781593: step 3426, loss 0.147734, acc 0.96
2016-09-06T23:37:36.455741: step 3427, loss 0.023195, acc 0.98
2016-09-06T23:37:37.170736: step 3428, loss 0.0145773, acc 0.98
2016-09-06T23:37:37.846450: step 3429, loss 0.00213338, acc 1
2016-09-06T23:37:38.524561: step 3430, loss 0.124241, acc 0.94
2016-09-06T23:37:39.253489: step 3431, loss 0.00171749, acc 1
2016-09-06T23:37:39.930433: step 3432, loss 0.0423561, acc 0.96
2016-09-06T23:37:40.637528: step 3433, loss 0.0402211, acc 0.96
2016-09-06T23:37:41.318059: step 3434, loss 0.0737643, acc 0.98
2016-09-06T23:37:42.025231: step 3435, loss 0.0657977, acc 0.98
2016-09-06T23:37:42.688623: step 3436, loss 0.0352712, acc 1
2016-09-06T23:37:43.377957: step 3437, loss 0.126819, acc 0.96
2016-09-06T23:37:44.063210: step 3438, loss 0.0142591, acc 1
2016-09-06T23:37:44.745950: step 3439, loss 0.0333313, acc 0.96
2016-09-06T23:37:45.436563: step 3440, loss 0.0249967, acc 0.98
2016-09-06T23:37:46.105540: step 3441, loss 0.0962121, acc 0.96
2016-09-06T23:37:46.786448: step 3442, loss 0.0884602, acc 0.92
2016-09-06T23:37:47.470917: step 3443, loss 0.0302182, acc 0.98
2016-09-06T23:37:48.160408: step 3444, loss 0.0321219, acc 0.98
2016-09-06T23:37:48.865363: step 3445, loss 0.0117612, acc 1
2016-09-06T23:37:49.554895: step 3446, loss 0.00554544, acc 1
2016-09-06T23:37:50.236338: step 3447, loss 0.0061193, acc 1
2016-09-06T23:37:50.936297: step 3448, loss 0.0941761, acc 0.98
2016-09-06T23:37:51.651706: step 3449, loss 0.0189806, acc 1
2016-09-06T23:37:52.340435: step 3450, loss 0.074378, acc 0.98
2016-09-06T23:37:53.039714: step 3451, loss 0.0205609, acc 1
2016-09-06T23:37:53.748688: step 3452, loss 0.0468434, acc 0.98
2016-09-06T23:37:54.454338: step 3453, loss 0.00589381, acc 1
2016-09-06T23:37:55.153969: step 3454, loss 0.0378538, acc 0.98
2016-09-06T23:37:55.831397: step 3455, loss 0.0663853, acc 0.98
2016-09-06T23:37:56.498591: step 3456, loss 0.00236227, acc 1
2016-09-06T23:37:57.188874: step 3457, loss 0.00418871, acc 1
2016-09-06T23:37:57.869350: step 3458, loss 0.0387075, acc 0.98
2016-09-06T23:37:58.543020: step 3459, loss 0.11025, acc 0.96
2016-09-06T23:37:59.230558: step 3460, loss 0.0483068, acc 0.96
2016-09-06T23:37:59.917462: step 3461, loss 0.0235521, acc 0.98
2016-09-06T23:38:00.632845: step 3462, loss 0.0275055, acc 1
2016-09-06T23:38:01.333213: step 3463, loss 0.0307471, acc 0.98
2016-09-06T23:38:02.020096: step 3464, loss 0.0554752, acc 0.98
2016-09-06T23:38:02.699002: step 3465, loss 0.0200646, acc 1
2016-09-06T23:38:03.382732: step 3466, loss 0.00327598, acc 1
2016-09-06T23:38:04.073963: step 3467, loss 0.00758332, acc 1
2016-09-06T23:38:04.784205: step 3468, loss 0.00190038, acc 1
2016-09-06T23:38:05.436703: step 3469, loss 0.0485442, acc 0.98
2016-09-06T23:38:06.131913: step 3470, loss 0.0647196, acc 0.94
2016-09-06T23:38:06.787710: step 3471, loss 0.00224059, acc 1
2016-09-06T23:38:07.478153: step 3472, loss 0.00166364, acc 1
2016-09-06T23:38:08.177852: step 3473, loss 0.0290576, acc 1
2016-09-06T23:38:08.859010: step 3474, loss 0.0247837, acc 0.98
2016-09-06T23:38:09.548336: step 3475, loss 0.0669237, acc 0.96
2016-09-06T23:38:10.211477: step 3476, loss 0.0219823, acc 0.98
2016-09-06T23:38:10.933487: step 3477, loss 0.00732858, acc 1
2016-09-06T23:38:11.635354: step 3478, loss 0.0230224, acc 1
2016-09-06T23:38:12.325081: step 3479, loss 0.00208259, acc 1
2016-09-06T23:38:13.018797: step 3480, loss 0.00977918, acc 1
2016-09-06T23:38:13.708402: step 3481, loss 0.0385114, acc 0.98
2016-09-06T23:38:14.422081: step 3482, loss 0.00883918, acc 1
2016-09-06T23:38:15.095430: step 3483, loss 0.00103733, acc 1
2016-09-06T23:38:15.789872: step 3484, loss 0.0183507, acc 1
2016-09-06T23:38:16.493335: step 3485, loss 0.00585086, acc 1
2016-09-06T23:38:17.188614: step 3486, loss 0.0495611, acc 0.98
2016-09-06T23:38:17.896033: step 3487, loss 0.030564, acc 0.98
2016-09-06T23:38:18.591488: step 3488, loss 0.171639, acc 0.96
2016-09-06T23:38:19.301080: step 3489, loss 0.0391939, acc 0.96
2016-09-06T23:38:20.001004: step 3490, loss 0.00942324, acc 1
2016-09-06T23:38:20.687012: step 3491, loss 0.00112086, acc 1
2016-09-06T23:38:21.365866: step 3492, loss 0.126064, acc 0.96
2016-09-06T23:38:22.054765: step 3493, loss 0.0339023, acc 0.96
2016-09-06T23:38:22.750885: step 3494, loss 0.0141321, acc 1
2016-09-06T23:38:23.415960: step 3495, loss 0.0101531, acc 1
2016-09-06T23:38:24.147124: step 3496, loss 0.000936453, acc 1
2016-09-06T23:38:24.826213: step 3497, loss 0.0646807, acc 0.98
2016-09-06T23:38:25.505684: step 3498, loss 0.00573682, acc 1
2016-09-06T23:38:26.192810: step 3499, loss 0.111549, acc 0.98
2016-09-06T23:38:26.869377: step 3500, loss 0.00140553, acc 1

Evaluation:
2016-09-06T23:38:30.036564: step 3500, loss 1.91081, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-3500

2016-09-06T23:38:31.668479: step 3501, loss 0.0736051, acc 0.94
2016-09-06T23:38:32.363952: step 3502, loss 0.00467455, acc 1
2016-09-06T23:38:33.043302: step 3503, loss 0.0444595, acc 0.98
2016-09-06T23:38:33.757221: step 3504, loss 0.0643181, acc 0.96
2016-09-06T23:38:34.438471: step 3505, loss 0.0412349, acc 1
2016-09-06T23:38:35.116936: step 3506, loss 0.0379443, acc 1
2016-09-06T23:38:35.808300: step 3507, loss 0.0190761, acc 1
2016-09-06T23:38:36.508931: step 3508, loss 0.0398658, acc 0.98
2016-09-06T23:38:37.215266: step 3509, loss 0.00169018, acc 1
2016-09-06T23:38:37.864087: step 3510, loss 0.0108363, acc 1
2016-09-06T23:38:38.563905: step 3511, loss 0.0649563, acc 0.98
2016-09-06T23:38:39.240718: step 3512, loss 0.0167971, acc 1
2016-09-06T23:38:39.908323: step 3513, loss 0.0258937, acc 0.98
2016-09-06T23:38:40.591992: step 3514, loss 0.000796327, acc 1
2016-09-06T23:38:41.271283: step 3515, loss 0.00919751, acc 1
2016-09-06T23:38:41.988459: step 3516, loss 0.00316112, acc 1
2016-09-06T23:38:42.672927: step 3517, loss 0.00793848, acc 1
2016-09-06T23:38:43.382015: step 3518, loss 0.0286491, acc 0.98
2016-09-06T23:38:44.060645: step 3519, loss 0.0536339, acc 0.96
2016-09-06T23:38:44.735390: step 3520, loss 0.17695, acc 0.96
2016-09-06T23:38:45.419205: step 3521, loss 0.025764, acc 1
2016-09-06T23:38:46.107754: step 3522, loss 0.161058, acc 0.94
2016-09-06T23:38:46.797423: step 3523, loss 0.0443178, acc 0.98
2016-09-06T23:38:47.464664: step 3524, loss 0.0355402, acc 0.98
2016-09-06T23:38:48.177550: step 3525, loss 0.00168708, acc 1
2016-09-06T23:38:48.857759: step 3526, loss 0.0294116, acc 0.98
2016-09-06T23:38:49.542248: step 3527, loss 0.0321332, acc 1
2016-09-06T23:38:50.227910: step 3528, loss 0.016566, acc 0.98
2016-09-06T23:38:50.931002: step 3529, loss 0.010603, acc 1
2016-09-06T23:38:51.614621: step 3530, loss 0.00693374, acc 1
2016-09-06T23:38:52.275242: step 3531, loss 0.0215152, acc 0.98
2016-09-06T23:38:52.970972: step 3532, loss 0.0476034, acc 0.98
2016-09-06T23:38:53.644615: step 3533, loss 0.0558313, acc 0.96
2016-09-06T23:38:54.322830: step 3534, loss 0.0163654, acc 1
2016-09-06T23:38:55.013404: step 3535, loss 0.022139, acc 0.98
2016-09-06T23:38:55.713235: step 3536, loss 0.0504345, acc 0.98
2016-09-06T23:38:56.408278: step 3537, loss 0.0319836, acc 1
2016-09-06T23:38:57.078432: step 3538, loss 0.00835147, acc 1
2016-09-06T23:38:57.783003: step 3539, loss 0.0226736, acc 1
2016-09-06T23:38:58.439747: step 3540, loss 0.0108417, acc 1
2016-09-06T23:38:59.129995: step 3541, loss 0.0317914, acc 1
2016-09-06T23:38:59.826226: step 3542, loss 0.0448286, acc 0.98
2016-09-06T23:39:00.537047: step 3543, loss 0.0131818, acc 1
2016-09-06T23:39:01.222926: step 3544, loss 0.0113013, acc 1
2016-09-06T23:39:01.893097: step 3545, loss 0.0280726, acc 0.98
2016-09-06T23:39:02.597917: step 3546, loss 0.0327056, acc 0.98
2016-09-06T23:39:03.265853: step 3547, loss 0.0619972, acc 0.98
2016-09-06T23:39:03.953847: step 3548, loss 0.0493775, acc 0.98
2016-09-06T23:39:04.641338: step 3549, loss 0.0186486, acc 0.98
2016-09-06T23:39:05.327488: step 3550, loss 0.153875, acc 0.92
2016-09-06T23:39:06.017459: step 3551, loss 0.107795, acc 0.9
2016-09-06T23:39:06.691177: step 3552, loss 0.0206686, acc 1
2016-09-06T23:39:07.379383: step 3553, loss 0.0227397, acc 1
2016-09-06T23:39:08.052049: step 3554, loss 0.0198364, acc 1
2016-09-06T23:39:08.742293: step 3555, loss 0.0129796, acc 1
2016-09-06T23:39:09.424066: step 3556, loss 0.0151564, acc 1
2016-09-06T23:39:10.087723: step 3557, loss 0.0417049, acc 0.98
2016-09-06T23:39:10.767216: step 3558, loss 0.0181125, acc 1
2016-09-06T23:39:11.468422: step 3559, loss 0.0340588, acc 1
2016-09-06T23:39:12.196782: step 3560, loss 0.109316, acc 0.94
2016-09-06T23:39:12.870271: step 3561, loss 0.113459, acc 0.96
2016-09-06T23:39:13.569586: step 3562, loss 0.00921539, acc 1
2016-09-06T23:39:14.277860: step 3563, loss 0.0364939, acc 0.98
2016-09-06T23:39:14.966467: step 3564, loss 0.0234533, acc 0.98
2016-09-06T23:39:15.633748: step 3565, loss 0.0799756, acc 0.98
2016-09-06T23:39:16.332709: step 3566, loss 0.0157561, acc 1
2016-09-06T23:39:17.043816: step 3567, loss 0.043179, acc 0.98
2016-09-06T23:39:17.740374: step 3568, loss 0.0161117, acc 0.98
2016-09-06T23:39:18.421357: step 3569, loss 0.0106389, acc 1
2016-09-06T23:39:19.132917: step 3570, loss 0.0298235, acc 0.96
2016-09-06T23:39:19.820647: step 3571, loss 0.0288146, acc 0.98
2016-09-06T23:39:20.524388: step 3572, loss 0.0959297, acc 0.96
2016-09-06T23:39:21.192963: step 3573, loss 0.0480677, acc 0.98
2016-09-06T23:39:21.907470: step 3574, loss 0.0143776, acc 1
2016-09-06T23:39:22.601641: step 3575, loss 0.0216949, acc 0.98
2016-09-06T23:39:23.295318: step 3576, loss 0.0147144, acc 1
2016-09-06T23:39:23.988582: step 3577, loss 0.0451895, acc 0.98
2016-09-06T23:39:24.686102: step 3578, loss 0.0145298, acc 1
2016-09-06T23:39:25.409395: step 3579, loss 0.00906503, acc 1
2016-09-06T23:39:26.097655: step 3580, loss 0.0212602, acc 0.98
2016-09-06T23:39:26.787426: step 3581, loss 0.00466067, acc 1
2016-09-06T23:39:27.465347: step 3582, loss 0.0354143, acc 0.98
2016-09-06T23:39:28.139162: step 3583, loss 0.0168063, acc 1
2016-09-06T23:39:28.814442: step 3584, loss 0.00427429, acc 1
2016-09-06T23:39:29.484808: step 3585, loss 0.0141218, acc 1
2016-09-06T23:39:30.166401: step 3586, loss 0.035072, acc 0.98
2016-09-06T23:39:30.843416: step 3587, loss 0.0644343, acc 0.96
2016-09-06T23:39:31.552889: step 3588, loss 0.0261471, acc 1
2016-09-06T23:39:32.241090: step 3589, loss 0.0393136, acc 0.98
2016-09-06T23:39:32.936202: step 3590, loss 0.0154228, acc 1
2016-09-06T23:39:33.635071: step 3591, loss 0.00573244, acc 1
2016-09-06T23:39:34.325182: step 3592, loss 0.00517143, acc 1
2016-09-06T23:39:35.037233: step 3593, loss 0.0328071, acc 1
2016-09-06T23:39:35.703510: step 3594, loss 0.00327677, acc 1
2016-09-06T23:39:36.382069: step 3595, loss 0.0297119, acc 0.98
2016-09-06T23:39:37.080416: step 3596, loss 0.0103751, acc 1
2016-09-06T23:39:37.759412: step 3597, loss 0.0247796, acc 1
2016-09-06T23:39:38.446485: step 3598, loss 0.0525019, acc 0.98
2016-09-06T23:39:39.144626: step 3599, loss 0.0204504, acc 0.98
2016-09-06T23:39:39.850019: step 3600, loss 0.00440322, acc 1

Evaluation:
2016-09-06T23:39:43.003467: step 3600, loss 1.96595, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-3600

2016-09-06T23:39:44.698761: step 3601, loss 0.0373006, acc 0.98
2016-09-06T23:39:45.390444: step 3602, loss 0.0125197, acc 1
2016-09-06T23:39:46.086348: step 3603, loss 0.0753523, acc 0.98
2016-09-06T23:39:46.790847: step 3604, loss 0.0759102, acc 0.96
2016-09-06T23:39:47.458512: step 3605, loss 0.030192, acc 1
2016-09-06T23:39:48.147254: step 3606, loss 0.000644612, acc 1
2016-09-06T23:39:48.793341: step 3607, loss 0.0237733, acc 1
2016-09-06T23:39:49.492787: step 3608, loss 0.0225837, acc 0.98
2016-09-06T23:39:50.171229: step 3609, loss 0.0266501, acc 0.98
2016-09-06T23:39:50.852631: step 3610, loss 0.0153517, acc 1
2016-09-06T23:39:51.528470: step 3611, loss 0.109304, acc 0.96
2016-09-06T23:39:52.197583: step 3612, loss 0.002238, acc 1
2016-09-06T23:39:52.897228: step 3613, loss 0.0084462, acc 1
2016-09-06T23:39:53.593924: step 3614, loss 0.0334691, acc 1
2016-09-06T23:39:54.298051: step 3615, loss 0.025343, acc 0.98
2016-09-06T23:39:54.993700: step 3616, loss 0.102485, acc 0.96
2016-09-06T23:39:55.679109: step 3617, loss 0.00274024, acc 1
2016-09-06T23:39:56.374999: step 3618, loss 0.000912138, acc 1
2016-09-06T23:39:57.053884: step 3619, loss 0.0280183, acc 0.98
2016-09-06T23:39:57.739184: step 3620, loss 0.0152205, acc 1
2016-09-06T23:39:58.440147: step 3621, loss 0.00564328, acc 1
2016-09-06T23:39:59.125450: step 3622, loss 0.0459903, acc 0.96
2016-09-06T23:39:59.828473: step 3623, loss 0.0312983, acc 0.98
2016-09-06T23:40:00.536856: step 3624, loss 0.0179677, acc 1
2016-09-06T23:40:01.224359: step 3625, loss 0.0192905, acc 0.98
2016-09-06T23:40:01.917121: step 3626, loss 0.102813, acc 0.94
2016-09-06T23:40:02.595562: step 3627, loss 0.0231758, acc 0.98
2016-09-06T23:40:03.251498: step 3628, loss 0.0134267, acc 1
2016-09-06T23:40:03.966028: step 3629, loss 0.0205213, acc 0.98
2016-09-06T23:40:04.646298: step 3630, loss 0.0423606, acc 1
2016-09-06T23:40:05.340940: step 3631, loss 0.0414792, acc 0.98
2016-09-06T23:40:06.033106: step 3632, loss 0.0821793, acc 0.98
2016-09-06T23:40:06.733845: step 3633, loss 0.00336173, acc 1
2016-09-06T23:40:07.428617: step 3634, loss 0.0260116, acc 0.98
2016-09-06T23:40:08.105206: step 3635, loss 0.0252103, acc 0.98
2016-09-06T23:40:08.807245: step 3636, loss 0.0730648, acc 0.96
2016-09-06T23:40:09.514968: step 3637, loss 0.00394957, acc 1
2016-09-06T23:40:10.198421: step 3638, loss 0.0668312, acc 0.96
2016-09-06T23:40:10.878241: step 3639, loss 0.020342, acc 0.98
2016-09-06T23:40:11.558353: step 3640, loss 0.0445064, acc 0.98
2016-09-06T23:40:12.249098: step 3641, loss 0.0690248, acc 0.98
2016-09-06T23:40:12.931987: step 3642, loss 0.041531, acc 0.98
2016-09-06T23:40:13.656894: step 3643, loss 0.0350208, acc 0.98
2016-09-06T23:40:14.366673: step 3644, loss 0.00360177, acc 1
2016-09-06T23:40:15.048330: step 3645, loss 0.0390864, acc 0.98
2016-09-06T23:40:15.734274: step 3646, loss 0.0873861, acc 0.92
2016-09-06T23:40:16.440481: step 3647, loss 0.179029, acc 0.98
2016-09-06T23:40:17.098861: step 3648, loss 0.0130984, acc 1
2016-09-06T23:40:17.795272: step 3649, loss 0.00880894, acc 1
2016-09-06T23:40:18.474441: step 3650, loss 0.101718, acc 0.94
2016-09-06T23:40:19.164719: step 3651, loss 0.0734425, acc 0.96
2016-09-06T23:40:19.866581: step 3652, loss 0.0412407, acc 0.98
2016-09-06T23:40:20.541748: step 3653, loss 0.00576929, acc 1
2016-09-06T23:40:21.249127: step 3654, loss 0.0304085, acc 0.98
2016-09-06T23:40:21.948866: step 3655, loss 0.0155132, acc 0.98
2016-09-06T23:40:22.628700: step 3656, loss 0.183757, acc 0.92
2016-09-06T23:40:23.325386: step 3657, loss 0.00782573, acc 1
2016-09-06T23:40:24.015660: step 3658, loss 0.0133429, acc 1
2016-09-06T23:40:24.686640: step 3659, loss 0.0206384, acc 1
2016-09-06T23:40:25.377419: step 3660, loss 0.0200997, acc 1
2016-09-06T23:40:26.058116: step 3661, loss 0.049051, acc 0.96
2016-09-06T23:40:26.740889: step 3662, loss 0.070581, acc 0.96
2016-09-06T23:40:27.405874: step 3663, loss 0.0402259, acc 0.98
2016-09-06T23:40:28.073153: step 3664, loss 0.0215482, acc 0.98
2016-09-06T23:40:28.785420: step 3665, loss 0.0231133, acc 1
2016-09-06T23:40:29.464876: step 3666, loss 0.0254057, acc 0.98
2016-09-06T23:40:30.146504: step 3667, loss 0.00632161, acc 1
2016-09-06T23:40:30.825133: step 3668, loss 0.0536256, acc 0.98
2016-09-06T23:40:31.527505: step 3669, loss 0.0655077, acc 0.96
2016-09-06T23:40:32.198400: step 3670, loss 0.0435725, acc 0.96
2016-09-06T23:40:32.875281: step 3671, loss 0.0129027, acc 1
2016-09-06T23:40:33.551969: step 3672, loss 0.0148839, acc 1
2016-09-06T23:40:34.258710: step 3673, loss 0.0372708, acc 0.98
2016-09-06T23:40:34.922168: step 3674, loss 0.0139282, acc 1
2016-09-06T23:40:35.619397: step 3675, loss 0.00257372, acc 1
2016-09-06T23:40:36.316757: step 3676, loss 0.00613878, acc 1
2016-09-06T23:40:37.015468: step 3677, loss 0.051178, acc 0.98
2016-09-06T23:40:37.684515: step 3678, loss 0.0181573, acc 1
2016-09-06T23:40:38.370669: step 3679, loss 0.34376, acc 0.96
2016-09-06T23:40:39.052913: step 3680, loss 0.0266291, acc 1
2016-09-06T23:40:39.731871: step 3681, loss 0.0482765, acc 0.96
2016-09-06T23:40:40.412532: step 3682, loss 0.0231447, acc 0.98
2016-09-06T23:40:41.123646: step 3683, loss 0.0234802, acc 0.98
2016-09-06T23:40:41.809096: step 3684, loss 0.0363418, acc 0.96
2016-09-06T23:40:42.506196: step 3685, loss 0.0271217, acc 0.98
2016-09-06T23:40:43.242463: step 3686, loss 0.0340618, acc 0.98
2016-09-06T23:40:43.944524: step 3687, loss 0.00556502, acc 1
2016-09-06T23:40:44.648659: step 3688, loss 0.0025287, acc 1
2016-09-06T23:40:45.325053: step 3689, loss 0.0383872, acc 0.96
2016-09-06T23:40:46.043946: step 3690, loss 0.0593387, acc 0.98
2016-09-06T23:40:46.714470: step 3691, loss 0.0120651, acc 1
2016-09-06T23:40:47.392825: step 3692, loss 0.0228789, acc 1
2016-09-06T23:40:48.090055: step 3693, loss 0.0232461, acc 0.98
2016-09-06T23:40:48.793134: step 3694, loss 0.0178005, acc 1
2016-09-06T23:40:49.451777: step 3695, loss 0.0166094, acc 0.98
2016-09-06T23:40:50.117609: step 3696, loss 0.0489718, acc 0.98
2016-09-06T23:40:50.837535: step 3697, loss 0.0181207, acc 0.98
2016-09-06T23:40:51.494465: step 3698, loss 0.00391108, acc 1
2016-09-06T23:40:52.181998: step 3699, loss 0.0167886, acc 1
2016-09-06T23:40:52.847664: step 3700, loss 0.0916396, acc 0.98

Evaluation:
2016-09-06T23:40:55.975772: step 3700, loss 1.77603, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-3700

2016-09-06T23:40:57.692634: step 3701, loss 0.0104052, acc 1
2016-09-06T23:40:58.384509: step 3702, loss 0.0285981, acc 1
2016-09-06T23:40:59.079645: step 3703, loss 0.0797322, acc 0.96
2016-09-06T23:40:59.735491: step 3704, loss 0.0143689, acc 1
2016-09-06T23:41:00.465983: step 3705, loss 0.107909, acc 0.98
2016-09-06T23:41:01.129156: step 3706, loss 0.0157193, acc 1
2016-09-06T23:41:01.797071: step 3707, loss 0.0226406, acc 1
2016-09-06T23:41:02.493238: step 3708, loss 0.152521, acc 0.96
2016-09-06T23:41:03.161063: step 3709, loss 0.0141484, acc 1
2016-09-06T23:41:03.831630: step 3710, loss 0.0207605, acc 0.98
2016-09-06T23:41:04.521985: step 3711, loss 0.0375276, acc 0.98
2016-09-06T23:41:05.217948: step 3712, loss 0.0286741, acc 1
2016-09-06T23:41:05.896520: step 3713, loss 0.0903194, acc 0.98
2016-09-06T23:41:06.582085: step 3714, loss 0.103869, acc 0.96
2016-09-06T23:41:07.284564: step 3715, loss 0.0161212, acc 0.98
2016-09-06T23:41:08.000097: step 3716, loss 0.0251192, acc 0.98
2016-09-06T23:41:08.675929: step 3717, loss 0.00326566, acc 1
2016-09-06T23:41:09.350455: step 3718, loss 0.0260338, acc 0.98
2016-09-06T23:41:10.040026: step 3719, loss 0.11012, acc 0.98
2016-09-06T23:41:10.692382: step 3720, loss 0.0283573, acc 1
2016-09-06T23:41:11.393836: step 3721, loss 0.0159294, acc 0.98
2016-09-06T23:41:12.092325: step 3722, loss 0.0441583, acc 0.98
2016-09-06T23:41:12.773155: step 3723, loss 0.0192425, acc 1
2016-09-06T23:41:13.467739: step 3724, loss 0.0768655, acc 0.98
2016-09-06T23:41:14.142661: step 3725, loss 0.0122003, acc 1
2016-09-06T23:41:14.826982: step 3726, loss 0.00133677, acc 1
2016-09-06T23:41:15.497381: step 3727, loss 0.0005939, acc 1
2016-09-06T23:41:16.182396: step 3728, loss 0.0216164, acc 1
2016-09-06T23:41:16.864194: step 3729, loss 0.023889, acc 1
2016-09-06T23:41:17.535675: step 3730, loss 0.0600885, acc 0.98
2016-09-06T23:41:18.234256: step 3731, loss 0.101905, acc 0.96
2016-09-06T23:41:18.924655: step 3732, loss 0.0018192, acc 1
2016-09-06T23:41:19.621494: step 3733, loss 0.104012, acc 0.98
2016-09-06T23:41:20.282420: step 3734, loss 0.0248835, acc 1
2016-09-06T23:41:20.990620: step 3735, loss 0.0155424, acc 1
2016-09-06T23:41:21.671953: step 3736, loss 0.0906567, acc 0.96
2016-09-06T23:41:22.363948: step 3737, loss 0.0328592, acc 0.98
2016-09-06T23:41:23.065040: step 3738, loss 0.0154007, acc 1
2016-09-06T23:41:23.746228: step 3739, loss 0.0451162, acc 0.98
2016-09-06T23:41:24.455603: step 3740, loss 0.0052089, acc 1
2016-09-06T23:41:25.168681: step 3741, loss 0.0129609, acc 1
2016-09-06T23:41:25.875386: step 3742, loss 0.0111163, acc 1
2016-09-06T23:41:26.554879: step 3743, loss 0.118273, acc 0.94
2016-09-06T23:41:27.253357: step 3744, loss 0.0135677, acc 1
2016-09-06T23:41:27.950991: step 3745, loss 0.0589593, acc 0.98
2016-09-06T23:41:28.611013: step 3746, loss 0.0172383, acc 1
2016-09-06T23:41:29.299143: step 3747, loss 0.0204825, acc 0.98
2016-09-06T23:41:29.984592: step 3748, loss 0.0297486, acc 1
2016-09-06T23:41:30.708596: step 3749, loss 0.0226043, acc 0.98
2016-09-06T23:41:31.405410: step 3750, loss 0.0401597, acc 0.98
2016-09-06T23:41:32.085482: step 3751, loss 0.0591942, acc 0.96
2016-09-06T23:41:32.784477: step 3752, loss 0.0875461, acc 0.94
2016-09-06T23:41:33.444508: step 3753, loss 0.115687, acc 0.96
2016-09-06T23:41:34.135479: step 3754, loss 0.0127735, acc 1
2016-09-06T23:41:34.803242: step 3755, loss 0.0276111, acc 0.98
2016-09-06T23:41:35.488272: step 3756, loss 0.0125066, acc 1
2016-09-06T23:41:36.174630: step 3757, loss 0.143438, acc 0.96
2016-09-06T23:41:36.852477: step 3758, loss 0.0609408, acc 0.94
2016-09-06T23:41:37.530639: step 3759, loss 0.0359581, acc 1
2016-09-06T23:41:38.200185: step 3760, loss 0.00932822, acc 1
2016-09-06T23:41:38.891777: step 3761, loss 0.0608722, acc 0.96
2016-09-06T23:41:39.545389: step 3762, loss 0.0439655, acc 0.98
2016-09-06T23:41:40.228357: step 3763, loss 0.0533838, acc 0.96
2016-09-06T23:41:40.912919: step 3764, loss 0.0306552, acc 0.98
2016-09-06T23:41:41.583682: step 3765, loss 0.0295975, acc 0.98
2016-09-06T23:41:42.285482: step 3766, loss 0.00246591, acc 1
2016-09-06T23:41:42.950632: step 3767, loss 0.0279237, acc 0.98
2016-09-06T23:41:43.648528: step 3768, loss 0.0813985, acc 0.96
2016-09-06T23:41:44.314309: step 3769, loss 0.0198969, acc 0.98
2016-09-06T23:41:45.007848: step 3770, loss 0.0169772, acc 0.98
2016-09-06T23:41:45.706208: step 3771, loss 0.04147, acc 0.98
2016-09-06T23:41:46.411760: step 3772, loss 0.0288835, acc 1
2016-09-06T23:41:47.113366: step 3773, loss 0.0327611, acc 0.98
2016-09-06T23:41:47.812165: step 3774, loss 0.0287339, acc 0.98
2016-09-06T23:41:48.511185: step 3775, loss 0.0192034, acc 1
2016-09-06T23:41:49.198908: step 3776, loss 0.139529, acc 0.96
2016-09-06T23:41:49.873804: step 3777, loss 0.0536673, acc 0.98
2016-09-06T23:41:50.538669: step 3778, loss 0.0556164, acc 0.96
2016-09-06T23:41:51.218416: step 3779, loss 0.0568615, acc 0.96
2016-09-06T23:41:51.904443: step 3780, loss 0.071916, acc 0.96
2016-09-06T23:41:52.567561: step 3781, loss 0.030335, acc 1
2016-09-06T23:41:53.257820: step 3782, loss 0.0482368, acc 0.98
2016-09-06T23:41:53.918586: step 3783, loss 0.00167481, acc 1
2016-09-06T23:41:54.622602: step 3784, loss 0.0479405, acc 0.96
2016-09-06T23:41:55.299409: step 3785, loss 0.0414797, acc 0.98
2016-09-06T23:41:56.003735: step 3786, loss 0.0211115, acc 1
2016-09-06T23:41:56.699626: step 3787, loss 0.0313183, acc 0.96
2016-09-06T23:41:57.376103: step 3788, loss 0.0850196, acc 0.96
2016-09-06T23:41:58.087601: step 3789, loss 0.014639, acc 1
2016-09-06T23:41:58.758576: step 3790, loss 0.0200368, acc 0.98
2016-09-06T23:41:59.466930: step 3791, loss 0.017786, acc 0.98
2016-09-06T23:42:00.168625: step 3792, loss 0.00466632, acc 1
2016-09-06T23:42:00.892463: step 3793, loss 0.0583167, acc 0.96
2016-09-06T23:42:01.583682: step 3794, loss 0.0318696, acc 0.98
2016-09-06T23:42:02.283575: step 3795, loss 0.0196874, acc 1
2016-09-06T23:42:02.978408: step 3796, loss 0.058401, acc 0.98
2016-09-06T23:42:03.650978: step 3797, loss 0.0205793, acc 1
2016-09-06T23:42:04.322246: step 3798, loss 0.00624019, acc 1
2016-09-06T23:42:05.000424: step 3799, loss 0.0438272, acc 0.98
2016-09-06T23:42:05.701052: step 3800, loss 0.0266481, acc 1

Evaluation:
2016-09-06T23:42:08.868542: step 3800, loss 1.70691, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-3800

2016-09-06T23:42:10.496652: step 3801, loss 0.00980602, acc 1
2016-09-06T23:42:11.173270: step 3802, loss 0.0442575, acc 0.98
2016-09-06T23:42:11.856055: step 3803, loss 0.0433576, acc 0.98
2016-09-06T23:42:12.560009: step 3804, loss 0.0164275, acc 1
2016-09-06T23:42:13.221347: step 3805, loss 0.0489166, acc 0.98
2016-09-06T23:42:13.920083: step 3806, loss 0.025104, acc 1
2016-09-06T23:42:14.620556: step 3807, loss 0.0229387, acc 1
2016-09-06T23:42:15.317471: step 3808, loss 0.0901021, acc 0.98
2016-09-06T23:42:16.009424: step 3809, loss 0.00206067, acc 1
2016-09-06T23:42:16.694302: step 3810, loss 0.0571698, acc 0.98
2016-09-06T23:42:17.432487: step 3811, loss 0.0262754, acc 0.98
2016-09-06T23:42:18.094214: step 3812, loss 0.00517006, acc 1
2016-09-06T23:42:18.798718: step 3813, loss 0.0579959, acc 0.98
2016-09-06T23:42:19.479114: step 3814, loss 0.0433889, acc 0.98
2016-09-06T23:42:20.178528: step 3815, loss 0.00375627, acc 1
2016-09-06T23:42:20.882482: step 3816, loss 0.0356803, acc 1
2016-09-06T23:42:21.578186: step 3817, loss 0.0590043, acc 0.98
2016-09-06T23:42:22.295781: step 3818, loss 0.0332443, acc 0.98
2016-09-06T23:42:22.974840: step 3819, loss 0.0195862, acc 0.98
2016-09-06T23:42:23.665195: step 3820, loss 0.0151647, acc 1
2016-09-06T23:42:24.362019: step 3821, loss 0.00325428, acc 1
2016-09-06T23:42:25.041778: step 3822, loss 0.156699, acc 0.94
2016-09-06T23:42:25.740814: step 3823, loss 0.000946064, acc 1
2016-09-06T23:42:26.415549: step 3824, loss 0.0179225, acc 1
2016-09-06T23:42:27.098681: step 3825, loss 0.0313044, acc 0.98
2016-09-06T23:42:27.782431: step 3826, loss 0.0584533, acc 0.98
2016-09-06T23:42:28.465598: step 3827, loss 0.00870126, acc 1
2016-09-06T23:42:29.131006: step 3828, loss 0.0277129, acc 0.98
2016-09-06T23:42:29.814095: step 3829, loss 0.00770741, acc 1
2016-09-06T23:42:30.517371: step 3830, loss 0.0201357, acc 0.98
2016-09-06T23:42:31.174833: step 3831, loss 0.0163365, acc 1
2016-09-06T23:42:31.882508: step 3832, loss 0.0478804, acc 0.98
2016-09-06T23:42:32.581027: step 3833, loss 0.00567775, acc 1
2016-09-06T23:42:33.278475: step 3834, loss 0.0237417, acc 1
2016-09-06T23:42:33.951770: step 3835, loss 0.0184914, acc 1
2016-09-06T23:42:34.637182: step 3836, loss 0.0112153, acc 1
2016-09-06T23:42:35.346517: step 3837, loss 0.0659838, acc 0.96
2016-09-06T23:42:36.027242: step 3838, loss 0.0281926, acc 1
2016-09-06T23:42:36.739454: step 3839, loss 0.0138996, acc 1
2016-09-06T23:42:37.398901: step 3840, loss 0.0137682, acc 1
2016-09-06T23:42:38.092744: step 3841, loss 0.0377469, acc 0.98
2016-09-06T23:42:38.775680: step 3842, loss 0.0319606, acc 0.98
2016-09-06T23:42:39.460886: step 3843, loss 0.0250207, acc 1
2016-09-06T23:42:40.149712: step 3844, loss 0.0494705, acc 0.94
2016-09-06T23:42:40.825792: step 3845, loss 0.0295497, acc 0.98
2016-09-06T23:42:41.533465: step 3846, loss 0.0283781, acc 1
2016-09-06T23:42:42.215748: step 3847, loss 0.0128057, acc 1
2016-09-06T23:42:42.886844: step 3848, loss 0.0394146, acc 0.98
2016-09-06T23:42:43.585108: step 3849, loss 0.0131861, acc 1
2016-09-06T23:42:44.257273: step 3850, loss 0.00977952, acc 1
2016-09-06T23:42:44.952252: step 3851, loss 0.00736571, acc 1
2016-09-06T23:42:45.635848: step 3852, loss 0.0266994, acc 0.98
2016-09-06T23:42:46.333333: step 3853, loss 0.043509, acc 0.98
2016-09-06T23:42:47.031104: step 3854, loss 0.050401, acc 0.96
2016-09-06T23:42:47.724615: step 3855, loss 0.0026631, acc 1
2016-09-06T23:42:48.426630: step 3856, loss 0.00843272, acc 1
2016-09-06T23:42:49.137596: step 3857, loss 0.122604, acc 0.96
2016-09-06T23:42:49.853314: step 3858, loss 0.0119523, acc 1
2016-09-06T23:42:50.517627: step 3859, loss 0.0302674, acc 0.98
2016-09-06T23:42:51.196888: step 3860, loss 0.0152997, acc 0.98
2016-09-06T23:42:51.898916: step 3861, loss 0.0606531, acc 0.98
2016-09-06T23:42:52.599426: step 3862, loss 0.0175582, acc 0.98
2016-09-06T23:42:53.299382: step 3863, loss 0.0182157, acc 0.98
2016-09-06T23:42:53.985565: step 3864, loss 0.00240031, acc 1
2016-09-06T23:42:54.703695: step 3865, loss 0.00524868, acc 1
2016-09-06T23:42:55.374862: step 3866, loss 0.0150995, acc 1
2016-09-06T23:42:56.058590: step 3867, loss 0.00107748, acc 1
2016-09-06T23:42:56.743503: step 3868, loss 0.056287, acc 0.98
2016-09-06T23:42:57.417147: step 3869, loss 0.0548561, acc 0.98
2016-09-06T23:42:58.099909: step 3870, loss 0.0229399, acc 1
2016-09-06T23:42:58.791600: step 3871, loss 0.00183718, acc 1
2016-09-06T23:42:59.495294: step 3872, loss 0.0325636, acc 0.98
2016-09-06T23:43:00.172100: step 3873, loss 0.031537, acc 1
2016-09-06T23:43:00.891577: step 3874, loss 0.000182716, acc 1
2016-09-06T23:43:01.600300: step 3875, loss 0.0142426, acc 1
2016-09-06T23:43:02.301788: step 3876, loss 0.0112401, acc 1
2016-09-06T23:43:03.039846: step 3877, loss 0.0299808, acc 0.98
2016-09-06T23:43:03.712931: step 3878, loss 0.0126072, acc 1
2016-09-06T23:43:04.401180: step 3879, loss 0.00113179, acc 1
2016-09-06T23:43:05.079347: step 3880, loss 0.0229533, acc 0.98
2016-09-06T23:43:05.762821: step 3881, loss 0.133309, acc 0.96
2016-09-06T23:43:06.442191: step 3882, loss 0.0579477, acc 0.98
2016-09-06T23:43:07.124241: step 3883, loss 0.0023743, acc 1
2016-09-06T23:43:07.823331: step 3884, loss 0.0717892, acc 0.98
2016-09-06T23:43:08.519473: step 3885, loss 0.0161432, acc 0.98
2016-09-06T23:43:09.218601: step 3886, loss 0.0291367, acc 0.98
2016-09-06T23:43:09.918597: step 3887, loss 0.0115597, acc 1
2016-09-06T23:43:10.608325: step 3888, loss 0.0191324, acc 1
2016-09-06T23:43:11.286698: step 3889, loss 0.27326, acc 0.96
2016-09-06T23:43:11.974729: step 3890, loss 0.0392957, acc 0.98
2016-09-06T23:43:12.672846: step 3891, loss 0.0204174, acc 0.98
2016-09-06T23:43:13.348956: step 3892, loss 0.000824005, acc 1
2016-09-06T23:43:14.026716: step 3893, loss 0.00584154, acc 1
2016-09-06T23:43:14.721333: step 3894, loss 0.0305211, acc 0.98
2016-09-06T23:43:15.430324: step 3895, loss 0.00554046, acc 1
2016-09-06T23:43:16.121811: step 3896, loss 0.0520243, acc 0.98
2016-09-06T23:43:16.813440: step 3897, loss 0.0513124, acc 0.98
2016-09-06T23:43:17.542645: step 3898, loss 0.00016876, acc 1
2016-09-06T23:43:18.225420: step 3899, loss 0.00885516, acc 1
2016-09-06T23:43:18.906278: step 3900, loss 0.0416074, acc 0.98

Evaluation:
2016-09-06T23:43:22.043453: step 3900, loss 1.72709, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-3900

2016-09-06T23:43:23.674517: step 3901, loss 0.0263828, acc 1
2016-09-06T23:43:24.370001: step 3902, loss 0.0477223, acc 0.96
2016-09-06T23:43:25.062880: step 3903, loss 0.0839235, acc 0.94
2016-09-06T23:43:25.752290: step 3904, loss 0.0172114, acc 1
2016-09-06T23:43:26.424529: step 3905, loss 0.0202686, acc 1
2016-09-06T23:43:27.141197: step 3906, loss 0.0292388, acc 1
2016-09-06T23:43:27.818038: step 3907, loss 0.0143553, acc 1
2016-09-06T23:43:28.526505: step 3908, loss 0.013663, acc 1
2016-09-06T23:43:29.201962: step 3909, loss 0.0328341, acc 0.98
2016-09-06T23:43:29.872504: step 3910, loss 0.023009, acc 1
2016-09-06T23:43:30.561956: step 3911, loss 0.0342364, acc 0.98
2016-09-06T23:43:31.231788: step 3912, loss 0.0620392, acc 0.98
2016-09-06T23:43:31.946457: step 3913, loss 0.0703034, acc 0.98
2016-09-06T23:43:32.616079: step 3914, loss 0.036714, acc 0.98
2016-09-06T23:43:33.334207: step 3915, loss 0.0390329, acc 0.98
2016-09-06T23:43:34.027906: step 3916, loss 0.115602, acc 0.92
2016-09-06T23:43:34.732566: step 3917, loss 0.0197683, acc 1
2016-09-06T23:43:35.427148: step 3918, loss 0.053511, acc 0.96
2016-09-06T23:43:36.098942: step 3919, loss 0.0124758, acc 1
2016-09-06T23:43:36.812599: step 3920, loss 0.0149687, acc 0.98
2016-09-06T23:43:37.500797: step 3921, loss 0.0168173, acc 0.98
2016-09-06T23:43:38.201038: step 3922, loss 0.0148223, acc 1
2016-09-06T23:43:38.898678: step 3923, loss 0.0141476, acc 1
2016-09-06T23:43:39.575384: step 3924, loss 0.0701234, acc 0.98
2016-09-06T23:43:40.270103: step 3925, loss 0.0187447, acc 0.98
2016-09-06T23:43:40.943991: step 3926, loss 0.0214603, acc 0.98
2016-09-06T23:43:41.641443: step 3927, loss 0.1049, acc 0.98
2016-09-06T23:43:42.324411: step 3928, loss 0.00584904, acc 1
2016-09-06T23:43:42.990819: step 3929, loss 0.029918, acc 1
2016-09-06T23:43:43.666439: step 3930, loss 0.0486893, acc 0.98
2016-09-06T23:43:44.347176: step 3931, loss 0.055391, acc 0.96
2016-09-06T23:43:45.028788: step 3932, loss 0.0792823, acc 0.98
2016-09-06T23:43:45.681263: step 3933, loss 0.0105196, acc 1
2016-09-06T23:43:46.375799: step 3934, loss 0.0238046, acc 0.98
2016-09-06T23:43:47.041820: step 3935, loss 0.019675, acc 1
2016-09-06T23:43:47.731741: step 3936, loss 0.0262759, acc 1
2016-09-06T23:43:48.412748: step 3937, loss 0.0180176, acc 0.98
2016-09-06T23:43:49.096065: step 3938, loss 0.011952, acc 1
2016-09-06T23:43:49.829445: step 3939, loss 0.0110241, acc 1
2016-09-06T23:43:50.490540: step 3940, loss 0.0448556, acc 0.98
2016-09-06T23:43:51.178435: step 3941, loss 0.00626519, acc 1
2016-09-06T23:43:51.852243: step 3942, loss 0.00115411, acc 1
2016-09-06T23:43:52.530571: step 3943, loss 0.0278982, acc 0.98
2016-09-06T23:43:53.220494: step 3944, loss 0.0176938, acc 1
2016-09-06T23:43:53.926634: step 3945, loss 0.0308142, acc 0.98
2016-09-06T23:43:54.616793: step 3946, loss 0.0381797, acc 0.98
2016-09-06T23:43:55.286681: step 3947, loss 0.000435791, acc 1
2016-09-06T23:43:56.000972: step 3948, loss 0.0330341, acc 0.98
2016-09-06T23:43:56.693853: step 3949, loss 0.0301911, acc 1
2016-09-06T23:43:57.398444: step 3950, loss 0.0284642, acc 0.98
2016-09-06T23:43:58.094793: step 3951, loss 0.0732351, acc 0.98
2016-09-06T23:43:58.789089: step 3952, loss 0.175233, acc 0.96
2016-09-06T23:43:59.464814: step 3953, loss 0.090881, acc 0.98
2016-09-06T23:44:00.122332: step 3954, loss 0.053815, acc 0.98
2016-09-06T23:44:00.856007: step 3955, loss 0.0203315, acc 0.98
2016-09-06T23:44:01.555456: step 3956, loss 0.103567, acc 0.96
2016-09-06T23:44:02.271272: step 3957, loss 0.0216489, acc 0.98
2016-09-06T23:44:02.974436: step 3958, loss 0.0314831, acc 0.98
2016-09-06T23:44:03.668152: step 3959, loss 0.00413223, acc 1
2016-09-06T23:44:04.375081: step 3960, loss 0.0476905, acc 0.98
2016-09-06T23:44:05.063339: step 3961, loss 0.0266367, acc 0.98
2016-09-06T23:44:05.770967: step 3962, loss 0.0587329, acc 0.98
2016-09-06T23:44:06.466372: step 3963, loss 0.00257698, acc 1
2016-09-06T23:44:07.166042: step 3964, loss 0.0104187, acc 1
2016-09-06T23:44:07.873660: step 3965, loss 0.0410096, acc 1
2016-09-06T23:44:08.569841: step 3966, loss 0.0389366, acc 0.96
2016-09-06T23:44:09.286388: step 3967, loss 0.0411065, acc 0.98
2016-09-06T23:44:09.982806: step 3968, loss 0.127301, acc 0.96
2016-09-06T23:44:10.660886: step 3969, loss 0.0614709, acc 0.98
2016-09-06T23:44:11.360569: step 3970, loss 0.0463767, acc 0.98
2016-09-06T23:44:12.066139: step 3971, loss 0.134319, acc 0.94
2016-09-06T23:44:12.770048: step 3972, loss 0.057506, acc 0.98
2016-09-06T23:44:13.441714: step 3973, loss 0.0742402, acc 0.96
2016-09-06T23:44:14.137469: step 3974, loss 0.074039, acc 0.96
2016-09-06T23:44:14.826686: step 3975, loss 0.0048626, acc 1
2016-09-06T23:44:15.518982: step 3976, loss 0.0260259, acc 1
2016-09-06T23:44:16.209580: step 3977, loss 0.0478124, acc 0.98
2016-09-06T23:44:16.888214: step 3978, loss 0.0116058, acc 1
2016-09-06T23:44:17.588738: step 3979, loss 0.0307192, acc 0.98
2016-09-06T23:44:18.284858: step 3980, loss 0.00482674, acc 1
2016-09-06T23:44:18.995667: step 3981, loss 0.0347288, acc 0.98
2016-09-06T23:44:19.691518: step 3982, loss 0.00346546, acc 1
2016-09-06T23:44:20.394199: step 3983, loss 0.0322512, acc 0.98
2016-09-06T23:44:21.089285: step 3984, loss 0.029928, acc 1
2016-09-06T23:44:21.757756: step 3985, loss 0.0752748, acc 0.98
2016-09-06T23:44:22.477060: step 3986, loss 0.00510559, acc 1
2016-09-06T23:44:23.168218: step 3987, loss 0.0235656, acc 1
2016-09-06T23:44:23.852377: step 3988, loss 0.0723349, acc 0.96
2016-09-06T23:44:24.541556: step 3989, loss 0.0791734, acc 0.96
2016-09-06T23:44:25.230388: step 3990, loss 0.0422314, acc 0.98
2016-09-06T23:44:25.925918: step 3991, loss 0.0132275, acc 1
2016-09-06T23:44:26.600477: step 3992, loss 0.0446433, acc 0.98
2016-09-06T23:44:27.330478: step 3993, loss 0.0103931, acc 1
2016-09-06T23:44:28.007750: step 3994, loss 0.0347144, acc 0.98
2016-09-06T23:44:28.693298: step 3995, loss 0.0142483, acc 1
2016-09-06T23:44:29.380540: step 3996, loss 0.00307843, acc 1
2016-09-06T23:44:30.060668: step 3997, loss 0.0201771, acc 0.98
2016-09-06T23:44:30.745825: step 3998, loss 0.126005, acc 0.94
2016-09-06T23:44:31.407699: step 3999, loss 0.0753937, acc 0.98
2016-09-06T23:44:32.120994: step 4000, loss 0.0463015, acc 0.98

Evaluation:
2016-09-06T23:44:35.258294: step 4000, loss 1.82108, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-4000

2016-09-06T23:44:36.967030: step 4001, loss 0.000611873, acc 1
2016-09-06T23:44:37.691197: step 4002, loss 0.021241, acc 1
2016-09-06T23:44:38.375159: step 4003, loss 0.0559895, acc 0.98
2016-09-06T23:44:39.068933: step 4004, loss 0.0100339, acc 1
2016-09-06T23:44:39.789063: step 4005, loss 0.016439, acc 0.98
2016-09-06T23:44:40.496389: step 4006, loss 0.0429952, acc 0.98
2016-09-06T23:44:41.180399: step 4007, loss 0.0181849, acc 0.98
2016-09-06T23:44:41.876247: step 4008, loss 0.0601595, acc 0.96
2016-09-06T23:44:42.562934: step 4009, loss 0.0400622, acc 0.98
2016-09-06T23:44:43.232220: step 4010, loss 0.0190593, acc 0.98
2016-09-06T23:44:43.944422: step 4011, loss 0.0195001, acc 0.98
2016-09-06T23:44:44.598282: step 4012, loss 0.0238318, acc 0.98
2016-09-06T23:44:45.303727: step 4013, loss 0.0220576, acc 0.98
2016-09-06T23:44:45.980275: step 4014, loss 0.0485334, acc 0.96
2016-09-06T23:44:46.666919: step 4015, loss 0.00548145, acc 1
2016-09-06T23:44:47.357277: step 4016, loss 0.0367463, acc 0.98
2016-09-06T23:44:48.042707: step 4017, loss 0.013611, acc 1
2016-09-06T23:44:48.729941: step 4018, loss 0.120399, acc 0.94
2016-09-06T23:44:49.416700: step 4019, loss 0.0199054, acc 1
2016-09-06T23:44:50.123708: step 4020, loss 0.00344916, acc 1
2016-09-06T23:44:50.800910: step 4021, loss 0.0353789, acc 0.96
2016-09-06T23:44:51.512665: step 4022, loss 0.0153403, acc 1
2016-09-06T23:44:52.211155: step 4023, loss 0.000300533, acc 1
2016-09-06T23:44:52.875987: step 4024, loss 0.0412765, acc 1
2016-09-06T23:44:53.557534: step 4025, loss 0.0491679, acc 0.98
2016-09-06T23:44:54.217249: step 4026, loss 0.0115751, acc 1
2016-09-06T23:44:54.901126: step 4027, loss 0.0545377, acc 0.96
2016-09-06T23:44:55.596856: step 4028, loss 0.00547342, acc 1
2016-09-06T23:44:56.274854: step 4029, loss 0.0125668, acc 1
2016-09-06T23:44:56.958895: step 4030, loss 0.0304426, acc 0.98
2016-09-06T23:44:57.670205: step 4031, loss 0.0192985, acc 0.98
2016-09-06T23:44:58.346812: step 4032, loss 0.000223403, acc 1
2016-09-06T23:44:59.037219: step 4033, loss 0.0311373, acc 0.98
2016-09-06T23:44:59.742582: step 4034, loss 0.0117842, acc 1
2016-09-06T23:45:00.449818: step 4035, loss 0.00379934, acc 1
2016-09-06T23:45:01.119719: step 4036, loss 0.205943, acc 0.96
2016-09-06T23:45:01.813280: step 4037, loss 0.0185477, acc 1
2016-09-06T23:45:02.500442: step 4038, loss 0.0634656, acc 0.96
2016-09-06T23:45:03.189724: step 4039, loss 0.0101392, acc 1
2016-09-06T23:45:03.850897: step 4040, loss 0.0321259, acc 0.98
2016-09-06T23:45:04.555108: step 4041, loss 0.0132324, acc 1
2016-09-06T23:45:05.233929: step 4042, loss 0.0153357, acc 0.98
2016-09-06T23:45:05.923556: step 4043, loss 0.00251111, acc 1
2016-09-06T23:45:06.596896: step 4044, loss 0.0257356, acc 0.98
2016-09-06T23:45:07.295417: step 4045, loss 0.0278116, acc 0.98
2016-09-06T23:45:08.015647: step 4046, loss 0.0434873, acc 0.98
2016-09-06T23:45:08.687497: step 4047, loss 0.0105446, acc 1
2016-09-06T23:45:09.395505: step 4048, loss 0.00938329, acc 1
2016-09-06T23:45:10.069488: step 4049, loss 0.00888056, acc 1
2016-09-06T23:45:10.777895: step 4050, loss 0.0613524, acc 0.96
2016-09-06T23:45:11.465366: step 4051, loss 0.0151508, acc 1
2016-09-06T23:45:12.151904: step 4052, loss 0.0434952, acc 0.98
2016-09-06T23:45:12.859203: step 4053, loss 0.00827448, acc 1
2016-09-06T23:45:13.518798: step 4054, loss 0.0338257, acc 0.98
2016-09-06T23:45:14.226707: step 4055, loss 0.00697283, acc 1
2016-09-06T23:45:14.924440: step 4056, loss 0.0225692, acc 0.98
2016-09-06T23:45:15.604625: step 4057, loss 0.037526, acc 0.98
2016-09-06T23:45:16.314724: step 4058, loss 0.00703001, acc 1
2016-09-06T23:45:17.007889: step 4059, loss 0.0107858, acc 1
2016-09-06T23:45:17.718168: step 4060, loss 0.00231571, acc 1
2016-09-06T23:45:18.397509: step 4061, loss 0.0432683, acc 0.98
2016-09-06T23:45:19.095584: step 4062, loss 0.014172, acc 1
2016-09-06T23:45:19.816968: step 4063, loss 0.0173632, acc 1
2016-09-06T23:45:20.492837: step 4064, loss 0.0530801, acc 0.98
2016-09-06T23:45:21.196706: step 4065, loss 0.0348877, acc 0.96
2016-09-06T23:45:21.862550: step 4066, loss 0.0285088, acc 0.98
2016-09-06T23:45:22.604467: step 4067, loss 0.131413, acc 0.96
2016-09-06T23:45:23.274267: step 4068, loss 0.0470455, acc 0.96
2016-09-06T23:45:23.955270: step 4069, loss 0.039727, acc 0.98
2016-09-06T23:45:24.643760: step 4070, loss 0.0266814, acc 1
2016-09-06T23:45:25.333733: step 4071, loss 0.0622261, acc 0.96
2016-09-06T23:45:26.009410: step 4072, loss 0.0208143, acc 1
2016-09-06T23:45:26.656012: step 4073, loss 0.0661147, acc 0.96
2016-09-06T23:45:27.346291: step 4074, loss 0.00105749, acc 1
2016-09-06T23:45:28.033810: step 4075, loss 0.0175542, acc 0.98
2016-09-06T23:45:28.719557: step 4076, loss 0.00158241, acc 1
2016-09-06T23:45:29.404455: step 4077, loss 0.0156498, acc 0.98
2016-09-06T23:45:30.091841: step 4078, loss 0.0359431, acc 0.96
2016-09-06T23:45:30.767665: step 4079, loss 0.0117993, acc 1
2016-09-06T23:45:31.447094: step 4080, loss 0.0183682, acc 1
2016-09-06T23:45:32.146255: step 4081, loss 0.041267, acc 0.98
2016-09-06T23:45:32.820077: step 4082, loss 0.020169, acc 0.98
2016-09-06T23:45:33.498881: step 4083, loss 0.0115904, acc 1
2016-09-06T23:45:34.177776: step 4084, loss 0.0118758, acc 1
2016-09-06T23:45:34.873311: step 4085, loss 0.0153597, acc 0.98
2016-09-06T23:45:35.571767: step 4086, loss 0.0438069, acc 0.98
2016-09-06T23:45:36.283464: step 4087, loss 0.00887976, acc 1
2016-09-06T23:45:36.986895: step 4088, loss 0.01339, acc 1
2016-09-06T23:45:37.658380: step 4089, loss 0.0295831, acc 0.98
2016-09-06T23:45:38.358646: step 4090, loss 0.00292319, acc 1
2016-09-06T23:45:39.046596: step 4091, loss 0.0131136, acc 1
2016-09-06T23:45:39.745476: step 4092, loss 0.0279702, acc 0.98
2016-09-06T23:45:40.443494: step 4093, loss 0.071274, acc 0.98
2016-09-06T23:45:41.118044: step 4094, loss 0.0156988, acc 0.98
2016-09-06T23:45:41.804981: step 4095, loss 0.0988718, acc 0.98
2016-09-06T23:45:42.479288: step 4096, loss 0.0882079, acc 0.98
2016-09-06T23:45:43.154763: step 4097, loss 0.0435636, acc 0.98
2016-09-06T23:45:43.852150: step 4098, loss 0.0144312, acc 1
2016-09-06T23:45:44.538538: step 4099, loss 0.00532544, acc 1
2016-09-06T23:45:45.251268: step 4100, loss 0.00391333, acc 1

Evaluation:
2016-09-06T23:45:48.359996: step 4100, loss 2.07698, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-4100

2016-09-06T23:45:50.006581: step 4101, loss 0.00140317, acc 1
2016-09-06T23:45:50.678777: step 4102, loss 0.069613, acc 0.96
2016-09-06T23:45:51.378986: step 4103, loss 0.0610501, acc 0.98
2016-09-06T23:45:52.057709: step 4104, loss 0.0536659, acc 0.98
2016-09-06T23:45:52.777916: step 4105, loss 5.74085e-05, acc 1
2016-09-06T23:45:53.502623: step 4106, loss 0.000727827, acc 1
2016-09-06T23:45:54.196375: step 4107, loss 0.00752014, acc 1
2016-09-06T23:45:54.890085: step 4108, loss 0.0140874, acc 1
2016-09-06T23:45:55.570182: step 4109, loss 0.0400314, acc 0.98
2016-09-06T23:45:56.288652: step 4110, loss 0.0184613, acc 1
2016-09-06T23:45:56.992866: step 4111, loss 0.000630304, acc 1
2016-09-06T23:45:57.659824: step 4112, loss 0.0139104, acc 1
2016-09-06T23:45:58.339403: step 4113, loss 0.0266255, acc 0.98
2016-09-06T23:45:59.017483: step 4114, loss 0.0329605, acc 0.98
2016-09-06T23:45:59.707141: step 4115, loss 0.0187972, acc 1
2016-09-06T23:46:00.423204: step 4116, loss 0.0020183, acc 1
2016-09-06T23:46:01.130570: step 4117, loss 0.0229212, acc 0.98
2016-09-06T23:46:01.814144: step 4118, loss 0.0260376, acc 1
2016-09-06T23:46:02.513507: step 4119, loss 0.172458, acc 0.96
2016-09-06T23:46:03.187783: step 4120, loss 0.0538379, acc 0.94
2016-09-06T23:46:03.862541: step 4121, loss 0.0231728, acc 0.98
2016-09-06T23:46:04.548740: step 4122, loss 0.0417601, acc 1
2016-09-06T23:46:05.210887: step 4123, loss 0.000169692, acc 1
2016-09-06T23:46:05.924904: step 4124, loss 0.0231555, acc 0.98
2016-09-06T23:46:06.591127: step 4125, loss 0.0312028, acc 0.98
2016-09-06T23:46:07.282212: step 4126, loss 0.014102, acc 1
2016-09-06T23:46:07.963710: step 4127, loss 0.0271446, acc 1
2016-09-06T23:46:08.630127: step 4128, loss 0.0227878, acc 1
2016-09-06T23:46:09.357944: step 4129, loss 0.0163079, acc 1
2016-09-06T23:46:10.017629: step 4130, loss 0.0194533, acc 1
2016-09-06T23:46:10.742162: step 4131, loss 0.0230084, acc 0.98
2016-09-06T23:46:11.431457: step 4132, loss 0.0690645, acc 0.98
2016-09-06T23:46:12.113640: step 4133, loss 0.0352459, acc 0.98
2016-09-06T23:46:12.799144: step 4134, loss 0.00121025, acc 1
2016-09-06T23:46:13.480289: step 4135, loss 0.124775, acc 0.98
2016-09-06T23:46:14.154925: step 4136, loss 0.0734774, acc 0.98
2016-09-06T23:46:14.809355: step 4137, loss 0.0383356, acc 0.98
2016-09-06T23:46:15.503008: step 4138, loss 0.0399159, acc 0.96
2016-09-06T23:46:16.173163: step 4139, loss 0.00256298, acc 1
2016-09-06T23:46:16.849948: step 4140, loss 0.203744, acc 0.96
2016-09-06T23:46:17.541213: step 4141, loss 0.0137412, acc 1
2016-09-06T23:46:18.228152: step 4142, loss 0.0153579, acc 1
2016-09-06T23:46:18.921965: step 4143, loss 0.0290462, acc 1
2016-09-06T23:46:19.607035: step 4144, loss 0.0296599, acc 0.98
2016-09-06T23:46:20.308557: step 4145, loss 0.0900444, acc 0.96
2016-09-06T23:46:20.988915: step 4146, loss 0.0395123, acc 0.98
2016-09-06T23:46:21.692557: step 4147, loss 0.00128249, acc 1
2016-09-06T23:46:22.390436: step 4148, loss 0.0409628, acc 0.96
2016-09-06T23:46:23.075792: step 4149, loss 0.016512, acc 0.98
2016-09-06T23:46:23.773942: step 4150, loss 0.0108093, acc 1
2016-09-06T23:46:24.442706: step 4151, loss 0.103666, acc 0.94
2016-09-06T23:46:25.163175: step 4152, loss 0.0546508, acc 0.98
2016-09-06T23:46:25.856308: step 4153, loss 0.0194798, acc 1
2016-09-06T23:46:26.535647: step 4154, loss 0.0168387, acc 0.98
2016-09-06T23:46:27.223146: step 4155, loss 0.00656304, acc 1
2016-09-06T23:46:27.910118: step 4156, loss 0.0792053, acc 0.96
2016-09-06T23:46:28.578816: step 4157, loss 0.0113277, acc 1
2016-09-06T23:46:29.253980: step 4158, loss 0.0251657, acc 0.98
2016-09-06T23:46:29.967227: step 4159, loss 0.0171357, acc 1
2016-09-06T23:46:30.650783: step 4160, loss 0.0335852, acc 1
2016-09-06T23:46:31.332830: step 4161, loss 0.0131489, acc 1
2016-09-06T23:46:32.030208: step 4162, loss 0.0519988, acc 0.96
2016-09-06T23:46:32.729415: step 4163, loss 0.0495104, acc 0.98
2016-09-06T23:46:33.446648: step 4164, loss 0.0224494, acc 1
2016-09-06T23:46:34.114744: step 4165, loss 0.0349906, acc 1
2016-09-06T23:46:34.824801: step 4166, loss 0.044277, acc 0.96
2016-09-06T23:46:35.518063: step 4167, loss 0.0122874, acc 1
2016-09-06T23:46:36.209918: step 4168, loss 0.00580077, acc 1
2016-09-06T23:46:36.899265: step 4169, loss 0.00156087, acc 1
2016-09-06T23:46:37.571028: step 4170, loss 0.00579325, acc 1
2016-09-06T23:46:38.263460: step 4171, loss 0.0556125, acc 0.96
2016-09-06T23:46:38.947607: step 4172, loss 0.0264042, acc 0.98
2016-09-06T23:46:39.640977: step 4173, loss 0.0402919, acc 0.96
2016-09-06T23:46:40.327689: step 4174, loss 0.0153844, acc 1
2016-09-06T23:46:41.009332: step 4175, loss 0.0447648, acc 0.98
2016-09-06T23:46:41.706457: step 4176, loss 0.0394015, acc 0.98
2016-09-06T23:46:42.406780: step 4177, loss 0.0496267, acc 0.98
2016-09-06T23:46:43.111909: step 4178, loss 0.0086628, acc 1
2016-09-06T23:46:43.775280: step 4179, loss 0.00256749, acc 1
2016-09-06T23:46:44.472061: step 4180, loss 0.066884, acc 0.96
2016-09-06T23:46:45.140633: step 4181, loss 0.045814, acc 0.98
2016-09-06T23:46:45.826445: step 4182, loss 0.00218933, acc 1
2016-09-06T23:46:46.522194: step 4183, loss 0.0439803, acc 0.98
2016-09-06T23:46:47.200959: step 4184, loss 0.0154024, acc 0.98
2016-09-06T23:46:47.907428: step 4185, loss 0.000176325, acc 1
2016-09-06T23:46:48.582665: step 4186, loss 0.0182837, acc 0.98
2016-09-06T23:46:49.263222: step 4187, loss 0.00866641, acc 1
2016-09-06T23:46:49.943410: step 4188, loss 0.000817562, acc 1
2016-09-06T23:46:50.640443: step 4189, loss 0.000122968, acc 1
2016-09-06T23:46:51.328872: step 4190, loss 0.0335639, acc 0.98
2016-09-06T23:46:52.040468: step 4191, loss 0.0201758, acc 1
2016-09-06T23:46:52.739172: step 4192, loss 0.0909344, acc 0.96
2016-09-06T23:46:53.422807: step 4193, loss 0.0012814, acc 1
2016-09-06T23:46:54.110171: step 4194, loss 0.00679253, acc 1
2016-09-06T23:46:54.829768: step 4195, loss 0.031806, acc 1
2016-09-06T23:46:55.525815: step 4196, loss 0.0184519, acc 0.98
2016-09-06T23:46:56.215139: step 4197, loss 0.0306936, acc 1
2016-09-06T23:46:56.881653: step 4198, loss 0.0121648, acc 1
2016-09-06T23:46:57.578199: step 4199, loss 0.0184922, acc 1
2016-09-06T23:46:58.265854: step 4200, loss 0.081102, acc 0.96

Evaluation:
2016-09-06T23:47:01.463203: step 4200, loss 2.20204, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-4200

2016-09-06T23:47:03.083191: step 4201, loss 0.182438, acc 0.98
2016-09-06T23:47:03.796317: step 4202, loss 0.0288789, acc 0.98
2016-09-06T23:47:04.504453: step 4203, loss 0.0129624, acc 1
2016-09-06T23:47:05.186313: step 4204, loss 0.0136824, acc 1
2016-09-06T23:47:05.870199: step 4205, loss 0.012329, acc 1
2016-09-06T23:47:06.558195: step 4206, loss 0.177178, acc 0.96
2016-09-06T23:47:07.261133: step 4207, loss 0.0728235, acc 0.96
2016-09-06T23:47:07.926917: step 4208, loss 0.073633, acc 0.98
2016-09-06T23:47:08.628899: step 4209, loss 0.0505196, acc 0.98
2016-09-06T23:47:09.313923: step 4210, loss 0.0363888, acc 0.98
2016-09-06T23:47:10.019725: step 4211, loss 0.0233484, acc 0.98
2016-09-06T23:47:10.712866: step 4212, loss 0.0242014, acc 1
2016-09-06T23:47:11.409679: step 4213, loss 0.248035, acc 0.98
2016-09-06T23:47:12.105643: step 4214, loss 0.0170699, acc 1
2016-09-06T23:47:12.772654: step 4215, loss 0.0255079, acc 1
2016-09-06T23:47:13.456526: step 4216, loss 0.0213423, acc 1
2016-09-06T23:47:14.136757: step 4217, loss 0.026914, acc 0.98
2016-09-06T23:47:14.814666: step 4218, loss 0.0141065, acc 1
2016-09-06T23:47:15.513528: step 4219, loss 0.057105, acc 0.98
2016-09-06T23:47:16.193687: step 4220, loss 0.00306171, acc 1
2016-09-06T23:47:16.889159: step 4221, loss 0.0696326, acc 0.98
2016-09-06T23:47:17.573860: step 4222, loss 0.0433056, acc 0.98
2016-09-06T23:47:18.276199: step 4223, loss 0.0275394, acc 0.98
2016-09-06T23:47:18.910524: step 4224, loss 0.0782981, acc 0.954545
2016-09-06T23:47:19.613353: step 4225, loss 0.0196572, acc 1
2016-09-06T23:47:20.290493: step 4226, loss 0.0294324, acc 0.98
2016-09-06T23:47:20.973759: step 4227, loss 0.186451, acc 0.94
2016-09-06T23:47:21.682411: step 4228, loss 0.101941, acc 0.96
2016-09-06T23:47:22.359572: step 4229, loss 0.0199654, acc 1
2016-09-06T23:47:23.041140: step 4230, loss 0.0821816, acc 0.96
2016-09-06T23:47:23.710049: step 4231, loss 0.0173197, acc 0.98
2016-09-06T23:47:24.400556: step 4232, loss 0.0235522, acc 1
2016-09-06T23:47:25.102771: step 4233, loss 0.0172385, acc 1
2016-09-06T23:47:25.778336: step 4234, loss 0.0289294, acc 0.98
2016-09-06T23:47:26.493230: step 4235, loss 0.0397267, acc 0.98
2016-09-06T23:47:27.155174: step 4236, loss 0.0189673, acc 0.98
2016-09-06T23:47:27.858510: step 4237, loss 0.0467576, acc 0.96
2016-09-06T23:47:28.532130: step 4238, loss 0.0855381, acc 0.96
2016-09-06T23:47:29.232544: step 4239, loss 0.00778467, acc 1
2016-09-06T23:47:29.938055: step 4240, loss 0.0105168, acc 1
2016-09-06T23:47:30.643128: step 4241, loss 0.0136285, acc 1
2016-09-06T23:47:31.341678: step 4242, loss 0.064201, acc 0.96
2016-09-06T23:47:32.028559: step 4243, loss 0.0845444, acc 0.98
2016-09-06T23:47:32.707353: step 4244, loss 0.00542226, acc 1
2016-09-06T23:47:33.387558: step 4245, loss 0.0163769, acc 1
2016-09-06T23:47:34.067577: step 4246, loss 0.0171795, acc 1
2016-09-06T23:47:34.739166: step 4247, loss 0.0163167, acc 1
2016-09-06T23:47:35.415279: step 4248, loss 0.00314044, acc 1
2016-09-06T23:47:36.101805: step 4249, loss 0.0231968, acc 0.98
2016-09-06T23:47:36.763965: step 4250, loss 0.0396708, acc 0.98
2016-09-06T23:47:37.450548: step 4251, loss 0.00328001, acc 1
2016-09-06T23:47:38.155748: step 4252, loss 0.0112776, acc 1
2016-09-06T23:47:38.831802: step 4253, loss 0.159254, acc 0.94
2016-09-06T23:47:39.516302: step 4254, loss 0.0930531, acc 0.96
2016-09-06T23:47:40.225205: step 4255, loss 0.00241124, acc 1
2016-09-06T23:47:40.911088: step 4256, loss 0.0195419, acc 1
2016-09-06T23:47:41.609729: step 4257, loss 0.0498357, acc 0.98
2016-09-06T23:47:42.278123: step 4258, loss 0.0341696, acc 0.96
2016-09-06T23:47:42.961538: step 4259, loss 0.00342823, acc 1
2016-09-06T23:47:43.644649: step 4260, loss 0.00566458, acc 1
2016-09-06T23:47:44.340439: step 4261, loss 0.0377514, acc 0.98
2016-09-06T23:47:45.034546: step 4262, loss 0.0357458, acc 0.98
2016-09-06T23:47:45.733907: step 4263, loss 0.01812, acc 1
2016-09-06T23:47:46.394946: step 4264, loss 0.000315714, acc 1
2016-09-06T23:47:47.092127: step 4265, loss 0.0359436, acc 0.98
2016-09-06T23:47:47.764810: step 4266, loss 0.0259813, acc 0.98
2016-09-06T23:47:48.462455: step 4267, loss 0.0477266, acc 0.98
2016-09-06T23:47:49.164294: step 4268, loss 0.0113848, acc 1
2016-09-06T23:47:49.848182: step 4269, loss 0.0524483, acc 0.96
2016-09-06T23:47:50.542018: step 4270, loss 0.0270544, acc 1
2016-09-06T23:47:51.212247: step 4271, loss 0.00994091, acc 1
2016-09-06T23:47:51.900381: step 4272, loss 0.00341995, acc 1
2016-09-06T23:47:52.598371: step 4273, loss 0.00212938, acc 1
2016-09-06T23:47:53.273293: step 4274, loss 0.0539392, acc 0.96
2016-09-06T23:47:53.965598: step 4275, loss 0.112071, acc 0.98
2016-09-06T23:47:54.663429: step 4276, loss 0.000590949, acc 1
2016-09-06T23:47:55.360448: step 4277, loss 0.0131075, acc 1
2016-09-06T23:47:56.023974: step 4278, loss 0.0223919, acc 0.98
2016-09-06T23:47:56.713473: step 4279, loss 0.0378189, acc 0.98
2016-09-06T23:47:57.379124: step 4280, loss 0.0663238, acc 0.98
2016-09-06T23:47:58.052719: step 4281, loss 0.0150242, acc 1
2016-09-06T23:47:58.744493: step 4282, loss 0.220997, acc 0.96
2016-09-06T23:47:59.423765: step 4283, loss 0.033867, acc 1
2016-09-06T23:48:00.120296: step 4284, loss 0.00442497, acc 1
2016-09-06T23:48:00.834232: step 4285, loss 0.0720438, acc 0.96
2016-09-06T23:48:01.516835: step 4286, loss 0.0207041, acc 1
2016-09-06T23:48:02.202836: step 4287, loss 0.0795434, acc 0.96
2016-09-06T23:48:02.877094: step 4288, loss 0.0178015, acc 0.98
2016-09-06T23:48:03.561768: step 4289, loss 0.0068307, acc 1
2016-09-06T23:48:04.250010: step 4290, loss 0.0215336, acc 0.98
2016-09-06T23:48:04.963672: step 4291, loss 0.00649908, acc 1
2016-09-06T23:48:05.655042: step 4292, loss 0.000732091, acc 1
2016-09-06T23:48:06.326340: step 4293, loss 0.0262824, acc 1
2016-09-06T23:48:07.012332: step 4294, loss 0.0649238, acc 0.96
2016-09-06T23:48:07.703895: step 4295, loss 0.0199161, acc 1
2016-09-06T23:48:08.382448: step 4296, loss 0.01007, acc 1
2016-09-06T23:48:09.073395: step 4297, loss 0.000910994, acc 1
2016-09-06T23:48:09.764971: step 4298, loss 0.00243945, acc 1
2016-09-06T23:48:10.448298: step 4299, loss 0.0277622, acc 0.98
2016-09-06T23:48:11.129588: step 4300, loss 0.0395627, acc 0.98

Evaluation:
2016-09-06T23:48:14.246179: step 4300, loss 1.75324, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-4300

2016-09-06T23:48:15.981116: step 4301, loss 0.0859062, acc 0.94
2016-09-06T23:48:16.663118: step 4302, loss 0.0142122, acc 1
2016-09-06T23:48:17.348900: step 4303, loss 0.0159526, acc 1
2016-09-06T23:48:18.033737: step 4304, loss 0.060974, acc 0.96
2016-09-06T23:48:18.715840: step 4305, loss 0.0568367, acc 0.96
2016-09-06T23:48:19.415151: step 4306, loss 0.0520724, acc 0.98
2016-09-06T23:48:20.082787: step 4307, loss 0.0718425, acc 0.96
2016-09-06T23:48:20.767711: step 4308, loss 0.00562233, acc 1
2016-09-06T23:48:21.456975: step 4309, loss 0.0312324, acc 0.98
2016-09-06T23:48:22.135758: step 4310, loss 0.0167949, acc 1
2016-09-06T23:48:22.838666: step 4311, loss 0.0880887, acc 0.94
2016-09-06T23:48:23.527321: step 4312, loss 0.00392144, acc 1
2016-09-06T23:48:24.215049: step 4313, loss 0.0058043, acc 1
2016-09-06T23:48:24.891147: step 4314, loss 0.0129612, acc 1
2016-09-06T23:48:25.600324: step 4315, loss 0.00306211, acc 1
2016-09-06T23:48:26.290455: step 4316, loss 0.0208767, acc 1
2016-09-06T23:48:26.987802: step 4317, loss 0.0238317, acc 0.98
2016-09-06T23:48:27.695461: step 4318, loss 0.0241028, acc 0.98
2016-09-06T23:48:28.345570: step 4319, loss 0.0302287, acc 1
2016-09-06T23:48:29.082590: step 4320, loss 0.0177371, acc 0.98
2016-09-06T23:48:29.766828: step 4321, loss 0.148599, acc 0.96
2016-09-06T23:48:30.453429: step 4322, loss 0.0111762, acc 1
2016-09-06T23:48:31.151008: step 4323, loss 0.075997, acc 0.96
2016-09-06T23:48:31.827121: step 4324, loss 0.0461425, acc 0.98
2016-09-06T23:48:32.512481: step 4325, loss 0.0096878, acc 1
2016-09-06T23:48:33.171125: step 4326, loss 0.00876667, acc 1
2016-09-06T23:48:33.863657: step 4327, loss 0.0635898, acc 0.98
2016-09-06T23:48:34.545064: step 4328, loss 0.0530107, acc 0.98
2016-09-06T23:48:35.228227: step 4329, loss 0.0291927, acc 0.98
2016-09-06T23:48:35.916379: step 4330, loss 0.000831466, acc 1
2016-09-06T23:48:36.607935: step 4331, loss 0.108311, acc 0.98
2016-09-06T23:48:37.317146: step 4332, loss 0.0312596, acc 0.98
2016-09-06T23:48:38.011508: step 4333, loss 0.0510604, acc 0.98
2016-09-06T23:48:38.708917: step 4334, loss 0.0578452, acc 0.96
2016-09-06T23:48:39.369398: step 4335, loss 0.0362387, acc 0.96
2016-09-06T23:48:40.042366: step 4336, loss 0.00769422, acc 1
2016-09-06T23:48:40.730383: step 4337, loss 0.0715138, acc 0.96
2016-09-06T23:48:41.398925: step 4338, loss 0.0374358, acc 0.98
2016-09-06T23:48:42.081280: step 4339, loss 0.0158926, acc 1
2016-09-06T23:48:42.774723: step 4340, loss 0.0663242, acc 0.96
2016-09-06T23:48:43.467717: step 4341, loss 0.00281605, acc 1
2016-09-06T23:48:44.129356: step 4342, loss 0.0348304, acc 0.98
2016-09-06T23:48:44.785491: step 4343, loss 0.00856595, acc 1
2016-09-06T23:48:45.481619: step 4344, loss 0.0132524, acc 1
2016-09-06T23:48:46.157378: step 4345, loss 0.035066, acc 0.98
2016-09-06T23:48:46.876532: step 4346, loss 0.0585357, acc 0.98
2016-09-06T23:48:47.564580: step 4347, loss 0.0134825, acc 1
2016-09-06T23:48:48.256702: step 4348, loss 0.0156378, acc 1
2016-09-06T23:48:48.951538: step 4349, loss 0.0319562, acc 0.98
2016-09-06T23:48:49.635631: step 4350, loss 0.0238603, acc 0.98
2016-09-06T23:48:50.306473: step 4351, loss 0.0207887, acc 1
2016-09-06T23:48:50.984854: step 4352, loss 0.0952931, acc 0.98
2016-09-06T23:48:51.663430: step 4353, loss 0.0180878, acc 1
2016-09-06T23:48:52.381776: step 4354, loss 0.0704739, acc 0.96
2016-09-06T23:48:53.083569: step 4355, loss 0.0479451, acc 1
2016-09-06T23:48:53.765125: step 4356, loss 0.0535144, acc 0.96
2016-09-06T23:48:54.454945: step 4357, loss 0.108178, acc 0.92
2016-09-06T23:48:55.137477: step 4358, loss 0.0241176, acc 1
2016-09-06T23:48:55.826566: step 4359, loss 0.0376942, acc 0.98
2016-09-06T23:48:56.509976: step 4360, loss 0.0806034, acc 0.98
2016-09-06T23:48:57.183168: step 4361, loss 0.0834048, acc 0.98
2016-09-06T23:48:57.894437: step 4362, loss 0.00703865, acc 1
2016-09-06T23:48:58.576992: step 4363, loss 0.0517507, acc 0.98
2016-09-06T23:48:59.256228: step 4364, loss 0.00254263, acc 1
2016-09-06T23:48:59.958835: step 4365, loss 0.101891, acc 0.96
2016-09-06T23:49:00.687681: step 4366, loss 0.0100898, acc 1
2016-09-06T23:49:01.384129: step 4367, loss 0.167733, acc 0.98
2016-09-06T23:49:02.063744: step 4368, loss 0.015909, acc 1
2016-09-06T23:49:02.775690: step 4369, loss 0.0504597, acc 0.98
2016-09-06T23:49:03.471086: step 4370, loss 0.0134779, acc 1
2016-09-06T23:49:04.145824: step 4371, loss 0.0130675, acc 1
2016-09-06T23:49:04.830899: step 4372, loss 0.0251991, acc 0.98
2016-09-06T23:49:05.513318: step 4373, loss 0.0487236, acc 0.98
2016-09-06T23:49:06.191836: step 4374, loss 0.00548317, acc 1
2016-09-06T23:49:06.850417: step 4375, loss 0.0546856, acc 0.98
2016-09-06T23:49:07.559730: step 4376, loss 0.0153352, acc 1
2016-09-06T23:49:08.248631: step 4377, loss 0.0915572, acc 0.94
2016-09-06T23:49:08.924698: step 4378, loss 0.0228387, acc 1
2016-09-06T23:49:09.612437: step 4379, loss 0.0421919, acc 0.98
2016-09-06T23:49:10.310985: step 4380, loss 0.0235747, acc 0.98
2016-09-06T23:49:10.993877: step 4381, loss 0.0619624, acc 0.96
2016-09-06T23:49:11.678509: step 4382, loss 0.0376144, acc 0.98
2016-09-06T23:49:12.387427: step 4383, loss 0.0217848, acc 1
2016-09-06T23:49:13.058231: step 4384, loss 0.042641, acc 0.98
2016-09-06T23:49:13.774879: step 4385, loss 0.0713683, acc 0.96
2016-09-06T23:49:14.456849: step 4386, loss 0.0395705, acc 0.98
2016-09-06T23:49:15.165241: step 4387, loss 0.0468265, acc 0.96
2016-09-06T23:49:15.889464: step 4388, loss 0.02223, acc 1
2016-09-06T23:49:16.571837: step 4389, loss 0.0204272, acc 1
2016-09-06T23:49:17.253116: step 4390, loss 0.0702646, acc 0.96
2016-09-06T23:49:17.941372: step 4391, loss 0.00315814, acc 1
2016-09-06T23:49:18.622455: step 4392, loss 0.0181408, acc 1
2016-09-06T23:49:19.320134: step 4393, loss 0.0495247, acc 0.96
2016-09-06T23:49:20.014819: step 4394, loss 0.0298674, acc 1
2016-09-06T23:49:20.731765: step 4395, loss 0.00859027, acc 1
2016-09-06T23:49:21.411857: step 4396, loss 0.0062452, acc 1
2016-09-06T23:49:22.095331: step 4397, loss 0.0697564, acc 0.92
2016-09-06T23:49:22.800352: step 4398, loss 0.173209, acc 0.96
2016-09-06T23:49:23.483831: step 4399, loss 0.0151481, acc 1
2016-09-06T23:49:24.170406: step 4400, loss 0.0320637, acc 0.98

Evaluation:
2016-09-06T23:49:27.296391: step 4400, loss 1.88376, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-4400

2016-09-06T23:49:28.946115: step 4401, loss 0.102151, acc 0.96
2016-09-06T23:49:29.637265: step 4402, loss 0.0201293, acc 1
2016-09-06T23:49:30.335882: step 4403, loss 0.00232267, acc 1
2016-09-06T23:49:31.018716: step 4404, loss 0.00634653, acc 1
2016-09-06T23:49:31.722147: step 4405, loss 0.0209368, acc 0.98
2016-09-06T23:49:32.402537: step 4406, loss 0.0493607, acc 0.98
2016-09-06T23:49:33.103587: step 4407, loss 0.0221332, acc 1
2016-09-06T23:49:33.780037: step 4408, loss 0.03972, acc 0.98
2016-09-06T23:49:34.472775: step 4409, loss 0.0203943, acc 1
2016-09-06T23:49:35.166401: step 4410, loss 0.0277693, acc 0.98
2016-09-06T23:49:35.838065: step 4411, loss 0.0339091, acc 1
2016-09-06T23:49:36.526061: step 4412, loss 0.000418268, acc 1
2016-09-06T23:49:37.196147: step 4413, loss 0.0340428, acc 0.98
2016-09-06T23:49:37.882552: step 4414, loss 0.129258, acc 0.94
2016-09-06T23:49:38.572951: step 4415, loss 0.0155681, acc 1
2016-09-06T23:49:39.242295: step 4416, loss 0.000181364, acc 1
2016-09-06T23:49:39.976027: step 4417, loss 0.0207449, acc 1
2016-09-06T23:49:40.648786: step 4418, loss 0.0609968, acc 0.94
2016-09-06T23:49:41.319950: step 4419, loss 0.0659907, acc 0.96
2016-09-06T23:49:42.002329: step 4420, loss 0.0691488, acc 0.96
2016-09-06T23:49:42.684694: step 4421, loss 0.0944278, acc 0.98
2016-09-06T23:49:43.375389: step 4422, loss 0.107422, acc 0.96
2016-09-06T23:49:44.042967: step 4423, loss 0.0172149, acc 0.98
2016-09-06T23:49:44.738376: step 4424, loss 0.0226428, acc 1
2016-09-06T23:49:45.414385: step 4425, loss 0.0885894, acc 0.92
2016-09-06T23:49:46.109109: step 4426, loss 0.0354208, acc 0.98
2016-09-06T23:49:46.801618: step 4427, loss 0.0580072, acc 0.94
2016-09-06T23:49:47.507745: step 4428, loss 0.0187572, acc 1
2016-09-06T23:49:48.194498: step 4429, loss 0.0318749, acc 0.98
2016-09-06T23:49:48.885844: step 4430, loss 0.0402185, acc 0.98
2016-09-06T23:49:49.593608: step 4431, loss 0.0659904, acc 0.98
2016-09-06T23:49:50.283680: step 4432, loss 0.00973864, acc 1
2016-09-06T23:49:50.965910: step 4433, loss 0.0504962, acc 0.98
2016-09-06T23:49:51.665808: step 4434, loss 0.00567402, acc 1
2016-09-06T23:49:52.341868: step 4435, loss 0.0569729, acc 0.96
2016-09-06T23:49:53.026698: step 4436, loss 0.0551245, acc 0.96
2016-09-06T23:49:53.681350: step 4437, loss 0.0744342, acc 0.96
2016-09-06T23:49:54.386218: step 4438, loss 0.00718166, acc 1
2016-09-06T23:49:55.066087: step 4439, loss 0.019538, acc 0.98
2016-09-06T23:49:55.766803: step 4440, loss 0.0300298, acc 1
2016-09-06T23:49:56.458351: step 4441, loss 0.0186456, acc 0.98
2016-09-06T23:49:57.154983: step 4442, loss 0.0323649, acc 0.98
2016-09-06T23:49:57.827982: step 4443, loss 0.00535664, acc 1
2016-09-06T23:49:58.520489: step 4444, loss 0.00132397, acc 1
2016-09-06T23:49:59.221096: step 4445, loss 0.0672682, acc 0.96
2016-09-06T23:49:59.894987: step 4446, loss 0.0347327, acc 1
2016-09-06T23:50:00.580685: step 4447, loss 0.0290979, acc 0.98
2016-09-06T23:50:01.289946: step 4448, loss 0.0400207, acc 0.98
2016-09-06T23:50:01.971330: step 4449, loss 0.0435723, acc 0.96
2016-09-06T23:50:02.655122: step 4450, loss 0.0359827, acc 1
2016-09-06T23:50:03.325555: step 4451, loss 0.00134625, acc 1
2016-09-06T23:50:04.026319: step 4452, loss 0.0245854, acc 1
2016-09-06T23:50:04.703558: step 4453, loss 0.00543314, acc 1
2016-09-06T23:50:05.369101: step 4454, loss 0.0362564, acc 0.98
2016-09-06T23:50:06.074666: step 4455, loss 0.00585053, acc 1
2016-09-06T23:50:06.749242: step 4456, loss 0.0106977, acc 1
2016-09-06T23:50:07.432330: step 4457, loss 0.0454633, acc 0.98
2016-09-06T23:50:08.111458: step 4458, loss 0.0112556, acc 1
2016-09-06T23:50:08.810912: step 4459, loss 0.00412107, acc 1
2016-09-06T23:50:09.485166: step 4460, loss 0.00227333, acc 1
2016-09-06T23:50:10.193332: step 4461, loss 0.0342871, acc 0.98
2016-09-06T23:50:10.875379: step 4462, loss 0.0251057, acc 0.98
2016-09-06T23:50:11.561790: step 4463, loss 0.0154963, acc 1
2016-09-06T23:50:12.255613: step 4464, loss 0.0514584, acc 0.98
2016-09-06T23:50:12.951767: step 4465, loss 0.00968531, acc 1
2016-09-06T23:50:13.661798: step 4466, loss 0.109775, acc 0.98
2016-09-06T23:50:14.346683: step 4467, loss 0.027781, acc 0.98
2016-09-06T23:50:15.042139: step 4468, loss 0.025149, acc 0.98
2016-09-06T23:50:15.734101: step 4469, loss 0.0376317, acc 0.98
2016-09-06T23:50:16.430942: step 4470, loss 0.00830331, acc 1
2016-09-06T23:50:17.121419: step 4471, loss 0.0205484, acc 1
2016-09-06T23:50:17.830361: step 4472, loss 0.0131839, acc 1
2016-09-06T23:50:18.539179: step 4473, loss 0.0388567, acc 0.98
2016-09-06T23:50:19.238672: step 4474, loss 0.0266859, acc 0.98
2016-09-06T23:50:19.928291: step 4475, loss 0.0127487, acc 1
2016-09-06T23:50:20.644064: step 4476, loss 0.00334669, acc 1
2016-09-06T23:50:21.350239: step 4477, loss 0.0942144, acc 0.96
2016-09-06T23:50:22.065750: step 4478, loss 0.00373019, acc 1
2016-09-06T23:50:22.716279: step 4479, loss 0.0190318, acc 0.98
2016-09-06T23:50:23.428528: step 4480, loss 0.0492148, acc 0.98
2016-09-06T23:50:24.115526: step 4481, loss 0.0614433, acc 0.98
2016-09-06T23:50:24.807145: step 4482, loss 0.0814097, acc 0.98
2016-09-06T23:50:25.497108: step 4483, loss 0.00661592, acc 1
2016-09-06T23:50:26.185915: step 4484, loss 0.0410132, acc 0.98
2016-09-06T23:50:26.903980: step 4485, loss 0.00504417, acc 1
2016-09-06T23:50:27.580897: step 4486, loss 0.0176787, acc 1
2016-09-06T23:50:28.262132: step 4487, loss 0.0234058, acc 1
2016-09-06T23:50:28.945361: step 4488, loss 0.00287122, acc 1
2016-09-06T23:50:29.615301: step 4489, loss 0.0101919, acc 1
2016-09-06T23:50:30.317108: step 4490, loss 0.0602597, acc 0.96
2016-09-06T23:50:31.006496: step 4491, loss 0.087969, acc 0.98
2016-09-06T23:50:31.702175: step 4492, loss 0.0585144, acc 0.94
2016-09-06T23:50:32.387274: step 4493, loss 0.00234628, acc 1
2016-09-06T23:50:33.058259: step 4494, loss 0.00275287, acc 1
2016-09-06T23:50:33.742578: step 4495, loss 0.0239376, acc 1
2016-09-06T23:50:34.426253: step 4496, loss 0.0145859, acc 1
2016-09-06T23:50:35.153921: step 4497, loss 0.0480678, acc 0.96
2016-09-06T23:50:35.823441: step 4498, loss 0.00666136, acc 1
2016-09-06T23:50:36.526743: step 4499, loss 0.0401361, acc 0.98
2016-09-06T23:50:37.203562: step 4500, loss 0.0589571, acc 0.96

Evaluation:
2016-09-06T23:50:40.373803: step 4500, loss 2.01566, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-4500

2016-09-06T23:50:42.056236: step 4501, loss 0.0333198, acc 0.98
2016-09-06T23:50:42.743721: step 4502, loss 0.0743887, acc 0.96
2016-09-06T23:50:43.422923: step 4503, loss 0.0400787, acc 0.98
2016-09-06T23:50:44.113559: step 4504, loss 0.000682236, acc 1
2016-09-06T23:50:44.796891: step 4505, loss 0.0122518, acc 1
2016-09-06T23:50:45.475927: step 4506, loss 0.00355118, acc 1
2016-09-06T23:50:46.179984: step 4507, loss 0.00969698, acc 1
2016-09-06T23:50:46.866496: step 4508, loss 0.0694686, acc 0.96
2016-09-06T23:50:47.542909: step 4509, loss 0.00113276, acc 1
2016-09-06T23:50:48.244372: step 4510, loss 0.0126332, acc 1
2016-09-06T23:50:48.935228: step 4511, loss 0.0358605, acc 1
2016-09-06T23:50:49.622035: step 4512, loss 0.0090069, acc 1
2016-09-06T23:50:50.302962: step 4513, loss 0.0369069, acc 0.98
2016-09-06T23:50:51.010061: step 4514, loss 0.019353, acc 1
2016-09-06T23:50:51.692967: step 4515, loss 0.0106638, acc 1
2016-09-06T23:50:52.364916: step 4516, loss 0.0169832, acc 0.98
2016-09-06T23:50:53.040250: step 4517, loss 0.0339596, acc 0.98
2016-09-06T23:50:53.720779: step 4518, loss 0.0908803, acc 0.98
2016-09-06T23:50:54.398456: step 4519, loss 0.0320927, acc 0.98
2016-09-06T23:50:55.079571: step 4520, loss 0.0771681, acc 0.96
2016-09-06T23:50:55.800779: step 4521, loss 0.0348903, acc 0.98
2016-09-06T23:50:56.488003: step 4522, loss 0.00561076, acc 1
2016-09-06T23:50:57.174148: step 4523, loss 0.0316811, acc 0.98
2016-09-06T23:50:57.878379: step 4524, loss 0.040415, acc 0.98
2016-09-06T23:50:58.556852: step 4525, loss 0.0185123, acc 1
2016-09-06T23:50:59.256369: step 4526, loss 0.0225558, acc 0.98
2016-09-06T23:50:59.953456: step 4527, loss 0.0109687, acc 1
2016-09-06T23:51:00.693820: step 4528, loss 0.0280245, acc 0.98
2016-09-06T23:51:01.377376: step 4529, loss 0.166237, acc 0.98
2016-09-06T23:51:02.057019: step 4530, loss 0.0343235, acc 0.96
2016-09-06T23:51:02.749087: step 4531, loss 0.0296304, acc 0.98
2016-09-06T23:51:03.423173: step 4532, loss 0.025358, acc 0.98
2016-09-06T23:51:04.114007: step 4533, loss 0.00927207, acc 1
2016-09-06T23:51:04.792523: step 4534, loss 0.004119, acc 1
2016-09-06T23:51:05.493848: step 4535, loss 0.0435972, acc 0.96
2016-09-06T23:51:06.173563: step 4536, loss 0.022284, acc 1
2016-09-06T23:51:06.849630: step 4537, loss 0.0429955, acc 0.98
2016-09-06T23:51:07.526416: step 4538, loss 0.0387942, acc 0.96
2016-09-06T23:51:08.213719: step 4539, loss 0.000231634, acc 1
2016-09-06T23:51:08.909313: step 4540, loss 0.0230412, acc 1
2016-09-06T23:51:09.577597: step 4541, loss 0.0056198, acc 1
2016-09-06T23:51:10.288346: step 4542, loss 0.0681778, acc 0.94
2016-09-06T23:51:10.978632: step 4543, loss 0.00333566, acc 1
2016-09-06T23:51:11.671798: step 4544, loss 0.0435567, acc 0.98
2016-09-06T23:51:12.354616: step 4545, loss 0.00283478, acc 1
2016-09-06T23:51:13.038412: step 4546, loss 0.030702, acc 0.98
2016-09-06T23:51:13.726492: step 4547, loss 0.0400101, acc 0.96
2016-09-06T23:51:14.390618: step 4548, loss 0.0645987, acc 0.98
2016-09-06T23:51:15.080611: step 4549, loss 0.0276052, acc 1
2016-09-06T23:51:15.774390: step 4550, loss 0.0263275, acc 1
2016-09-06T23:51:16.460380: step 4551, loss 0.000419368, acc 1
2016-09-06T23:51:17.135327: step 4552, loss 0.0178684, acc 1
2016-09-06T23:51:17.837094: step 4553, loss 0.0080832, acc 1
2016-09-06T23:51:18.509203: step 4554, loss 0.000235379, acc 1
2016-09-06T23:51:19.170995: step 4555, loss 0.0227758, acc 0.98
2016-09-06T23:51:19.858415: step 4556, loss 5.61855e-05, acc 1
2016-09-06T23:51:20.533218: step 4557, loss 0.0157134, acc 1
2016-09-06T23:51:21.212245: step 4558, loss 0.00125657, acc 1
2016-09-06T23:51:21.887774: step 4559, loss 0.0024683, acc 1
2016-09-06T23:51:22.593379: step 4560, loss 0.0103042, acc 1
2016-09-06T23:51:23.301812: step 4561, loss 0.0215487, acc 0.98
2016-09-06T23:51:23.970526: step 4562, loss 0.0377747, acc 0.98
2016-09-06T23:51:24.672444: step 4563, loss 0.00221532, acc 1
2016-09-06T23:51:25.342160: step 4564, loss 0.016674, acc 0.98
2016-09-06T23:51:26.024511: step 4565, loss 0.0519594, acc 0.96
2016-09-06T23:51:26.705166: step 4566, loss 0.00134031, acc 1
2016-09-06T23:51:27.384269: step 4567, loss 0.0657446, acc 0.98
2016-09-06T23:51:28.069203: step 4568, loss 0.0385588, acc 1
2016-09-06T23:51:28.760167: step 4569, loss 0.00510573, acc 1
2016-09-06T23:51:29.456379: step 4570, loss 0.00113336, acc 1
2016-09-06T23:51:30.120391: step 4571, loss 0.0298108, acc 0.98
2016-09-06T23:51:30.814562: step 4572, loss 8.77026e-05, acc 1
2016-09-06T23:51:31.493924: step 4573, loss 0.0022803, acc 1
2016-09-06T23:51:32.203182: step 4574, loss 0.0150895, acc 0.98
2016-09-06T23:51:32.918326: step 4575, loss 0.000779088, acc 1
2016-09-06T23:51:33.590237: step 4576, loss 0.0586082, acc 0.98
2016-09-06T23:51:34.281571: step 4577, loss 0.0168905, acc 0.98
2016-09-06T23:51:34.967406: step 4578, loss 0.251698, acc 0.98
2016-09-06T23:51:35.661864: step 4579, loss 0.070806, acc 0.98
2016-09-06T23:51:36.340650: step 4580, loss 0.00611947, acc 1
2016-09-06T23:51:37.036329: step 4581, loss 0.0715094, acc 0.98
2016-09-06T23:51:37.728460: step 4582, loss 0.0255476, acc 0.98
2016-09-06T23:51:38.399490: step 4583, loss 0.00708858, acc 1
2016-09-06T23:51:39.100791: step 4584, loss 0.0442123, acc 0.98
2016-09-06T23:51:39.785025: step 4585, loss 0.0344703, acc 0.98
2016-09-06T23:51:40.470033: step 4586, loss 0.0332033, acc 1
2016-09-06T23:51:41.170436: step 4587, loss 0.00725893, acc 1
2016-09-06T23:51:41.855606: step 4588, loss 0.0136243, acc 1
2016-09-06T23:51:42.560241: step 4589, loss 0.0154387, acc 0.98
2016-09-06T23:51:43.237269: step 4590, loss 0.0300837, acc 0.98
2016-09-06T23:51:43.952795: step 4591, loss 0.000633306, acc 1
2016-09-06T23:51:44.649335: step 4592, loss 0.0265741, acc 0.98
2016-09-06T23:51:45.313491: step 4593, loss 0.0231862, acc 0.98
2016-09-06T23:51:45.986873: step 4594, loss 0.0188126, acc 1
2016-09-06T23:51:46.672261: step 4595, loss 0.000763962, acc 1
2016-09-06T23:51:47.370710: step 4596, loss 0.0515995, acc 0.98
2016-09-06T23:51:48.033083: step 4597, loss 0.0215232, acc 0.98
2016-09-06T23:51:48.755227: step 4598, loss 0.00288121, acc 1
2016-09-06T23:51:49.435780: step 4599, loss 9.93948e-05, acc 1
2016-09-06T23:51:50.124734: step 4600, loss 0.0401799, acc 1

Evaluation:
2016-09-06T23:51:53.267229: step 4600, loss 2.16265, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-4600

2016-09-06T23:51:55.013315: step 4601, loss 0.0189384, acc 0.98
2016-09-06T23:51:55.697510: step 4602, loss 0.146169, acc 0.94
2016-09-06T23:51:56.372383: step 4603, loss 0.00324747, acc 1
2016-09-06T23:51:57.077878: step 4604, loss 0.00836948, acc 1
2016-09-06T23:51:57.742991: step 4605, loss 0.0360945, acc 0.98
2016-09-06T23:51:58.453669: step 4606, loss 0.0586724, acc 0.96
2016-09-06T23:51:59.141867: step 4607, loss 0.0408412, acc 0.98
2016-09-06T23:51:59.810083: step 4608, loss 0.000814226, acc 1
2016-09-06T23:52:00.526972: step 4609, loss 0.0157572, acc 1
2016-09-06T23:52:01.194844: step 4610, loss 0.0050661, acc 1
2016-09-06T23:52:01.905996: step 4611, loss 0.0540412, acc 1
2016-09-06T23:52:02.593444: step 4612, loss 0.00828766, acc 1
2016-09-06T23:52:03.267872: step 4613, loss 0.0385006, acc 0.98
2016-09-06T23:52:03.952816: step 4614, loss 0.0145464, acc 1
2016-09-06T23:52:04.635763: step 4615, loss 0.121491, acc 0.96
2016-09-06T23:52:05.328912: step 4616, loss 0.00534075, acc 1
2016-09-06T23:52:06.003240: step 4617, loss 0.00412041, acc 1
2016-09-06T23:52:06.687815: step 4618, loss 0.0149266, acc 1
2016-09-06T23:52:07.361981: step 4619, loss 0.00678478, acc 1
2016-09-06T23:52:08.048055: step 4620, loss 0.00949976, acc 1
2016-09-06T23:52:08.740618: step 4621, loss 0.173483, acc 0.96
2016-09-06T23:52:09.419726: step 4622, loss 0.0558977, acc 0.98
2016-09-06T23:52:10.124252: step 4623, loss 0.0154258, acc 0.98
2016-09-06T23:52:10.810877: step 4624, loss 0.00376208, acc 1
2016-09-06T23:52:11.533553: step 4625, loss 0.0634548, acc 0.98
2016-09-06T23:52:12.239434: step 4626, loss 0.00191038, acc 1
2016-09-06T23:52:12.928358: step 4627, loss 0.0173532, acc 1
2016-09-06T23:52:13.612138: step 4628, loss 0.013372, acc 1
2016-09-06T23:52:14.306475: step 4629, loss 0.00346291, acc 1
2016-09-06T23:52:14.994171: step 4630, loss 0.0925378, acc 0.98
2016-09-06T23:52:15.647125: step 4631, loss 0.0073866, acc 1
2016-09-06T23:52:16.349427: step 4632, loss 0.00162043, acc 1
2016-09-06T23:52:17.015188: step 4633, loss 0.00243926, acc 1
2016-09-06T23:52:17.689677: step 4634, loss 0.0117223, acc 1
2016-09-06T23:52:18.379385: step 4635, loss 0.00528686, acc 1
2016-09-06T23:52:19.049520: step 4636, loss 0.0113322, acc 1
2016-09-06T23:52:19.743791: step 4637, loss 0.0367176, acc 0.96
2016-09-06T23:52:20.448099: step 4638, loss 0.00441407, acc 1
2016-09-06T23:52:21.152028: step 4639, loss 0.0217445, acc 1
2016-09-06T23:52:21.799598: step 4640, loss 0.0444176, acc 0.96
2016-09-06T23:52:22.490671: step 4641, loss 0.00370411, acc 1
2016-09-06T23:52:23.185931: step 4642, loss 0.0224525, acc 1
2016-09-06T23:52:23.870510: step 4643, loss 0.00444354, acc 1
2016-09-06T23:52:24.550191: step 4644, loss 0.0237917, acc 0.98
2016-09-06T23:52:25.257757: step 4645, loss 0.00922619, acc 1
2016-09-06T23:52:25.969043: step 4646, loss 0.00698401, acc 1
2016-09-06T23:52:26.640300: step 4647, loss 0.00683729, acc 1
2016-09-06T23:52:27.324710: step 4648, loss 0.000212623, acc 1
2016-09-06T23:52:28.017271: step 4649, loss 0.0140185, acc 1
2016-09-06T23:52:28.706803: step 4650, loss 0.0243634, acc 0.98
2016-09-06T23:52:29.400275: step 4651, loss 0.04701, acc 0.98
2016-09-06T23:52:30.081412: step 4652, loss 0.0151353, acc 1
2016-09-06T23:52:30.776552: step 4653, loss 0.017898, acc 0.98
2016-09-06T23:52:31.468604: step 4654, loss 0.0755583, acc 0.98
2016-09-06T23:52:32.155309: step 4655, loss 0.0018634, acc 1
2016-09-06T23:52:32.835738: step 4656, loss 0.0708856, acc 0.96
2016-09-06T23:52:33.529792: step 4657, loss 0.00706406, acc 1
2016-09-06T23:52:34.211074: step 4658, loss 0.0377685, acc 0.96
2016-09-06T23:52:34.888564: step 4659, loss 0.0188845, acc 1
2016-09-06T23:52:35.585755: step 4660, loss 0.0381497, acc 0.98
2016-09-06T23:52:36.261364: step 4661, loss 0.0613411, acc 0.94
2016-09-06T23:52:36.942071: step 4662, loss 0.0737215, acc 0.96
2016-09-06T23:52:37.620689: step 4663, loss 0.000903067, acc 1
2016-09-06T23:52:38.281829: step 4664, loss 0.0193796, acc 1
2016-09-06T23:52:38.951295: step 4665, loss 0.0283221, acc 0.98
2016-09-06T23:52:39.629738: step 4666, loss 0.000562004, acc 1
2016-09-06T23:52:40.320450: step 4667, loss 0.00656482, acc 1
2016-09-06T23:52:40.986119: step 4668, loss 0.00243796, acc 1
2016-09-06T23:52:41.682328: step 4669, loss 0.0559236, acc 0.98
2016-09-06T23:52:42.393613: step 4670, loss 0.0492655, acc 0.98
2016-09-06T23:52:43.084197: step 4671, loss 0.0262444, acc 0.98
2016-09-06T23:52:43.771861: step 4672, loss 0.111451, acc 0.94
2016-09-06T23:52:44.442570: step 4673, loss 0.0293223, acc 0.98
2016-09-06T23:52:45.117549: step 4674, loss 0.0614712, acc 0.96
2016-09-06T23:52:45.777306: step 4675, loss 0.012963, acc 1
2016-09-06T23:52:46.488685: step 4676, loss 0.0670321, acc 0.98
2016-09-06T23:52:47.175347: step 4677, loss 0.00206509, acc 1
2016-09-06T23:52:47.876398: step 4678, loss 0.0437793, acc 0.98
2016-09-06T23:52:48.552190: step 4679, loss 0.0152743, acc 1
2016-09-06T23:52:49.249152: step 4680, loss 0.0253656, acc 0.98
2016-09-06T23:52:49.982348: step 4681, loss 0.00951149, acc 1
2016-09-06T23:52:50.654757: step 4682, loss 0.0334104, acc 0.98
2016-09-06T23:52:51.352457: step 4683, loss 0.00492832, acc 1
2016-09-06T23:52:52.023152: step 4684, loss 0.147774, acc 0.96
2016-09-06T23:52:52.723890: step 4685, loss 0.024021, acc 1
2016-09-06T23:52:53.413248: step 4686, loss 0.00242956, acc 1
2016-09-06T23:52:54.110529: step 4687, loss 0.0025196, acc 1
2016-09-06T23:52:54.830072: step 4688, loss 0.0300208, acc 1
2016-09-06T23:52:55.506501: step 4689, loss 0.0109131, acc 1
2016-09-06T23:52:56.199149: step 4690, loss 0.0307791, acc 0.98
2016-09-06T23:52:56.886184: step 4691, loss 0.0330424, acc 0.98
2016-09-06T23:52:57.583318: step 4692, loss 0.0178366, acc 0.98
2016-09-06T23:52:58.258882: step 4693, loss 0.0136242, acc 1
2016-09-06T23:52:58.936768: step 4694, loss 0.020932, acc 0.98
2016-09-06T23:52:59.648034: step 4695, loss 0.0197568, acc 0.98
2016-09-06T23:53:00.358256: step 4696, loss 0.00185461, acc 1
2016-09-06T23:53:01.036372: step 4697, loss 0.0146107, acc 1
2016-09-06T23:53:01.728718: step 4698, loss 0.0680567, acc 0.98
2016-09-06T23:53:02.403500: step 4699, loss 0.00262543, acc 1
2016-09-06T23:53:03.090387: step 4700, loss 0.0993657, acc 0.96

Evaluation:
2016-09-06T23:53:06.213079: step 4700, loss 2.12666, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-4700

2016-09-06T23:53:07.995817: step 4701, loss 0.0598391, acc 0.98
2016-09-06T23:53:08.662078: step 4702, loss 0.00036618, acc 1
2016-09-06T23:53:09.353267: step 4703, loss 0.0361903, acc 0.98
2016-09-06T23:53:10.050572: step 4704, loss 0.0209166, acc 0.98
2016-09-06T23:53:10.745287: step 4705, loss 0.0123012, acc 1
2016-09-06T23:53:11.424289: step 4706, loss 0.0363598, acc 0.98
2016-09-06T23:53:12.110626: step 4707, loss 0.000530253, acc 1
2016-09-06T23:53:12.831181: step 4708, loss 0.0276997, acc 0.98
2016-09-06T23:53:13.512640: step 4709, loss 0.0668864, acc 0.98
2016-09-06T23:53:14.216349: step 4710, loss 0.00118225, acc 1
2016-09-06T23:53:14.920826: step 4711, loss 0.0531784, acc 0.98
2016-09-06T23:53:15.620880: step 4712, loss 0.0260764, acc 0.98
2016-09-06T23:53:16.314367: step 4713, loss 0.00100789, acc 1
2016-09-06T23:53:17.001384: step 4714, loss 0.0121186, acc 1
2016-09-06T23:53:17.704904: step 4715, loss 0.0203054, acc 0.98
2016-09-06T23:53:18.372417: step 4716, loss 0.036356, acc 0.98
2016-09-06T23:53:19.060839: step 4717, loss 0.0497439, acc 0.98
2016-09-06T23:53:19.742618: step 4718, loss 0.00723752, acc 1
2016-09-06T23:53:20.444727: step 4719, loss 0.00676115, acc 1
2016-09-06T23:53:21.144738: step 4720, loss 0.0468124, acc 0.98
2016-09-06T23:53:21.801076: step 4721, loss 0.0354624, acc 0.98
2016-09-06T23:53:22.505510: step 4722, loss 0.0763037, acc 0.96
2016-09-06T23:53:23.191627: step 4723, loss 0.028171, acc 0.98
2016-09-06T23:53:23.877092: step 4724, loss 0.0082383, acc 1
2016-09-06T23:53:24.581109: step 4725, loss 4.36429e-05, acc 1
2016-09-06T23:53:25.284565: step 4726, loss 0.00332819, acc 1
2016-09-06T23:53:25.974067: step 4727, loss 0.0355193, acc 0.98
2016-09-06T23:53:26.635162: step 4728, loss 0.0476477, acc 0.96
2016-09-06T23:53:27.336162: step 4729, loss 0.0137062, acc 1
2016-09-06T23:53:28.025938: step 4730, loss 0.019722, acc 1
2016-09-06T23:53:28.714263: step 4731, loss 0.0489297, acc 0.98
2016-09-06T23:53:29.387029: step 4732, loss 0.0331718, acc 1
2016-09-06T23:53:30.078970: step 4733, loss 0.0231769, acc 0.98
2016-09-06T23:53:30.744938: step 4734, loss 0.0820605, acc 0.98
2016-09-06T23:53:31.412301: step 4735, loss 0.00584319, acc 1
2016-09-06T23:53:32.149737: step 4736, loss 0.0172905, acc 1
2016-09-06T23:53:32.822973: step 4737, loss 8.37335e-05, acc 1
2016-09-06T23:53:33.510001: step 4738, loss 0.043843, acc 0.98
2016-09-06T23:53:34.210515: step 4739, loss 0.0361091, acc 0.96
2016-09-06T23:53:34.906843: step 4740, loss 0.0468238, acc 0.98
2016-09-06T23:53:35.611382: step 4741, loss 0.0339129, acc 1
2016-09-06T23:53:36.283485: step 4742, loss 0.0831042, acc 0.98
2016-09-06T23:53:37.002729: step 4743, loss 0.024949, acc 0.98
2016-09-06T23:53:37.689820: step 4744, loss 0.00320609, acc 1
2016-09-06T23:53:38.390126: step 4745, loss 0.0573324, acc 0.98
2016-09-06T23:53:39.090879: step 4746, loss 0.0130598, acc 1
2016-09-06T23:53:39.782275: step 4747, loss 0.0315172, acc 0.98
2016-09-06T23:53:40.485773: step 4748, loss 0.00204912, acc 1
2016-09-06T23:53:41.148477: step 4749, loss 0.0413327, acc 0.96
2016-09-06T23:53:41.844734: step 4750, loss 0.0170927, acc 0.98
2016-09-06T23:53:42.548700: step 4751, loss 0.00493283, acc 1
2016-09-06T23:53:43.260857: step 4752, loss 0.0320637, acc 0.98
2016-09-06T23:53:43.943957: step 4753, loss 0.00624401, acc 1
2016-09-06T23:53:44.622587: step 4754, loss 0.000438953, acc 1
2016-09-06T23:53:45.317669: step 4755, loss 0.0137162, acc 1
2016-09-06T23:53:45.979412: step 4756, loss 0.0579591, acc 0.96
2016-09-06T23:53:46.681157: step 4757, loss 0.0407406, acc 1
2016-09-06T23:53:47.365487: step 4758, loss 0.010863, acc 1
2016-09-06T23:53:48.053365: step 4759, loss 0.0345499, acc 0.98
2016-09-06T23:53:48.752312: step 4760, loss 0.00807909, acc 1
2016-09-06T23:53:49.441975: step 4761, loss 0.0229445, acc 0.98
2016-09-06T23:53:50.124191: step 4762, loss 0.0103758, acc 1
2016-09-06T23:53:50.798882: step 4763, loss 0.0465029, acc 0.96
2016-09-06T23:53:51.484614: step 4764, loss 0.131441, acc 0.98
2016-09-06T23:53:52.167944: step 4765, loss 0.000168329, acc 1
2016-09-06T23:53:52.882396: step 4766, loss 0.0275651, acc 0.98
2016-09-06T23:53:53.563699: step 4767, loss 0.0257129, acc 1
2016-09-06T23:53:54.252870: step 4768, loss 0.0280027, acc 0.98
2016-09-06T23:53:54.955871: step 4769, loss 0.00300459, acc 1
2016-09-06T23:53:55.630026: step 4770, loss 0.017699, acc 0.98
2016-09-06T23:53:56.307561: step 4771, loss 0.0201625, acc 1
2016-09-06T23:53:57.003670: step 4772, loss 0.0178059, acc 0.98
2016-09-06T23:53:57.688592: step 4773, loss 0.0371106, acc 0.98
2016-09-06T23:53:58.389214: step 4774, loss 0.0161199, acc 1
2016-09-06T23:53:59.081554: step 4775, loss 0.0626185, acc 0.98
2016-09-06T23:53:59.787307: step 4776, loss 0.0340058, acc 1
2016-09-06T23:54:00.506696: step 4777, loss 0.115474, acc 0.98
2016-09-06T23:54:01.184177: step 4778, loss 0.0166288, acc 1
2016-09-06T23:54:01.892547: step 4779, loss 0.00102753, acc 1
2016-09-06T23:54:02.582293: step 4780, loss 0.000728027, acc 1
2016-09-06T23:54:03.279517: step 4781, loss 0.0282764, acc 0.98
2016-09-06T23:54:03.949610: step 4782, loss 0.0250057, acc 0.98
2016-09-06T23:54:04.642281: step 4783, loss 0.029949, acc 0.98
2016-09-06T23:54:05.331224: step 4784, loss 0.0722685, acc 0.94
2016-09-06T23:54:06.017154: step 4785, loss 0.280519, acc 0.96
2016-09-06T23:54:06.708921: step 4786, loss 0.0284207, acc 0.98
2016-09-06T23:54:07.384593: step 4787, loss 0.00909994, acc 1
2016-09-06T23:54:08.089220: step 4788, loss 0.0117403, acc 1
2016-09-06T23:54:08.755638: step 4789, loss 0.00328222, acc 1
2016-09-06T23:54:09.473671: step 4790, loss 0.00269211, acc 1
2016-09-06T23:54:10.144638: step 4791, loss 0.0107065, acc 1
2016-09-06T23:54:10.844812: step 4792, loss 0.0352118, acc 1
2016-09-06T23:54:11.550507: step 4793, loss 0.0181872, acc 0.98
2016-09-06T23:54:12.237282: step 4794, loss 0.0321655, acc 0.98
2016-09-06T23:54:12.954870: step 4795, loss 0.0444865, acc 0.96
2016-09-06T23:54:13.626842: step 4796, loss 0.0021247, acc 1
2016-09-06T23:54:14.333961: step 4797, loss 0.0831138, acc 0.96
2016-09-06T23:54:15.030431: step 4798, loss 0.0902717, acc 0.98
2016-09-06T23:54:15.715833: step 4799, loss 0.0420636, acc 0.98
2016-09-06T23:54:16.345511: step 4800, loss 0.00666931, acc 1

Evaluation:
2016-09-06T23:54:19.494542: step 4800, loss 1.91829, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-4800

2016-09-06T23:54:21.200930: step 4801, loss 0.0177592, acc 1
2016-09-06T23:54:21.897139: step 4802, loss 0.0742554, acc 0.96
2016-09-06T23:54:22.598961: step 4803, loss 0.0864432, acc 0.98
2016-09-06T23:54:23.282095: step 4804, loss 0.000876942, acc 1
2016-09-06T23:54:23.960399: step 4805, loss 0.0270632, acc 1
2016-09-06T23:54:24.650189: step 4806, loss 0.060757, acc 0.98
2016-09-06T23:54:25.336799: step 4807, loss 0.000421578, acc 1
2016-09-06T23:54:26.017396: step 4808, loss 0.0267674, acc 0.98
2016-09-06T23:54:26.699964: step 4809, loss 0.00124662, acc 1
2016-09-06T23:54:27.414085: step 4810, loss 0.0275229, acc 0.98
2016-09-06T23:54:28.111349: step 4811, loss 0.085697, acc 0.98
2016-09-06T23:54:28.788316: step 4812, loss 0.0065616, acc 1
2016-09-06T23:54:29.470179: step 4813, loss 0.114195, acc 0.96
2016-09-06T23:54:30.148633: step 4814, loss 0.0370782, acc 0.98
2016-09-06T23:54:30.853290: step 4815, loss 0.00940863, acc 1
2016-09-06T23:54:31.514785: step 4816, loss 0.0383658, acc 0.98
2016-09-06T23:54:32.211573: step 4817, loss 0.0099785, acc 1
2016-09-06T23:54:32.877381: step 4818, loss 0.0312452, acc 0.98
2016-09-06T23:54:33.567280: step 4819, loss 0.0127444, acc 1
2016-09-06T23:54:34.252499: step 4820, loss 0.0233207, acc 1
2016-09-06T23:54:34.946450: step 4821, loss 0.0156971, acc 1
2016-09-06T23:54:35.630215: step 4822, loss 0.0298889, acc 1
2016-09-06T23:54:36.341850: step 4823, loss 0.0258081, acc 0.98
2016-09-06T23:54:37.065364: step 4824, loss 0.0307369, acc 0.98
2016-09-06T23:54:37.764299: step 4825, loss 0.0166922, acc 1
2016-09-06T23:54:38.450000: step 4826, loss 0.00490257, acc 1
2016-09-06T23:54:39.135192: step 4827, loss 0.0450299, acc 0.98
2016-09-06T23:54:39.849260: step 4828, loss 0.017064, acc 0.98
2016-09-06T23:54:40.532796: step 4829, loss 0.0226229, acc 0.98
2016-09-06T23:54:41.187297: step 4830, loss 0.00940265, acc 1
2016-09-06T23:54:41.873621: step 4831, loss 0.0377815, acc 0.98
2016-09-06T23:54:42.552702: step 4832, loss 0.0623531, acc 0.96
2016-09-06T23:54:43.233762: step 4833, loss 0.0104166, acc 1
2016-09-06T23:54:43.923564: step 4834, loss 0.047578, acc 0.98
2016-09-06T23:54:44.608131: step 4835, loss 0.00350234, acc 1
2016-09-06T23:54:45.290756: step 4836, loss 0.000328339, acc 1
2016-09-06T23:54:45.959642: step 4837, loss 0.091475, acc 0.98
2016-09-06T23:54:46.676327: step 4838, loss 0.0148958, acc 0.98
2016-09-06T23:54:47.365383: step 4839, loss 0.00210304, acc 1
2016-09-06T23:54:48.066674: step 4840, loss 0.00227927, acc 1
2016-09-06T23:54:48.753715: step 4841, loss 0.0326337, acc 0.98
2016-09-06T23:54:49.438207: step 4842, loss 0.0238493, acc 0.98
2016-09-06T23:54:50.122029: step 4843, loss 0.00059385, acc 1
2016-09-06T23:54:50.797951: step 4844, loss 0.0254253, acc 0.98
2016-09-06T23:54:51.489127: step 4845, loss 0.0944954, acc 0.96
2016-09-06T23:54:52.155889: step 4846, loss 0.0256355, acc 0.98
2016-09-06T23:54:52.829010: step 4847, loss 0.0149001, acc 1
2016-09-06T23:54:53.528624: step 4848, loss 0.0641406, acc 0.94
2016-09-06T23:54:54.240825: step 4849, loss 0.00506724, acc 1
2016-09-06T23:54:54.920885: step 4850, loss 0.0268131, acc 1
2016-09-06T23:54:55.605372: step 4851, loss 0.0244437, acc 1
2016-09-06T23:54:56.336014: step 4852, loss 0.013626, acc 1
2016-09-06T23:54:57.049013: step 4853, loss 0.0628226, acc 0.98
2016-09-06T23:54:57.744468: step 4854, loss 0.0427713, acc 0.98
2016-09-06T23:54:58.447460: step 4855, loss 0.01618, acc 0.98
2016-09-06T23:54:59.155273: step 4856, loss 0.125216, acc 0.96
2016-09-06T23:54:59.861651: step 4857, loss 0.0749713, acc 0.96
2016-09-06T23:55:00.574917: step 4858, loss 6.85299e-05, acc 1
2016-09-06T23:55:01.256890: step 4859, loss 0.00437728, acc 1
2016-09-06T23:55:01.938247: step 4860, loss 0.0158101, acc 1
2016-09-06T23:55:02.615528: step 4861, loss 0.0219544, acc 1
2016-09-06T23:55:03.303013: step 4862, loss 0.0699404, acc 0.96
2016-09-06T23:55:03.988475: step 4863, loss 0.0175481, acc 1
2016-09-06T23:55:04.693853: step 4864, loss 0.052768, acc 0.98
2016-09-06T23:55:05.379564: step 4865, loss 0.0512358, acc 0.94
2016-09-06T23:55:06.089102: step 4866, loss 0.045162, acc 0.98
2016-09-06T23:55:06.799060: step 4867, loss 0.00108847, acc 1
2016-09-06T23:55:07.478930: step 4868, loss 0.0072484, acc 1
2016-09-06T23:55:08.175633: step 4869, loss 0.0245669, acc 1
2016-09-06T23:55:08.851049: step 4870, loss 0.0180538, acc 0.98
2016-09-06T23:55:09.553789: step 4871, loss 0.00830778, acc 1
2016-09-06T23:55:10.221348: step 4872, loss 0.0879965, acc 0.98
2016-09-06T23:55:10.894149: step 4873, loss 0.00257564, acc 1
2016-09-06T23:55:11.585425: step 4874, loss 0.00986285, acc 1
2016-09-06T23:55:12.262296: step 4875, loss 0.00424593, acc 1
2016-09-06T23:55:12.965406: step 4876, loss 0.0749561, acc 0.96
2016-09-06T23:55:13.633967: step 4877, loss 0.0344761, acc 1
2016-09-06T23:55:14.345190: step 4878, loss 0.0877325, acc 0.96
2016-09-06T23:55:15.045166: step 4879, loss 0.0463093, acc 0.96
2016-09-06T23:55:15.737492: step 4880, loss 0.043487, acc 0.96
2016-09-06T23:55:16.429930: step 4881, loss 0.016758, acc 1
2016-09-06T23:55:17.130476: step 4882, loss 0.0276441, acc 0.98
2016-09-06T23:55:17.810745: step 4883, loss 0.0151204, acc 1
2016-09-06T23:55:18.482936: step 4884, loss 0.0015198, acc 1
2016-09-06T23:55:19.187230: step 4885, loss 0.0143773, acc 1
2016-09-06T23:55:19.872394: step 4886, loss 0.0146005, acc 1
2016-09-06T23:55:20.560908: step 4887, loss 0.0274751, acc 1
2016-09-06T23:55:21.235400: step 4888, loss 0.00130266, acc 1
2016-09-06T23:55:21.933059: step 4889, loss 0.0254013, acc 0.98
2016-09-06T23:55:22.612785: step 4890, loss 0.0242242, acc 0.98
2016-09-06T23:55:23.279880: step 4891, loss 0.0222336, acc 0.98
2016-09-06T23:55:23.974961: step 4892, loss 0.0282083, acc 0.98
2016-09-06T23:55:24.663365: step 4893, loss 0.00926393, acc 1
2016-09-06T23:55:25.368580: step 4894, loss 0.0184057, acc 1
2016-09-06T23:55:26.050843: step 4895, loss 0.0640127, acc 0.98
2016-09-06T23:55:26.756734: step 4896, loss 3.24458e-05, acc 1
2016-09-06T23:55:27.435085: step 4897, loss 0.0913685, acc 0.96
2016-09-06T23:55:28.094973: step 4898, loss 0.00863281, acc 1
2016-09-06T23:55:28.814966: step 4899, loss 0.0299352, acc 1
2016-09-06T23:55:29.483653: step 4900, loss 0.0298296, acc 0.98

Evaluation:
2016-09-06T23:55:32.647683: step 4900, loss 2.06222, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-4900

2016-09-06T23:55:34.316629: step 4901, loss 0.0327564, acc 0.98
2016-09-06T23:55:35.006164: step 4902, loss 0.00546941, acc 1
2016-09-06T23:55:35.696760: step 4903, loss 0.0242265, acc 0.98
2016-09-06T23:55:36.400723: step 4904, loss 0.00452436, acc 1
2016-09-06T23:55:37.086877: step 4905, loss 0.00729195, acc 1
2016-09-06T23:55:37.753414: step 4906, loss 0.011962, acc 1
2016-09-06T23:55:38.445753: step 4907, loss 0.0645759, acc 0.96
2016-09-06T23:55:39.143998: step 4908, loss 0.0221433, acc 0.98
2016-09-06T23:55:39.840412: step 4909, loss 0.0330415, acc 0.98
2016-09-06T23:55:40.537777: step 4910, loss 0.00494216, acc 1
2016-09-06T23:55:41.214732: step 4911, loss 0.0119217, acc 1
2016-09-06T23:55:41.932998: step 4912, loss 0.00509936, acc 1
2016-09-06T23:55:42.608470: step 4913, loss 0.0472424, acc 0.98
2016-09-06T23:55:43.318014: step 4914, loss 0.0139897, acc 1
2016-09-06T23:55:44.018089: step 4915, loss 0.0235633, acc 0.98
2016-09-06T23:55:44.708425: step 4916, loss 0.03274, acc 0.98
2016-09-06T23:55:45.402949: step 4917, loss 0.011099, acc 1
2016-09-06T23:55:46.093913: step 4918, loss 0.00304409, acc 1
2016-09-06T23:55:46.783667: step 4919, loss 0.0091734, acc 1
2016-09-06T23:55:47.480369: step 4920, loss 0.00593399, acc 1
2016-09-06T23:55:48.151846: step 4921, loss 0.00907033, acc 1
2016-09-06T23:55:48.838544: step 4922, loss 0.0338461, acc 0.98
2016-09-06T23:55:49.527450: step 4923, loss 0.00158928, acc 1
2016-09-06T23:55:50.215433: step 4924, loss 0.0126958, acc 1
2016-09-06T23:55:50.897919: step 4925, loss 0.000720302, acc 1
2016-09-06T23:55:51.597617: step 4926, loss 0.0699665, acc 0.96
2016-09-06T23:55:52.278601: step 4927, loss 0.0374632, acc 0.98
2016-09-06T23:55:52.963988: step 4928, loss 0.0220379, acc 1
2016-09-06T23:55:53.645787: step 4929, loss 0.0477512, acc 0.98
2016-09-06T23:55:54.324428: step 4930, loss 0.102089, acc 0.94
2016-09-06T23:55:55.005948: step 4931, loss 0.0808767, acc 0.96
2016-09-06T23:55:55.705164: step 4932, loss 0.00704935, acc 1
2016-09-06T23:55:56.418685: step 4933, loss 0.0222155, acc 1
2016-09-06T23:55:57.091391: step 4934, loss 0.0172068, acc 1
2016-09-06T23:55:57.765567: step 4935, loss 0.000755093, acc 1
2016-09-06T23:55:58.455034: step 4936, loss 0.00057053, acc 1
2016-09-06T23:55:59.159439: step 4937, loss 0.00962451, acc 1
2016-09-06T23:55:59.845505: step 4938, loss 0.0542955, acc 0.98
2016-09-06T23:56:00.559888: step 4939, loss 0.0188543, acc 0.98
2016-09-06T23:56:01.245050: step 4940, loss 0.00613799, acc 1
2016-09-06T23:56:01.917568: step 4941, loss 0.0152958, acc 1
2016-09-06T23:56:02.591123: step 4942, loss 0.0250638, acc 0.98
2016-09-06T23:56:03.283874: step 4943, loss 0.00411387, acc 1
2016-09-06T23:56:03.977913: step 4944, loss 0.00503203, acc 1
2016-09-06T23:56:04.669749: step 4945, loss 0.0122875, acc 1
2016-09-06T23:56:05.355903: step 4946, loss 0.0073646, acc 1
2016-09-06T23:56:06.068315: step 4947, loss 0.0188743, acc 0.98
2016-09-06T23:56:06.717754: step 4948, loss 0.00548555, acc 1
2016-09-06T23:56:07.395215: step 4949, loss 0.0354551, acc 0.98
2016-09-06T23:56:08.091447: step 4950, loss 0.0230151, acc 0.98
2016-09-06T23:56:08.770317: step 4951, loss 0.0184764, acc 1
2016-09-06T23:56:09.467498: step 4952, loss 0.0217235, acc 1
2016-09-06T23:56:10.161891: step 4953, loss 0.0215326, acc 0.98
2016-09-06T23:56:10.878217: step 4954, loss 0.00287929, acc 1
2016-09-06T23:56:11.544689: step 4955, loss 0.027998, acc 1
2016-09-06T23:56:12.251079: step 4956, loss 0.0329435, acc 0.98
2016-09-06T23:56:12.950516: step 4957, loss 0.0355408, acc 0.98
2016-09-06T23:56:13.644446: step 4958, loss 0.0546045, acc 0.98
2016-09-06T23:56:14.324643: step 4959, loss 0.0242577, acc 0.98
2016-09-06T23:56:14.986206: step 4960, loss 0.0710646, acc 0.94
2016-09-06T23:56:15.676493: step 4961, loss 0.0323495, acc 0.98
2016-09-06T23:56:16.344263: step 4962, loss 0.0120911, acc 1
2016-09-06T23:56:17.055049: step 4963, loss 0.00650449, acc 1
2016-09-06T23:56:17.756254: step 4964, loss 0.000378056, acc 1
2016-09-06T23:56:18.431965: step 4965, loss 0.14473, acc 0.96
2016-09-06T23:56:19.113934: step 4966, loss 0.0164336, acc 0.98
2016-09-06T23:56:19.775637: step 4967, loss 0.00842834, acc 1
2016-09-06T23:56:20.509445: step 4968, loss 0.0173026, acc 1
2016-09-06T23:56:21.203940: step 4969, loss 0.0176316, acc 1
2016-09-06T23:56:21.885865: step 4970, loss 0.00494682, acc 1
2016-09-06T23:56:22.554141: step 4971, loss 0.0122582, acc 1
2016-09-06T23:56:23.237183: step 4972, loss 0.0902132, acc 0.98
2016-09-06T23:56:23.929367: step 4973, loss 0.0337444, acc 0.98
2016-09-06T23:56:24.592286: step 4974, loss 0.0137882, acc 1
2016-09-06T23:56:25.300489: step 4975, loss 0.00772545, acc 1
2016-09-06T23:56:25.992350: step 4976, loss 0.00089174, acc 1
2016-09-06T23:56:26.666556: step 4977, loss 0.0515552, acc 0.96
2016-09-06T23:56:27.364309: step 4978, loss 0.0843726, acc 0.94
2016-09-06T23:56:28.072643: step 4979, loss 0.0257446, acc 0.98
2016-09-06T23:56:28.827757: step 4980, loss 0.00569772, acc 1
2016-09-06T23:56:29.568187: step 4981, loss 0.0204646, acc 1
2016-09-06T23:56:30.250188: step 4982, loss 0.00482302, acc 1
2016-09-06T23:56:30.933284: step 4983, loss 0.0319867, acc 0.98
2016-09-06T23:56:31.595574: step 4984, loss 0.0367733, acc 0.98
2016-09-06T23:56:32.276240: step 4985, loss 0.10163, acc 0.96
2016-09-06T23:56:32.965457: step 4986, loss 0.026717, acc 0.98
2016-09-06T23:56:33.689773: step 4987, loss 0.0395899, acc 0.98
2016-09-06T23:56:34.368094: step 4988, loss 0.00546536, acc 1
2016-09-06T23:56:35.068481: step 4989, loss 0.0122032, acc 1
2016-09-06T23:56:35.755370: step 4990, loss 0.069852, acc 0.98
2016-09-06T23:56:36.461897: step 4991, loss 0.00948642, acc 1
2016-09-06T23:56:37.108385: step 4992, loss 0.00180055, acc 1
2016-09-06T23:56:37.807790: step 4993, loss 0.00761243, acc 1
2016-09-06T23:56:38.520864: step 4994, loss 0.0649649, acc 0.98
2016-09-06T23:56:39.212092: step 4995, loss 0.0954378, acc 0.96
2016-09-06T23:56:39.888832: step 4996, loss 0.0297437, acc 0.98
2016-09-06T23:56:40.577392: step 4997, loss 0.0230641, acc 1
2016-09-06T23:56:41.266465: step 4998, loss 0.11937, acc 0.94
2016-09-06T23:56:41.979861: step 4999, loss 0.104099, acc 0.98
2016-09-06T23:56:42.654243: step 5000, loss 0.00292611, acc 1

Evaluation:
2016-09-06T23:56:45.845273: step 5000, loss 1.95827, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-5000

2016-09-06T23:56:47.589482: step 5001, loss 0.00524467, acc 1
2016-09-06T23:56:48.258572: step 5002, loss 0.0411579, acc 0.98
2016-09-06T23:56:48.965291: step 5003, loss 0.00540531, acc 1
2016-09-06T23:56:49.643468: step 5004, loss 0.0624164, acc 0.98
2016-09-06T23:56:50.320150: step 5005, loss 0.0719609, acc 0.98
2016-09-06T23:56:50.999890: step 5006, loss 0.0785198, acc 0.96
2016-09-06T23:56:51.700357: step 5007, loss 0.000454563, acc 1
2016-09-06T23:56:52.378129: step 5008, loss 0.00913959, acc 1
2016-09-06T23:56:53.059044: step 5009, loss 0.0271031, acc 0.98
2016-09-06T23:56:53.747730: step 5010, loss 0.00395897, acc 1
2016-09-06T23:56:54.460688: step 5011, loss 0.0151493, acc 1
2016-09-06T23:56:55.161826: step 5012, loss 0.00249054, acc 1
2016-09-06T23:56:55.852063: step 5013, loss 0.0341317, acc 0.98
2016-09-06T23:56:56.563071: step 5014, loss 0.0200977, acc 1
2016-09-06T23:56:57.247792: step 5015, loss 0.000596898, acc 1
2016-09-06T23:56:57.930305: step 5016, loss 0.0313476, acc 0.98
2016-09-06T23:56:58.623044: step 5017, loss 0.112517, acc 0.96
2016-09-06T23:56:59.318613: step 5018, loss 3.88614e-06, acc 1
2016-09-06T23:57:00.037380: step 5019, loss 0.0202773, acc 1
2016-09-06T23:57:00.742113: step 5020, loss 0.020034, acc 0.98
2016-09-06T23:57:01.455644: step 5021, loss 0.0173495, acc 0.98
2016-09-06T23:57:02.141058: step 5022, loss 0.0227126, acc 1
2016-09-06T23:57:02.815415: step 5023, loss 0.0508688, acc 0.96
2016-09-06T23:57:03.491118: step 5024, loss 0.058046, acc 0.98
2016-09-06T23:57:04.175851: step 5025, loss 0.0183752, acc 1
2016-09-06T23:57:04.890275: step 5026, loss 0.10797, acc 0.96
2016-09-06T23:57:05.567530: step 5027, loss 0.0156804, acc 1
2016-09-06T23:57:06.300290: step 5028, loss 0.022065, acc 1
2016-09-06T23:57:07.006928: step 5029, loss 0.0328003, acc 0.96
2016-09-06T23:57:07.683833: step 5030, loss 0.106001, acc 0.94
2016-09-06T23:57:08.366636: step 5031, loss 0.00461576, acc 1
2016-09-06T23:57:09.039412: step 5032, loss 0.000733622, acc 1
2016-09-06T23:57:09.748586: step 5033, loss 0.0244076, acc 0.98
2016-09-06T23:57:10.414728: step 5034, loss 0.0692316, acc 0.94
2016-09-06T23:57:11.102365: step 5035, loss 0.011999, acc 1
2016-09-06T23:57:11.790498: step 5036, loss 0.0134413, acc 1
2016-09-06T23:57:12.493344: step 5037, loss 0.0332026, acc 0.98
2016-09-06T23:57:13.178836: step 5038, loss 0.033123, acc 1
2016-09-06T23:57:13.877439: step 5039, loss 0.0122524, acc 1
2016-09-06T23:57:14.589084: step 5040, loss 0.0248678, acc 1
2016-09-06T23:57:15.237602: step 5041, loss 0.00129925, acc 1
2016-09-06T23:57:15.929925: step 5042, loss 0.01147, acc 1
2016-09-06T23:57:16.610302: step 5043, loss 0.0360899, acc 1
2016-09-06T23:57:17.297671: step 5044, loss 0.0326786, acc 0.98
2016-09-06T23:57:18.000870: step 5045, loss 0.00492518, acc 1
2016-09-06T23:57:18.697638: step 5046, loss 0.00486828, acc 1
2016-09-06T23:57:19.407701: step 5047, loss 0.00425255, acc 1
2016-09-06T23:57:20.072978: step 5048, loss 0.00137635, acc 1
2016-09-06T23:57:20.751629: step 5049, loss 0.0870218, acc 0.96
2016-09-06T23:57:21.450333: step 5050, loss 0.00909932, acc 1
2016-09-06T23:57:22.147902: step 5051, loss 0.00705863, acc 1
2016-09-06T23:57:22.805925: step 5052, loss 0.129394, acc 0.98
2016-09-06T23:57:23.502912: step 5053, loss 0.0406932, acc 0.98
2016-09-06T23:57:24.226303: step 5054, loss 0.0108452, acc 1
2016-09-06T23:57:24.904124: step 5055, loss 0.02758, acc 1
2016-09-06T23:57:25.584898: step 5056, loss 0.0318372, acc 0.98
2016-09-06T23:57:26.284449: step 5057, loss 0.0290824, acc 1
2016-09-06T23:57:26.982920: step 5058, loss 0.0425989, acc 0.98
2016-09-06T23:57:27.679227: step 5059, loss 0.00179407, acc 1
2016-09-06T23:57:28.343826: step 5060, loss 0.00497129, acc 1
2016-09-06T23:57:29.049977: step 5061, loss 0.000886728, acc 1
2016-09-06T23:57:29.707434: step 5062, loss 0.0131534, acc 1
2016-09-06T23:57:30.373901: step 5063, loss 0.0131609, acc 1
2016-09-06T23:57:31.060335: step 5064, loss 0.0365276, acc 0.98
2016-09-06T23:57:31.747347: step 5065, loss 0.000141221, acc 1
2016-09-06T23:57:32.422356: step 5066, loss 0.000184239, acc 1
2016-09-06T23:57:33.113954: step 5067, loss 0.0244867, acc 0.98
2016-09-06T23:57:33.822288: step 5068, loss 0.026948, acc 0.98
2016-09-06T23:57:34.499074: step 5069, loss 0.000891595, acc 1
2016-09-06T23:57:35.184317: step 5070, loss 0.00215702, acc 1
2016-09-06T23:57:35.895261: step 5071, loss 0.0183675, acc 1
2016-09-06T23:57:36.573857: step 5072, loss 0.0243444, acc 1
2016-09-06T23:57:37.250197: step 5073, loss 0.0011596, acc 1
2016-09-06T23:57:37.925936: step 5074, loss 0.0125361, acc 1
2016-09-06T23:57:38.649897: step 5075, loss 0.0163742, acc 0.98
2016-09-06T23:57:39.313587: step 5076, loss 0.0152634, acc 0.98
2016-09-06T23:57:39.995831: step 5077, loss 0.000240583, acc 1
2016-09-06T23:57:40.695533: step 5078, loss 0.2069, acc 0.92
2016-09-06T23:57:41.383153: step 5079, loss 0.00384149, acc 1
2016-09-06T23:57:42.079633: step 5080, loss 0.0545527, acc 0.98
2016-09-06T23:57:42.737977: step 5081, loss 0.00544266, acc 1
2016-09-06T23:57:43.437613: step 5082, loss 0.00718844, acc 1
2016-09-06T23:57:44.126662: step 5083, loss 0.013217, acc 1
2016-09-06T23:57:44.818384: step 5084, loss 0.0434589, acc 0.96
2016-09-06T23:57:45.596597: step 5085, loss 0.0110884, acc 1
2016-09-06T23:57:46.282194: step 5086, loss 0.0042428, acc 1
2016-09-06T23:57:46.966610: step 5087, loss 0.0330933, acc 0.98
2016-09-06T23:57:47.639730: step 5088, loss 0.0046062, acc 1
2016-09-06T23:57:48.346043: step 5089, loss 0.014074, acc 1
2016-09-06T23:57:49.042119: step 5090, loss 0.0227231, acc 0.98
2016-09-06T23:57:49.723881: step 5091, loss 0.0850661, acc 0.98
2016-09-06T23:57:50.401121: step 5092, loss 0.0348921, acc 1
2016-09-06T23:57:51.091143: step 5093, loss 0.0171065, acc 0.98
2016-09-06T23:57:51.765517: step 5094, loss 0.00034805, acc 1
2016-09-06T23:57:52.430737: step 5095, loss 0.0373292, acc 0.98
2016-09-06T23:57:53.134366: step 5096, loss 0.0451047, acc 0.96
2016-09-06T23:57:53.805814: step 5097, loss 0.00486945, acc 1
2016-09-06T23:57:54.514123: step 5098, loss 0.0273626, acc 0.98
2016-09-06T23:57:55.207961: step 5099, loss 0.00826666, acc 1
2016-09-06T23:57:55.895629: step 5100, loss 0.0124197, acc 1

Evaluation:
2016-09-06T23:57:59.038155: step 5100, loss 2.07588, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-5100

2016-09-06T23:58:00.733533: step 5101, loss 0.000392322, acc 1
2016-09-06T23:58:01.423007: step 5102, loss 0.0323135, acc 0.96
2016-09-06T23:58:02.090779: step 5103, loss 0.00509175, acc 1
2016-09-06T23:58:02.797737: step 5104, loss 0.0119361, acc 1
2016-09-06T23:58:03.498452: step 5105, loss 0.0107212, acc 1
2016-09-06T23:58:04.176966: step 5106, loss 0.00975695, acc 1
2016-09-06T23:58:04.879840: step 5107, loss 0.0354986, acc 0.98
2016-09-06T23:58:05.571145: step 5108, loss 0.053475, acc 0.96
2016-09-06T23:58:06.270346: step 5109, loss 0.0113415, acc 1
2016-09-06T23:58:06.939369: step 5110, loss 0.0045757, acc 1
2016-09-06T23:58:07.633696: step 5111, loss 0.0269875, acc 1
2016-09-06T23:58:08.344810: step 5112, loss 0.0421856, acc 0.98
2016-09-06T23:58:09.053153: step 5113, loss 0.0193737, acc 1
2016-09-06T23:58:09.751776: step 5114, loss 0.0628898, acc 0.96
2016-09-06T23:58:10.422959: step 5115, loss 0.0117765, acc 1
2016-09-06T23:58:11.132593: step 5116, loss 0.0296935, acc 1
2016-09-06T23:58:11.808553: step 5117, loss 0.0005138, acc 1
2016-09-06T23:58:12.496657: step 5118, loss 0.0158749, acc 1
2016-09-06T23:58:13.199396: step 5119, loss 0.0237418, acc 1
2016-09-06T23:58:13.888767: step 5120, loss 0.0958775, acc 0.96
2016-09-06T23:58:14.572943: step 5121, loss 0.0788842, acc 0.96
2016-09-06T23:58:15.231491: step 5122, loss 0.024673, acc 1
2016-09-06T23:58:15.933991: step 5123, loss 0.105225, acc 0.92
2016-09-06T23:58:16.599904: step 5124, loss 0.0559717, acc 0.98
2016-09-06T23:58:17.282057: step 5125, loss 0.049027, acc 0.98
2016-09-06T23:58:17.969009: step 5126, loss 0.00835948, acc 1
2016-09-06T23:58:18.664522: step 5127, loss 0.000817453, acc 1
2016-09-06T23:58:19.343590: step 5128, loss 0.00224256, acc 1
2016-09-06T23:58:20.014716: step 5129, loss 0.0270215, acc 1
2016-09-06T23:58:20.737028: step 5130, loss 0.000339171, acc 1
2016-09-06T23:58:21.444195: step 5131, loss 0.0192878, acc 0.98
2016-09-06T23:58:22.113255: step 5132, loss 0.00965718, acc 1
2016-09-06T23:58:22.787124: step 5133, loss 0.0264297, acc 0.98
2016-09-06T23:58:23.460884: step 5134, loss 0.0564328, acc 0.96
2016-09-06T23:58:24.157111: step 5135, loss 0.00117955, acc 1
2016-09-06T23:58:24.849284: step 5136, loss 0.03328, acc 0.98
2016-09-06T23:58:25.563055: step 5137, loss 0.00299988, acc 1
2016-09-06T23:58:26.257588: step 5138, loss 0.00772822, acc 1
2016-09-06T23:58:26.917622: step 5139, loss 0.0030744, acc 1
2016-09-06T23:58:27.619192: step 5140, loss 0.0557984, acc 0.98
2016-09-06T23:58:28.307714: step 5141, loss 0.106911, acc 0.96
2016-09-06T23:58:29.013872: step 5142, loss 0.0680047, acc 0.98
2016-09-06T23:58:29.733597: step 5143, loss 0.0222243, acc 0.98
2016-09-06T23:58:30.442142: step 5144, loss 0.0215187, acc 0.98
2016-09-06T23:58:31.142648: step 5145, loss 0.0154675, acc 1
2016-09-06T23:58:31.848092: step 5146, loss 0.00191918, acc 1
2016-09-06T23:58:32.568884: step 5147, loss 0.00779358, acc 1
2016-09-06T23:58:33.236554: step 5148, loss 0.000869956, acc 1
2016-09-06T23:58:33.931016: step 5149, loss 0.00770942, acc 1
2016-09-06T23:58:34.590735: step 5150, loss 0.0496477, acc 0.96
2016-09-06T23:58:35.277811: step 5151, loss 0.0122367, acc 1
2016-09-06T23:58:35.991305: step 5152, loss 0.00295689, acc 1
2016-09-06T23:58:36.685885: step 5153, loss 0.000560991, acc 1
2016-09-06T23:58:37.357728: step 5154, loss 0.0189548, acc 0.98
2016-09-06T23:58:38.050437: step 5155, loss 0.0254448, acc 1
2016-09-06T23:58:38.754242: step 5156, loss 0.00986172, acc 1
2016-09-06T23:58:39.430165: step 5157, loss 0.0447, acc 0.96
2016-09-06T23:58:40.111903: step 5158, loss 0.00334479, acc 1
2016-09-06T23:58:40.805924: step 5159, loss 0.0330602, acc 0.98
2016-09-06T23:58:41.500493: step 5160, loss 0.133768, acc 0.96
2016-09-06T23:58:42.172927: step 5161, loss 0.0913735, acc 0.94
2016-09-06T23:58:42.853899: step 5162, loss 0.0206565, acc 1
2016-09-06T23:58:43.557620: step 5163, loss 0.000308984, acc 1
2016-09-06T23:58:44.240752: step 5164, loss 0.0827637, acc 0.96
2016-09-06T23:58:44.950946: step 5165, loss 0.0180171, acc 1
2016-09-06T23:58:45.624398: step 5166, loss 0.0527926, acc 0.94
2016-09-06T23:58:46.338107: step 5167, loss 0.0164964, acc 1
2016-09-06T23:58:47.029618: step 5168, loss 0.027309, acc 1
2016-09-06T23:58:47.703239: step 5169, loss 0.00855838, acc 1
2016-09-06T23:58:48.416172: step 5170, loss 0.0613459, acc 0.96
2016-09-06T23:58:49.092177: step 5171, loss 0.0225421, acc 0.98
2016-09-06T23:58:49.800237: step 5172, loss 0.0198894, acc 1
2016-09-06T23:58:50.491645: step 5173, loss 0.0115333, acc 1
2016-09-06T23:58:51.196597: step 5174, loss 0.0856144, acc 0.98
2016-09-06T23:58:51.890345: step 5175, loss 0.00090361, acc 1
2016-09-06T23:58:52.554369: step 5176, loss 0.0261348, acc 0.98
2016-09-06T23:58:53.261838: step 5177, loss 0.0117658, acc 1
2016-09-06T23:58:53.940999: step 5178, loss 0.00270816, acc 1
2016-09-06T23:58:54.615863: step 5179, loss 0.051882, acc 0.98
2016-09-06T23:58:55.292270: step 5180, loss 0.0201891, acc 1
2016-09-06T23:58:55.990545: step 5181, loss 0.00578899, acc 1
2016-09-06T23:58:56.684046: step 5182, loss 0.00507334, acc 1
2016-09-06T23:58:57.354295: step 5183, loss 0.0119471, acc 1
2016-09-06T23:58:58.002230: step 5184, loss 0.0344321, acc 0.977273
2016-09-06T23:58:58.676881: step 5185, loss 0.0336282, acc 0.98
2016-09-06T23:58:59.356803: step 5186, loss 0.0233817, acc 1
2016-09-06T23:59:00.089722: step 5187, loss 0.0308326, acc 0.96
2016-09-06T23:59:00.790358: step 5188, loss 0.0223084, acc 1
2016-09-06T23:59:01.482921: step 5189, loss 0.196267, acc 0.98
2016-09-06T23:59:02.178317: step 5190, loss 0.0188087, acc 1
2016-09-06T23:59:02.868658: step 5191, loss 0.0130357, acc 1
2016-09-06T23:59:03.537012: step 5192, loss 0.122632, acc 0.98
2016-09-06T23:59:04.202099: step 5193, loss 0.0131912, acc 1
2016-09-06T23:59:04.892153: step 5194, loss 0.0132249, acc 1
2016-09-06T23:59:05.566282: step 5195, loss 0.0195598, acc 0.98
2016-09-06T23:59:06.255400: step 5196, loss 0.0264568, acc 0.98
2016-09-06T23:59:06.936302: step 5197, loss 0.0609162, acc 0.96
2016-09-06T23:59:07.647009: step 5198, loss 0.0254327, acc 0.98
2016-09-06T23:59:08.305955: step 5199, loss 0.00900713, acc 1
2016-09-06T23:59:08.980248: step 5200, loss 0.00184555, acc 1

Evaluation:
2016-09-06T23:59:12.110948: step 5200, loss 2.24007, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-5200

2016-09-06T23:59:13.771102: step 5201, loss 0.0664399, acc 0.98
2016-09-06T23:59:14.467734: step 5202, loss 0.000629364, acc 1
2016-09-06T23:59:15.161504: step 5203, loss 0.0203118, acc 1
2016-09-06T23:59:15.836423: step 5204, loss 0.0229746, acc 1
2016-09-06T23:59:16.509683: step 5205, loss 0.0201427, acc 0.98
2016-09-06T23:59:17.200069: step 5206, loss 0.0221428, acc 0.98
2016-09-06T23:59:17.878342: step 5207, loss 0.00295324, acc 1
2016-09-06T23:59:18.589741: step 5208, loss 0.019399, acc 1
2016-09-06T23:59:19.277415: step 5209, loss 0.0190956, acc 1
2016-09-06T23:59:19.988118: step 5210, loss 0.00627191, acc 1
2016-09-06T23:59:20.680809: step 5211, loss 0.0133322, acc 1
2016-09-06T23:59:21.371172: step 5212, loss 0.00633121, acc 1
2016-09-06T23:59:22.092430: step 5213, loss 0.0178243, acc 1
2016-09-06T23:59:22.755024: step 5214, loss 0.0201338, acc 1
2016-09-06T23:59:23.438980: step 5215, loss 0.00531464, acc 1
2016-09-06T23:59:24.136558: step 5216, loss 0.0252017, acc 1
2016-09-06T23:59:24.828737: step 5217, loss 0.0345876, acc 0.98
2016-09-06T23:59:25.513444: step 5218, loss 0.109124, acc 0.98
2016-09-06T23:59:26.175375: step 5219, loss 0.00254282, acc 1
2016-09-06T23:59:26.874463: step 5220, loss 0.0545586, acc 0.98
2016-09-06T23:59:27.562892: step 5221, loss 0.00959941, acc 1
2016-09-06T23:59:28.256371: step 5222, loss 0.0275683, acc 0.98
2016-09-06T23:59:28.944596: step 5223, loss 0.0101535, acc 1
2016-09-06T23:59:29.622718: step 5224, loss 0.024656, acc 0.98
2016-09-06T23:59:30.324592: step 5225, loss 0.0377807, acc 0.96
2016-09-06T23:59:30.987445: step 5226, loss 0.0933741, acc 0.96
2016-09-06T23:59:31.670775: step 5227, loss 0.0121569, acc 1
2016-09-06T23:59:32.346076: step 5228, loss 0.015599, acc 0.98
2016-09-06T23:59:33.008395: step 5229, loss 0.0335003, acc 0.96
2016-09-06T23:59:33.707046: step 5230, loss 0.0222612, acc 1
2016-09-06T23:59:34.394254: step 5231, loss 0.0302786, acc 0.98
2016-09-06T23:59:35.079981: step 5232, loss 0.0286719, acc 1
2016-09-06T23:59:35.785283: step 5233, loss 0.000451475, acc 1
2016-09-06T23:59:36.489143: step 5234, loss 0.00738704, acc 1
2016-09-06T23:59:37.182960: step 5235, loss 0.00294633, acc 1
2016-09-06T23:59:37.871550: step 5236, loss 0.016076, acc 1
2016-09-06T23:59:38.588927: step 5237, loss 0.0128517, acc 1
2016-09-06T23:59:39.262879: step 5238, loss 0.0111007, acc 1
2016-09-06T23:59:39.948530: step 5239, loss 0.00862262, acc 1
2016-09-06T23:59:40.615963: step 5240, loss 0.00831516, acc 1
2016-09-06T23:59:41.327897: step 5241, loss 0.0216104, acc 0.98
2016-09-06T23:59:42.009092: step 5242, loss 0.000156067, acc 1
2016-09-06T23:59:42.720321: step 5243, loss 0.169452, acc 0.92
2016-09-06T23:59:43.420686: step 5244, loss 0.00163698, acc 1
2016-09-06T23:59:44.101330: step 5245, loss 0.0101333, acc 1
2016-09-06T23:59:44.779206: step 5246, loss 0.0257764, acc 0.98
2016-09-06T23:59:45.466153: step 5247, loss 0.0209329, acc 0.98
2016-09-06T23:59:46.156177: step 5248, loss 0.00916662, acc 1
2016-09-06T23:59:46.836435: step 5249, loss 0.0390261, acc 0.98
2016-09-06T23:59:47.513221: step 5250, loss 0.0185062, acc 1
2016-09-06T23:59:48.191561: step 5251, loss 0.031404, acc 0.98
2016-09-06T23:59:48.896585: step 5252, loss 0.000199564, acc 1
2016-09-06T23:59:49.601473: step 5253, loss 0.00227442, acc 1
2016-09-06T23:59:50.288932: step 5254, loss 0.0844796, acc 0.94
2016-09-06T23:59:51.014966: step 5255, loss 0.000296286, acc 1
2016-09-06T23:59:51.723317: step 5256, loss 0.0145004, acc 0.98
2016-09-06T23:59:52.415774: step 5257, loss 0.00101303, acc 1
2016-09-06T23:59:53.116389: step 5258, loss 0.0284811, acc 0.98
2016-09-06T23:59:53.818001: step 5259, loss 0.0268331, acc 0.98
2016-09-06T23:59:54.537987: step 5260, loss 0.027003, acc 0.98
2016-09-06T23:59:55.199695: step 5261, loss 0.0618999, acc 0.96
2016-09-06T23:59:55.908013: step 5262, loss 0.000138405, acc 1
2016-09-06T23:59:56.572769: step 5263, loss 0.0126545, acc 1
2016-09-06T23:59:57.265940: step 5264, loss 0.0848005, acc 0.94
2016-09-06T23:59:57.965122: step 5265, loss 0.0254473, acc 0.98
2016-09-06T23:59:58.632609: step 5266, loss 0.0015873, acc 1
2016-09-06T23:59:59.332334: step 5267, loss 0.0150363, acc 0.98
2016-09-07T00:00:00.003485: step 5268, loss 0.00128395, acc 1
2016-09-07T00:00:00.713849: step 5269, loss 0.00482052, acc 1
2016-09-07T00:00:01.402018: step 5270, loss 0.0143746, acc 1
2016-09-07T00:00:02.090844: step 5271, loss 0.0395322, acc 0.98
2016-09-07T00:00:02.782768: step 5272, loss 0.0411535, acc 0.98
2016-09-07T00:00:03.450564: step 5273, loss 0.0365716, acc 0.98
2016-09-07T00:00:04.191236: step 5274, loss 0.0631257, acc 0.98
2016-09-07T00:00:04.916229: step 5275, loss 0.00366026, acc 1
2016-09-07T00:00:05.606252: step 5276, loss 0.0111889, acc 1
2016-09-07T00:00:06.292622: step 5277, loss 0.022255, acc 1
2016-09-07T00:00:06.973412: step 5278, loss 0.0382374, acc 0.98
2016-09-07T00:00:07.664611: step 5279, loss 0.019745, acc 1
2016-09-07T00:00:08.319128: step 5280, loss 0.00635785, acc 1
2016-09-07T00:00:09.024292: step 5281, loss 0.0347624, acc 1
2016-09-07T00:00:09.729542: step 5282, loss 0.0104481, acc 1
2016-09-07T00:00:10.398542: step 5283, loss 0.0639483, acc 0.98
2016-09-07T00:00:11.080670: step 5284, loss 0.0173463, acc 1
2016-09-07T00:00:11.781981: step 5285, loss 0.0482968, acc 0.96
2016-09-07T00:00:12.475001: step 5286, loss 0.00142584, acc 1
2016-09-07T00:00:13.148523: step 5287, loss 0.108293, acc 0.98
2016-09-07T00:00:13.867291: step 5288, loss 0.0259667, acc 0.98
2016-09-07T00:00:14.565494: step 5289, loss 0.049062, acc 0.96
2016-09-07T00:00:15.249338: step 5290, loss 0.0171747, acc 1
2016-09-07T00:00:15.940053: step 5291, loss 0.0496901, acc 0.96
2016-09-07T00:00:16.622384: step 5292, loss 0.026615, acc 1
2016-09-07T00:00:17.325606: step 5293, loss 0.0585186, acc 0.96
2016-09-07T00:00:18.004124: step 5294, loss 0.0107911, acc 1
2016-09-07T00:00:18.688738: step 5295, loss 0.00746396, acc 1
2016-09-07T00:00:19.403616: step 5296, loss 0.0065407, acc 1
2016-09-07T00:00:20.076414: step 5297, loss 0.0115996, acc 1
2016-09-07T00:00:20.760599: step 5298, loss 0.0860098, acc 0.94
2016-09-07T00:00:21.440759: step 5299, loss 0.0101986, acc 1
2016-09-07T00:00:22.146678: step 5300, loss 0.0106278, acc 1

Evaluation:
2016-09-07T00:00:25.286409: step 5300, loss 2.20728, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-5300

2016-09-07T00:00:27.067686: step 5301, loss 0.00389843, acc 1
2016-09-07T00:00:27.745918: step 5302, loss 0.00271556, acc 1
2016-09-07T00:00:28.453010: step 5303, loss 0.0390221, acc 0.98
2016-09-07T00:00:29.122109: step 5304, loss 0.0143964, acc 1
2016-09-07T00:00:29.821432: step 5305, loss 0.0502842, acc 0.98
2016-09-07T00:00:30.524180: step 5306, loss 0.000192536, acc 1
2016-09-07T00:00:31.210476: step 5307, loss 0.0224173, acc 1
2016-09-07T00:00:31.899177: step 5308, loss 0.0196637, acc 0.98
2016-09-07T00:00:32.609412: step 5309, loss 0.0151913, acc 0.98
2016-09-07T00:00:33.304125: step 5310, loss 0.0488191, acc 0.98
2016-09-07T00:00:33.989492: step 5311, loss 0.091431, acc 0.98
2016-09-07T00:00:34.658743: step 5312, loss 0.00964967, acc 1
2016-09-07T00:00:35.357472: step 5313, loss 0.0192111, acc 1
2016-09-07T00:00:36.026639: step 5314, loss 0.0684609, acc 0.98
2016-09-07T00:00:36.738996: step 5315, loss 0.133331, acc 0.92
2016-09-07T00:00:37.434391: step 5316, loss 0.0253238, acc 0.98
2016-09-07T00:00:38.122723: step 5317, loss 0.0399023, acc 0.96
2016-09-07T00:00:38.798311: step 5318, loss 0.00513441, acc 1
2016-09-07T00:00:39.493341: step 5319, loss 0.00663664, acc 1
2016-09-07T00:00:40.203465: step 5320, loss 0.000311891, acc 1
2016-09-07T00:00:40.881067: step 5321, loss 0.0124073, acc 1
2016-09-07T00:00:41.582821: step 5322, loss 0.00115894, acc 1
2016-09-07T00:00:42.264604: step 5323, loss 0.0186673, acc 1
2016-09-07T00:00:42.962831: step 5324, loss 0.0120822, acc 1
2016-09-07T00:00:43.650988: step 5325, loss 0.0433532, acc 0.96
2016-09-07T00:00:44.332076: step 5326, loss 0.00945149, acc 1
2016-09-07T00:00:45.041774: step 5327, loss 0.0307801, acc 0.98
2016-09-07T00:00:45.727642: step 5328, loss 0.0159084, acc 1
2016-09-07T00:00:46.420667: step 5329, loss 0.0169594, acc 1
2016-09-07T00:00:47.129553: step 5330, loss 0.000230581, acc 1
2016-09-07T00:00:47.816081: step 5331, loss 0.0119254, acc 1
2016-09-07T00:00:48.517796: step 5332, loss 0.111327, acc 0.92
2016-09-07T00:00:49.227175: step 5333, loss 0.0347048, acc 0.98
2016-09-07T00:00:49.941383: step 5334, loss 0.00252491, acc 1
2016-09-07T00:00:50.658402: step 5335, loss 0.180223, acc 0.98
2016-09-07T00:00:51.338849: step 5336, loss 0.0229091, acc 0.98
2016-09-07T00:00:52.013612: step 5337, loss 0.100142, acc 0.98
2016-09-07T00:00:52.714541: step 5338, loss 0.000256012, acc 1
2016-09-07T00:00:53.404168: step 5339, loss 0.0405028, acc 0.98
2016-09-07T00:00:54.087996: step 5340, loss 0.0045129, acc 1
2016-09-07T00:00:54.786878: step 5341, loss 0.0386384, acc 0.98
2016-09-07T00:00:55.472640: step 5342, loss 0.134266, acc 0.96
2016-09-07T00:00:56.163767: step 5343, loss 0.00324096, acc 1
2016-09-07T00:00:56.858599: step 5344, loss 0.00794149, acc 1
2016-09-07T00:00:57.559776: step 5345, loss 0.0171698, acc 0.98
2016-09-07T00:00:58.274069: step 5346, loss 0.0174757, acc 0.98
2016-09-07T00:00:58.954047: step 5347, loss 0.0106373, acc 1
2016-09-07T00:00:59.642020: step 5348, loss 0.0146331, acc 1
2016-09-07T00:01:00.346857: step 5349, loss 0.0377111, acc 0.98
2016-09-07T00:01:01.029227: step 5350, loss 0.056067, acc 0.96
2016-09-07T00:01:01.725053: step 5351, loss 0.00014528, acc 1
2016-09-07T00:01:02.405907: step 5352, loss 0.034771, acc 0.98
2016-09-07T00:01:03.120096: step 5353, loss 0.0249611, acc 0.98
2016-09-07T00:01:03.794394: step 5354, loss 0.110527, acc 0.98
2016-09-07T00:01:04.478158: step 5355, loss 0.00638414, acc 1
2016-09-07T00:01:05.174511: step 5356, loss 0.0244132, acc 0.98
2016-09-07T00:01:05.862281: step 5357, loss 0.0358177, acc 0.96
2016-09-07T00:01:06.570601: step 5358, loss 0.0385385, acc 0.98
2016-09-07T00:01:07.249411: step 5359, loss 0.0021519, acc 1
2016-09-07T00:01:07.957328: step 5360, loss 0.0321008, acc 0.96
2016-09-07T00:01:08.650749: step 5361, loss 0.0110947, acc 1
2016-09-07T00:01:09.333283: step 5362, loss 0.0225949, acc 0.98
2016-09-07T00:01:10.021503: step 5363, loss 0.0632147, acc 0.98
2016-09-07T00:01:10.722104: step 5364, loss 0.0251571, acc 0.98
2016-09-07T00:01:11.399881: step 5365, loss 0.00648386, acc 1
2016-09-07T00:01:12.071365: step 5366, loss 0.00243309, acc 1
2016-09-07T00:01:12.792942: step 5367, loss 0.0124527, acc 1
2016-09-07T00:01:13.474621: step 5368, loss 0.0314435, acc 0.98
2016-09-07T00:01:14.177943: step 5369, loss 0.00560284, acc 1
2016-09-07T00:01:14.852456: step 5370, loss 0.00120283, acc 1
2016-09-07T00:01:15.529809: step 5371, loss 0.0393354, acc 0.98
2016-09-07T00:01:16.239553: step 5372, loss 0.000499254, acc 1
2016-09-07T00:01:16.902994: step 5373, loss 0.0813631, acc 0.98
2016-09-07T00:01:17.612441: step 5374, loss 0.0212026, acc 0.98
2016-09-07T00:01:18.300650: step 5375, loss 0.0226036, acc 1
2016-09-07T00:01:18.941687: step 5376, loss 0.000483687, acc 1
2016-09-07T00:01:19.636986: step 5377, loss 0.0401923, acc 0.96
2016-09-07T00:01:20.314159: step 5378, loss 0.0258506, acc 1
2016-09-07T00:01:20.988951: step 5379, loss 0.0782331, acc 0.92
2016-09-07T00:01:21.647172: step 5380, loss 0.00328529, acc 1
2016-09-07T00:01:22.366749: step 5381, loss 0.0304132, acc 1
2016-09-07T00:01:23.054584: step 5382, loss 0.053769, acc 0.98
2016-09-07T00:01:23.730130: step 5383, loss 0.00333915, acc 1
2016-09-07T00:01:24.437237: step 5384, loss 0.109312, acc 0.96
2016-09-07T00:01:25.119475: step 5385, loss 0.00811386, acc 1
2016-09-07T00:01:25.802991: step 5386, loss 0.0107002, acc 1
2016-09-07T00:01:26.459637: step 5387, loss 0.0212883, acc 0.98
2016-09-07T00:01:27.182190: step 5388, loss 0.0140521, acc 1
2016-09-07T00:01:27.873072: step 5389, loss 0.0422458, acc 0.98
2016-09-07T00:01:28.549516: step 5390, loss 0.106364, acc 0.96
2016-09-07T00:01:29.264637: step 5391, loss 0.037894, acc 0.98
2016-09-07T00:01:29.943812: step 5392, loss 0.00467532, acc 1
2016-09-07T00:01:30.622886: step 5393, loss 0.00417029, acc 1
2016-09-07T00:01:31.292248: step 5394, loss 0.0470973, acc 0.96
2016-09-07T00:01:31.999088: step 5395, loss 0.0194383, acc 0.98
2016-09-07T00:01:32.666792: step 5396, loss 0.0181502, acc 0.98
2016-09-07T00:01:33.358191: step 5397, loss 0.00992689, acc 1
2016-09-07T00:01:34.041356: step 5398, loss 0.0420597, acc 0.96
2016-09-07T00:01:34.724358: step 5399, loss 0.00224836, acc 1
2016-09-07T00:01:35.412326: step 5400, loss 0.0305704, acc 0.98

Evaluation:
2016-09-07T00:01:38.510818: step 5400, loss 1.98494, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-5400

2016-09-07T00:01:40.283070: step 5401, loss 0.012124, acc 1
2016-09-07T00:01:40.957410: step 5402, loss 0.10846, acc 0.98
2016-09-07T00:01:41.658963: step 5403, loss 0.0485403, acc 0.96
2016-09-07T00:01:42.358358: step 5404, loss 0.000491688, acc 1
2016-09-07T00:01:43.045203: step 5405, loss 0.0747099, acc 0.96
2016-09-07T00:01:43.715013: step 5406, loss 0.0466915, acc 0.98
2016-09-07T00:01:44.434679: step 5407, loss 0.0101513, acc 1
2016-09-07T00:01:45.148997: step 5408, loss 0.175096, acc 0.92
2016-09-07T00:01:45.813604: step 5409, loss 0.0354034, acc 0.98
2016-09-07T00:01:46.477524: step 5410, loss 0.0289269, acc 0.98
2016-09-07T00:01:47.165937: step 5411, loss 0.0326608, acc 0.98
2016-09-07T00:01:47.863510: step 5412, loss 0.0428282, acc 0.98
2016-09-07T00:01:48.562511: step 5413, loss 0.029798, acc 0.98
2016-09-07T00:01:49.260591: step 5414, loss 0.0319537, acc 1
2016-09-07T00:01:49.958180: step 5415, loss 0.0183659, acc 0.98
2016-09-07T00:01:50.608093: step 5416, loss 0.00279827, acc 1
2016-09-07T00:01:51.311427: step 5417, loss 0.0322619, acc 0.98
2016-09-07T00:01:51.993191: step 5418, loss 0.0335491, acc 0.98
2016-09-07T00:01:52.677919: step 5419, loss 0.000536483, acc 1
2016-09-07T00:01:53.341604: step 5420, loss 0.0186044, acc 1
2016-09-07T00:01:54.023102: step 5421, loss 0.0227424, acc 0.98
2016-09-07T00:01:54.712447: step 5422, loss 0.0266623, acc 0.98
2016-09-07T00:01:55.373989: step 5423, loss 0.0227813, acc 0.98
2016-09-07T00:01:56.079077: step 5424, loss 0.0176334, acc 1
2016-09-07T00:01:56.766579: step 5425, loss 0.096062, acc 0.94
2016-09-07T00:01:57.450004: step 5426, loss 0.03716, acc 0.98
2016-09-07T00:01:58.131796: step 5427, loss 0.00632981, acc 1
2016-09-07T00:01:58.825558: step 5428, loss 0.0442944, acc 0.98
2016-09-07T00:01:59.529889: step 5429, loss 0.000225412, acc 1
2016-09-07T00:02:00.226293: step 5430, loss 0.000323894, acc 1
2016-09-07T00:02:00.919441: step 5431, loss 0.0045709, acc 1
2016-09-07T00:02:01.614083: step 5432, loss 0.00348839, acc 1
2016-09-07T00:02:02.300369: step 5433, loss 0.0340681, acc 0.98
2016-09-07T00:02:02.984780: step 5434, loss 0.00748543, acc 1
2016-09-07T00:02:03.653594: step 5435, loss 0.0292218, acc 0.98
2016-09-07T00:02:04.344771: step 5436, loss 0.0282001, acc 1
2016-09-07T00:02:05.002474: step 5437, loss 0.029568, acc 0.98
2016-09-07T00:02:05.706888: step 5438, loss 0.00581912, acc 1
2016-09-07T00:02:06.410785: step 5439, loss 0.100261, acc 0.98
2016-09-07T00:02:07.078130: step 5440, loss 0.151251, acc 0.94
2016-09-07T00:02:07.793448: step 5441, loss 0.00579798, acc 1
2016-09-07T00:02:08.466770: step 5442, loss 0.0415419, acc 0.96
2016-09-07T00:02:09.169872: step 5443, loss 0.035572, acc 0.98
2016-09-07T00:02:09.855793: step 5444, loss 0.000361966, acc 1
2016-09-07T00:02:10.578405: step 5445, loss 0.0108402, acc 1
2016-09-07T00:02:11.284637: step 5446, loss 0.032562, acc 0.96
2016-09-07T00:02:11.992665: step 5447, loss 0.0216655, acc 0.98
2016-09-07T00:02:12.694931: step 5448, loss 0.0156363, acc 1
2016-09-07T00:02:13.366228: step 5449, loss 0.0072675, acc 1
2016-09-07T00:02:14.056225: step 5450, loss 0.000914297, acc 1
2016-09-07T00:02:14.723270: step 5451, loss 0.0404024, acc 0.96
2016-09-07T00:02:15.429677: step 5452, loss 0.0431277, acc 0.96
2016-09-07T00:02:16.105465: step 5453, loss 0.00383465, acc 1
2016-09-07T00:02:16.792980: step 5454, loss 0.000397483, acc 1
2016-09-07T00:02:17.482372: step 5455, loss 0.0609241, acc 0.96
2016-09-07T00:02:18.142269: step 5456, loss 0.0417476, acc 0.96
2016-09-07T00:02:18.837912: step 5457, loss 0.00151513, acc 1
2016-09-07T00:02:19.495270: step 5458, loss 0.00537505, acc 1
2016-09-07T00:02:20.186658: step 5459, loss 0.043026, acc 0.98
2016-09-07T00:02:20.864510: step 5460, loss 0.0124772, acc 1
2016-09-07T00:02:21.566579: step 5461, loss 0.0640649, acc 0.94
2016-09-07T00:02:22.262239: step 5462, loss 0.0747016, acc 0.98
2016-09-07T00:02:22.922682: step 5463, loss 0.0317195, acc 0.98
2016-09-07T00:02:23.611776: step 5464, loss 0.0696842, acc 0.98
2016-09-07T00:02:24.299422: step 5465, loss 0.00986738, acc 1
2016-09-07T00:02:25.013051: step 5466, loss 0.0233374, acc 1
2016-09-07T00:02:25.693351: step 5467, loss 0.00647989, acc 1
2016-09-07T00:02:26.374800: step 5468, loss 0.0494122, acc 0.96
2016-09-07T00:02:27.076094: step 5469, loss 0.0068696, acc 1
2016-09-07T00:02:27.756703: step 5470, loss 0.00635721, acc 1
2016-09-07T00:02:28.445587: step 5471, loss 0.0168071, acc 1
2016-09-07T00:02:29.117951: step 5472, loss 0.0122907, acc 1
2016-09-07T00:02:29.788788: step 5473, loss 0.0186697, acc 1
2016-09-07T00:02:30.491117: step 5474, loss 0.035868, acc 0.98
2016-09-07T00:02:31.171812: step 5475, loss 0.0376454, acc 0.98
2016-09-07T00:02:31.864538: step 5476, loss 0.0257922, acc 0.98
2016-09-07T00:02:32.553630: step 5477, loss 0.00706303, acc 1
2016-09-07T00:02:33.275533: step 5478, loss 0.0200071, acc 0.98
2016-09-07T00:02:33.965375: step 5479, loss 0.00904395, acc 1
2016-09-07T00:02:34.652773: step 5480, loss 0.000557624, acc 1
2016-09-07T00:02:35.321873: step 5481, loss 0.00297577, acc 1
2016-09-07T00:02:36.003681: step 5482, loss 0.0281508, acc 0.98
2016-09-07T00:02:36.694241: step 5483, loss 0.0554884, acc 0.98
2016-09-07T00:02:37.370348: step 5484, loss 0.0174885, acc 1
2016-09-07T00:02:38.069541: step 5485, loss 0.00946379, acc 1
2016-09-07T00:02:38.744842: step 5486, loss 0.0213891, acc 1
2016-09-07T00:02:39.426874: step 5487, loss 0.0380698, acc 0.98
2016-09-07T00:02:40.109292: step 5488, loss 0.000346021, acc 1
2016-09-07T00:02:40.818715: step 5489, loss 0.0115231, acc 1
2016-09-07T00:02:41.511855: step 5490, loss 0.0466794, acc 0.94
2016-09-07T00:02:42.164684: step 5491, loss 0.0225678, acc 1
2016-09-07T00:02:42.878352: step 5492, loss 0.0323999, acc 0.98
2016-09-07T00:02:43.564535: step 5493, loss 0.00565135, acc 1
2016-09-07T00:02:44.277381: step 5494, loss 0.00269565, acc 1
2016-09-07T00:02:44.963555: step 5495, loss 0.00773084, acc 1
2016-09-07T00:02:45.644478: step 5496, loss 0.0045888, acc 1
2016-09-07T00:02:46.344039: step 5497, loss 0.00926699, acc 1
2016-09-07T00:02:47.003532: step 5498, loss 0.0203283, acc 0.98
2016-09-07T00:02:47.709121: step 5499, loss 0.000113447, acc 1
2016-09-07T00:02:48.400720: step 5500, loss 0.0366642, acc 0.98

Evaluation:
2016-09-07T00:02:51.534363: step 5500, loss 2.34834, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-5500

2016-09-07T00:02:53.222853: step 5501, loss 0.0160591, acc 1
2016-09-07T00:02:53.889590: step 5502, loss 0.123603, acc 0.94
2016-09-07T00:02:54.587364: step 5503, loss 0.00121348, acc 1
2016-09-07T00:02:55.283847: step 5504, loss 0.0172542, acc 1
2016-09-07T00:02:55.965909: step 5505, loss 9.8999e-05, acc 1
2016-09-07T00:02:56.618872: step 5506, loss 0.0131486, acc 1
2016-09-07T00:02:57.320835: step 5507, loss 0.0361347, acc 1
2016-09-07T00:02:58.007704: step 5508, loss 0.01667, acc 1
2016-09-07T00:02:58.704107: step 5509, loss 0.0668793, acc 0.96
2016-09-07T00:02:59.390434: step 5510, loss 0.0226376, acc 1
2016-09-07T00:03:00.077740: step 5511, loss 0.0137541, acc 1
2016-09-07T00:03:00.803071: step 5512, loss 0.000485961, acc 1
2016-09-07T00:03:01.473548: step 5513, loss 0.0206894, acc 0.98
2016-09-07T00:03:02.172682: step 5514, loss 0.00235619, acc 1
2016-09-07T00:03:02.861204: step 5515, loss 0.00859768, acc 1
2016-09-07T00:03:03.557832: step 5516, loss 0.00559543, acc 1
2016-09-07T00:03:04.245515: step 5517, loss 0.0646829, acc 0.96
2016-09-07T00:03:04.926745: step 5518, loss 0.00218479, acc 1
2016-09-07T00:03:05.620093: step 5519, loss 0.0191406, acc 1
2016-09-07T00:03:06.307626: step 5520, loss 0.0663637, acc 0.98
2016-09-07T00:03:07.006238: step 5521, loss 0.00377597, acc 1
2016-09-07T00:03:07.723297: step 5522, loss 0.0295469, acc 0.98
2016-09-07T00:03:08.406389: step 5523, loss 0.0110239, acc 1
2016-09-07T00:03:09.094625: step 5524, loss 0.0528992, acc 0.96
2016-09-07T00:03:09.777712: step 5525, loss 0.027674, acc 1
2016-09-07T00:03:10.505157: step 5526, loss 0.0366291, acc 0.98
2016-09-07T00:03:11.194039: step 5527, loss 0.0201126, acc 0.98
2016-09-07T00:03:11.908699: step 5528, loss 0.00727975, acc 1
2016-09-07T00:03:12.627577: step 5529, loss 0.00950984, acc 1
2016-09-07T00:03:13.316243: step 5530, loss 0.165364, acc 0.96
2016-09-07T00:03:14.018625: step 5531, loss 0.0528261, acc 0.98
2016-09-07T00:03:14.696201: step 5532, loss 0.00542993, acc 1
2016-09-07T00:03:15.400244: step 5533, loss 0.00916627, acc 1
2016-09-07T00:03:16.087385: step 5534, loss 0.0442583, acc 0.98
2016-09-07T00:03:16.766910: step 5535, loss 0.0011958, acc 1
2016-09-07T00:03:17.466795: step 5536, loss 0.0375628, acc 0.98
2016-09-07T00:03:18.128229: step 5537, loss 0.0397877, acc 0.98
2016-09-07T00:03:18.827347: step 5538, loss 0.0155941, acc 0.98
2016-09-07T00:03:19.493053: step 5539, loss 0.0337556, acc 0.98
2016-09-07T00:03:20.202710: step 5540, loss 0.0432173, acc 0.98
2016-09-07T00:03:20.871644: step 5541, loss 0.0198288, acc 1
2016-09-07T00:03:21.579171: step 5542, loss 0.0154469, acc 0.98
2016-09-07T00:03:22.253054: step 5543, loss 0.00780435, acc 1
2016-09-07T00:03:22.928646: step 5544, loss 0.0683423, acc 0.94
2016-09-07T00:03:23.612271: step 5545, loss 0.0601334, acc 0.98
2016-09-07T00:03:24.281631: step 5546, loss 0.0138978, acc 1
2016-09-07T00:03:24.989221: step 5547, loss 0.00561328, acc 1
2016-09-07T00:03:25.686089: step 5548, loss 0.00450992, acc 1
2016-09-07T00:03:26.369992: step 5549, loss 0.00195385, acc 1
2016-09-07T00:03:27.065716: step 5550, loss 0.00340237, acc 1
2016-09-07T00:03:27.780900: step 5551, loss 0.0180268, acc 1
2016-09-07T00:03:28.471378: step 5552, loss 0.0437819, acc 0.96
2016-09-07T00:03:29.143975: step 5553, loss 0.0291442, acc 0.98
2016-09-07T00:03:29.838224: step 5554, loss 0.117673, acc 0.96
2016-09-07T00:03:30.507905: step 5555, loss 0.00276208, acc 1
2016-09-07T00:03:31.188868: step 5556, loss 0.0259798, acc 0.98
2016-09-07T00:03:31.874964: step 5557, loss 0.0207567, acc 0.98
2016-09-07T00:03:32.559522: step 5558, loss 0.00621597, acc 1
2016-09-07T00:03:33.255103: step 5559, loss 0.0173568, acc 0.98
2016-09-07T00:03:33.905253: step 5560, loss 0.0263275, acc 1
2016-09-07T00:03:34.619693: step 5561, loss 0.011664, acc 1
2016-09-07T00:03:35.291234: step 5562, loss 0.00248608, acc 1
2016-09-07T00:03:35.956755: step 5563, loss 0.024002, acc 0.98
2016-09-07T00:03:36.658092: step 5564, loss 0.000611437, acc 1
2016-09-07T00:03:37.347051: step 5565, loss 0.000254746, acc 1
2016-09-07T00:03:38.029583: step 5566, loss 0.0448082, acc 0.98
2016-09-07T00:03:38.701469: step 5567, loss 0.0108936, acc 1
2016-09-07T00:03:39.358773: step 5568, loss 0.155905, acc 0.977273
2016-09-07T00:03:40.041418: step 5569, loss 0.00651668, acc 1
2016-09-07T00:03:40.734555: step 5570, loss 0.00696779, acc 1
2016-09-07T00:03:41.424442: step 5571, loss 0.04694, acc 0.98
2016-09-07T00:03:42.129253: step 5572, loss 0.0350786, acc 0.98
2016-09-07T00:03:42.820889: step 5573, loss 0.00742968, acc 1
2016-09-07T00:03:43.498195: step 5574, loss 0.0483901, acc 0.98
2016-09-07T00:03:44.203330: step 5575, loss 0.00350142, acc 1
2016-09-07T00:03:44.882550: step 5576, loss 0.0884617, acc 0.96
2016-09-07T00:03:45.553020: step 5577, loss 0.0264921, acc 0.98
2016-09-07T00:03:46.230868: step 5578, loss 0.00634177, acc 1
2016-09-07T00:03:46.916644: step 5579, loss 0.0241543, acc 1
2016-09-07T00:03:47.605255: step 5580, loss 0.00285361, acc 1
2016-09-07T00:03:48.302568: step 5581, loss 0.0159222, acc 1
2016-09-07T00:03:49.014919: step 5582, loss 0.190149, acc 0.94
2016-09-07T00:03:49.681398: step 5583, loss 0.00579104, acc 1
2016-09-07T00:03:50.355349: step 5584, loss 0.0392758, acc 0.98
2016-09-07T00:03:51.063341: step 5585, loss 0.156592, acc 0.94
2016-09-07T00:03:51.768311: step 5586, loss 0.00336584, acc 1
2016-09-07T00:03:52.479957: step 5587, loss 0.0170872, acc 1
2016-09-07T00:03:53.131739: step 5588, loss 0.00836252, acc 1
2016-09-07T00:03:53.833017: step 5589, loss 0.0167585, acc 1
2016-09-07T00:03:54.497622: step 5590, loss 0.0341515, acc 0.98
2016-09-07T00:03:55.187929: step 5591, loss 0.0971509, acc 0.98
2016-09-07T00:03:55.888364: step 5592, loss 0.0678043, acc 0.96
2016-09-07T00:03:56.592108: step 5593, loss 0.0616872, acc 0.96
2016-09-07T00:03:57.311651: step 5594, loss 0.00486265, acc 1
2016-09-07T00:03:57.985215: step 5595, loss 0.00168475, acc 1
2016-09-07T00:03:58.694329: step 5596, loss 0.0234486, acc 0.98
2016-09-07T00:03:59.373947: step 5597, loss 0.0441778, acc 0.98
2016-09-07T00:04:00.072365: step 5598, loss 0.0387756, acc 0.98
2016-09-07T00:04:00.790564: step 5599, loss 0.0410794, acc 0.98
2016-09-07T00:04:01.488200: step 5600, loss 0.0662775, acc 0.96

Evaluation:
2016-09-07T00:04:04.646163: step 5600, loss 1.68724, acc 0.75985

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-5600

2016-09-07T00:04:06.495123: step 5601, loss 0.0152742, acc 0.98
2016-09-07T00:04:07.181877: step 5602, loss 0.00924689, acc 1
2016-09-07T00:04:07.877526: step 5603, loss 0.0481746, acc 0.98
2016-09-07T00:04:08.585608: step 5604, loss 0.040179, acc 0.98
2016-09-07T00:04:09.269688: step 5605, loss 0.00282183, acc 1
2016-09-07T00:04:09.954547: step 5606, loss 0.0122377, acc 1
2016-09-07T00:04:10.662002: step 5607, loss 0.00635344, acc 1
2016-09-07T00:04:11.333524: step 5608, loss 0.00149437, acc 1
2016-09-07T00:04:12.017622: step 5609, loss 0.0996691, acc 0.96
2016-09-07T00:04:12.713053: step 5610, loss 0.00777675, acc 1
2016-09-07T00:04:13.394158: step 5611, loss 0.00941807, acc 1
2016-09-07T00:04:14.094581: step 5612, loss 0.00828318, acc 1
2016-09-07T00:04:14.764154: step 5613, loss 0.0536475, acc 0.98
2016-09-07T00:04:15.449659: step 5614, loss 0.0214743, acc 1
2016-09-07T00:04:16.121958: step 5615, loss 0.0237988, acc 0.98
2016-09-07T00:04:16.817937: step 5616, loss 0.00065379, acc 1
2016-09-07T00:04:17.504754: step 5617, loss 0.00104241, acc 1
2016-09-07T00:04:18.202673: step 5618, loss 0.0266013, acc 1
2016-09-07T00:04:18.882581: step 5619, loss 0.0561826, acc 0.98
2016-09-07T00:04:19.562352: step 5620, loss 0.0335234, acc 0.98
2016-09-07T00:04:20.256662: step 5621, loss 0.0112577, acc 1
2016-09-07T00:04:20.930487: step 5622, loss 0.0280458, acc 0.98
2016-09-07T00:04:21.604913: step 5623, loss 0.00606055, acc 1
2016-09-07T00:04:22.286044: step 5624, loss 0.0243348, acc 0.98
2016-09-07T00:04:22.965819: step 5625, loss 0.00702572, acc 1
2016-09-07T00:04:23.671133: step 5626, loss 0.000474819, acc 1
2016-09-07T00:04:24.349759: step 5627, loss 0.00335817, acc 1
2016-09-07T00:04:25.047602: step 5628, loss 0.00609828, acc 1
2016-09-07T00:04:25.738243: step 5629, loss 0.0452239, acc 0.98
2016-09-07T00:04:26.437641: step 5630, loss 3.55376e-05, acc 1
2016-09-07T00:04:27.120085: step 5631, loss 0.111903, acc 0.94
2016-09-07T00:04:27.787146: step 5632, loss 0.0121997, acc 1
2016-09-07T00:04:28.473877: step 5633, loss 0.0771129, acc 0.94
2016-09-07T00:04:29.149039: step 5634, loss 0.0633391, acc 0.98
2016-09-07T00:04:29.833967: step 5635, loss 0.00232238, acc 1
2016-09-07T00:04:30.516945: step 5636, loss 0.00398906, acc 1
2016-09-07T00:04:31.207216: step 5637, loss 0.00292273, acc 1
2016-09-07T00:04:31.903179: step 5638, loss 0.112639, acc 0.96
2016-09-07T00:04:32.586826: step 5639, loss 0.0355702, acc 0.98
2016-09-07T00:04:33.274190: step 5640, loss 0.0116541, acc 1
2016-09-07T00:04:33.957660: step 5641, loss 0.00157934, acc 1
2016-09-07T00:04:34.648539: step 5642, loss 0.0228468, acc 0.98
2016-09-07T00:04:35.323847: step 5643, loss 0.0606064, acc 0.98
2016-09-07T00:04:36.006593: step 5644, loss 0.0338579, acc 0.98
2016-09-07T00:04:36.688446: step 5645, loss 0.0133725, acc 1
2016-09-07T00:04:37.403682: step 5646, loss 0.00282269, acc 1
2016-09-07T00:04:38.100644: step 5647, loss 0.00145532, acc 1
2016-09-07T00:04:38.788719: step 5648, loss 0.0224572, acc 1
2016-09-07T00:04:39.500412: step 5649, loss 0.0294122, acc 0.98
2016-09-07T00:04:40.171463: step 5650, loss 0.0138125, acc 1
2016-09-07T00:04:40.908665: step 5651, loss 0.00929872, acc 1
2016-09-07T00:04:41.600043: step 5652, loss 0.0204755, acc 0.98
2016-09-07T00:04:42.283574: step 5653, loss 0.00284927, acc 1
2016-09-07T00:04:42.968590: step 5654, loss 0.0492673, acc 0.96
2016-09-07T00:04:43.636493: step 5655, loss 0.0692777, acc 0.96
2016-09-07T00:04:44.358397: step 5656, loss 0.000759938, acc 1
2016-09-07T00:04:45.027101: step 5657, loss 0.0174465, acc 1
2016-09-07T00:04:45.732215: step 5658, loss 0.0422724, acc 0.96
2016-09-07T00:04:46.428739: step 5659, loss 0.022981, acc 0.98
2016-09-07T00:04:47.096766: step 5660, loss 0.00685436, acc 1
2016-09-07T00:04:47.770248: step 5661, loss 0.0066472, acc 1
2016-09-07T00:04:48.438776: step 5662, loss 0.0113921, acc 1
2016-09-07T00:04:49.129375: step 5663, loss 0.0862553, acc 0.96
2016-09-07T00:04:49.793422: step 5664, loss 0.0192753, acc 0.98
2016-09-07T00:04:50.468428: step 5665, loss 0.0311276, acc 1
2016-09-07T00:04:51.183314: step 5666, loss 0.0588623, acc 0.98
2016-09-07T00:04:51.866499: step 5667, loss 0.0240179, acc 0.98
2016-09-07T00:04:52.540658: step 5668, loss 0.0191811, acc 1
2016-09-07T00:04:53.230813: step 5669, loss 0.00693114, acc 1
2016-09-07T00:04:53.926073: step 5670, loss 0.00149394, acc 1
2016-09-07T00:04:54.596137: step 5671, loss 0.00276504, acc 1
2016-09-07T00:04:55.280471: step 5672, loss 0.0114789, acc 1
2016-09-07T00:04:55.967482: step 5673, loss 0.114316, acc 0.96
2016-09-07T00:04:56.677404: step 5674, loss 0.0187999, acc 0.98
2016-09-07T00:04:57.357601: step 5675, loss 0.0122305, acc 1
2016-09-07T00:04:58.066857: step 5676, loss 0.0302813, acc 0.98
2016-09-07T00:04:58.769626: step 5677, loss 0.0274656, acc 1
2016-09-07T00:04:59.452609: step 5678, loss 0.0332776, acc 0.98
2016-09-07T00:05:00.140231: step 5679, loss 0.00720552, acc 1
2016-09-07T00:05:00.842352: step 5680, loss 0.0220825, acc 1
2016-09-07T00:05:01.529789: step 5681, loss 0.0326059, acc 0.98
2016-09-07T00:05:02.211587: step 5682, loss 0.0369084, acc 1
2016-09-07T00:05:02.897040: step 5683, loss 0.00272179, acc 1
2016-09-07T00:05:03.601684: step 5684, loss 0.00670534, acc 1
2016-09-07T00:05:04.283732: step 5685, loss 8.79064e-05, acc 1
2016-09-07T00:05:04.969153: step 5686, loss 0.0154535, acc 1
2016-09-07T00:05:05.652946: step 5687, loss 0.0301648, acc 0.98
2016-09-07T00:05:06.346162: step 5688, loss 0.0123695, acc 1
2016-09-07T00:05:07.047151: step 5689, loss 0.0169559, acc 1
2016-09-07T00:05:07.711951: step 5690, loss 0.0195095, acc 1
2016-09-07T00:05:08.416262: step 5691, loss 0.0152721, acc 0.98
2016-09-07T00:05:09.081602: step 5692, loss 0.0266653, acc 1
2016-09-07T00:05:09.774205: step 5693, loss 0.0785599, acc 0.98
2016-09-07T00:05:10.484987: step 5694, loss 0.0269468, acc 0.98
2016-09-07T00:05:11.181110: step 5695, loss 0.000904637, acc 1
2016-09-07T00:05:11.861554: step 5696, loss 0.0826315, acc 0.96
2016-09-07T00:05:12.530192: step 5697, loss 0.00156309, acc 1
2016-09-07T00:05:13.263036: step 5698, loss 0.070398, acc 0.98
2016-09-07T00:05:13.963036: step 5699, loss 0.118398, acc 0.92
2016-09-07T00:05:14.651234: step 5700, loss 0.0201743, acc 1

Evaluation:
2016-09-07T00:05:17.787450: step 5700, loss 2.18067, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473173739/checkpoints/model-5700

2016-09-07T00:05:19.494590: step 5701, loss 0.122477, acc 0.96
2016-09-07T00:05:20.203632: step 5702, loss 0.0236094, acc 0.98
2016-09-07T00:05:20.894322: step 5703, loss 0.0919049, acc 0.96
2016-09-07T00:05:21.594270: step 5704, loss 0.00596858, acc 1
2016-09-07T00:05:22.281455: step 5705, loss 0.0043477, acc 1
2016-09-07T00:05:22.964776: step 5706, loss 0.0112455, acc 1
2016-09-07T00:05:23.650886: step 5707, loss 0.0169237, acc 1
2016-09-07T00:05:24.343176: step 5708, loss 0.0297222, acc 0.98
2016-09-07T00:05:25.050094: step 5709, loss 0.00609553, acc 1
2016-09-07T00:05:25.707565: step 5710, loss 0.00108813, acc 1
2016-09-07T00:05:26.408436: step 5711, loss 0.0265588, acc 0.98
2016-09-07T00:05:27.118637: step 5712, loss 0.0382878, acc 0.96
2016-09-07T00:05:27.802962: step 5713, loss 0.0230995, acc 0.98
2016-09-07T00:05:28.502788: step 5714, loss 0.000339765, acc 1
2016-09-07T00:05:29.181128: step 5715, loss 0.0144643, acc 1
2016-09-07T00:05:29.888122: step 5716, loss 0.0480492, acc 0.98
2016-09-07T00:05:30.568445: step 5717, loss 0.0468552, acc 0.96
2016-09-07T00:05:31.282821: step 5718, loss 0.0167255, acc 0.98
2016-09-07T00:05:31.991488: step 5719, loss 0.0604068, acc 0.98
2016-09-07T00:05:32.681670: step 5720, loss 0.00639257, acc 1
2016-09-07T00:05:33.389435: step 5721, loss 4.86217e-05, acc 1
2016-09-07T00:05:34.078246: step 5722, loss 0.0111895, acc 1
2016-09-07T00:05:34.769922: step 5723, loss 0.00508709, acc 1
2016-09-07T00:05:35.436617: step 5724, loss 0.00720616, acc 1
2016-09-07T00:05:36.131596: step 5725, loss 0.000147954, acc 1
2016-09-07T00:05:36.804116: step 5726, loss 0.0166655, acc 1
2016-09-07T00:05:37.509330: step 5727, loss 0.00596189, acc 1
2016-09-07T00:05:38.178336: step 5728, loss 0.0299267, acc 1
2016-09-07T00:05:38.867241: step 5729, loss 0.0321962, acc 0.98
2016-09-07T00:05:39.578455: step 5730, loss 0.000333275, acc 1
2016-09-07T00:05:40.236768: step 5731, loss 0.00789358, acc 1
2016-09-07T00:05:40.950706: step 5732, loss 0.109507, acc 0.98
2016-09-07T00:05:41.637921: step 5733, loss 0.0426102, acc 0.96
2016-09-07T00:05:42.318019: step 5734, loss 0.0150855, acc 1
2016-09-07T00:05:43.003997: step 5735, loss 0.102551, acc 0.98
2016-09-07T00:05:43.694356: step 5736, loss 0.0158098, acc 1
2016-09-07T00:05:44.395602: step 5737, loss 0.021571, acc 1
2016-09-07T00:05:45.066660: step 5738, loss 0.00417638, acc 1
2016-09-07T00:05:45.745155: step 5739, loss 0.0116051, acc 1
2016-09-07T00:05:46.428245: step 5740, loss 0.0267327, acc 0.98
2016-09-07T00:05:47.109541: step 5741, loss 0.108507, acc 0.98
2016-09-07T00:05:47.796330: step 5742, loss 0.0380058, acc 0.98
2016-09-07T00:05:48.467608: step 5743, loss 0.0356105, acc 0.96
2016-09-07T00:05:49.181263: step 5744, loss 0.0284708, acc 0.98
2016-09-07T00:05:49.881733: step 5745, loss 0.0149198, acc 0.98
2016-09-07T00:05:50.567914: step 5746, loss 0.00508111, acc 1
2016-09-07T00:05:51.257235: step 5747, loss 0.00965171, acc 1
2016-09-07T00:05:51.941634: step 5748, loss 0.0740688, acc 0.98
2016-09-07T00:05:52.631827: step 5749, loss 0.0138948, acc 1
2016-09-07T00:05:53.319058: step 5750, loss 0.00519204, acc 1
2016-09-07T00:05:54.026653: step 5751, loss 0.0481995, acc 0.98
2016-09-07T00:05:54.720576: step 5752, loss 0.056132, acc 0.96
2016-09-07T00:05:55.406030: step 5753, loss 0.0210638, acc 1
2016-09-07T00:05:56.091781: step 5754, loss 0.0473965, acc 0.98
2016-09-07T00:05:56.788567: step 5755, loss 0.00625073, acc 1
2016-09-07T00:05:57.476447: step 5756, loss 0.00301605, acc 1
2016-09-07T00:05:58.150596: step 5757, loss 0.0150871, acc 1
2016-09-07T00:05:58.840672: step 5758, loss 0.0237495, acc 0.98
2016-09-07T00:05:59.512528: step 5759, loss 0.0122428, acc 1
2016-09-07T00:06:00.147956: step 5760, loss 0.00474818, acc 1

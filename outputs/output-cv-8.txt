WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f080f52ce90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f080f52ce50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
CV_INDEX=8
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=30
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9594/1066
Writing to /home/cil/lstm-context-embeddings/runs/1473190736

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-07T03:39:16.375098: step 1, loss 0.693147, acc 0.46
2016-09-07T03:39:17.053907: step 2, loss 0.691156, acc 0.56
2016-09-07T03:39:17.738376: step 3, loss 0.68045, acc 0.58
2016-09-07T03:39:18.399752: step 4, loss 0.711051, acc 0.48
2016-09-07T03:39:19.092180: step 5, loss 0.716976, acc 0.54
2016-09-07T03:39:19.763888: step 6, loss 0.70091, acc 0.42
2016-09-07T03:39:20.463828: step 7, loss 0.681243, acc 0.58
2016-09-07T03:39:21.140055: step 8, loss 0.74385, acc 0.4
2016-09-07T03:39:21.811192: step 9, loss 0.67811, acc 0.56
2016-09-07T03:39:22.475071: step 10, loss 0.696246, acc 0.52
2016-09-07T03:39:23.149192: step 11, loss 0.718172, acc 0.5
2016-09-07T03:39:23.837742: step 12, loss 0.69218, acc 0.6
2016-09-07T03:39:24.494258: step 13, loss 0.691984, acc 0.44
2016-09-07T03:39:25.192331: step 14, loss 0.710885, acc 0.56
2016-09-07T03:39:25.864907: step 15, loss 0.714522, acc 0.5
2016-09-07T03:39:26.555955: step 16, loss 0.691966, acc 0.52
2016-09-07T03:39:27.265972: step 17, loss 0.658007, acc 0.68
2016-09-07T03:39:27.951278: step 18, loss 0.68547, acc 0.62
2016-09-07T03:39:28.625225: step 19, loss 0.680573, acc 0.52
2016-09-07T03:39:29.311019: step 20, loss 0.659216, acc 0.6
2016-09-07T03:39:30.025376: step 21, loss 0.670003, acc 0.64
2016-09-07T03:39:30.718695: step 22, loss 0.684415, acc 0.58
2016-09-07T03:39:31.389509: step 23, loss 0.659573, acc 0.62
2016-09-07T03:39:32.086369: step 24, loss 0.663709, acc 0.58
2016-09-07T03:39:32.765763: step 25, loss 0.691393, acc 0.56
2016-09-07T03:39:33.449420: step 26, loss 0.573613, acc 0.72
2016-09-07T03:39:34.131179: step 27, loss 0.739649, acc 0.56
2016-09-07T03:39:34.826360: step 28, loss 0.633998, acc 0.56
2016-09-07T03:39:35.495002: step 29, loss 0.633476, acc 0.7
2016-09-07T03:39:36.173073: step 30, loss 0.638677, acc 0.64
2016-09-07T03:39:36.855793: step 31, loss 0.752445, acc 0.58
2016-09-07T03:39:37.545900: step 32, loss 0.733867, acc 0.5
2016-09-07T03:39:38.240428: step 33, loss 0.788463, acc 0.46
2016-09-07T03:39:38.930083: step 34, loss 0.701539, acc 0.54
2016-09-07T03:39:39.646924: step 35, loss 0.726425, acc 0.5
2016-09-07T03:39:40.330448: step 36, loss 0.68062, acc 0.52
2016-09-07T03:39:41.010202: step 37, loss 0.79444, acc 0.38
2016-09-07T03:39:41.695148: step 38, loss 0.68376, acc 0.54
2016-09-07T03:39:42.373194: step 39, loss 0.62233, acc 0.6
2016-09-07T03:39:43.052597: step 40, loss 0.713642, acc 0.44
2016-09-07T03:39:43.722660: step 41, loss 0.621755, acc 0.64
2016-09-07T03:39:44.436036: step 42, loss 0.626276, acc 0.58
2016-09-07T03:39:45.108175: step 43, loss 0.640487, acc 0.66
2016-09-07T03:39:45.793723: step 44, loss 0.58852, acc 0.74
2016-09-07T03:39:46.478240: step 45, loss 0.709781, acc 0.64
2016-09-07T03:39:47.169600: step 46, loss 0.555408, acc 0.68
2016-09-07T03:39:47.859039: step 47, loss 0.564537, acc 0.7
2016-09-07T03:39:48.549951: step 48, loss 0.521518, acc 0.74
2016-09-07T03:39:49.258783: step 49, loss 0.696764, acc 0.66
2016-09-07T03:39:49.932895: step 50, loss 0.694288, acc 0.68
2016-09-07T03:39:50.622426: step 51, loss 0.650391, acc 0.58
2016-09-07T03:39:51.307402: step 52, loss 0.458311, acc 0.82
2016-09-07T03:39:51.993183: step 53, loss 0.579951, acc 0.74
2016-09-07T03:39:52.714793: step 54, loss 0.574098, acc 0.74
2016-09-07T03:39:53.397539: step 55, loss 0.573867, acc 0.74
2016-09-07T03:39:54.109981: step 56, loss 0.666001, acc 0.6
2016-09-07T03:39:54.776770: step 57, loss 0.525451, acc 0.8
2016-09-07T03:39:55.440544: step 58, loss 0.649212, acc 0.62
2016-09-07T03:39:56.121850: step 59, loss 0.518749, acc 0.82
2016-09-07T03:39:56.827726: step 60, loss 0.546073, acc 0.74
2016-09-07T03:39:57.534025: step 61, loss 0.579904, acc 0.76
2016-09-07T03:39:58.187304: step 62, loss 0.466569, acc 0.76
2016-09-07T03:39:58.877557: step 63, loss 0.524849, acc 0.66
2016-09-07T03:39:59.558055: step 64, loss 0.511822, acc 0.74
2016-09-07T03:40:00.251757: step 65, loss 0.732852, acc 0.62
2016-09-07T03:40:00.918775: step 66, loss 0.705194, acc 0.6
2016-09-07T03:40:01.605218: step 67, loss 0.585282, acc 0.64
2016-09-07T03:40:02.293293: step 68, loss 0.449212, acc 0.82
2016-09-07T03:40:02.955430: step 69, loss 0.569182, acc 0.7
2016-09-07T03:40:03.662344: step 70, loss 0.587759, acc 0.74
2016-09-07T03:40:04.325492: step 71, loss 0.517206, acc 0.7
2016-09-07T03:40:04.998877: step 72, loss 0.416854, acc 0.84
2016-09-07T03:40:05.688782: step 73, loss 0.560053, acc 0.7
2016-09-07T03:40:06.388453: step 74, loss 0.492636, acc 0.72
2016-09-07T03:40:07.083964: step 75, loss 0.433327, acc 0.82
2016-09-07T03:40:07.754292: step 76, loss 0.648512, acc 0.64
2016-09-07T03:40:08.451950: step 77, loss 0.492754, acc 0.76
2016-09-07T03:40:09.130864: step 78, loss 0.62855, acc 0.64
2016-09-07T03:40:09.816010: step 79, loss 0.663343, acc 0.62
2016-09-07T03:40:10.500356: step 80, loss 0.573498, acc 0.64
2016-09-07T03:40:11.196281: step 81, loss 0.433381, acc 0.84
2016-09-07T03:40:11.880996: step 82, loss 0.60922, acc 0.72
2016-09-07T03:40:12.550697: step 83, loss 0.606806, acc 0.72
2016-09-07T03:40:13.235477: step 84, loss 0.591661, acc 0.7
2016-09-07T03:40:13.913270: step 85, loss 0.636146, acc 0.64
2016-09-07T03:40:14.612722: step 86, loss 0.539905, acc 0.76
2016-09-07T03:40:15.299241: step 87, loss 0.562171, acc 0.72
2016-09-07T03:40:15.983551: step 88, loss 0.475371, acc 0.8
2016-09-07T03:40:16.675046: step 89, loss 0.524465, acc 0.68
2016-09-07T03:40:17.382502: step 90, loss 0.531836, acc 0.74
2016-09-07T03:40:18.091072: step 91, loss 0.520719, acc 0.76
2016-09-07T03:40:18.790192: step 92, loss 0.519684, acc 0.74
2016-09-07T03:40:19.474383: step 93, loss 0.491532, acc 0.76
2016-09-07T03:40:20.157554: step 94, loss 0.651554, acc 0.62
2016-09-07T03:40:20.846634: step 95, loss 0.608245, acc 0.6
2016-09-07T03:40:21.518385: step 96, loss 0.467246, acc 0.72
2016-09-07T03:40:22.188070: step 97, loss 0.563643, acc 0.7
2016-09-07T03:40:22.898980: step 98, loss 0.47415, acc 0.82
2016-09-07T03:40:23.564804: step 99, loss 0.473145, acc 0.76
2016-09-07T03:40:24.254611: step 100, loss 0.505064, acc 0.78

Evaluation:
2016-09-07T03:40:27.812624: step 100, loss 0.497573, acc 0.771107

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-100

2016-09-07T03:40:29.573037: step 101, loss 0.364479, acc 0.86
2016-09-07T03:40:30.256468: step 102, loss 0.436057, acc 0.78
2016-09-07T03:40:30.959525: step 103, loss 0.476826, acc 0.72
2016-09-07T03:40:31.657287: step 104, loss 0.609164, acc 0.68
2016-09-07T03:40:32.341357: step 105, loss 0.458393, acc 0.8
2016-09-07T03:40:33.019408: step 106, loss 0.574249, acc 0.66
2016-09-07T03:40:33.717136: step 107, loss 0.672511, acc 0.6
2016-09-07T03:40:34.391044: step 108, loss 0.622149, acc 0.6
2016-09-07T03:40:35.082457: step 109, loss 0.581651, acc 0.78
2016-09-07T03:40:35.780642: step 110, loss 0.542286, acc 0.76
2016-09-07T03:40:36.495750: step 111, loss 0.479834, acc 0.76
2016-09-07T03:40:37.172874: step 112, loss 0.478206, acc 0.8
2016-09-07T03:40:37.827947: step 113, loss 0.462951, acc 0.78
2016-09-07T03:40:38.537044: step 114, loss 0.548834, acc 0.72
2016-09-07T03:40:39.203874: step 115, loss 0.520949, acc 0.74
2016-09-07T03:40:39.911903: step 116, loss 0.573415, acc 0.64
2016-09-07T03:40:40.619415: step 117, loss 0.492681, acc 0.76
2016-09-07T03:40:41.328225: step 118, loss 0.417483, acc 0.76
2016-09-07T03:40:42.055995: step 119, loss 0.483112, acc 0.8
2016-09-07T03:40:42.717287: step 120, loss 0.474568, acc 0.8
2016-09-07T03:40:43.428340: step 121, loss 0.425174, acc 0.82
2016-09-07T03:40:44.113695: step 122, loss 0.510717, acc 0.72
2016-09-07T03:40:44.801670: step 123, loss 0.491101, acc 0.76
2016-09-07T03:40:45.481211: step 124, loss 0.54323, acc 0.72
2016-09-07T03:40:46.190214: step 125, loss 0.548259, acc 0.76
2016-09-07T03:40:46.878062: step 126, loss 0.480684, acc 0.72
2016-09-07T03:40:47.545001: step 127, loss 0.51414, acc 0.72
2016-09-07T03:40:48.237821: step 128, loss 0.443599, acc 0.76
2016-09-07T03:40:48.933153: step 129, loss 0.598161, acc 0.7
2016-09-07T03:40:49.620784: step 130, loss 0.519414, acc 0.74
2016-09-07T03:40:50.327426: step 131, loss 0.394031, acc 0.78
2016-09-07T03:40:51.012855: step 132, loss 0.573887, acc 0.78
2016-09-07T03:40:51.723964: step 133, loss 0.482006, acc 0.78
2016-09-07T03:40:52.378516: step 134, loss 0.53477, acc 0.76
2016-09-07T03:40:53.081509: step 135, loss 0.498456, acc 0.84
2016-09-07T03:40:53.762123: step 136, loss 0.522769, acc 0.7
2016-09-07T03:40:54.455732: step 137, loss 0.558801, acc 0.68
2016-09-07T03:40:55.155888: step 138, loss 0.489024, acc 0.8
2016-09-07T03:40:55.850528: step 139, loss 0.51754, acc 0.76
2016-09-07T03:40:56.544214: step 140, loss 0.472031, acc 0.76
2016-09-07T03:40:57.226930: step 141, loss 0.531466, acc 0.78
2016-09-07T03:40:57.916479: step 142, loss 0.492649, acc 0.72
2016-09-07T03:40:58.599466: step 143, loss 0.512078, acc 0.76
2016-09-07T03:40:59.281718: step 144, loss 0.444488, acc 0.8
2016-09-07T03:40:59.983001: step 145, loss 0.450799, acc 0.82
2016-09-07T03:41:00.718932: step 146, loss 0.658122, acc 0.64
2016-09-07T03:41:01.422854: step 147, loss 0.488451, acc 0.76
2016-09-07T03:41:02.093219: step 148, loss 0.665128, acc 0.58
2016-09-07T03:41:02.773574: step 149, loss 0.522278, acc 0.68
2016-09-07T03:41:03.445278: step 150, loss 0.573511, acc 0.7
2016-09-07T03:41:04.129118: step 151, loss 0.56286, acc 0.68
2016-09-07T03:41:04.805496: step 152, loss 0.626286, acc 0.64
2016-09-07T03:41:05.501635: step 153, loss 0.580593, acc 0.7
2016-09-07T03:41:06.182586: step 154, loss 0.540323, acc 0.7
2016-09-07T03:41:06.847849: step 155, loss 0.548766, acc 0.72
2016-09-07T03:41:07.536278: step 156, loss 0.565016, acc 0.76
2016-09-07T03:41:08.211435: step 157, loss 0.457786, acc 0.82
2016-09-07T03:41:08.899963: step 158, loss 0.573862, acc 0.68
2016-09-07T03:41:09.581099: step 159, loss 0.418836, acc 0.8
2016-09-07T03:41:10.257891: step 160, loss 0.494413, acc 0.74
2016-09-07T03:41:10.948349: step 161, loss 0.516394, acc 0.78
2016-09-07T03:41:11.630619: step 162, loss 0.503836, acc 0.72
2016-09-07T03:41:12.337100: step 163, loss 0.479772, acc 0.7
2016-09-07T03:41:13.023868: step 164, loss 0.364213, acc 0.88
2016-09-07T03:41:13.706981: step 165, loss 0.4154, acc 0.82
2016-09-07T03:41:14.387208: step 166, loss 0.514403, acc 0.7
2016-09-07T03:41:15.069608: step 167, loss 0.438361, acc 0.78
2016-09-07T03:41:15.768462: step 168, loss 0.399042, acc 0.84
2016-09-07T03:41:16.436892: step 169, loss 0.354602, acc 0.84
2016-09-07T03:41:17.158768: step 170, loss 0.429077, acc 0.78
2016-09-07T03:41:17.857421: step 171, loss 0.437829, acc 0.86
2016-09-07T03:41:18.558367: step 172, loss 0.54651, acc 0.76
2016-09-07T03:41:19.255918: step 173, loss 0.407206, acc 0.78
2016-09-07T03:41:19.932030: step 174, loss 0.429179, acc 0.82
2016-09-07T03:41:20.637924: step 175, loss 0.518033, acc 0.74
2016-09-07T03:41:21.297806: step 176, loss 0.551189, acc 0.8
2016-09-07T03:41:21.974965: step 177, loss 0.418437, acc 0.82
2016-09-07T03:41:22.660484: step 178, loss 0.507255, acc 0.76
2016-09-07T03:41:23.364496: step 179, loss 0.49292, acc 0.76
2016-09-07T03:41:24.051932: step 180, loss 0.397401, acc 0.86
2016-09-07T03:41:24.734674: step 181, loss 0.536492, acc 0.7
2016-09-07T03:41:25.425087: step 182, loss 0.407707, acc 0.82
2016-09-07T03:41:26.098023: step 183, loss 0.507569, acc 0.76
2016-09-07T03:41:26.796007: step 184, loss 0.655253, acc 0.66
2016-09-07T03:41:27.488461: step 185, loss 0.539109, acc 0.76
2016-09-07T03:41:28.169028: step 186, loss 0.433231, acc 0.8
2016-09-07T03:41:28.856155: step 187, loss 0.428387, acc 0.8
2016-09-07T03:41:29.541600: step 188, loss 0.470026, acc 0.8
2016-09-07T03:41:30.259920: step 189, loss 0.495253, acc 0.8
2016-09-07T03:41:30.923362: step 190, loss 0.448243, acc 0.78
2016-09-07T03:41:31.599787: step 191, loss 0.430041, acc 0.82
2016-09-07T03:41:32.267791: step 192, loss 0.504818, acc 0.727273
2016-09-07T03:41:32.955496: step 193, loss 0.485059, acc 0.72
2016-09-07T03:41:33.645727: step 194, loss 0.237407, acc 0.94
2016-09-07T03:41:34.333835: step 195, loss 0.411593, acc 0.84
2016-09-07T03:41:35.039925: step 196, loss 0.335269, acc 0.9
2016-09-07T03:41:35.697010: step 197, loss 0.412896, acc 0.86
2016-09-07T03:41:36.409624: step 198, loss 0.255354, acc 0.92
2016-09-07T03:41:37.089409: step 199, loss 0.330748, acc 0.9
2016-09-07T03:41:37.795326: step 200, loss 0.347027, acc 0.88

Evaluation:
2016-09-07T03:41:41.378419: step 200, loss 0.484033, acc 0.790807

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-200

2016-09-07T03:41:43.090075: step 201, loss 0.416259, acc 0.88
2016-09-07T03:41:43.772272: step 202, loss 0.228304, acc 0.9
2016-09-07T03:41:44.458141: step 203, loss 0.430407, acc 0.84
2016-09-07T03:41:45.149665: step 204, loss 0.508325, acc 0.78
2016-09-07T03:41:45.843087: step 205, loss 0.390059, acc 0.84
2016-09-07T03:41:46.516613: step 206, loss 0.299124, acc 0.82
2016-09-07T03:41:47.204431: step 207, loss 0.535459, acc 0.68
2016-09-07T03:41:47.901245: step 208, loss 0.224865, acc 0.94
2016-09-07T03:41:48.606013: step 209, loss 0.325977, acc 0.86
2016-09-07T03:41:49.306460: step 210, loss 0.360391, acc 0.8
2016-09-07T03:41:50.005297: step 211, loss 0.421503, acc 0.74
2016-09-07T03:41:50.728655: step 212, loss 0.266457, acc 0.86
2016-09-07T03:41:51.387812: step 213, loss 0.577235, acc 0.66
2016-09-07T03:41:52.065374: step 214, loss 0.466161, acc 0.8
2016-09-07T03:41:52.755884: step 215, loss 0.355993, acc 0.86
2016-09-07T03:41:53.432578: step 216, loss 0.206609, acc 0.92
2016-09-07T03:41:54.122019: step 217, loss 0.335201, acc 0.88
2016-09-07T03:41:54.808114: step 218, loss 0.308122, acc 0.9
2016-09-07T03:41:55.513295: step 219, loss 0.225321, acc 0.9
2016-09-07T03:41:56.189761: step 220, loss 0.348743, acc 0.84
2016-09-07T03:41:56.879464: step 221, loss 0.352028, acc 0.84
2016-09-07T03:41:57.564309: step 222, loss 0.425872, acc 0.82
2016-09-07T03:41:58.259702: step 223, loss 0.365905, acc 0.84
2016-09-07T03:41:58.940145: step 224, loss 0.415409, acc 0.74
2016-09-07T03:41:59.619944: step 225, loss 0.391779, acc 0.84
2016-09-07T03:42:00.346454: step 226, loss 0.296647, acc 0.88
2016-09-07T03:42:01.041394: step 227, loss 0.339158, acc 0.84
2016-09-07T03:42:01.726873: step 228, loss 0.399332, acc 0.84
2016-09-07T03:42:02.434411: step 229, loss 0.373509, acc 0.82
2016-09-07T03:42:03.130703: step 230, loss 0.345731, acc 0.82
2016-09-07T03:42:03.846605: step 231, loss 0.301253, acc 0.84
2016-09-07T03:42:04.524881: step 232, loss 0.246618, acc 0.92
2016-09-07T03:42:05.216123: step 233, loss 0.429132, acc 0.76
2016-09-07T03:42:05.905708: step 234, loss 0.353834, acc 0.82
2016-09-07T03:42:06.587323: step 235, loss 0.451273, acc 0.82
2016-09-07T03:42:07.268499: step 236, loss 0.309415, acc 0.9
2016-09-07T03:42:07.981141: step 237, loss 0.291015, acc 0.92
2016-09-07T03:42:08.704293: step 238, loss 0.615437, acc 0.74
2016-09-07T03:42:09.381379: step 239, loss 0.535245, acc 0.74
2016-09-07T03:42:10.067317: step 240, loss 0.368507, acc 0.82
2016-09-07T03:42:10.758356: step 241, loss 0.457617, acc 0.8
2016-09-07T03:42:11.448265: step 242, loss 0.417453, acc 0.8
2016-09-07T03:42:12.146114: step 243, loss 0.455261, acc 0.76
2016-09-07T03:42:12.859177: step 244, loss 0.365176, acc 0.88
2016-09-07T03:42:13.562758: step 245, loss 0.35821, acc 0.86
2016-09-07T03:42:14.249691: step 246, loss 0.419022, acc 0.8
2016-09-07T03:42:14.950947: step 247, loss 0.366577, acc 0.84
2016-09-07T03:42:15.630041: step 248, loss 0.345765, acc 0.78
2016-09-07T03:42:16.315182: step 249, loss 0.300327, acc 0.88
2016-09-07T03:42:17.015731: step 250, loss 0.364528, acc 0.84
2016-09-07T03:42:17.666789: step 251, loss 0.346792, acc 0.84
2016-09-07T03:42:18.384895: step 252, loss 0.328618, acc 0.84
2016-09-07T03:42:19.082379: step 253, loss 0.314678, acc 0.92
2016-09-07T03:42:19.766704: step 254, loss 0.313443, acc 0.86
2016-09-07T03:42:20.503713: step 255, loss 0.374952, acc 0.84
2016-09-07T03:42:21.176804: step 256, loss 0.291632, acc 0.86
2016-09-07T03:42:21.862311: step 257, loss 0.513, acc 0.74
2016-09-07T03:42:22.526272: step 258, loss 0.324911, acc 0.86
2016-09-07T03:42:23.238858: step 259, loss 0.507185, acc 0.8
2016-09-07T03:42:23.905099: step 260, loss 0.484054, acc 0.74
2016-09-07T03:42:24.605319: step 261, loss 0.204214, acc 0.94
2016-09-07T03:42:25.290006: step 262, loss 0.428105, acc 0.76
2016-09-07T03:42:26.006055: step 263, loss 0.415035, acc 0.8
2016-09-07T03:42:26.704691: step 264, loss 0.286667, acc 0.88
2016-09-07T03:42:27.365844: step 265, loss 0.398969, acc 0.84
2016-09-07T03:42:28.074195: step 266, loss 0.369963, acc 0.88
2016-09-07T03:42:28.769796: step 267, loss 0.368722, acc 0.82
2016-09-07T03:42:29.462946: step 268, loss 0.422238, acc 0.84
2016-09-07T03:42:30.160265: step 269, loss 0.47017, acc 0.82
2016-09-07T03:42:30.852752: step 270, loss 0.308952, acc 0.9
2016-09-07T03:42:31.553246: step 271, loss 0.412854, acc 0.88
2016-09-07T03:42:32.232306: step 272, loss 0.310611, acc 0.9
2016-09-07T03:42:32.926320: step 273, loss 0.280861, acc 0.86
2016-09-07T03:42:33.610801: step 274, loss 0.381339, acc 0.86
2016-09-07T03:42:34.302783: step 275, loss 0.409747, acc 0.84
2016-09-07T03:42:34.993652: step 276, loss 0.393129, acc 0.84
2016-09-07T03:42:35.675545: step 277, loss 0.371123, acc 0.8
2016-09-07T03:42:36.375534: step 278, loss 0.63085, acc 0.66
2016-09-07T03:42:37.071754: step 279, loss 0.291777, acc 0.88
2016-09-07T03:42:37.774014: step 280, loss 0.329502, acc 0.86
2016-09-07T03:42:38.469360: step 281, loss 0.253443, acc 0.9
2016-09-07T03:42:39.153669: step 282, loss 0.346358, acc 0.9
2016-09-07T03:42:39.845394: step 283, loss 0.296301, acc 0.86
2016-09-07T03:42:40.554032: step 284, loss 0.313542, acc 0.92
2016-09-07T03:42:41.271434: step 285, loss 0.517833, acc 0.74
2016-09-07T03:42:41.937445: step 286, loss 0.310293, acc 0.9
2016-09-07T03:42:42.637784: step 287, loss 0.311899, acc 0.86
2016-09-07T03:42:43.349417: step 288, loss 0.401084, acc 0.82
2016-09-07T03:42:44.046657: step 289, loss 0.386602, acc 0.82
2016-09-07T03:42:44.739648: step 290, loss 0.373412, acc 0.86
2016-09-07T03:42:45.406792: step 291, loss 0.411942, acc 0.8
2016-09-07T03:42:46.100939: step 292, loss 0.319758, acc 0.82
2016-09-07T03:42:46.795162: step 293, loss 0.435309, acc 0.8
2016-09-07T03:42:47.479419: step 294, loss 0.614042, acc 0.74
2016-09-07T03:42:48.156903: step 295, loss 0.476001, acc 0.8
2016-09-07T03:42:48.851029: step 296, loss 0.350553, acc 0.82
2016-09-07T03:42:49.549902: step 297, loss 0.38526, acc 0.8
2016-09-07T03:42:50.231496: step 298, loss 0.409512, acc 0.82
2016-09-07T03:42:50.928309: step 299, loss 0.390309, acc 0.84
2016-09-07T03:42:51.618507: step 300, loss 0.293562, acc 0.9

Evaluation:
2016-09-07T03:42:55.221780: step 300, loss 0.448092, acc 0.795497

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-300

2016-09-07T03:42:56.969577: step 301, loss 0.357155, acc 0.84
2016-09-07T03:42:57.685839: step 302, loss 0.371085, acc 0.84
2016-09-07T03:42:58.364840: step 303, loss 0.38002, acc 0.84
2016-09-07T03:42:59.063397: step 304, loss 0.393547, acc 0.82
2016-09-07T03:42:59.751583: step 305, loss 0.341829, acc 0.86
2016-09-07T03:43:00.500835: step 306, loss 0.385433, acc 0.9
2016-09-07T03:43:01.193740: step 307, loss 0.21429, acc 0.9
2016-09-07T03:43:01.889271: step 308, loss 0.416, acc 0.88
2016-09-07T03:43:02.587045: step 309, loss 0.323177, acc 0.88
2016-09-07T03:43:03.284714: step 310, loss 0.373408, acc 0.82
2016-09-07T03:43:03.980262: step 311, loss 0.301375, acc 0.86
2016-09-07T03:43:04.661773: step 312, loss 0.410115, acc 0.82
2016-09-07T03:43:05.376568: step 313, loss 0.377593, acc 0.88
2016-09-07T03:43:06.075287: step 314, loss 0.3716, acc 0.82
2016-09-07T03:43:06.751948: step 315, loss 0.336778, acc 0.82
2016-09-07T03:43:07.428730: step 316, loss 0.330246, acc 0.88
2016-09-07T03:43:08.120914: step 317, loss 0.494152, acc 0.82
2016-09-07T03:43:08.803754: step 318, loss 0.380994, acc 0.84
2016-09-07T03:43:09.489577: step 319, loss 0.302793, acc 0.84
2016-09-07T03:43:10.188129: step 320, loss 0.334138, acc 0.86
2016-09-07T03:43:10.888533: step 321, loss 0.20948, acc 0.94
2016-09-07T03:43:11.569930: step 322, loss 0.395644, acc 0.76
2016-09-07T03:43:12.262587: step 323, loss 0.344211, acc 0.8
2016-09-07T03:43:12.950729: step 324, loss 0.410998, acc 0.8
2016-09-07T03:43:13.652913: step 325, loss 0.449695, acc 0.76
2016-09-07T03:43:14.332689: step 326, loss 0.230633, acc 0.94
2016-09-07T03:43:15.039516: step 327, loss 0.53138, acc 0.74
2016-09-07T03:43:15.738197: step 328, loss 0.498587, acc 0.74
2016-09-07T03:43:16.446005: step 329, loss 0.420844, acc 0.8
2016-09-07T03:43:17.152199: step 330, loss 0.49981, acc 0.78
2016-09-07T03:43:17.836446: step 331, loss 0.218754, acc 0.94
2016-09-07T03:43:18.522971: step 332, loss 0.489383, acc 0.78
2016-09-07T03:43:19.218215: step 333, loss 0.730125, acc 0.66
2016-09-07T03:43:19.900928: step 334, loss 0.279201, acc 0.9
2016-09-07T03:43:20.583710: step 335, loss 0.516842, acc 0.7
2016-09-07T03:43:21.267745: step 336, loss 0.296135, acc 0.88
2016-09-07T03:43:21.973116: step 337, loss 0.319369, acc 0.82
2016-09-07T03:43:22.635450: step 338, loss 0.569805, acc 0.68
2016-09-07T03:43:23.346020: step 339, loss 0.33142, acc 0.9
2016-09-07T03:43:24.014679: step 340, loss 0.409584, acc 0.84
2016-09-07T03:43:24.698727: step 341, loss 0.326289, acc 0.86
2016-09-07T03:43:25.405318: step 342, loss 0.364616, acc 0.82
2016-09-07T03:43:26.112787: step 343, loss 0.374429, acc 0.84
2016-09-07T03:43:26.812213: step 344, loss 0.308927, acc 0.9
2016-09-07T03:43:27.479008: step 345, loss 0.473896, acc 0.8
2016-09-07T03:43:28.176442: step 346, loss 0.333889, acc 0.88
2016-09-07T03:43:28.878575: step 347, loss 0.374689, acc 0.82
2016-09-07T03:43:29.557518: step 348, loss 0.354728, acc 0.8
2016-09-07T03:43:30.235594: step 349, loss 0.534353, acc 0.76
2016-09-07T03:43:30.924577: step 350, loss 0.428519, acc 0.74
2016-09-07T03:43:31.629386: step 351, loss 0.40766, acc 0.86
2016-09-07T03:43:32.299064: step 352, loss 0.34569, acc 0.82
2016-09-07T03:43:32.997425: step 353, loss 0.219024, acc 0.92
2016-09-07T03:43:33.663050: step 354, loss 0.222986, acc 0.9
2016-09-07T03:43:34.367809: step 355, loss 0.458658, acc 0.84
2016-09-07T03:43:35.065165: step 356, loss 0.379522, acc 0.84
2016-09-07T03:43:35.752358: step 357, loss 0.290218, acc 0.92
2016-09-07T03:43:36.455692: step 358, loss 0.289725, acc 0.9
2016-09-07T03:43:37.135082: step 359, loss 0.324211, acc 0.86
2016-09-07T03:43:37.851022: step 360, loss 0.30837, acc 0.88
2016-09-07T03:43:38.540085: step 361, loss 0.327577, acc 0.88
2016-09-07T03:43:39.231651: step 362, loss 0.35846, acc 0.82
2016-09-07T03:43:39.941384: step 363, loss 0.309124, acc 0.92
2016-09-07T03:43:40.621600: step 364, loss 0.47893, acc 0.78
2016-09-07T03:43:41.326743: step 365, loss 0.262506, acc 0.88
2016-09-07T03:43:42.013447: step 366, loss 0.251908, acc 0.86
2016-09-07T03:43:42.695585: step 367, loss 0.607251, acc 0.72
2016-09-07T03:43:43.392071: step 368, loss 0.360278, acc 0.8
2016-09-07T03:43:44.072517: step 369, loss 0.395822, acc 0.8
2016-09-07T03:43:44.781973: step 370, loss 0.495281, acc 0.74
2016-09-07T03:43:45.485253: step 371, loss 0.295967, acc 0.88
2016-09-07T03:43:46.199160: step 372, loss 0.547204, acc 0.72
2016-09-07T03:43:46.877232: step 373, loss 0.397651, acc 0.78
2016-09-07T03:43:47.546021: step 374, loss 0.293916, acc 0.86
2016-09-07T03:43:48.237599: step 375, loss 0.288125, acc 0.9
2016-09-07T03:43:48.930713: step 376, loss 0.406665, acc 0.82
2016-09-07T03:43:49.636142: step 377, loss 0.400551, acc 0.84
2016-09-07T03:43:50.315772: step 378, loss 0.462023, acc 0.76
2016-09-07T03:43:51.011985: step 379, loss 0.435382, acc 0.82
2016-09-07T03:43:51.689711: step 380, loss 0.420303, acc 0.82
2016-09-07T03:43:52.368477: step 381, loss 0.266771, acc 0.94
2016-09-07T03:43:53.086193: step 382, loss 0.338935, acc 0.84
2016-09-07T03:43:53.770772: step 383, loss 0.445702, acc 0.78
2016-09-07T03:43:54.408229: step 384, loss 0.346965, acc 0.818182
2016-09-07T03:43:55.111081: step 385, loss 0.195015, acc 0.96
2016-09-07T03:43:55.811387: step 386, loss 0.18356, acc 0.98
2016-09-07T03:43:56.480922: step 387, loss 0.266786, acc 0.84
2016-09-07T03:43:57.162659: step 388, loss 0.214323, acc 0.92
2016-09-07T03:43:57.864671: step 389, loss 0.238325, acc 0.96
2016-09-07T03:43:58.572459: step 390, loss 0.23803, acc 0.88
2016-09-07T03:43:59.262528: step 391, loss 0.121676, acc 0.96
2016-09-07T03:43:59.952153: step 392, loss 0.12471, acc 0.96
2016-09-07T03:44:00.696965: step 393, loss 0.249705, acc 0.9
2016-09-07T03:44:01.384163: step 394, loss 0.162792, acc 0.96
2016-09-07T03:44:02.066040: step 395, loss 0.209869, acc 0.9
2016-09-07T03:44:02.746346: step 396, loss 0.141269, acc 0.9
2016-09-07T03:44:03.434867: step 397, loss 0.137262, acc 0.92
2016-09-07T03:44:04.154408: step 398, loss 0.0753193, acc 0.96
2016-09-07T03:44:04.835128: step 399, loss 0.304374, acc 0.94
2016-09-07T03:44:05.544156: step 400, loss 0.274454, acc 0.88

Evaluation:
2016-09-07T03:44:09.125831: step 400, loss 0.609748, acc 0.789869

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-400

2016-09-07T03:44:10.873908: step 401, loss 0.184245, acc 0.9
2016-09-07T03:44:11.565932: step 402, loss 0.238407, acc 0.94
2016-09-07T03:44:12.258021: step 403, loss 0.192125, acc 0.9
2016-09-07T03:44:12.950591: step 404, loss 0.178061, acc 0.92
2016-09-07T03:44:13.669861: step 405, loss 0.358675, acc 0.84
2016-09-07T03:44:14.345763: step 406, loss 0.211224, acc 0.88
2016-09-07T03:44:15.041811: step 407, loss 0.301817, acc 0.9
2016-09-07T03:44:15.712464: step 408, loss 0.14407, acc 0.96
2016-09-07T03:44:16.406656: step 409, loss 0.239409, acc 0.86
2016-09-07T03:44:17.102215: step 410, loss 0.138214, acc 0.96
2016-09-07T03:44:17.787997: step 411, loss 0.140712, acc 0.94
2016-09-07T03:44:18.480593: step 412, loss 0.187143, acc 0.88
2016-09-07T03:44:19.142650: step 413, loss 0.403646, acc 0.84
2016-09-07T03:44:19.842124: step 414, loss 0.328884, acc 0.82
2016-09-07T03:44:20.511842: step 415, loss 0.0538034, acc 0.98
2016-09-07T03:44:21.193111: step 416, loss 0.219372, acc 0.9
2016-09-07T03:44:21.891361: step 417, loss 0.291746, acc 0.84
2016-09-07T03:44:22.591213: step 418, loss 0.262594, acc 0.86
2016-09-07T03:44:23.293702: step 419, loss 0.195031, acc 0.96
2016-09-07T03:44:23.969026: step 420, loss 0.141182, acc 0.96
2016-09-07T03:44:24.678245: step 421, loss 0.208285, acc 0.9
2016-09-07T03:44:25.347508: step 422, loss 0.262236, acc 0.9
2016-09-07T03:44:26.046933: step 423, loss 0.338756, acc 0.86
2016-09-07T03:44:26.761944: step 424, loss 0.257364, acc 0.84
2016-09-07T03:44:27.447644: step 425, loss 0.319308, acc 0.88
2016-09-07T03:44:28.137226: step 426, loss 0.19235, acc 0.92
2016-09-07T03:44:28.805830: step 427, loss 0.20442, acc 0.94
2016-09-07T03:44:29.504903: step 428, loss 0.188539, acc 0.9
2016-09-07T03:44:30.189945: step 429, loss 0.169447, acc 0.94
2016-09-07T03:44:30.873062: step 430, loss 0.222114, acc 0.88
2016-09-07T03:44:31.570698: step 431, loss 0.368738, acc 0.88
2016-09-07T03:44:32.259687: step 432, loss 0.216939, acc 0.9
2016-09-07T03:44:32.982588: step 433, loss 0.316864, acc 0.84
2016-09-07T03:44:33.659921: step 434, loss 0.167464, acc 0.94
2016-09-07T03:44:34.373135: step 435, loss 0.244977, acc 0.94
2016-09-07T03:44:35.064912: step 436, loss 0.159418, acc 0.96
2016-09-07T03:44:35.746018: step 437, loss 0.366874, acc 0.82
2016-09-07T03:44:36.417895: step 438, loss 0.337513, acc 0.88
2016-09-07T03:44:37.121022: step 439, loss 0.147135, acc 0.92
2016-09-07T03:44:37.844121: step 440, loss 0.272629, acc 0.86
2016-09-07T03:44:38.523557: step 441, loss 0.232698, acc 0.92
2016-09-07T03:44:39.212880: step 442, loss 0.236627, acc 0.9
2016-09-07T03:44:39.902353: step 443, loss 0.428641, acc 0.86
2016-09-07T03:44:40.607370: step 444, loss 0.0856472, acc 1
2016-09-07T03:44:41.285352: step 445, loss 0.294577, acc 0.86
2016-09-07T03:44:41.972567: step 446, loss 0.227414, acc 0.96
2016-09-07T03:44:42.674596: step 447, loss 0.266158, acc 0.88
2016-09-07T03:44:43.350513: step 448, loss 0.226478, acc 0.9
2016-09-07T03:44:44.069496: step 449, loss 0.285053, acc 0.88
2016-09-07T03:44:44.770637: step 450, loss 0.28018, acc 0.92
2016-09-07T03:44:45.455662: step 451, loss 0.192959, acc 0.92
2016-09-07T03:44:46.160199: step 452, loss 0.334929, acc 0.8
2016-09-07T03:44:46.808703: step 453, loss 0.213507, acc 0.92
2016-09-07T03:44:47.524721: step 454, loss 0.285819, acc 0.84
2016-09-07T03:44:48.226183: step 455, loss 0.321279, acc 0.88
2016-09-07T03:44:48.913988: step 456, loss 0.223648, acc 0.92
2016-09-07T03:44:49.607530: step 457, loss 0.266983, acc 0.86
2016-09-07T03:44:50.309417: step 458, loss 0.167275, acc 0.92
2016-09-07T03:44:51.010333: step 459, loss 0.247958, acc 0.92
2016-09-07T03:44:51.664321: step 460, loss 0.314425, acc 0.86
2016-09-07T03:44:52.377863: step 461, loss 0.248728, acc 0.96
2016-09-07T03:44:53.071583: step 462, loss 0.187822, acc 0.94
2016-09-07T03:44:53.774815: step 463, loss 0.178365, acc 0.9
2016-09-07T03:44:54.476043: step 464, loss 0.338141, acc 0.86
2016-09-07T03:44:55.194025: step 465, loss 0.211601, acc 0.9
2016-09-07T03:44:55.913976: step 466, loss 0.239422, acc 0.88
2016-09-07T03:44:56.603962: step 467, loss 0.28337, acc 0.9
2016-09-07T03:44:57.281421: step 468, loss 0.281814, acc 0.9
2016-09-07T03:44:57.970613: step 469, loss 0.193492, acc 0.92
2016-09-07T03:44:58.665391: step 470, loss 0.26291, acc 0.88
2016-09-07T03:44:59.364466: step 471, loss 0.351354, acc 0.82
2016-09-07T03:45:00.039037: step 472, loss 0.157066, acc 0.96
2016-09-07T03:45:00.787242: step 473, loss 0.250232, acc 0.86
2016-09-07T03:45:01.480276: step 474, loss 0.282522, acc 0.88
2016-09-07T03:45:02.167484: step 475, loss 0.354313, acc 0.86
2016-09-07T03:45:02.876521: step 476, loss 0.398458, acc 0.9
2016-09-07T03:45:03.566819: step 477, loss 0.118445, acc 0.96
2016-09-07T03:45:04.284818: step 478, loss 0.207231, acc 0.9
2016-09-07T03:45:04.969770: step 479, loss 0.171016, acc 0.92
2016-09-07T03:45:05.650255: step 480, loss 0.299985, acc 0.82
2016-09-07T03:45:06.341382: step 481, loss 0.343371, acc 0.86
2016-09-07T03:45:07.036598: step 482, loss 0.210346, acc 0.88
2016-09-07T03:45:07.733531: step 483, loss 0.32191, acc 0.88
2016-09-07T03:45:08.428054: step 484, loss 0.330251, acc 0.86
2016-09-07T03:45:09.130821: step 485, loss 0.231668, acc 0.9
2016-09-07T03:45:09.813023: step 486, loss 0.295306, acc 0.86
2016-09-07T03:45:10.488417: step 487, loss 0.158439, acc 1
2016-09-07T03:45:11.194107: step 488, loss 0.328586, acc 0.92
2016-09-07T03:45:11.870751: step 489, loss 0.105108, acc 0.96
2016-09-07T03:45:12.588325: step 490, loss 0.168144, acc 0.96
2016-09-07T03:45:13.260271: step 491, loss 0.431942, acc 0.86
2016-09-07T03:45:13.983671: step 492, loss 0.307445, acc 0.84
2016-09-07T03:45:14.682973: step 493, loss 0.232458, acc 0.88
2016-09-07T03:45:15.369269: step 494, loss 0.175993, acc 0.94
2016-09-07T03:45:16.070015: step 495, loss 0.288127, acc 0.84
2016-09-07T03:45:16.762439: step 496, loss 0.177423, acc 0.92
2016-09-07T03:45:17.443579: step 497, loss 0.177533, acc 0.94
2016-09-07T03:45:18.122305: step 498, loss 0.303606, acc 0.9
2016-09-07T03:45:18.826767: step 499, loss 0.329672, acc 0.84
2016-09-07T03:45:19.509915: step 500, loss 0.496215, acc 0.84

Evaluation:
2016-09-07T03:45:23.141094: step 500, loss 0.509929, acc 0.786116

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-500

2016-09-07T03:45:24.915372: step 501, loss 0.338965, acc 0.8
2016-09-07T03:45:25.635426: step 502, loss 0.215194, acc 0.9
2016-09-07T03:45:26.319517: step 503, loss 0.308109, acc 0.9
2016-09-07T03:45:27.028830: step 504, loss 0.28395, acc 0.82
2016-09-07T03:45:27.721947: step 505, loss 0.243047, acc 0.92
2016-09-07T03:45:28.453245: step 506, loss 0.258944, acc 0.88
2016-09-07T03:45:29.162599: step 507, loss 0.191034, acc 0.96
2016-09-07T03:45:29.862258: step 508, loss 0.2464, acc 0.92
2016-09-07T03:45:30.564042: step 509, loss 0.407598, acc 0.78
2016-09-07T03:45:31.253264: step 510, loss 0.229376, acc 0.94
2016-09-07T03:45:31.962193: step 511, loss 0.210959, acc 0.92
2016-09-07T03:45:32.663508: step 512, loss 0.165223, acc 0.96
2016-09-07T03:45:33.344778: step 513, loss 0.271932, acc 0.86
2016-09-07T03:45:34.058176: step 514, loss 0.185947, acc 0.96
2016-09-07T03:45:34.747160: step 515, loss 0.221177, acc 0.9
2016-09-07T03:45:35.447670: step 516, loss 0.12726, acc 0.94
2016-09-07T03:45:36.136976: step 517, loss 0.193683, acc 0.94
2016-09-07T03:45:36.845614: step 518, loss 0.205956, acc 0.92
2016-09-07T03:45:37.536388: step 519, loss 0.246275, acc 0.92
2016-09-07T03:45:38.205439: step 520, loss 0.139856, acc 0.96
2016-09-07T03:45:38.898073: step 521, loss 0.232531, acc 0.84
2016-09-07T03:45:39.597765: step 522, loss 0.208154, acc 0.9
2016-09-07T03:45:40.315341: step 523, loss 0.348864, acc 0.82
2016-09-07T03:45:40.996020: step 524, loss 0.289238, acc 0.92
2016-09-07T03:45:41.687295: step 525, loss 0.306271, acc 0.84
2016-09-07T03:45:42.378344: step 526, loss 0.354278, acc 0.86
2016-09-07T03:45:43.069969: step 527, loss 0.311613, acc 0.86
2016-09-07T03:45:43.757656: step 528, loss 0.110848, acc 0.98
2016-09-07T03:45:44.449038: step 529, loss 0.549582, acc 0.76
2016-09-07T03:45:45.167157: step 530, loss 0.250062, acc 0.84
2016-09-07T03:45:45.833354: step 531, loss 0.316345, acc 0.82
2016-09-07T03:45:46.532337: step 532, loss 0.248201, acc 0.88
2016-09-07T03:45:47.222285: step 533, loss 0.195524, acc 0.9
2016-09-07T03:45:47.914668: step 534, loss 0.181238, acc 0.9
2016-09-07T03:45:48.625138: step 535, loss 0.377075, acc 0.86
2016-09-07T03:45:49.320039: step 536, loss 0.225586, acc 0.86
2016-09-07T03:45:50.033695: step 537, loss 0.253539, acc 0.84
2016-09-07T03:45:50.726677: step 538, loss 0.228894, acc 0.94
2016-09-07T03:45:51.430293: step 539, loss 0.246874, acc 0.9
2016-09-07T03:45:52.121239: step 540, loss 0.307313, acc 0.84
2016-09-07T03:45:52.824215: step 541, loss 0.141296, acc 0.98
2016-09-07T03:45:53.493432: step 542, loss 0.133346, acc 0.96
2016-09-07T03:45:54.156947: step 543, loss 0.479272, acc 0.8
2016-09-07T03:45:54.862680: step 544, loss 0.255812, acc 0.9
2016-09-07T03:45:55.545660: step 545, loss 0.276968, acc 0.9
2016-09-07T03:45:56.236612: step 546, loss 0.402321, acc 0.84
2016-09-07T03:45:56.922447: step 547, loss 0.515043, acc 0.78
2016-09-07T03:45:57.604743: step 548, loss 0.318291, acc 0.86
2016-09-07T03:45:58.294862: step 549, loss 0.346291, acc 0.84
2016-09-07T03:45:58.970197: step 550, loss 0.202554, acc 0.96
2016-09-07T03:45:59.690363: step 551, loss 0.172552, acc 0.98
2016-09-07T03:46:00.412893: step 552, loss 0.282186, acc 0.82
2016-09-07T03:46:01.118536: step 553, loss 0.381601, acc 0.84
2016-09-07T03:46:01.812139: step 554, loss 0.364469, acc 0.86
2016-09-07T03:46:02.513743: step 555, loss 0.227905, acc 0.9
2016-09-07T03:46:03.216636: step 556, loss 0.362798, acc 0.84
2016-09-07T03:46:03.901879: step 557, loss 0.243312, acc 0.9
2016-09-07T03:46:04.590689: step 558, loss 0.319953, acc 0.88
2016-09-07T03:46:05.296910: step 559, loss 0.192419, acc 0.94
2016-09-07T03:46:05.995497: step 560, loss 0.229488, acc 0.92
2016-09-07T03:46:06.699309: step 561, loss 0.206133, acc 0.94
2016-09-07T03:46:07.366660: step 562, loss 0.24657, acc 0.88
2016-09-07T03:46:08.072108: step 563, loss 0.40886, acc 0.84
2016-09-07T03:46:08.754336: step 564, loss 0.364804, acc 0.88
2016-09-07T03:46:09.442027: step 565, loss 0.252187, acc 0.94
2016-09-07T03:46:10.140480: step 566, loss 0.212427, acc 0.94
2016-09-07T03:46:10.841256: step 567, loss 0.189756, acc 0.92
2016-09-07T03:46:11.543763: step 568, loss 0.325014, acc 0.88
2016-09-07T03:46:12.204964: step 569, loss 0.208377, acc 0.94
2016-09-07T03:46:12.918194: step 570, loss 0.186108, acc 0.94
2016-09-07T03:46:13.626623: step 571, loss 0.158499, acc 0.94
2016-09-07T03:46:14.305442: step 572, loss 0.219687, acc 0.9
2016-09-07T03:46:15.012854: step 573, loss 0.280643, acc 0.86
2016-09-07T03:46:15.710886: step 574, loss 0.220708, acc 0.84
2016-09-07T03:46:16.411607: step 575, loss 0.144907, acc 0.96
2016-09-07T03:46:17.043687: step 576, loss 0.243899, acc 0.886364
2016-09-07T03:46:17.754673: step 577, loss 0.0829768, acc 0.98
2016-09-07T03:46:18.490945: step 578, loss 0.114534, acc 0.92
2016-09-07T03:46:19.183753: step 579, loss 0.186862, acc 0.92
2016-09-07T03:46:19.869968: step 580, loss 0.231206, acc 0.92
2016-09-07T03:46:20.560128: step 581, loss 0.134188, acc 0.94
2016-09-07T03:46:21.265417: step 582, loss 0.162558, acc 0.96
2016-09-07T03:46:21.944374: step 583, loss 0.207272, acc 0.9
2016-09-07T03:46:22.622109: step 584, loss 0.187499, acc 0.88
2016-09-07T03:46:23.335899: step 585, loss 0.113832, acc 0.96
2016-09-07T03:46:24.039581: step 586, loss 0.253035, acc 0.88
2016-09-07T03:46:24.754143: step 587, loss 0.113519, acc 0.94
2016-09-07T03:46:25.463225: step 588, loss 0.0732268, acc 0.98
2016-09-07T03:46:26.168542: step 589, loss 0.11506, acc 0.96
2016-09-07T03:46:26.853208: step 590, loss 0.144529, acc 0.94
2016-09-07T03:46:27.536643: step 591, loss 0.148389, acc 0.96
2016-09-07T03:46:28.220161: step 592, loss 0.105343, acc 0.96
2016-09-07T03:46:28.945279: step 593, loss 0.148312, acc 0.94
2016-09-07T03:46:29.662612: step 594, loss 0.242009, acc 0.92
2016-09-07T03:46:30.347923: step 595, loss 0.282203, acc 0.94
2016-09-07T03:46:31.050180: step 596, loss 0.156277, acc 0.92
2016-09-07T03:46:31.738091: step 597, loss 0.169416, acc 0.9
2016-09-07T03:46:32.422604: step 598, loss 0.316835, acc 0.84
2016-09-07T03:46:33.093776: step 599, loss 0.120254, acc 0.94
2016-09-07T03:46:33.800336: step 600, loss 0.120516, acc 0.94

Evaluation:
2016-09-07T03:46:37.428894: step 600, loss 0.621094, acc 0.76454

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-600

2016-09-07T03:46:39.295620: step 601, loss 0.067733, acc 1
2016-09-07T03:46:39.979574: step 602, loss 0.135112, acc 0.94
2016-09-07T03:46:40.666476: step 603, loss 0.0767511, acc 0.96
2016-09-07T03:46:41.338511: step 604, loss 0.311891, acc 0.88
2016-09-07T03:46:42.010968: step 605, loss 0.122967, acc 0.94
2016-09-07T03:46:42.704724: step 606, loss 0.190286, acc 0.94
2016-09-07T03:46:43.388186: step 607, loss 0.18703, acc 0.92
2016-09-07T03:46:44.096769: step 608, loss 0.213481, acc 0.94
2016-09-07T03:46:44.774017: step 609, loss 0.0879848, acc 0.98
2016-09-07T03:46:45.456513: step 610, loss 0.131359, acc 0.92
2016-09-07T03:46:46.153678: step 611, loss 0.161754, acc 0.94
2016-09-07T03:46:46.843545: step 612, loss 0.203881, acc 0.86
2016-09-07T03:46:47.578857: step 613, loss 0.0646899, acc 0.98
2016-09-07T03:46:48.276506: step 614, loss 0.14505, acc 0.94
2016-09-07T03:46:48.987290: step 615, loss 0.0351366, acc 1
2016-09-07T03:46:49.663369: step 616, loss 0.102924, acc 0.96
2016-09-07T03:46:50.344429: step 617, loss 0.158024, acc 0.98
2016-09-07T03:46:51.025115: step 618, loss 0.165845, acc 0.92
2016-09-07T03:46:51.719771: step 619, loss 0.239052, acc 0.9
2016-09-07T03:46:52.401326: step 620, loss 0.231916, acc 0.9
2016-09-07T03:46:53.068214: step 621, loss 0.24806, acc 0.9
2016-09-07T03:46:53.779655: step 622, loss 0.1145, acc 0.94
2016-09-07T03:46:54.499714: step 623, loss 0.179487, acc 0.92
2016-09-07T03:46:55.192249: step 624, loss 0.166598, acc 0.96
2016-09-07T03:46:55.887321: step 625, loss 0.0845635, acc 0.96
2016-09-07T03:46:56.584209: step 626, loss 0.1233, acc 0.92
2016-09-07T03:46:57.294617: step 627, loss 0.261745, acc 0.84
2016-09-07T03:46:57.971626: step 628, loss 0.252919, acc 0.88
2016-09-07T03:46:58.684815: step 629, loss 0.240787, acc 0.88
2016-09-07T03:46:59.363977: step 630, loss 0.182116, acc 0.96
2016-09-07T03:47:00.061616: step 631, loss 0.116179, acc 0.96
2016-09-07T03:47:00.793025: step 632, loss 0.114051, acc 0.94
2016-09-07T03:47:01.447193: step 633, loss 0.138354, acc 0.96
2016-09-07T03:47:02.156975: step 634, loss 0.0530546, acc 1
2016-09-07T03:47:02.852758: step 635, loss 0.155585, acc 0.92
2016-09-07T03:47:03.540309: step 636, loss 0.155518, acc 0.94
2016-09-07T03:47:04.235388: step 637, loss 0.24509, acc 0.9
2016-09-07T03:47:04.920783: step 638, loss 0.363993, acc 0.84
2016-09-07T03:47:05.619811: step 639, loss 0.238222, acc 0.94
2016-09-07T03:47:06.293821: step 640, loss 0.163909, acc 0.92
2016-09-07T03:47:07.004441: step 641, loss 0.213333, acc 0.94
2016-09-07T03:47:07.728294: step 642, loss 0.343586, acc 0.84
2016-09-07T03:47:08.415533: step 643, loss 0.174134, acc 0.9
2016-09-07T03:47:09.103313: step 644, loss 0.211407, acc 0.88
2016-09-07T03:47:09.786596: step 645, loss 0.208865, acc 0.86
2016-09-07T03:47:10.486391: step 646, loss 0.220054, acc 0.94
2016-09-07T03:47:11.170122: step 647, loss 0.251651, acc 0.9
2016-09-07T03:47:11.889884: step 648, loss 0.128931, acc 0.94
2016-09-07T03:47:12.590465: step 649, loss 0.146501, acc 0.94
2016-09-07T03:47:13.292733: step 650, loss 0.0588536, acc 1
2016-09-07T03:47:13.987676: step 651, loss 0.122873, acc 0.94
2016-09-07T03:47:14.673541: step 652, loss 0.198565, acc 0.92
2016-09-07T03:47:15.377370: step 653, loss 0.0763185, acc 0.98
2016-09-07T03:47:16.057659: step 654, loss 0.203581, acc 0.9
2016-09-07T03:47:16.736227: step 655, loss 0.12047, acc 0.94
2016-09-07T03:47:17.428906: step 656, loss 0.224104, acc 0.9
2016-09-07T03:47:18.113129: step 657, loss 0.0724807, acc 1
2016-09-07T03:47:18.808362: step 658, loss 0.0802221, acc 1
2016-09-07T03:47:19.484610: step 659, loss 0.151239, acc 0.96
2016-09-07T03:47:20.179887: step 660, loss 0.263537, acc 0.88
2016-09-07T03:47:20.875700: step 661, loss 0.119919, acc 0.94
2016-09-07T03:47:21.573817: step 662, loss 0.140787, acc 0.96
2016-09-07T03:47:22.277787: step 663, loss 0.0556379, acc 0.98
2016-09-07T03:47:22.967694: step 664, loss 0.122895, acc 0.96
2016-09-07T03:47:23.662616: step 665, loss 0.221446, acc 0.92
2016-09-07T03:47:24.314625: step 666, loss 0.189778, acc 0.94
2016-09-07T03:47:25.018469: step 667, loss 0.272902, acc 0.82
2016-09-07T03:47:25.718143: step 668, loss 0.0839095, acc 0.96
2016-09-07T03:47:26.404244: step 669, loss 0.0901319, acc 0.98
2016-09-07T03:47:27.103299: step 670, loss 0.0955479, acc 0.94
2016-09-07T03:47:27.798967: step 671, loss 0.261252, acc 0.9
2016-09-07T03:47:28.494131: step 672, loss 0.1549, acc 0.96
2016-09-07T03:47:29.148380: step 673, loss 0.231207, acc 0.92
2016-09-07T03:47:29.853780: step 674, loss 0.194438, acc 0.88
2016-09-07T03:47:30.575283: step 675, loss 0.251936, acc 0.94
2016-09-07T03:47:31.251624: step 676, loss 0.0950002, acc 0.98
2016-09-07T03:47:31.949063: step 677, loss 0.16437, acc 0.94
2016-09-07T03:47:32.654515: step 678, loss 0.281735, acc 0.94
2016-09-07T03:47:33.373590: step 679, loss 0.105658, acc 0.94
2016-09-07T03:47:34.027735: step 680, loss 0.109103, acc 0.96
2016-09-07T03:47:34.720891: step 681, loss 0.0981449, acc 0.94
2016-09-07T03:47:35.413487: step 682, loss 0.159756, acc 0.92
2016-09-07T03:47:36.105033: step 683, loss 0.151094, acc 0.94
2016-09-07T03:47:36.785417: step 684, loss 0.200828, acc 0.88
2016-09-07T03:47:37.465405: step 685, loss 0.106933, acc 0.98
2016-09-07T03:47:38.178440: step 686, loss 0.179533, acc 0.92
2016-09-07T03:47:38.849394: step 687, loss 0.0880593, acc 0.96
2016-09-07T03:47:39.546584: step 688, loss 0.0902905, acc 0.96
2016-09-07T03:47:40.229764: step 689, loss 0.136663, acc 0.92
2016-09-07T03:47:40.925202: step 690, loss 0.369563, acc 0.92
2016-09-07T03:47:41.626528: step 691, loss 0.317355, acc 0.92
2016-09-07T03:47:42.339021: step 692, loss 0.158784, acc 0.9
2016-09-07T03:47:43.049105: step 693, loss 0.0200424, acc 1
2016-09-07T03:47:43.742833: step 694, loss 0.198313, acc 0.88
2016-09-07T03:47:44.431734: step 695, loss 0.192958, acc 0.88
2016-09-07T03:47:45.133534: step 696, loss 0.200557, acc 0.9
2016-09-07T03:47:45.827036: step 697, loss 0.180033, acc 0.92
2016-09-07T03:47:46.543759: step 698, loss 0.175517, acc 0.96
2016-09-07T03:47:47.204363: step 699, loss 0.111054, acc 0.96
2016-09-07T03:47:47.907758: step 700, loss 0.164957, acc 0.92

Evaluation:
2016-09-07T03:47:51.512426: step 700, loss 0.669715, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-700

2016-09-07T03:47:53.254480: step 701, loss 0.12349, acc 0.94
2016-09-07T03:47:53.955827: step 702, loss 0.158026, acc 0.94
2016-09-07T03:47:54.661043: step 703, loss 0.291583, acc 0.84
2016-09-07T03:47:55.348171: step 704, loss 0.282006, acc 0.86
2016-09-07T03:47:56.060703: step 705, loss 0.220871, acc 0.9
2016-09-07T03:47:56.742598: step 706, loss 0.179956, acc 0.94
2016-09-07T03:47:57.441548: step 707, loss 0.162398, acc 0.9
2016-09-07T03:47:58.108050: step 708, loss 0.138532, acc 0.94
2016-09-07T03:47:58.790837: step 709, loss 0.255621, acc 0.9
2016-09-07T03:47:59.490400: step 710, loss 0.108285, acc 0.96
2016-09-07T03:48:00.188414: step 711, loss 0.295348, acc 0.84
2016-09-07T03:48:00.894896: step 712, loss 0.24891, acc 0.92
2016-09-07T03:48:01.550647: step 713, loss 0.185804, acc 0.94
2016-09-07T03:48:02.257573: step 714, loss 0.0925978, acc 0.98
2016-09-07T03:48:02.961217: step 715, loss 0.0985092, acc 0.98
2016-09-07T03:48:03.678802: step 716, loss 0.145263, acc 0.96
2016-09-07T03:48:04.374474: step 717, loss 0.212771, acc 0.96
2016-09-07T03:48:05.059847: step 718, loss 0.273119, acc 0.86
2016-09-07T03:48:05.766175: step 719, loss 0.23764, acc 0.9
2016-09-07T03:48:06.427853: step 720, loss 0.132393, acc 0.96
2016-09-07T03:48:07.158059: step 721, loss 0.165974, acc 0.96
2016-09-07T03:48:07.853424: step 722, loss 0.135617, acc 0.94
2016-09-07T03:48:08.543254: step 723, loss 0.185406, acc 0.94
2016-09-07T03:48:09.223431: step 724, loss 0.091246, acc 0.98
2016-09-07T03:48:09.900948: step 725, loss 0.0577139, acc 1
2016-09-07T03:48:10.600550: step 726, loss 0.119611, acc 0.94
2016-09-07T03:48:11.282637: step 727, loss 0.118062, acc 0.96
2016-09-07T03:48:12.006684: step 728, loss 0.214102, acc 0.92
2016-09-07T03:48:12.715962: step 729, loss 0.396879, acc 0.84
2016-09-07T03:48:13.392061: step 730, loss 0.30959, acc 0.88
2016-09-07T03:48:14.092350: step 731, loss 0.132451, acc 0.94
2016-09-07T03:48:14.765368: step 732, loss 0.1572, acc 0.94
2016-09-07T03:48:15.480280: step 733, loss 0.145841, acc 0.94
2016-09-07T03:48:16.170798: step 734, loss 0.197, acc 0.9
2016-09-07T03:48:16.866243: step 735, loss 0.0796492, acc 0.98
2016-09-07T03:48:17.560368: step 736, loss 0.110573, acc 0.94
2016-09-07T03:48:18.262880: step 737, loss 0.0809719, acc 0.96
2016-09-07T03:48:18.964091: step 738, loss 0.109795, acc 0.96
2016-09-07T03:48:19.627914: step 739, loss 0.187193, acc 0.92
2016-09-07T03:48:20.340328: step 740, loss 0.174395, acc 0.92
2016-09-07T03:48:21.003309: step 741, loss 0.096244, acc 0.96
2016-09-07T03:48:21.699240: step 742, loss 0.247132, acc 0.92
2016-09-07T03:48:22.377597: step 743, loss 0.171106, acc 0.96
2016-09-07T03:48:23.062342: step 744, loss 0.357084, acc 0.84
2016-09-07T03:48:23.748304: step 745, loss 0.13348, acc 0.94
2016-09-07T03:48:24.411234: step 746, loss 0.302679, acc 0.9
2016-09-07T03:48:25.126278: step 747, loss 0.0700614, acc 0.98
2016-09-07T03:48:25.799704: step 748, loss 0.174662, acc 0.94
2016-09-07T03:48:26.469359: step 749, loss 0.195546, acc 0.94
2016-09-07T03:48:27.177413: step 750, loss 0.165408, acc 0.96
2016-09-07T03:48:27.869161: step 751, loss 0.226527, acc 0.92
2016-09-07T03:48:28.554416: step 752, loss 0.19232, acc 0.92
2016-09-07T03:48:29.214489: step 753, loss 0.140576, acc 0.96
2016-09-07T03:48:29.912418: step 754, loss 0.172548, acc 0.98
2016-09-07T03:48:30.576489: step 755, loss 0.276362, acc 0.92
2016-09-07T03:48:31.273723: step 756, loss 0.218482, acc 0.94
2016-09-07T03:48:31.963032: step 757, loss 0.306843, acc 0.86
2016-09-07T03:48:32.658689: step 758, loss 0.0689817, acc 1
2016-09-07T03:48:33.357090: step 759, loss 0.138908, acc 0.94
2016-09-07T03:48:34.037784: step 760, loss 0.339218, acc 0.82
2016-09-07T03:48:34.758179: step 761, loss 0.195731, acc 0.92
2016-09-07T03:48:35.410119: step 762, loss 0.124401, acc 0.94
2016-09-07T03:48:36.108546: step 763, loss 0.241766, acc 0.9
2016-09-07T03:48:36.798899: step 764, loss 0.0763808, acc 1
2016-09-07T03:48:37.475969: step 765, loss 0.265696, acc 0.86
2016-09-07T03:48:38.162423: step 766, loss 0.273746, acc 0.88
2016-09-07T03:48:38.829031: step 767, loss 0.159084, acc 0.94
2016-09-07T03:48:39.502790: step 768, loss 0.0676881, acc 1
2016-09-07T03:48:40.173457: step 769, loss 0.16564, acc 0.9
2016-09-07T03:48:40.876366: step 770, loss 0.0557185, acc 1
2016-09-07T03:48:41.572957: step 771, loss 0.101102, acc 0.96
2016-09-07T03:48:42.294893: step 772, loss 0.0476655, acc 1
2016-09-07T03:48:42.985141: step 773, loss 0.108241, acc 0.96
2016-09-07T03:48:43.640080: step 774, loss 0.155259, acc 0.94
2016-09-07T03:48:44.335332: step 775, loss 0.023916, acc 1
2016-09-07T03:48:45.015900: step 776, loss 0.210011, acc 0.88
2016-09-07T03:48:45.695681: step 777, loss 0.109577, acc 0.96
2016-09-07T03:48:46.378454: step 778, loss 0.0826775, acc 0.96
2016-09-07T03:48:47.069942: step 779, loss 0.116305, acc 0.94
2016-09-07T03:48:47.752014: step 780, loss 0.0944195, acc 0.96
2016-09-07T03:48:48.434286: step 781, loss 0.0652606, acc 0.96
2016-09-07T03:48:49.112802: step 782, loss 0.0811416, acc 0.98
2016-09-07T03:48:49.798059: step 783, loss 0.072215, acc 0.96
2016-09-07T03:48:50.475864: step 784, loss 0.184429, acc 0.98
2016-09-07T03:48:51.145541: step 785, loss 0.196385, acc 0.96
2016-09-07T03:48:51.832323: step 786, loss 0.098457, acc 0.96
2016-09-07T03:48:52.525530: step 787, loss 0.118589, acc 0.96
2016-09-07T03:48:53.196914: step 788, loss 0.0596838, acc 0.98
2016-09-07T03:48:53.902302: step 789, loss 0.084637, acc 0.96
2016-09-07T03:48:54.581412: step 790, loss 0.134967, acc 0.94
2016-09-07T03:48:55.258381: step 791, loss 0.150548, acc 0.92
2016-09-07T03:48:55.964562: step 792, loss 0.0896941, acc 0.98
2016-09-07T03:48:56.654411: step 793, loss 0.0772421, acc 0.96
2016-09-07T03:48:57.340238: step 794, loss 0.133945, acc 0.94
2016-09-07T03:48:58.028371: step 795, loss 0.108238, acc 0.98
2016-09-07T03:48:58.738736: step 796, loss 0.0156327, acc 1
2016-09-07T03:48:59.436987: step 797, loss 0.286583, acc 0.9
2016-09-07T03:49:00.134222: step 798, loss 0.140767, acc 0.96
2016-09-07T03:49:00.865815: step 799, loss 0.0304899, acc 1
2016-09-07T03:49:01.551949: step 800, loss 0.176775, acc 0.9

Evaluation:
2016-09-07T03:49:05.130668: step 800, loss 0.782611, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-800

2016-09-07T03:49:06.894331: step 801, loss 0.083785, acc 0.96
2016-09-07T03:49:07.594751: step 802, loss 0.183494, acc 0.92
2016-09-07T03:49:08.320696: step 803, loss 0.239121, acc 0.9
2016-09-07T03:49:09.024283: step 804, loss 0.155303, acc 0.9
2016-09-07T03:49:09.718025: step 805, loss 0.226777, acc 0.92
2016-09-07T03:49:10.414056: step 806, loss 0.188129, acc 0.92
2016-09-07T03:49:11.115511: step 807, loss 0.141797, acc 0.98
2016-09-07T03:49:11.833922: step 808, loss 0.0596745, acc 1
2016-09-07T03:49:12.496871: step 809, loss 0.0849029, acc 0.94
2016-09-07T03:49:13.189528: step 810, loss 0.0639409, acc 0.96
2016-09-07T03:49:13.861755: step 811, loss 0.118597, acc 0.96
2016-09-07T03:49:14.541930: step 812, loss 0.0999703, acc 0.96
2016-09-07T03:49:15.238414: step 813, loss 0.134344, acc 0.94
2016-09-07T03:49:15.932023: step 814, loss 0.122202, acc 0.98
2016-09-07T03:49:16.649570: step 815, loss 0.156419, acc 0.92
2016-09-07T03:49:17.316848: step 816, loss 0.0452296, acc 1
2016-09-07T03:49:18.014056: step 817, loss 0.123317, acc 0.94
2016-09-07T03:49:18.712305: step 818, loss 0.0798373, acc 0.98
2016-09-07T03:49:19.400231: step 819, loss 0.0843822, acc 0.96
2016-09-07T03:49:20.090951: step 820, loss 0.0735979, acc 0.96
2016-09-07T03:49:20.773181: step 821, loss 0.0946955, acc 0.96
2016-09-07T03:49:21.462339: step 822, loss 0.0470918, acc 0.98
2016-09-07T03:49:22.159006: step 823, loss 0.108731, acc 0.96
2016-09-07T03:49:22.883460: step 824, loss 0.0967351, acc 0.96
2016-09-07T03:49:23.571523: step 825, loss 0.106269, acc 0.94
2016-09-07T03:49:24.263861: step 826, loss 0.076741, acc 0.96
2016-09-07T03:49:24.949793: step 827, loss 0.119113, acc 0.98
2016-09-07T03:49:25.655368: step 828, loss 0.0443613, acc 0.98
2016-09-07T03:49:26.386753: step 829, loss 0.144795, acc 0.9
2016-09-07T03:49:27.090018: step 830, loss 0.0871516, acc 0.92
2016-09-07T03:49:27.779295: step 831, loss 0.0720372, acc 0.94
2016-09-07T03:49:28.475115: step 832, loss 0.175264, acc 0.92
2016-09-07T03:49:29.179473: step 833, loss 0.0158831, acc 1
2016-09-07T03:49:29.876880: step 834, loss 0.0691287, acc 0.96
2016-09-07T03:49:30.580120: step 835, loss 0.0163724, acc 1
2016-09-07T03:49:31.285043: step 836, loss 0.0241223, acc 1
2016-09-07T03:49:31.970591: step 837, loss 0.213058, acc 0.94
2016-09-07T03:49:32.668062: step 838, loss 0.107162, acc 0.96
2016-09-07T03:49:33.369420: step 839, loss 0.223582, acc 0.92
2016-09-07T03:49:34.062091: step 840, loss 0.0319939, acc 0.98
2016-09-07T03:49:34.771297: step 841, loss 0.112967, acc 0.94
2016-09-07T03:49:35.444800: step 842, loss 0.0620075, acc 0.98
2016-09-07T03:49:36.166001: step 843, loss 0.0664203, acc 0.98
2016-09-07T03:49:36.859264: step 844, loss 0.0728419, acc 0.96
2016-09-07T03:49:37.557473: step 845, loss 0.110309, acc 0.92
2016-09-07T03:49:38.235481: step 846, loss 0.157112, acc 0.94
2016-09-07T03:49:38.932016: step 847, loss 0.235241, acc 0.92
2016-09-07T03:49:39.642137: step 848, loss 0.146574, acc 0.92
2016-09-07T03:49:40.336045: step 849, loss 0.167061, acc 0.96
2016-09-07T03:49:41.044827: step 850, loss 0.133454, acc 0.98
2016-09-07T03:49:41.746880: step 851, loss 0.0968659, acc 0.98
2016-09-07T03:49:42.434263: step 852, loss 0.172084, acc 0.92
2016-09-07T03:49:43.129954: step 853, loss 0.0818445, acc 0.96
2016-09-07T03:49:43.814355: step 854, loss 0.102791, acc 0.98
2016-09-07T03:49:44.539858: step 855, loss 0.137816, acc 0.96
2016-09-07T03:49:45.238816: step 856, loss 0.0793251, acc 0.98
2016-09-07T03:49:45.945165: step 857, loss 0.120404, acc 0.98
2016-09-07T03:49:46.631948: step 858, loss 0.163584, acc 0.92
2016-09-07T03:49:47.323844: step 859, loss 0.167568, acc 0.94
2016-09-07T03:49:48.033702: step 860, loss 0.117654, acc 0.96
2016-09-07T03:49:48.729376: step 861, loss 0.209793, acc 0.88
2016-09-07T03:49:49.430211: step 862, loss 0.128143, acc 0.92
2016-09-07T03:49:50.112711: step 863, loss 0.122059, acc 0.96
2016-09-07T03:49:50.813244: step 864, loss 0.181012, acc 0.92
2016-09-07T03:49:51.509653: step 865, loss 0.120003, acc 0.98
2016-09-07T03:49:52.164146: step 866, loss 0.179185, acc 0.94
2016-09-07T03:49:52.871179: step 867, loss 0.236227, acc 0.94
2016-09-07T03:49:53.567071: step 868, loss 0.0926725, acc 0.96
2016-09-07T03:49:54.250493: step 869, loss 0.0684102, acc 0.96
2016-09-07T03:49:54.936855: step 870, loss 0.188549, acc 0.9
2016-09-07T03:49:55.644490: step 871, loss 0.0620552, acc 1
2016-09-07T03:49:56.336352: step 872, loss 0.208474, acc 0.92
2016-09-07T03:49:56.995179: step 873, loss 0.184595, acc 0.9
2016-09-07T03:49:57.718658: step 874, loss 0.0684854, acc 0.98
2016-09-07T03:49:58.386732: step 875, loss 0.0397744, acc 1
2016-09-07T03:49:59.074834: step 876, loss 0.108282, acc 0.94
2016-09-07T03:49:59.766473: step 877, loss 0.152429, acc 0.88
2016-09-07T03:50:00.486234: step 878, loss 0.111585, acc 0.94
2016-09-07T03:50:01.192840: step 879, loss 0.10245, acc 0.96
2016-09-07T03:50:01.867360: step 880, loss 0.0893684, acc 0.96
2016-09-07T03:50:02.585131: step 881, loss 0.146806, acc 0.94
2016-09-07T03:50:03.270729: step 882, loss 0.17129, acc 0.92
2016-09-07T03:50:03.960392: step 883, loss 0.202337, acc 0.96
2016-09-07T03:50:04.650559: step 884, loss 0.194215, acc 0.92
2016-09-07T03:50:05.335364: step 885, loss 0.112107, acc 0.96
2016-09-07T03:50:06.069190: step 886, loss 0.0627086, acc 0.98
2016-09-07T03:50:06.757514: step 887, loss 0.180608, acc 0.94
2016-09-07T03:50:07.436173: step 888, loss 0.0806757, acc 0.96
2016-09-07T03:50:08.130025: step 889, loss 0.0990352, acc 0.96
2016-09-07T03:50:08.805762: step 890, loss 0.192677, acc 0.9
2016-09-07T03:50:09.500040: step 891, loss 0.084518, acc 0.92
2016-09-07T03:50:10.192292: step 892, loss 0.149787, acc 0.92
2016-09-07T03:50:10.893823: step 893, loss 0.228996, acc 0.9
2016-09-07T03:50:11.580219: step 894, loss 0.100726, acc 0.96
2016-09-07T03:50:12.252571: step 895, loss 0.303788, acc 0.86
2016-09-07T03:50:12.938914: step 896, loss 0.15644, acc 0.94
2016-09-07T03:50:13.642020: step 897, loss 0.0511704, acc 1
2016-09-07T03:50:14.335392: step 898, loss 0.132537, acc 0.94
2016-09-07T03:50:15.024434: step 899, loss 0.134644, acc 0.96
2016-09-07T03:50:15.722809: step 900, loss 0.145493, acc 0.96

Evaluation:
2016-09-07T03:50:19.360039: step 900, loss 0.664978, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-900

2016-09-07T03:50:21.094292: step 901, loss 0.107246, acc 0.98
2016-09-07T03:50:21.805839: step 902, loss 0.175837, acc 0.94
2016-09-07T03:50:22.504310: step 903, loss 0.219499, acc 0.9
2016-09-07T03:50:23.193830: step 904, loss 0.117252, acc 0.94
2016-09-07T03:50:23.909865: step 905, loss 0.112742, acc 0.96
2016-09-07T03:50:24.625446: step 906, loss 0.137335, acc 0.98
2016-09-07T03:50:25.345008: step 907, loss 0.143814, acc 0.98
2016-09-07T03:50:26.043470: step 908, loss 0.0955903, acc 0.98
2016-09-07T03:50:26.740715: step 909, loss 0.0734075, acc 0.98
2016-09-07T03:50:27.446222: step 910, loss 0.142779, acc 0.94
2016-09-07T03:50:28.161977: step 911, loss 0.058269, acc 0.98
2016-09-07T03:50:28.872746: step 912, loss 0.0794903, acc 0.96
2016-09-07T03:50:29.544250: step 913, loss 0.159626, acc 0.98
2016-09-07T03:50:30.256280: step 914, loss 0.122882, acc 0.96
2016-09-07T03:50:30.933333: step 915, loss 0.146423, acc 0.94
2016-09-07T03:50:31.619435: step 916, loss 0.0800128, acc 0.94
2016-09-07T03:50:32.291280: step 917, loss 0.166154, acc 0.94
2016-09-07T03:50:32.975530: step 918, loss 0.255045, acc 0.88
2016-09-07T03:50:33.658122: step 919, loss 0.120392, acc 0.94
2016-09-07T03:50:34.335195: step 920, loss 0.200537, acc 0.88
2016-09-07T03:50:35.031253: step 921, loss 0.0776107, acc 0.96
2016-09-07T03:50:35.709976: step 922, loss 0.165087, acc 0.92
2016-09-07T03:50:36.397936: step 923, loss 0.0777805, acc 0.96
2016-09-07T03:50:37.083080: step 924, loss 0.157757, acc 0.94
2016-09-07T03:50:37.766009: step 925, loss 0.158106, acc 0.94
2016-09-07T03:50:38.449165: step 926, loss 0.0540178, acc 0.98
2016-09-07T03:50:39.118394: step 927, loss 0.0299702, acc 0.98
2016-09-07T03:50:39.814531: step 928, loss 0.0986285, acc 0.96
2016-09-07T03:50:40.507749: step 929, loss 0.242387, acc 0.9
2016-09-07T03:50:41.198861: step 930, loss 0.0420824, acc 1
2016-09-07T03:50:41.909297: step 931, loss 0.126819, acc 0.92
2016-09-07T03:50:42.589686: step 932, loss 0.0698112, acc 0.96
2016-09-07T03:50:43.279282: step 933, loss 0.176765, acc 0.94
2016-09-07T03:50:43.947840: step 934, loss 0.225298, acc 0.9
2016-09-07T03:50:44.666292: step 935, loss 0.0330157, acc 0.98
2016-09-07T03:50:45.376849: step 936, loss 0.061963, acc 0.96
2016-09-07T03:50:46.087918: step 937, loss 0.0259437, acc 1
2016-09-07T03:50:46.778511: step 938, loss 0.233993, acc 0.92
2016-09-07T03:50:47.469045: step 939, loss 0.0995898, acc 0.94
2016-09-07T03:50:48.179494: step 940, loss 0.10618, acc 0.96
2016-09-07T03:50:48.849355: step 941, loss 0.0813622, acc 0.92
2016-09-07T03:50:49.540768: step 942, loss 0.23495, acc 0.92
2016-09-07T03:50:50.269044: step 943, loss 0.0904122, acc 0.98
2016-09-07T03:50:50.971756: step 944, loss 0.0966353, acc 0.94
2016-09-07T03:50:51.671117: step 945, loss 0.115736, acc 0.96
2016-09-07T03:50:52.358960: step 946, loss 0.0784824, acc 0.98
2016-09-07T03:50:53.070549: step 947, loss 0.199708, acc 0.92
2016-09-07T03:50:53.754694: step 948, loss 0.149858, acc 0.92
2016-09-07T03:50:54.427486: step 949, loss 0.0866542, acc 0.98
2016-09-07T03:50:55.129931: step 950, loss 0.14546, acc 0.9
2016-09-07T03:50:55.818847: step 951, loss 0.0745406, acc 0.96
2016-09-07T03:50:56.525458: step 952, loss 0.263708, acc 0.92
2016-09-07T03:50:57.207673: step 953, loss 0.151015, acc 0.92
2016-09-07T03:50:57.914396: step 954, loss 0.0880345, acc 0.94
2016-09-07T03:50:58.608593: step 955, loss 0.231596, acc 0.88
2016-09-07T03:50:59.295839: step 956, loss 0.0365505, acc 1
2016-09-07T03:50:59.997157: step 957, loss 0.151631, acc 0.92
2016-09-07T03:51:00.723735: step 958, loss 0.312374, acc 0.9
2016-09-07T03:51:01.449380: step 959, loss 0.175464, acc 0.94
2016-09-07T03:51:02.078693: step 960, loss 0.0949464, acc 0.954545
2016-09-07T03:51:02.765635: step 961, loss 0.0568663, acc 0.98
2016-09-07T03:51:03.448597: step 962, loss 0.0790957, acc 0.98
2016-09-07T03:51:04.128942: step 963, loss 0.116156, acc 0.96
2016-09-07T03:51:04.855209: step 964, loss 0.091459, acc 0.98
2016-09-07T03:51:05.509111: step 965, loss 0.0581266, acc 0.98
2016-09-07T03:51:06.223855: step 966, loss 0.112132, acc 0.96
2016-09-07T03:51:06.913366: step 967, loss 0.0227511, acc 1
2016-09-07T03:51:07.606796: step 968, loss 0.0813356, acc 0.98
2016-09-07T03:51:08.314024: step 969, loss 0.116624, acc 0.94
2016-09-07T03:51:08.994458: step 970, loss 0.112684, acc 0.96
2016-09-07T03:51:09.714936: step 971, loss 0.0569336, acc 0.96
2016-09-07T03:51:10.368360: step 972, loss 0.0562473, acc 0.98
2016-09-07T03:51:11.079298: step 973, loss 0.095373, acc 0.96
2016-09-07T03:51:11.773152: step 974, loss 0.113181, acc 0.96
2016-09-07T03:51:12.465959: step 975, loss 0.150667, acc 0.92
2016-09-07T03:51:13.160569: step 976, loss 0.050526, acc 0.98
2016-09-07T03:51:13.846453: step 977, loss 0.188741, acc 0.88
2016-09-07T03:51:14.537098: step 978, loss 0.056706, acc 1
2016-09-07T03:51:15.217235: step 979, loss 0.0611372, acc 1
2016-09-07T03:51:15.919751: step 980, loss 0.040561, acc 1
2016-09-07T03:51:16.606194: step 981, loss 0.0757301, acc 0.96
2016-09-07T03:51:17.297150: step 982, loss 0.0822402, acc 0.98
2016-09-07T03:51:17.989292: step 983, loss 0.0563161, acc 0.98
2016-09-07T03:51:18.682172: step 984, loss 0.0564577, acc 0.96
2016-09-07T03:51:19.375762: step 985, loss 0.0394136, acc 0.98
2016-09-07T03:51:20.056693: step 986, loss 0.0260684, acc 1
2016-09-07T03:51:20.764612: step 987, loss 0.112552, acc 0.94
2016-09-07T03:51:21.469512: step 988, loss 0.0654146, acc 0.96
2016-09-07T03:51:22.157821: step 989, loss 0.170503, acc 0.92
2016-09-07T03:51:22.836109: step 990, loss 0.0900383, acc 0.98
2016-09-07T03:51:23.522390: step 991, loss 0.0280038, acc 1
2016-09-07T03:51:24.231661: step 992, loss 0.101163, acc 0.96
2016-09-07T03:51:24.937188: step 993, loss 0.074026, acc 0.96
2016-09-07T03:51:25.630631: step 994, loss 0.12077, acc 0.94
2016-09-07T03:51:26.318555: step 995, loss 0.0297618, acc 0.98
2016-09-07T03:51:27.009097: step 996, loss 0.0861888, acc 0.96
2016-09-07T03:51:27.718984: step 997, loss 0.0143652, acc 1
2016-09-07T03:51:28.402397: step 998, loss 0.0554194, acc 1
2016-09-07T03:51:29.095812: step 999, loss 0.0653793, acc 0.96
2016-09-07T03:51:29.768574: step 1000, loss 0.204384, acc 0.94

Evaluation:
2016-09-07T03:51:33.341843: step 1000, loss 1.10261, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-1000

2016-09-07T03:51:35.087524: step 1001, loss 0.0836608, acc 0.98
2016-09-07T03:51:35.771798: step 1002, loss 0.0880267, acc 0.94
2016-09-07T03:51:36.449065: step 1003, loss 0.0656054, acc 0.96
2016-09-07T03:51:37.141385: step 1004, loss 0.0881359, acc 0.96
2016-09-07T03:51:37.853205: step 1005, loss 0.235244, acc 0.96
2016-09-07T03:51:38.555859: step 1006, loss 0.155536, acc 0.98
2016-09-07T03:51:39.231181: step 1007, loss 0.0934158, acc 0.92
2016-09-07T03:51:39.934639: step 1008, loss 0.0549843, acc 0.96
2016-09-07T03:51:40.624808: step 1009, loss 0.136701, acc 0.92
2016-09-07T03:51:41.287158: step 1010, loss 0.191986, acc 0.88
2016-09-07T03:51:41.978843: step 1011, loss 0.127502, acc 0.94
2016-09-07T03:51:42.691687: step 1012, loss 0.0762504, acc 0.96
2016-09-07T03:51:43.392980: step 1013, loss 0.0856618, acc 0.92
2016-09-07T03:51:44.051745: step 1014, loss 0.0595593, acc 0.98
2016-09-07T03:51:44.739477: step 1015, loss 0.0440054, acc 1
2016-09-07T03:51:45.426028: step 1016, loss 0.167986, acc 0.96
2016-09-07T03:51:46.122642: step 1017, loss 0.0861097, acc 0.94
2016-09-07T03:51:46.810811: step 1018, loss 0.0972226, acc 0.94
2016-09-07T03:51:47.494767: step 1019, loss 0.134987, acc 0.94
2016-09-07T03:51:48.195977: step 1020, loss 0.133017, acc 0.96
2016-09-07T03:51:48.878036: step 1021, loss 0.0604086, acc 0.98
2016-09-07T03:51:49.586457: step 1022, loss 0.0872792, acc 0.92
2016-09-07T03:51:50.284361: step 1023, loss 0.157208, acc 0.94
2016-09-07T03:51:51.003365: step 1024, loss 0.0788149, acc 0.98
2016-09-07T03:51:51.722335: step 1025, loss 0.154968, acc 0.92
2016-09-07T03:51:52.386396: step 1026, loss 0.20768, acc 0.9
2016-09-07T03:51:53.147801: step 1027, loss 0.0930677, acc 0.94
2016-09-07T03:51:53.844620: step 1028, loss 0.0813878, acc 0.96
2016-09-07T03:51:54.551102: step 1029, loss 0.0585347, acc 0.98
2016-09-07T03:51:55.233559: step 1030, loss 0.11486, acc 0.96
2016-09-07T03:51:55.925351: step 1031, loss 0.134024, acc 0.92
2016-09-07T03:51:56.625917: step 1032, loss 0.0528909, acc 0.96
2016-09-07T03:51:57.306558: step 1033, loss 0.0529243, acc 1
2016-09-07T03:51:57.988656: step 1034, loss 0.0985191, acc 0.96
2016-09-07T03:51:58.680264: step 1035, loss 0.0924913, acc 0.96
2016-09-07T03:51:59.354114: step 1036, loss 0.180278, acc 0.92
2016-09-07T03:52:00.036139: step 1037, loss 0.0693254, acc 0.98
2016-09-07T03:52:00.783510: step 1038, loss 0.219997, acc 0.92
2016-09-07T03:52:01.473148: step 1039, loss 0.0688615, acc 0.96
2016-09-07T03:52:02.157397: step 1040, loss 0.149233, acc 0.88
2016-09-07T03:52:02.847566: step 1041, loss 0.0821864, acc 0.96
2016-09-07T03:52:03.537503: step 1042, loss 0.108107, acc 0.94
2016-09-07T03:52:04.253973: step 1043, loss 0.0550344, acc 0.98
2016-09-07T03:52:04.949410: step 1044, loss 0.191406, acc 0.92
2016-09-07T03:52:05.619361: step 1045, loss 0.106622, acc 0.94
2016-09-07T03:52:06.321359: step 1046, loss 0.129888, acc 0.98
2016-09-07T03:52:07.014037: step 1047, loss 0.0854543, acc 0.94
2016-09-07T03:52:07.700731: step 1048, loss 0.0628632, acc 0.98
2016-09-07T03:52:08.390292: step 1049, loss 0.177036, acc 0.92
2016-09-07T03:52:09.081321: step 1050, loss 0.0981569, acc 0.96
2016-09-07T03:52:09.770380: step 1051, loss 0.0543622, acc 0.98
2016-09-07T03:52:10.445966: step 1052, loss 0.0185297, acc 1
2016-09-07T03:52:11.167202: step 1053, loss 0.0575983, acc 0.98
2016-09-07T03:52:11.835748: step 1054, loss 0.0970797, acc 0.96
2016-09-07T03:52:12.524310: step 1055, loss 0.15557, acc 0.94
2016-09-07T03:52:13.206167: step 1056, loss 0.191195, acc 0.9
2016-09-07T03:52:13.894183: step 1057, loss 0.417994, acc 0.9
2016-09-07T03:52:14.586987: step 1058, loss 0.133572, acc 0.94
2016-09-07T03:52:15.268073: step 1059, loss 0.0735205, acc 0.96
2016-09-07T03:52:15.974965: step 1060, loss 0.07266, acc 0.96
2016-09-07T03:52:16.631974: step 1061, loss 0.0750503, acc 0.98
2016-09-07T03:52:17.308014: step 1062, loss 0.0486423, acc 0.98
2016-09-07T03:52:18.006411: step 1063, loss 0.0894602, acc 0.96
2016-09-07T03:52:18.695810: step 1064, loss 0.0415567, acc 1
2016-09-07T03:52:19.374466: step 1065, loss 0.0752449, acc 1
2016-09-07T03:52:20.050699: step 1066, loss 0.100358, acc 0.96
2016-09-07T03:52:20.762850: step 1067, loss 0.0918056, acc 0.96
2016-09-07T03:52:21.433230: step 1068, loss 0.0634314, acc 0.96
2016-09-07T03:52:22.101055: step 1069, loss 0.10409, acc 0.96
2016-09-07T03:52:22.805798: step 1070, loss 0.128722, acc 0.94
2016-09-07T03:52:23.485479: step 1071, loss 0.122075, acc 0.94
2016-09-07T03:52:24.178324: step 1072, loss 0.0620775, acc 0.98
2016-09-07T03:52:24.858771: step 1073, loss 0.0269456, acc 1
2016-09-07T03:52:25.576366: step 1074, loss 0.035851, acc 0.98
2016-09-07T03:52:26.270568: step 1075, loss 0.0278128, acc 0.98
2016-09-07T03:52:26.974836: step 1076, loss 0.115778, acc 0.98
2016-09-07T03:52:27.666144: step 1077, loss 0.093734, acc 0.96
2016-09-07T03:52:28.337256: step 1078, loss 0.142502, acc 0.9
2016-09-07T03:52:29.023577: step 1079, loss 0.0811678, acc 0.96
2016-09-07T03:52:29.689665: step 1080, loss 0.125992, acc 0.94
2016-09-07T03:52:30.393724: step 1081, loss 0.112664, acc 0.96
2016-09-07T03:52:31.064832: step 1082, loss 0.240174, acc 0.92
2016-09-07T03:52:31.743252: step 1083, loss 0.0325155, acc 0.98
2016-09-07T03:52:32.425469: step 1084, loss 0.110261, acc 0.94
2016-09-07T03:52:33.134281: step 1085, loss 0.108078, acc 0.96
2016-09-07T03:52:33.826243: step 1086, loss 0.18716, acc 0.94
2016-09-07T03:52:34.488447: step 1087, loss 0.0204874, acc 1
2016-09-07T03:52:35.201513: step 1088, loss 0.102941, acc 0.94
2016-09-07T03:52:35.863347: step 1089, loss 0.103682, acc 0.94
2016-09-07T03:52:36.563375: step 1090, loss 0.080199, acc 0.98
2016-09-07T03:52:37.265656: step 1091, loss 0.149836, acc 0.9
2016-09-07T03:52:37.990920: step 1092, loss 0.065246, acc 0.98
2016-09-07T03:52:38.684692: step 1093, loss 0.0500493, acc 0.96
2016-09-07T03:52:39.367503: step 1094, loss 0.040454, acc 1
2016-09-07T03:52:40.067615: step 1095, loss 0.198333, acc 0.88
2016-09-07T03:52:40.766720: step 1096, loss 0.163257, acc 0.94
2016-09-07T03:52:41.446398: step 1097, loss 0.174528, acc 0.96
2016-09-07T03:52:42.131506: step 1098, loss 0.0633658, acc 0.98
2016-09-07T03:52:42.814527: step 1099, loss 0.0357488, acc 1
2016-09-07T03:52:43.514089: step 1100, loss 0.0198521, acc 1

Evaluation:
2016-09-07T03:52:46.660280: step 1100, loss 0.915639, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-1100

2016-09-07T03:52:48.446608: step 1101, loss 0.152176, acc 0.92
2016-09-07T03:52:49.129534: step 1102, loss 0.16004, acc 0.96
2016-09-07T03:52:49.823477: step 1103, loss 0.049754, acc 1
2016-09-07T03:52:50.513068: step 1104, loss 0.0494552, acc 0.98
2016-09-07T03:52:51.192812: step 1105, loss 0.201372, acc 0.9
2016-09-07T03:52:51.893237: step 1106, loss 0.2044, acc 0.9
2016-09-07T03:52:52.582720: step 1107, loss 0.0914916, acc 0.96
2016-09-07T03:52:53.293294: step 1108, loss 0.0738113, acc 0.96
2016-09-07T03:52:53.974191: step 1109, loss 0.203091, acc 0.94
2016-09-07T03:52:54.647047: step 1110, loss 0.137516, acc 0.9
2016-09-07T03:52:55.346866: step 1111, loss 0.065848, acc 0.96
2016-09-07T03:52:56.030615: step 1112, loss 0.185926, acc 0.92
2016-09-07T03:52:56.729437: step 1113, loss 0.0881709, acc 0.98
2016-09-07T03:52:57.391950: step 1114, loss 0.184376, acc 0.9
2016-09-07T03:52:58.093094: step 1115, loss 0.13528, acc 0.98
2016-09-07T03:52:58.787822: step 1116, loss 0.0476412, acc 0.98
2016-09-07T03:52:59.469700: step 1117, loss 0.0453201, acc 0.98
2016-09-07T03:53:00.143242: step 1118, loss 0.190891, acc 0.94
2016-09-07T03:53:00.852758: step 1119, loss 0.0217509, acc 1
2016-09-07T03:53:01.568755: step 1120, loss 0.0835773, acc 0.96
2016-09-07T03:53:02.238528: step 1121, loss 0.0597259, acc 1
2016-09-07T03:53:02.934413: step 1122, loss 0.12529, acc 0.92
2016-09-07T03:53:03.629549: step 1123, loss 0.117177, acc 0.94
2016-09-07T03:53:04.318863: step 1124, loss 0.26022, acc 0.88
2016-09-07T03:53:05.013573: step 1125, loss 0.0451435, acc 0.98
2016-09-07T03:53:05.702301: step 1126, loss 0.0545912, acc 0.98
2016-09-07T03:53:06.418387: step 1127, loss 0.10244, acc 0.96
2016-09-07T03:53:07.079100: step 1128, loss 0.0666276, acc 0.96
2016-09-07T03:53:07.766749: step 1129, loss 0.0998968, acc 0.94
2016-09-07T03:53:08.442352: step 1130, loss 0.0703826, acc 0.98
2016-09-07T03:53:09.123404: step 1131, loss 0.0476917, acc 0.98
2016-09-07T03:53:09.823103: step 1132, loss 0.086842, acc 0.98
2016-09-07T03:53:10.504927: step 1133, loss 0.134085, acc 0.94
2016-09-07T03:53:11.193529: step 1134, loss 0.262561, acc 0.86
2016-09-07T03:53:11.856815: step 1135, loss 0.109948, acc 0.96
2016-09-07T03:53:12.586958: step 1136, loss 0.033779, acc 1
2016-09-07T03:53:13.274411: step 1137, loss 0.0601454, acc 1
2016-09-07T03:53:13.963524: step 1138, loss 0.156085, acc 0.94
2016-09-07T03:53:14.679716: step 1139, loss 0.0676785, acc 0.98
2016-09-07T03:53:15.366867: step 1140, loss 0.0946842, acc 0.96
2016-09-07T03:53:16.064469: step 1141, loss 0.149767, acc 0.92
2016-09-07T03:53:16.759181: step 1142, loss 0.133639, acc 0.94
2016-09-07T03:53:17.479792: step 1143, loss 0.0543039, acc 0.98
2016-09-07T03:53:18.163265: step 1144, loss 0.1465, acc 0.92
2016-09-07T03:53:18.855901: step 1145, loss 0.106725, acc 0.96
2016-09-07T03:53:19.539545: step 1146, loss 0.180456, acc 0.92
2016-09-07T03:53:20.193531: step 1147, loss 0.0417215, acc 1
2016-09-07T03:53:20.902266: step 1148, loss 0.0520254, acc 0.98
2016-09-07T03:53:21.578643: step 1149, loss 0.128578, acc 0.96
2016-09-07T03:53:22.267803: step 1150, loss 0.0805784, acc 0.96
2016-09-07T03:53:22.950413: step 1151, loss 0.0529945, acc 0.96
2016-09-07T03:53:23.583396: step 1152, loss 0.0728613, acc 0.954545
2016-09-07T03:53:24.247984: step 1153, loss 0.0592804, acc 0.98
2016-09-07T03:53:24.956229: step 1154, loss 0.0409841, acc 0.98
2016-09-07T03:53:25.648858: step 1155, loss 0.181466, acc 0.88
2016-09-07T03:53:26.318576: step 1156, loss 0.0580155, acc 0.96
2016-09-07T03:53:27.022811: step 1157, loss 0.0276885, acc 1
2016-09-07T03:53:27.706349: step 1158, loss 0.121288, acc 0.94
2016-09-07T03:53:28.402557: step 1159, loss 0.0892776, acc 0.98
2016-09-07T03:53:29.064916: step 1160, loss 0.0539992, acc 1
2016-09-07T03:53:29.753220: step 1161, loss 0.0850912, acc 0.96
2016-09-07T03:53:30.448572: step 1162, loss 0.127605, acc 0.94
2016-09-07T03:53:31.143533: step 1163, loss 0.0353733, acc 0.98
2016-09-07T03:53:31.839035: step 1164, loss 0.0469615, acc 1
2016-09-07T03:53:32.531011: step 1165, loss 0.0946212, acc 0.94
2016-09-07T03:53:33.206492: step 1166, loss 0.032874, acc 1
2016-09-07T03:53:33.892899: step 1167, loss 0.0094995, acc 1
2016-09-07T03:53:34.580678: step 1168, loss 0.0254394, acc 0.98
2016-09-07T03:53:35.282238: step 1169, loss 0.0512754, acc 1
2016-09-07T03:53:35.948279: step 1170, loss 0.0600178, acc 0.98
2016-09-07T03:53:36.634513: step 1171, loss 0.0308384, acc 1
2016-09-07T03:53:37.325542: step 1172, loss 0.11415, acc 0.94
2016-09-07T03:53:38.014648: step 1173, loss 0.0423911, acc 1
2016-09-07T03:53:38.703907: step 1174, loss 0.153901, acc 0.88
2016-09-07T03:53:39.408711: step 1175, loss 0.0727832, acc 0.98
2016-09-07T03:53:40.107114: step 1176, loss 0.227942, acc 0.96
2016-09-07T03:53:40.781702: step 1177, loss 0.141829, acc 0.96
2016-09-07T03:53:41.482525: step 1178, loss 0.105448, acc 0.96
2016-09-07T03:53:42.169071: step 1179, loss 0.0387347, acc 0.98
2016-09-07T03:53:42.854898: step 1180, loss 0.0325566, acc 1
2016-09-07T03:53:43.537227: step 1181, loss 0.129551, acc 0.94
2016-09-07T03:53:44.258776: step 1182, loss 0.0908728, acc 0.96
2016-09-07T03:53:44.963116: step 1183, loss 0.0877619, acc 0.96
2016-09-07T03:53:45.645132: step 1184, loss 0.213237, acc 0.9
2016-09-07T03:53:46.325053: step 1185, loss 0.119409, acc 0.94
2016-09-07T03:53:47.022076: step 1186, loss 0.035468, acc 1
2016-09-07T03:53:47.704081: step 1187, loss 0.0276912, acc 1
2016-09-07T03:53:48.399565: step 1188, loss 0.0517024, acc 0.96
2016-09-07T03:53:49.083603: step 1189, loss 0.0237197, acc 0.98
2016-09-07T03:53:49.788403: step 1190, loss 0.0596106, acc 0.98
2016-09-07T03:53:50.477597: step 1191, loss 0.137768, acc 0.96
2016-09-07T03:53:51.160165: step 1192, loss 0.0516195, acc 0.96
2016-09-07T03:53:51.845529: step 1193, loss 0.076755, acc 0.96
2016-09-07T03:53:52.547995: step 1194, loss 0.073778, acc 0.96
2016-09-07T03:53:53.248117: step 1195, loss 0.092413, acc 0.96
2016-09-07T03:53:53.912184: step 1196, loss 0.0718796, acc 0.96
2016-09-07T03:53:54.620397: step 1197, loss 0.0627788, acc 0.98
2016-09-07T03:53:55.328471: step 1198, loss 0.0736464, acc 0.98
2016-09-07T03:53:56.008850: step 1199, loss 0.0405796, acc 0.98
2016-09-07T03:53:56.699203: step 1200, loss 0.12888, acc 0.96

Evaluation:
2016-09-07T03:53:59.847903: step 1200, loss 1.11818, acc 0.74015

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-1200

2016-09-07T03:54:01.556545: step 1201, loss 0.152548, acc 0.92
2016-09-07T03:54:02.243952: step 1202, loss 0.0923214, acc 0.94
2016-09-07T03:54:02.948948: step 1203, loss 0.107762, acc 0.98
2016-09-07T03:54:03.633761: step 1204, loss 0.0139717, acc 1
2016-09-07T03:54:04.300472: step 1205, loss 0.11093, acc 0.96
2016-09-07T03:54:04.991885: step 1206, loss 0.0384008, acc 0.98
2016-09-07T03:54:05.687996: step 1207, loss 0.0675886, acc 0.94
2016-09-07T03:54:06.379709: step 1208, loss 0.136822, acc 0.94
2016-09-07T03:54:07.066789: step 1209, loss 0.241371, acc 0.88
2016-09-07T03:54:07.787090: step 1210, loss 0.105802, acc 0.96
2016-09-07T03:54:08.459209: step 1211, loss 0.109474, acc 0.94
2016-09-07T03:54:09.154326: step 1212, loss 0.0706109, acc 0.96
2016-09-07T03:54:09.839403: step 1213, loss 0.0663106, acc 0.96
2016-09-07T03:54:10.535167: step 1214, loss 0.0443999, acc 0.96
2016-09-07T03:54:11.214925: step 1215, loss 0.216625, acc 0.92
2016-09-07T03:54:11.906938: step 1216, loss 0.0992072, acc 0.96
2016-09-07T03:54:12.621718: step 1217, loss 0.0556355, acc 1
2016-09-07T03:54:13.283235: step 1218, loss 0.0536697, acc 0.98
2016-09-07T03:54:13.966762: step 1219, loss 0.0746039, acc 0.94
2016-09-07T03:54:14.648144: step 1220, loss 0.0110966, acc 1
2016-09-07T03:54:15.335587: step 1221, loss 0.0276531, acc 1
2016-09-07T03:54:16.039418: step 1222, loss 0.0503297, acc 0.98
2016-09-07T03:54:16.718217: step 1223, loss 0.0318404, acc 1
2016-09-07T03:54:17.427311: step 1224, loss 0.0693471, acc 0.98
2016-09-07T03:54:18.108404: step 1225, loss 0.043177, acc 0.98
2016-09-07T03:54:18.785300: step 1226, loss 0.0916092, acc 0.96
2016-09-07T03:54:19.469561: step 1227, loss 0.0251986, acc 1
2016-09-07T03:54:20.131504: step 1228, loss 0.138563, acc 0.94
2016-09-07T03:54:20.820125: step 1229, loss 0.0518408, acc 0.98
2016-09-07T03:54:21.503657: step 1230, loss 0.0278055, acc 1
2016-09-07T03:54:22.205581: step 1231, loss 0.169537, acc 0.94
2016-09-07T03:54:22.882914: step 1232, loss 0.0201936, acc 1
2016-09-07T03:54:23.561414: step 1233, loss 0.0447728, acc 1
2016-09-07T03:54:24.258216: step 1234, loss 0.0354953, acc 0.98
2016-09-07T03:54:24.941808: step 1235, loss 0.0530276, acc 0.96
2016-09-07T03:54:25.630977: step 1236, loss 0.0951967, acc 0.94
2016-09-07T03:54:26.313531: step 1237, loss 0.0523028, acc 0.98
2016-09-07T03:54:27.023012: step 1238, loss 0.127874, acc 0.96
2016-09-07T03:54:27.687496: step 1239, loss 0.0555877, acc 0.98
2016-09-07T03:54:28.386247: step 1240, loss 0.0702014, acc 0.96
2016-09-07T03:54:29.080621: step 1241, loss 0.096531, acc 0.96
2016-09-07T03:54:29.755909: step 1242, loss 0.281696, acc 0.9
2016-09-07T03:54:30.445212: step 1243, loss 0.0826563, acc 0.98
2016-09-07T03:54:31.135557: step 1244, loss 0.125634, acc 0.94
2016-09-07T03:54:31.838636: step 1245, loss 0.0695894, acc 0.96
2016-09-07T03:54:32.536573: step 1246, loss 0.0432888, acc 0.96
2016-09-07T03:54:33.222182: step 1247, loss 0.033343, acc 0.98
2016-09-07T03:54:33.906051: step 1248, loss 0.0203593, acc 1
2016-09-07T03:54:34.618042: step 1249, loss 0.104315, acc 0.96
2016-09-07T03:54:35.318662: step 1250, loss 0.113475, acc 0.96
2016-09-07T03:54:35.976157: step 1251, loss 0.124676, acc 0.94
2016-09-07T03:54:36.693928: step 1252, loss 0.0704032, acc 0.98
2016-09-07T03:54:37.364106: step 1253, loss 0.160578, acc 0.98
2016-09-07T03:54:38.059912: step 1254, loss 0.0755212, acc 0.98
2016-09-07T03:54:38.754768: step 1255, loss 0.0795669, acc 0.98
2016-09-07T03:54:39.437058: step 1256, loss 0.155167, acc 0.92
2016-09-07T03:54:40.123818: step 1257, loss 0.0518508, acc 1
2016-09-07T03:54:40.819816: step 1258, loss 0.120695, acc 0.98
2016-09-07T03:54:41.518725: step 1259, loss 0.0753299, acc 0.96
2016-09-07T03:54:42.208769: step 1260, loss 0.059291, acc 0.98
2016-09-07T03:54:42.899500: step 1261, loss 0.12439, acc 0.98
2016-09-07T03:54:43.587563: step 1262, loss 0.0943862, acc 0.94
2016-09-07T03:54:44.281164: step 1263, loss 0.0966872, acc 0.96
2016-09-07T03:54:44.973862: step 1264, loss 0.0705211, acc 0.96
2016-09-07T03:54:45.661854: step 1265, loss 0.171165, acc 0.92
2016-09-07T03:54:46.387625: step 1266, loss 0.0595343, acc 0.96
2016-09-07T03:54:47.077960: step 1267, loss 0.0663234, acc 0.98
2016-09-07T03:54:47.769916: step 1268, loss 0.110869, acc 0.96
2016-09-07T03:54:48.447410: step 1269, loss 0.0641841, acc 0.98
2016-09-07T03:54:49.104823: step 1270, loss 0.0281124, acc 0.98
2016-09-07T03:54:49.797849: step 1271, loss 0.0678628, acc 0.96
2016-09-07T03:54:50.458101: step 1272, loss 0.200761, acc 0.9
2016-09-07T03:54:51.159160: step 1273, loss 0.0258365, acc 1
2016-09-07T03:54:51.852325: step 1274, loss 0.0748175, acc 0.96
2016-09-07T03:54:52.537483: step 1275, loss 0.0212512, acc 0.98
2016-09-07T03:54:53.229729: step 1276, loss 0.0448806, acc 0.98
2016-09-07T03:54:53.910151: step 1277, loss 0.129899, acc 0.98
2016-09-07T03:54:54.601587: step 1278, loss 0.0362956, acc 0.98
2016-09-07T03:54:55.266460: step 1279, loss 0.0867233, acc 0.98
2016-09-07T03:54:55.967130: step 1280, loss 0.0472734, acc 0.96
2016-09-07T03:54:56.682981: step 1281, loss 0.0438325, acc 1
2016-09-07T03:54:57.371589: step 1282, loss 0.10636, acc 0.94
2016-09-07T03:54:58.063153: step 1283, loss 0.0395099, acc 1
2016-09-07T03:54:58.764843: step 1284, loss 0.118665, acc 0.94
2016-09-07T03:54:59.459203: step 1285, loss 0.0983511, acc 0.98
2016-09-07T03:55:00.133388: step 1286, loss 0.0755722, acc 0.96
2016-09-07T03:55:00.838520: step 1287, loss 0.145854, acc 0.9
2016-09-07T03:55:01.539170: step 1288, loss 0.103856, acc 0.92
2016-09-07T03:55:02.216831: step 1289, loss 0.012372, acc 1
2016-09-07T03:55:02.898345: step 1290, loss 0.03842, acc 0.98
2016-09-07T03:55:03.583619: step 1291, loss 0.0565771, acc 0.98
2016-09-07T03:55:04.317886: step 1292, loss 0.0433177, acc 1
2016-09-07T03:55:04.997544: step 1293, loss 0.115076, acc 0.94
2016-09-07T03:55:05.680397: step 1294, loss 0.065007, acc 0.96
2016-09-07T03:55:06.364672: step 1295, loss 0.0412142, acc 0.98
2016-09-07T03:55:07.060443: step 1296, loss 0.0799975, acc 0.96
2016-09-07T03:55:07.756887: step 1297, loss 0.0472722, acc 1
2016-09-07T03:55:08.421439: step 1298, loss 0.117598, acc 0.96
2016-09-07T03:55:09.126103: step 1299, loss 0.0604065, acc 0.96
2016-09-07T03:55:09.798680: step 1300, loss 0.148317, acc 0.96

Evaluation:
2016-09-07T03:55:12.922499: step 1300, loss 1.23049, acc 0.731707

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-1300

2016-09-07T03:55:14.640264: step 1301, loss 0.0434811, acc 0.98
2016-09-07T03:55:15.328430: step 1302, loss 0.0504309, acc 1
2016-09-07T03:55:16.015018: step 1303, loss 0.036401, acc 0.98
2016-09-07T03:55:16.720908: step 1304, loss 0.0890623, acc 0.96
2016-09-07T03:55:17.448716: step 1305, loss 0.157597, acc 0.9
2016-09-07T03:55:18.126219: step 1306, loss 0.19778, acc 0.9
2016-09-07T03:55:18.828530: step 1307, loss 0.0325682, acc 0.98
2016-09-07T03:55:19.532623: step 1308, loss 0.0154855, acc 1
2016-09-07T03:55:20.231713: step 1309, loss 0.00717611, acc 1
2016-09-07T03:55:20.916218: step 1310, loss 0.241062, acc 0.92
2016-09-07T03:55:21.605332: step 1311, loss 0.075882, acc 0.96
2016-09-07T03:55:22.323159: step 1312, loss 0.137237, acc 0.94
2016-09-07T03:55:22.999133: step 1313, loss 0.0255394, acc 1
2016-09-07T03:55:23.675167: step 1314, loss 0.131291, acc 0.94
2016-09-07T03:55:24.358880: step 1315, loss 0.0690977, acc 0.98
2016-09-07T03:55:25.034444: step 1316, loss 0.0797796, acc 0.98
2016-09-07T03:55:25.722623: step 1317, loss 0.0119569, acc 1
2016-09-07T03:55:26.427485: step 1318, loss 0.0714735, acc 0.96
2016-09-07T03:55:27.150292: step 1319, loss 0.0939452, acc 0.94
2016-09-07T03:55:27.834625: step 1320, loss 0.0366247, acc 0.98
2016-09-07T03:55:28.521548: step 1321, loss 0.0698226, acc 0.96
2016-09-07T03:55:29.203459: step 1322, loss 0.189583, acc 0.92
2016-09-07T03:55:29.879251: step 1323, loss 0.0459996, acc 0.96
2016-09-07T03:55:30.566805: step 1324, loss 0.0448772, acc 0.98
2016-09-07T03:55:31.264200: step 1325, loss 0.103234, acc 0.98
2016-09-07T03:55:31.976111: step 1326, loss 0.0971666, acc 0.94
2016-09-07T03:55:32.686118: step 1327, loss 0.167462, acc 0.96
2016-09-07T03:55:33.401208: step 1328, loss 0.0891887, acc 0.96
2016-09-07T03:55:34.089826: step 1329, loss 0.12781, acc 0.94
2016-09-07T03:55:34.805973: step 1330, loss 0.100483, acc 0.98
2016-09-07T03:55:35.515322: step 1331, loss 0.0476878, acc 0.98
2016-09-07T03:55:36.178906: step 1332, loss 0.107541, acc 0.94
2016-09-07T03:55:36.871835: step 1333, loss 0.005237, acc 1
2016-09-07T03:55:37.549023: step 1334, loss 0.109471, acc 0.98
2016-09-07T03:55:38.236060: step 1335, loss 0.0648036, acc 0.96
2016-09-07T03:55:38.928737: step 1336, loss 0.173175, acc 0.92
2016-09-07T03:55:39.612786: step 1337, loss 0.153403, acc 0.98
2016-09-07T03:55:40.300865: step 1338, loss 0.153066, acc 0.88
2016-09-07T03:55:40.976084: step 1339, loss 0.16982, acc 0.94
2016-09-07T03:55:41.669497: step 1340, loss 0.0741778, acc 0.98
2016-09-07T03:55:42.360544: step 1341, loss 0.128233, acc 0.98
2016-09-07T03:55:43.023657: step 1342, loss 0.0877788, acc 0.98
2016-09-07T03:55:43.735745: step 1343, loss 0.0987801, acc 0.96
2016-09-07T03:55:44.370131: step 1344, loss 0.114444, acc 0.909091
2016-09-07T03:55:45.077651: step 1345, loss 0.144835, acc 0.96
2016-09-07T03:55:45.766388: step 1346, loss 0.189275, acc 0.96
2016-09-07T03:55:46.479217: step 1347, loss 0.129981, acc 0.92
2016-09-07T03:55:47.173723: step 1348, loss 0.0391274, acc 0.98
2016-09-07T03:55:47.878797: step 1349, loss 0.119582, acc 0.92
2016-09-07T03:55:48.578749: step 1350, loss 0.135022, acc 0.98
2016-09-07T03:55:49.263663: step 1351, loss 0.0292617, acc 1
2016-09-07T03:55:49.970468: step 1352, loss 0.0574666, acc 0.96
2016-09-07T03:55:50.668209: step 1353, loss 0.0560725, acc 0.98
2016-09-07T03:55:51.341715: step 1354, loss 0.047028, acc 0.98
2016-09-07T03:55:52.044797: step 1355, loss 0.0621677, acc 0.98
2016-09-07T03:55:52.747491: step 1356, loss 0.0400903, acc 1
2016-09-07T03:55:53.453037: step 1357, loss 0.0977621, acc 0.94
2016-09-07T03:55:54.117701: step 1358, loss 0.112511, acc 0.92
2016-09-07T03:55:54.818524: step 1359, loss 0.116102, acc 0.92
2016-09-07T03:55:55.488537: step 1360, loss 0.0370155, acc 0.98
2016-09-07T03:55:56.153454: step 1361, loss 0.108405, acc 0.96
2016-09-07T03:55:56.841747: step 1362, loss 0.137658, acc 0.94
2016-09-07T03:55:57.519081: step 1363, loss 0.0292704, acc 1
2016-09-07T03:55:58.208860: step 1364, loss 0.039461, acc 0.98
2016-09-07T03:55:58.908875: step 1365, loss 0.0609887, acc 0.98
2016-09-07T03:55:59.616547: step 1366, loss 0.0559249, acc 0.96
2016-09-07T03:56:00.286556: step 1367, loss 0.033868, acc 1
2016-09-07T03:56:00.982240: step 1368, loss 0.115923, acc 0.94
2016-09-07T03:56:01.680037: step 1369, loss 0.0129425, acc 1
2016-09-07T03:56:02.366045: step 1370, loss 0.0892734, acc 0.98
2016-09-07T03:56:03.047102: step 1371, loss 0.13988, acc 0.96
2016-09-07T03:56:03.737724: step 1372, loss 0.0712573, acc 0.96
2016-09-07T03:56:04.446004: step 1373, loss 0.0405473, acc 0.96
2016-09-07T03:56:05.114213: step 1374, loss 0.0174323, acc 1
2016-09-07T03:56:05.790148: step 1375, loss 0.0361525, acc 0.98
2016-09-07T03:56:06.464041: step 1376, loss 0.027559, acc 1
2016-09-07T03:56:07.150054: step 1377, loss 0.038672, acc 0.98
2016-09-07T03:56:07.841444: step 1378, loss 0.0186057, acc 1
2016-09-07T03:56:08.534863: step 1379, loss 0.0883224, acc 0.96
2016-09-07T03:56:09.238633: step 1380, loss 0.151154, acc 0.9
2016-09-07T03:56:09.896682: step 1381, loss 0.115973, acc 0.94
2016-09-07T03:56:10.579317: step 1382, loss 0.080525, acc 0.98
2016-09-07T03:56:11.273737: step 1383, loss 0.0431543, acc 0.98
2016-09-07T03:56:11.954061: step 1384, loss 0.0850926, acc 0.96
2016-09-07T03:56:12.648047: step 1385, loss 0.065476, acc 0.96
2016-09-07T03:56:13.338223: step 1386, loss 0.0473142, acc 0.98
2016-09-07T03:56:14.031341: step 1387, loss 0.0654215, acc 0.98
2016-09-07T03:56:14.700558: step 1388, loss 0.0926692, acc 0.96
2016-09-07T03:56:15.393669: step 1389, loss 0.0318124, acc 1
2016-09-07T03:56:16.087535: step 1390, loss 0.0693094, acc 0.96
2016-09-07T03:56:16.791063: step 1391, loss 0.0507351, acc 0.98
2016-09-07T03:56:17.485419: step 1392, loss 0.092652, acc 0.98
2016-09-07T03:56:18.166974: step 1393, loss 0.0794527, acc 0.96
2016-09-07T03:56:18.869978: step 1394, loss 0.179688, acc 0.94
2016-09-07T03:56:19.541228: step 1395, loss 0.0429996, acc 0.98
2016-09-07T03:56:20.241254: step 1396, loss 0.0531735, acc 1
2016-09-07T03:56:20.951147: step 1397, loss 0.0717642, acc 0.98
2016-09-07T03:56:21.636354: step 1398, loss 0.0737118, acc 0.98
2016-09-07T03:56:22.330241: step 1399, loss 0.04332, acc 0.98
2016-09-07T03:56:22.986475: step 1400, loss 0.0417618, acc 0.98

Evaluation:
2016-09-07T03:56:26.156144: step 1400, loss 1.10541, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-1400

2016-09-07T03:56:27.821027: step 1401, loss 0.0539057, acc 0.96
2016-09-07T03:56:28.520067: step 1402, loss 0.0208816, acc 1
2016-09-07T03:56:29.199595: step 1403, loss 0.0428995, acc 0.98
2016-09-07T03:56:29.897137: step 1404, loss 0.0481925, acc 0.98
2016-09-07T03:56:30.617000: step 1405, loss 0.10196, acc 0.98
2016-09-07T03:56:31.308180: step 1406, loss 0.120803, acc 0.96
2016-09-07T03:56:32.016094: step 1407, loss 0.0291393, acc 1
2016-09-07T03:56:32.705510: step 1408, loss 0.157507, acc 0.92
2016-09-07T03:56:33.398351: step 1409, loss 0.0632692, acc 0.98
2016-09-07T03:56:34.056179: step 1410, loss 0.0638002, acc 0.98
2016-09-07T03:56:34.743248: step 1411, loss 0.0406304, acc 1
2016-09-07T03:56:35.422762: step 1412, loss 0.0210059, acc 1
2016-09-07T03:56:36.118914: step 1413, loss 0.121312, acc 0.96
2016-09-07T03:56:36.813815: step 1414, loss 0.199399, acc 0.86
2016-09-07T03:56:37.486755: step 1415, loss 0.0594179, acc 0.98
2016-09-07T03:56:38.180748: step 1416, loss 0.0274229, acc 0.98
2016-09-07T03:56:38.874828: step 1417, loss 0.0472804, acc 0.96
2016-09-07T03:56:39.557320: step 1418, loss 0.0234389, acc 0.98
2016-09-07T03:56:40.240047: step 1419, loss 0.0594516, acc 0.96
2016-09-07T03:56:40.930306: step 1420, loss 0.0630434, acc 0.96
2016-09-07T03:56:41.625590: step 1421, loss 0.0802705, acc 0.94
2016-09-07T03:56:42.295636: step 1422, loss 0.0840147, acc 0.98
2016-09-07T03:56:43.023650: step 1423, loss 0.0640481, acc 0.96
2016-09-07T03:56:43.713029: step 1424, loss 0.0801832, acc 0.96
2016-09-07T03:56:44.410628: step 1425, loss 0.0562618, acc 0.98
2016-09-07T03:56:45.107684: step 1426, loss 0.0661371, acc 0.96
2016-09-07T03:56:45.804277: step 1427, loss 0.108763, acc 0.96
2016-09-07T03:56:46.536591: step 1428, loss 0.0458068, acc 0.98
2016-09-07T03:56:47.214328: step 1429, loss 0.0476905, acc 0.98
2016-09-07T03:56:47.894490: step 1430, loss 0.0410132, acc 0.98
2016-09-07T03:56:48.572238: step 1431, loss 0.0271383, acc 1
2016-09-07T03:56:49.244233: step 1432, loss 0.11099, acc 0.96
2016-09-07T03:56:49.961780: step 1433, loss 0.0921911, acc 0.96
2016-09-07T03:56:50.619290: step 1434, loss 0.111301, acc 0.94
2016-09-07T03:56:51.328119: step 1435, loss 0.0283953, acc 1
2016-09-07T03:56:52.009120: step 1436, loss 0.0506817, acc 0.98
2016-09-07T03:56:52.700452: step 1437, loss 0.030706, acc 0.98
2016-09-07T03:56:53.385253: step 1438, loss 0.120409, acc 0.92
2016-09-07T03:56:54.064097: step 1439, loss 0.117419, acc 0.94
2016-09-07T03:56:54.757478: step 1440, loss 0.018461, acc 1
2016-09-07T03:56:55.418156: step 1441, loss 0.0482346, acc 0.98
2016-09-07T03:56:56.109669: step 1442, loss 0.116052, acc 0.92
2016-09-07T03:56:56.784196: step 1443, loss 0.083896, acc 0.98
2016-09-07T03:56:57.476018: step 1444, loss 0.16074, acc 0.94
2016-09-07T03:56:58.203745: step 1445, loss 0.0289189, acc 1
2016-09-07T03:56:58.890405: step 1446, loss 0.128484, acc 0.98
2016-09-07T03:56:59.584304: step 1447, loss 0.108892, acc 0.94
2016-09-07T03:57:00.263995: step 1448, loss 0.087059, acc 0.94
2016-09-07T03:57:00.960129: step 1449, loss 0.036994, acc 0.98
2016-09-07T03:57:01.617964: step 1450, loss 0.0295069, acc 0.98
2016-09-07T03:57:02.294224: step 1451, loss 0.126664, acc 0.96
2016-09-07T03:57:02.974323: step 1452, loss 0.279437, acc 0.92
2016-09-07T03:57:03.657777: step 1453, loss 0.121873, acc 0.94
2016-09-07T03:57:04.344796: step 1454, loss 0.0393746, acc 0.98
2016-09-07T03:57:05.050858: step 1455, loss 0.0350015, acc 0.98
2016-09-07T03:57:05.751334: step 1456, loss 0.0353605, acc 0.98
2016-09-07T03:57:06.430660: step 1457, loss 0.0330588, acc 0.98
2016-09-07T03:57:07.124998: step 1458, loss 0.0954532, acc 0.98
2016-09-07T03:57:07.818018: step 1459, loss 0.0388525, acc 1
2016-09-07T03:57:08.511206: step 1460, loss 0.052912, acc 0.98
2016-09-07T03:57:09.198769: step 1461, loss 0.0548999, acc 0.98
2016-09-07T03:57:09.897506: step 1462, loss 0.0694626, acc 0.96
2016-09-07T03:57:10.591568: step 1463, loss 0.0723662, acc 0.98
2016-09-07T03:57:11.276045: step 1464, loss 0.0644064, acc 0.96
2016-09-07T03:57:11.952268: step 1465, loss 0.0970791, acc 0.96
2016-09-07T03:57:12.634402: step 1466, loss 0.0681939, acc 0.98
2016-09-07T03:57:13.330860: step 1467, loss 0.0139537, acc 1
2016-09-07T03:57:14.018536: step 1468, loss 0.185799, acc 0.98
2016-09-07T03:57:14.681861: step 1469, loss 0.140291, acc 0.92
2016-09-07T03:57:15.438644: step 1470, loss 0.0695529, acc 0.96
2016-09-07T03:57:16.146354: step 1471, loss 0.013358, acc 1
2016-09-07T03:57:16.843791: step 1472, loss 0.0647104, acc 0.98
2016-09-07T03:57:17.529179: step 1473, loss 0.0213569, acc 1
2016-09-07T03:57:18.208850: step 1474, loss 0.0572699, acc 1
2016-09-07T03:57:18.912659: step 1475, loss 0.187488, acc 0.92
2016-09-07T03:57:19.593582: step 1476, loss 0.0750573, acc 0.96
2016-09-07T03:57:20.297810: step 1477, loss 0.0443173, acc 0.96
2016-09-07T03:57:20.997820: step 1478, loss 0.115365, acc 0.94
2016-09-07T03:57:21.713663: step 1479, loss 0.10719, acc 0.94
2016-09-07T03:57:22.404662: step 1480, loss 0.256276, acc 0.88
2016-09-07T03:57:23.080822: step 1481, loss 0.135249, acc 0.9
2016-09-07T03:57:23.795085: step 1482, loss 0.0584009, acc 0.96
2016-09-07T03:57:24.493196: step 1483, loss 0.162032, acc 0.94
2016-09-07T03:57:25.195099: step 1484, loss 0.0640146, acc 0.98
2016-09-07T03:57:25.891402: step 1485, loss 0.104798, acc 0.94
2016-09-07T03:57:26.588369: step 1486, loss 0.0539562, acc 0.96
2016-09-07T03:57:27.284108: step 1487, loss 0.160328, acc 0.96
2016-09-07T03:57:27.950450: step 1488, loss 0.0775184, acc 0.96
2016-09-07T03:57:28.653212: step 1489, loss 0.0838494, acc 0.96
2016-09-07T03:57:29.329548: step 1490, loss 0.13798, acc 0.94
2016-09-07T03:57:30.015243: step 1491, loss 0.0803888, acc 0.96
2016-09-07T03:57:30.691816: step 1492, loss 0.0588835, acc 0.96
2016-09-07T03:57:31.373683: step 1493, loss 0.0712804, acc 0.98
2016-09-07T03:57:32.066553: step 1494, loss 0.116005, acc 0.96
2016-09-07T03:57:32.737589: step 1495, loss 0.125556, acc 0.92
2016-09-07T03:57:33.449591: step 1496, loss 0.104582, acc 0.98
2016-09-07T03:57:34.150459: step 1497, loss 0.144304, acc 0.9
2016-09-07T03:57:34.843858: step 1498, loss 0.137466, acc 0.92
2016-09-07T03:57:35.540024: step 1499, loss 0.0514762, acc 1
2016-09-07T03:57:36.243758: step 1500, loss 0.0245003, acc 1

Evaluation:
2016-09-07T03:57:39.404693: step 1500, loss 0.933933, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-1500

2016-09-07T03:57:41.122865: step 1501, loss 0.101268, acc 0.94
2016-09-07T03:57:41.825363: step 1502, loss 0.0997022, acc 0.94
2016-09-07T03:57:42.507980: step 1503, loss 0.17841, acc 0.9
2016-09-07T03:57:43.200408: step 1504, loss 0.0512317, acc 0.98
2016-09-07T03:57:43.873857: step 1505, loss 0.107929, acc 0.94
2016-09-07T03:57:44.548686: step 1506, loss 0.0581171, acc 0.96
2016-09-07T03:57:45.263066: step 1507, loss 0.00743217, acc 1
2016-09-07T03:57:45.932730: step 1508, loss 0.173482, acc 0.92
2016-09-07T03:57:46.625568: step 1509, loss 0.0706788, acc 0.96
2016-09-07T03:57:47.279330: step 1510, loss 0.118044, acc 0.96
2016-09-07T03:57:47.943657: step 1511, loss 0.0329714, acc 0.98
2016-09-07T03:57:48.632619: step 1512, loss 0.144785, acc 0.96
2016-09-07T03:57:49.308081: step 1513, loss 0.132312, acc 0.92
2016-09-07T03:57:49.994361: step 1514, loss 0.158438, acc 0.92
2016-09-07T03:57:50.688006: step 1515, loss 0.0147081, acc 1
2016-09-07T03:57:51.381723: step 1516, loss 0.0571974, acc 0.96
2016-09-07T03:57:52.058354: step 1517, loss 0.0275163, acc 1
2016-09-07T03:57:52.759946: step 1518, loss 0.090032, acc 0.98
2016-09-07T03:57:53.433053: step 1519, loss 0.060746, acc 0.96
2016-09-07T03:57:54.134856: step 1520, loss 0.0649555, acc 0.96
2016-09-07T03:57:54.802452: step 1521, loss 0.188179, acc 0.92
2016-09-07T03:57:55.504673: step 1522, loss 0.0587994, acc 0.96
2016-09-07T03:57:56.204443: step 1523, loss 0.130376, acc 0.96
2016-09-07T03:57:56.893510: step 1524, loss 0.0525881, acc 0.98
2016-09-07T03:57:57.576862: step 1525, loss 0.0466851, acc 0.98
2016-09-07T03:57:58.267774: step 1526, loss 0.0256838, acc 1
2016-09-07T03:57:58.950279: step 1527, loss 0.072392, acc 0.98
2016-09-07T03:57:59.629471: step 1528, loss 0.158535, acc 0.9
2016-09-07T03:58:00.348747: step 1529, loss 0.030453, acc 0.98
2016-09-07T03:58:01.075927: step 1530, loss 0.0259523, acc 1
2016-09-07T03:58:01.779006: step 1531, loss 0.0675837, acc 0.96
2016-09-07T03:58:02.474703: step 1532, loss 0.0153678, acc 1
2016-09-07T03:58:03.159846: step 1533, loss 0.0213053, acc 1
2016-09-07T03:58:03.840372: step 1534, loss 0.0460372, acc 1
2016-09-07T03:58:04.528629: step 1535, loss 0.0513941, acc 0.98
2016-09-07T03:58:05.179675: step 1536, loss 0.0230744, acc 1
2016-09-07T03:58:05.885270: step 1537, loss 0.0733548, acc 0.96
2016-09-07T03:58:06.541040: step 1538, loss 0.0119215, acc 1
2016-09-07T03:58:07.209924: step 1539, loss 0.0320096, acc 1
2016-09-07T03:58:07.902049: step 1540, loss 0.0103366, acc 1
2016-09-07T03:58:08.571588: step 1541, loss 0.13909, acc 0.92
2016-09-07T03:58:09.272346: step 1542, loss 0.0709652, acc 0.96
2016-09-07T03:58:09.957711: step 1543, loss 0.0381708, acc 0.98
2016-09-07T03:58:10.640691: step 1544, loss 0.136509, acc 0.96
2016-09-07T03:58:11.322970: step 1545, loss 0.0254405, acc 0.98
2016-09-07T03:58:12.022490: step 1546, loss 0.0169466, acc 1
2016-09-07T03:58:12.715169: step 1547, loss 0.0856343, acc 0.96
2016-09-07T03:58:13.407748: step 1548, loss 0.0662422, acc 0.96
2016-09-07T03:58:14.088817: step 1549, loss 0.0900953, acc 0.96
2016-09-07T03:58:14.780149: step 1550, loss 0.0350208, acc 1
2016-09-07T03:58:15.483533: step 1551, loss 0.29549, acc 0.94
2016-09-07T03:58:16.154494: step 1552, loss 0.0096477, acc 1
2016-09-07T03:58:16.829840: step 1553, loss 0.0383994, acc 0.98
2016-09-07T03:58:17.514958: step 1554, loss 0.138252, acc 0.94
2016-09-07T03:58:18.198814: step 1555, loss 0.108949, acc 0.94
2016-09-07T03:58:18.890595: step 1556, loss 0.0874312, acc 0.96
2016-09-07T03:58:19.571013: step 1557, loss 0.038114, acc 0.98
2016-09-07T03:58:20.278153: step 1558, loss 0.0853896, acc 0.98
2016-09-07T03:58:20.976311: step 1559, loss 0.035761, acc 1
2016-09-07T03:58:21.673522: step 1560, loss 0.0494208, acc 1
2016-09-07T03:58:22.350508: step 1561, loss 0.032233, acc 0.98
2016-09-07T03:58:23.036275: step 1562, loss 0.0289091, acc 1
2016-09-07T03:58:23.726773: step 1563, loss 0.119639, acc 0.96
2016-09-07T03:58:24.402883: step 1564, loss 0.00572048, acc 1
2016-09-07T03:58:25.099453: step 1565, loss 0.0533354, acc 0.96
2016-09-07T03:58:25.765305: step 1566, loss 0.0424525, acc 0.98
2016-09-07T03:58:26.468715: step 1567, loss 0.0790131, acc 0.96
2016-09-07T03:58:27.155588: step 1568, loss 0.156451, acc 0.9
2016-09-07T03:58:27.824808: step 1569, loss 0.0218986, acc 1
2016-09-07T03:58:28.517029: step 1570, loss 0.0168215, acc 1
2016-09-07T03:58:29.204438: step 1571, loss 0.0279226, acc 1
2016-09-07T03:58:29.922960: step 1572, loss 0.043136, acc 0.96
2016-09-07T03:58:30.592967: step 1573, loss 0.0404803, acc 0.98
2016-09-07T03:58:31.274727: step 1574, loss 0.0425086, acc 0.98
2016-09-07T03:58:31.953725: step 1575, loss 0.0541927, acc 0.98
2016-09-07T03:58:32.617794: step 1576, loss 0.0905407, acc 0.98
2016-09-07T03:58:33.300021: step 1577, loss 0.0801755, acc 0.96
2016-09-07T03:58:33.999823: step 1578, loss 0.0319414, acc 0.98
2016-09-07T03:58:34.700207: step 1579, loss 0.0323704, acc 1
2016-09-07T03:58:35.401532: step 1580, loss 0.0239656, acc 0.98
2016-09-07T03:58:36.097502: step 1581, loss 0.0261526, acc 0.98
2016-09-07T03:58:36.764455: step 1582, loss 0.0534322, acc 0.98
2016-09-07T03:58:37.449275: step 1583, loss 0.040375, acc 1
2016-09-07T03:58:38.121711: step 1584, loss 0.0458614, acc 1
2016-09-07T03:58:38.815048: step 1585, loss 0.0920244, acc 0.96
2016-09-07T03:58:39.540917: step 1586, loss 0.0795095, acc 0.94
2016-09-07T03:58:40.204004: step 1587, loss 0.0474062, acc 0.96
2016-09-07T03:58:40.886183: step 1588, loss 0.0361743, acc 0.98
2016-09-07T03:58:41.576767: step 1589, loss 0.13504, acc 0.96
2016-09-07T03:58:42.253237: step 1590, loss 0.0236791, acc 1
2016-09-07T03:58:42.930070: step 1591, loss 0.113854, acc 0.92
2016-09-07T03:58:43.621528: step 1592, loss 0.034609, acc 0.98
2016-09-07T03:58:44.338586: step 1593, loss 0.0222476, acc 0.98
2016-09-07T03:58:45.019861: step 1594, loss 0.0158741, acc 1
2016-09-07T03:58:45.705058: step 1595, loss 0.0259066, acc 0.98
2016-09-07T03:58:46.407461: step 1596, loss 0.0495594, acc 0.98
2016-09-07T03:58:47.089323: step 1597, loss 0.020248, acc 1
2016-09-07T03:58:47.797791: step 1598, loss 0.0438821, acc 0.98
2016-09-07T03:58:48.462707: step 1599, loss 0.0631482, acc 0.96
2016-09-07T03:58:49.147411: step 1600, loss 0.0306527, acc 1

Evaluation:
2016-09-07T03:58:52.292939: step 1600, loss 1.41809, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-1600

2016-09-07T03:58:53.955685: step 1601, loss 0.13864, acc 0.96
2016-09-07T03:58:54.625878: step 1602, loss 0.0627923, acc 0.98
2016-09-07T03:58:55.315657: step 1603, loss 0.105589, acc 0.92
2016-09-07T03:58:56.019419: step 1604, loss 0.123178, acc 0.96
2016-09-07T03:58:56.710174: step 1605, loss 0.0396683, acc 0.98
2016-09-07T03:58:57.410917: step 1606, loss 0.0711361, acc 0.98
2016-09-07T03:58:58.092353: step 1607, loss 0.0672173, acc 0.98
2016-09-07T03:58:58.819207: step 1608, loss 0.0220151, acc 0.98
2016-09-07T03:58:59.487738: step 1609, loss 0.00797294, acc 1
2016-09-07T03:59:00.167567: step 1610, loss 0.0552254, acc 0.98
2016-09-07T03:59:00.912302: step 1611, loss 0.0793875, acc 0.96
2016-09-07T03:59:01.594928: step 1612, loss 0.0330583, acc 0.98
2016-09-07T03:59:02.271297: step 1613, loss 0.0363784, acc 0.98
2016-09-07T03:59:02.950353: step 1614, loss 0.0270368, acc 1
2016-09-07T03:59:03.655823: step 1615, loss 0.0235528, acc 0.98
2016-09-07T03:59:04.346278: step 1616, loss 0.174091, acc 0.96
2016-09-07T03:59:05.036772: step 1617, loss 0.0606996, acc 0.94
2016-09-07T03:59:05.732850: step 1618, loss 0.0122392, acc 1
2016-09-07T03:59:06.419106: step 1619, loss 0.0890594, acc 0.94
2016-09-07T03:59:07.110428: step 1620, loss 0.0272721, acc 0.98
2016-09-07T03:59:07.786182: step 1621, loss 0.0202146, acc 1
2016-09-07T03:59:08.478422: step 1622, loss 0.0221817, acc 1
2016-09-07T03:59:09.190399: step 1623, loss 0.0657194, acc 0.98
2016-09-07T03:59:09.876888: step 1624, loss 0.074591, acc 0.96
2016-09-07T03:59:10.557754: step 1625, loss 0.119765, acc 0.94
2016-09-07T03:59:11.266312: step 1626, loss 0.0814445, acc 0.98
2016-09-07T03:59:11.983642: step 1627, loss 0.128879, acc 0.94
2016-09-07T03:59:12.651672: step 1628, loss 0.0349353, acc 1
2016-09-07T03:59:13.314119: step 1629, loss 0.0135586, acc 1
2016-09-07T03:59:13.982363: step 1630, loss 0.0358394, acc 0.98
2016-09-07T03:59:14.675399: step 1631, loss 0.00621755, acc 1
2016-09-07T03:59:15.349681: step 1632, loss 0.021215, acc 1
2016-09-07T03:59:16.045673: step 1633, loss 0.0460809, acc 0.98
2016-09-07T03:59:16.737658: step 1634, loss 0.0819466, acc 0.96
2016-09-07T03:59:17.415939: step 1635, loss 0.0939442, acc 0.94
2016-09-07T03:59:18.132763: step 1636, loss 0.126907, acc 0.94
2016-09-07T03:59:18.825765: step 1637, loss 0.0673518, acc 0.96
2016-09-07T03:59:19.506765: step 1638, loss 0.0421665, acc 0.98
2016-09-07T03:59:20.181878: step 1639, loss 0.0880929, acc 0.94
2016-09-07T03:59:20.861916: step 1640, loss 0.0177224, acc 1
2016-09-07T03:59:21.554990: step 1641, loss 0.0327096, acc 1
2016-09-07T03:59:22.235225: step 1642, loss 0.0686458, acc 0.98
2016-09-07T03:59:22.917351: step 1643, loss 0.0429704, acc 0.98
2016-09-07T03:59:23.603525: step 1644, loss 0.117319, acc 0.94
2016-09-07T03:59:24.312269: step 1645, loss 0.0320974, acc 0.98
2016-09-07T03:59:25.034660: step 1646, loss 0.117123, acc 0.96
2016-09-07T03:59:25.714732: step 1647, loss 0.336471, acc 0.92
2016-09-07T03:59:26.432838: step 1648, loss 0.00862482, acc 1
2016-09-07T03:59:27.130288: step 1649, loss 0.124887, acc 0.92
2016-09-07T03:59:27.825863: step 1650, loss 0.0543851, acc 0.96
2016-09-07T03:59:28.517803: step 1651, loss 0.0980627, acc 0.96
2016-09-07T03:59:29.221577: step 1652, loss 0.0883263, acc 0.94
2016-09-07T03:59:29.926401: step 1653, loss 0.0450291, acc 1
2016-09-07T03:59:30.613213: step 1654, loss 0.115406, acc 0.92
2016-09-07T03:59:31.322658: step 1655, loss 0.0633975, acc 0.98
2016-09-07T03:59:31.999642: step 1656, loss 0.0880461, acc 0.92
2016-09-07T03:59:32.673743: step 1657, loss 0.06243, acc 1
2016-09-07T03:59:33.371449: step 1658, loss 0.0769253, acc 0.96
2016-09-07T03:59:34.065847: step 1659, loss 0.0527947, acc 0.98
2016-09-07T03:59:34.810287: step 1660, loss 0.0741212, acc 0.96
2016-09-07T03:59:35.490048: step 1661, loss 0.108491, acc 0.92
2016-09-07T03:59:36.193972: step 1662, loss 0.248155, acc 0.92
2016-09-07T03:59:36.866243: step 1663, loss 0.0310076, acc 1
2016-09-07T03:59:37.559160: step 1664, loss 0.0697511, acc 0.96
2016-09-07T03:59:38.250268: step 1665, loss 0.0374844, acc 0.98
2016-09-07T03:59:38.940267: step 1666, loss 0.0650736, acc 0.98
2016-09-07T03:59:39.670951: step 1667, loss 0.228122, acc 0.92
2016-09-07T03:59:40.412831: step 1668, loss 0.0765398, acc 0.96
2016-09-07T03:59:41.082350: step 1669, loss 0.00314723, acc 1
2016-09-07T03:59:41.765248: step 1670, loss 0.0701913, acc 0.98
2016-09-07T03:59:42.464875: step 1671, loss 0.0213202, acc 1
2016-09-07T03:59:43.142428: step 1672, loss 0.0894692, acc 0.94
2016-09-07T03:59:43.800275: step 1673, loss 0.0363677, acc 0.98
2016-09-07T03:59:44.496829: step 1674, loss 0.0527367, acc 0.98
2016-09-07T03:59:45.164890: step 1675, loss 0.0901527, acc 0.94
2016-09-07T03:59:45.834766: step 1676, loss 0.0979884, acc 0.96
2016-09-07T03:59:46.519322: step 1677, loss 0.0186568, acc 1
2016-09-07T03:59:47.209017: step 1678, loss 0.034531, acc 0.98
2016-09-07T03:59:47.894878: step 1679, loss 0.0192307, acc 1
2016-09-07T03:59:48.560832: step 1680, loss 0.0748385, acc 0.98
2016-09-07T03:59:49.252484: step 1681, loss 0.0325191, acc 0.98
2016-09-07T03:59:49.939597: step 1682, loss 0.135343, acc 0.92
2016-09-07T03:59:50.627901: step 1683, loss 0.0393257, acc 0.96
2016-09-07T03:59:51.316309: step 1684, loss 0.0211479, acc 1
2016-09-07T03:59:51.985844: step 1685, loss 0.0686098, acc 0.96
2016-09-07T03:59:52.669954: step 1686, loss 0.0580922, acc 0.98
2016-09-07T03:59:53.365434: step 1687, loss 0.0983005, acc 0.98
2016-09-07T03:59:54.083977: step 1688, loss 0.0954769, acc 0.94
2016-09-07T03:59:54.750546: step 1689, loss 0.0605676, acc 0.96
2016-09-07T03:59:55.435438: step 1690, loss 0.171725, acc 0.94
2016-09-07T03:59:56.114373: step 1691, loss 0.0671884, acc 0.96
2016-09-07T03:59:56.816348: step 1692, loss 0.0247348, acc 0.98
2016-09-07T03:59:57.510682: step 1693, loss 0.166698, acc 0.92
2016-09-07T03:59:58.194955: step 1694, loss 0.0315825, acc 0.98
2016-09-07T03:59:58.900067: step 1695, loss 0.047598, acc 0.98
2016-09-07T03:59:59.586144: step 1696, loss 0.0485446, acc 0.98
2016-09-07T04:00:00.301903: step 1697, loss 0.131097, acc 0.94
2016-09-07T04:00:00.995008: step 1698, loss 0.165245, acc 0.94
2016-09-07T04:00:01.665762: step 1699, loss 0.00729029, acc 1
2016-09-07T04:00:02.346351: step 1700, loss 0.00587102, acc 1

Evaluation:
2016-09-07T04:00:05.503308: step 1700, loss 1.15761, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-1700

2016-09-07T04:00:07.198438: step 1701, loss 0.0575859, acc 0.98
2016-09-07T04:00:07.890823: step 1702, loss 0.112016, acc 0.92
2016-09-07T04:00:08.602284: step 1703, loss 0.0522585, acc 0.96
2016-09-07T04:00:09.308272: step 1704, loss 0.0823813, acc 0.96
2016-09-07T04:00:09.996026: step 1705, loss 0.0913932, acc 0.94
2016-09-07T04:00:10.714126: step 1706, loss 0.0917006, acc 0.98
2016-09-07T04:00:11.443250: step 1707, loss 0.0087002, acc 1
2016-09-07T04:00:12.155435: step 1708, loss 0.0139951, acc 1
2016-09-07T04:00:12.842092: step 1709, loss 0.0671131, acc 0.96
2016-09-07T04:00:13.531812: step 1710, loss 0.0605318, acc 0.98
2016-09-07T04:00:14.208518: step 1711, loss 0.147188, acc 0.94
2016-09-07T04:00:14.885692: step 1712, loss 0.0678172, acc 0.98
2016-09-07T04:00:15.584129: step 1713, loss 0.0769473, acc 0.96
2016-09-07T04:00:16.256034: step 1714, loss 0.0808076, acc 0.98
2016-09-07T04:00:16.946962: step 1715, loss 0.0214892, acc 1
2016-09-07T04:00:17.648543: step 1716, loss 0.0205627, acc 1
2016-09-07T04:00:18.337349: step 1717, loss 0.0994928, acc 0.98
2016-09-07T04:00:19.024510: step 1718, loss 0.0957289, acc 0.96
2016-09-07T04:00:19.718540: step 1719, loss 0.0545321, acc 0.98
2016-09-07T04:00:20.400377: step 1720, loss 0.0938487, acc 0.94
2016-09-07T04:00:21.082863: step 1721, loss 0.0878654, acc 0.98
2016-09-07T04:00:21.797052: step 1722, loss 0.0893621, acc 0.98
2016-09-07T04:00:22.482517: step 1723, loss 0.112846, acc 0.94
2016-09-07T04:00:23.178624: step 1724, loss 0.0112392, acc 1
2016-09-07T04:00:23.854913: step 1725, loss 0.176979, acc 0.96
2016-09-07T04:00:24.556950: step 1726, loss 0.0888859, acc 0.96
2016-09-07T04:00:25.275758: step 1727, loss 0.0322621, acc 1
2016-09-07T04:00:25.897293: step 1728, loss 0.151472, acc 0.977273
2016-09-07T04:00:26.622671: step 1729, loss 0.0667062, acc 0.98
2016-09-07T04:00:27.306166: step 1730, loss 0.0417162, acc 1
2016-09-07T04:00:27.987053: step 1731, loss 0.0968886, acc 0.94
2016-09-07T04:00:28.663932: step 1732, loss 0.142478, acc 0.94
2016-09-07T04:00:29.349894: step 1733, loss 0.02774, acc 1
2016-09-07T04:00:30.053733: step 1734, loss 0.0203646, acc 1
2016-09-07T04:00:30.735429: step 1735, loss 0.0374985, acc 0.98
2016-09-07T04:00:31.448723: step 1736, loss 0.113076, acc 0.96
2016-09-07T04:00:32.122717: step 1737, loss 0.0479102, acc 0.96
2016-09-07T04:00:32.823151: step 1738, loss 0.0805965, acc 0.96
2016-09-07T04:00:33.507327: step 1739, loss 0.0383215, acc 1
2016-09-07T04:00:34.203780: step 1740, loss 0.0441715, acc 0.98
2016-09-07T04:00:34.882759: step 1741, loss 0.0746209, acc 0.96
2016-09-07T04:00:35.556386: step 1742, loss 0.0713253, acc 0.96
2016-09-07T04:00:36.262408: step 1743, loss 0.0181312, acc 1
2016-09-07T04:00:36.964679: step 1744, loss 0.0345092, acc 0.98
2016-09-07T04:00:37.643375: step 1745, loss 0.0753932, acc 0.96
2016-09-07T04:00:38.327369: step 1746, loss 0.125693, acc 0.96
2016-09-07T04:00:39.005878: step 1747, loss 0.0176713, acc 1
2016-09-07T04:00:39.692487: step 1748, loss 0.0161949, acc 1
2016-09-07T04:00:40.365393: step 1749, loss 0.0534283, acc 0.98
2016-09-07T04:00:41.081499: step 1750, loss 0.103114, acc 0.96
2016-09-07T04:00:41.792708: step 1751, loss 0.00501967, acc 1
2016-09-07T04:00:42.478639: step 1752, loss 0.0136851, acc 1
2016-09-07T04:00:43.183548: step 1753, loss 0.36967, acc 0.94
2016-09-07T04:00:43.866815: step 1754, loss 0.0542283, acc 0.94
2016-09-07T04:00:44.571305: step 1755, loss 0.114882, acc 0.98
2016-09-07T04:00:45.259563: step 1756, loss 0.104578, acc 0.94
2016-09-07T04:00:45.950099: step 1757, loss 0.0356454, acc 1
2016-09-07T04:00:46.648920: step 1758, loss 0.0231352, acc 1
2016-09-07T04:00:47.357420: step 1759, loss 0.0828895, acc 0.96
2016-09-07T04:00:48.041044: step 1760, loss 0.0603336, acc 0.98
2016-09-07T04:00:48.725335: step 1761, loss 0.0857472, acc 0.98
2016-09-07T04:00:49.416303: step 1762, loss 0.0243317, acc 1
2016-09-07T04:00:50.086464: step 1763, loss 0.0493337, acc 0.98
2016-09-07T04:00:50.774601: step 1764, loss 0.0408781, acc 0.98
2016-09-07T04:00:51.486438: step 1765, loss 0.0233269, acc 1
2016-09-07T04:00:52.183685: step 1766, loss 0.0664824, acc 0.94
2016-09-07T04:00:52.883119: step 1767, loss 0.0568748, acc 0.98
2016-09-07T04:00:53.569516: step 1768, loss 0.051295, acc 0.98
2016-09-07T04:00:54.277565: step 1769, loss 0.0509521, acc 0.96
2016-09-07T04:00:54.953267: step 1770, loss 0.0469381, acc 0.98
2016-09-07T04:00:55.631326: step 1771, loss 0.025212, acc 1
2016-09-07T04:00:56.316459: step 1772, loss 0.136191, acc 0.98
2016-09-07T04:00:57.000167: step 1773, loss 0.0833279, acc 0.96
2016-09-07T04:00:57.703222: step 1774, loss 0.106911, acc 0.98
2016-09-07T04:00:58.370070: step 1775, loss 0.0504676, acc 0.98
2016-09-07T04:00:59.087956: step 1776, loss 0.063472, acc 0.96
2016-09-07T04:00:59.764157: step 1777, loss 0.0332717, acc 0.98
2016-09-07T04:01:00.483072: step 1778, loss 0.0883713, acc 0.96
2016-09-07T04:01:01.166281: step 1779, loss 0.0908358, acc 0.96
2016-09-07T04:01:01.855551: step 1780, loss 0.0296854, acc 0.98
2016-09-07T04:01:02.568429: step 1781, loss 0.109912, acc 0.96
2016-09-07T04:01:03.263505: step 1782, loss 0.0167683, acc 1
2016-09-07T04:01:03.937964: step 1783, loss 0.0301854, acc 0.98
2016-09-07T04:01:04.607138: step 1784, loss 0.0372805, acc 0.98
2016-09-07T04:01:05.295118: step 1785, loss 0.187974, acc 0.94
2016-09-07T04:01:05.988805: step 1786, loss 0.00334704, acc 1
2016-09-07T04:01:06.688847: step 1787, loss 0.0432084, acc 1
2016-09-07T04:01:07.377341: step 1788, loss 0.0291897, acc 0.98
2016-09-07T04:01:08.034968: step 1789, loss 0.0561439, acc 0.98
2016-09-07T04:01:08.736725: step 1790, loss 0.047468, acc 0.96
2016-09-07T04:01:09.447242: step 1791, loss 0.0447995, acc 1
2016-09-07T04:01:10.145478: step 1792, loss 0.0722306, acc 0.96
2016-09-07T04:01:10.824220: step 1793, loss 0.0255854, acc 1
2016-09-07T04:01:11.525141: step 1794, loss 0.0684107, acc 0.94
2016-09-07T04:01:12.229319: step 1795, loss 0.0889263, acc 0.98
2016-09-07T04:01:12.915000: step 1796, loss 0.0392796, acc 0.98
2016-09-07T04:01:13.597907: step 1797, loss 0.0319953, acc 0.98
2016-09-07T04:01:14.272677: step 1798, loss 0.0205168, acc 1
2016-09-07T04:01:14.967290: step 1799, loss 0.02245, acc 1
2016-09-07T04:01:15.690607: step 1800, loss 0.0305855, acc 0.98

Evaluation:
2016-09-07T04:01:18.854329: step 1800, loss 1.40168, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-1800

2016-09-07T04:01:20.629105: step 1801, loss 0.00709014, acc 1
2016-09-07T04:01:21.329194: step 1802, loss 0.0808321, acc 0.96
2016-09-07T04:01:22.024687: step 1803, loss 0.0404849, acc 0.98
2016-09-07T04:01:22.715236: step 1804, loss 0.0330206, acc 0.98
2016-09-07T04:01:23.394440: step 1805, loss 0.11007, acc 0.94
2016-09-07T04:01:24.103508: step 1806, loss 0.0027537, acc 1
2016-09-07T04:01:24.779024: step 1807, loss 0.0385095, acc 0.98
2016-09-07T04:01:25.494407: step 1808, loss 0.0299229, acc 1
2016-09-07T04:01:26.162076: step 1809, loss 0.0721581, acc 0.96
2016-09-07T04:01:26.846080: step 1810, loss 0.017101, acc 1
2016-09-07T04:01:27.554636: step 1811, loss 0.0769975, acc 0.98
2016-09-07T04:01:28.240341: step 1812, loss 0.0405524, acc 0.98
2016-09-07T04:01:28.919481: step 1813, loss 0.0507091, acc 0.98
2016-09-07T04:01:29.571447: step 1814, loss 0.0697694, acc 0.98
2016-09-07T04:01:30.280714: step 1815, loss 0.0740315, acc 0.96
2016-09-07T04:01:30.954033: step 1816, loss 0.0118541, acc 1
2016-09-07T04:01:31.623132: step 1817, loss 0.0651027, acc 0.98
2016-09-07T04:01:32.310554: step 1818, loss 0.10599, acc 0.94
2016-09-07T04:01:32.987593: step 1819, loss 0.0848579, acc 0.98
2016-09-07T04:01:33.692311: step 1820, loss 0.0645143, acc 0.98
2016-09-07T04:01:34.371213: step 1821, loss 0.0192196, acc 1
2016-09-07T04:01:35.078787: step 1822, loss 0.0197195, acc 1
2016-09-07T04:01:35.748788: step 1823, loss 0.0221755, acc 0.98
2016-09-07T04:01:36.427187: step 1824, loss 0.0862631, acc 0.96
2016-09-07T04:01:37.105148: step 1825, loss 0.0112668, acc 1
2016-09-07T04:01:37.810161: step 1826, loss 0.0678163, acc 0.94
2016-09-07T04:01:38.502012: step 1827, loss 0.0360734, acc 0.96
2016-09-07T04:01:39.191553: step 1828, loss 0.0113442, acc 1
2016-09-07T04:01:39.900229: step 1829, loss 0.0695645, acc 0.98
2016-09-07T04:01:40.607342: step 1830, loss 0.0336411, acc 1
2016-09-07T04:01:41.304158: step 1831, loss 0.0939757, acc 0.98
2016-09-07T04:01:41.989647: step 1832, loss 0.0588967, acc 0.98
2016-09-07T04:01:42.693319: step 1833, loss 0.0167758, acc 1
2016-09-07T04:01:43.390466: step 1834, loss 0.0102124, acc 1
2016-09-07T04:01:44.060139: step 1835, loss 0.0774402, acc 0.98
2016-09-07T04:01:44.746269: step 1836, loss 0.0116709, acc 1
2016-09-07T04:01:45.425152: step 1837, loss 0.0442527, acc 0.98
2016-09-07T04:01:46.122815: step 1838, loss 0.101543, acc 0.98
2016-09-07T04:01:46.791117: step 1839, loss 0.0765117, acc 0.96
2016-09-07T04:01:47.485155: step 1840, loss 0.0351352, acc 1
2016-09-07T04:01:48.175102: step 1841, loss 0.0278517, acc 0.98
2016-09-07T04:01:48.863790: step 1842, loss 0.0138214, acc 1
2016-09-07T04:01:49.561284: step 1843, loss 0.0801634, acc 0.98
2016-09-07T04:01:50.247627: step 1844, loss 0.179173, acc 0.94
2016-09-07T04:01:50.930644: step 1845, loss 0.0508233, acc 0.98
2016-09-07T04:01:51.620649: step 1846, loss 0.0473996, acc 0.96
2016-09-07T04:01:52.312768: step 1847, loss 0.0421631, acc 0.98
2016-09-07T04:01:53.012211: step 1848, loss 0.173039, acc 0.92
2016-09-07T04:01:53.718531: step 1849, loss 0.0312815, acc 0.98
2016-09-07T04:01:54.437677: step 1850, loss 0.0626281, acc 0.98
2016-09-07T04:01:55.144016: step 1851, loss 0.239547, acc 0.92
2016-09-07T04:01:55.823806: step 1852, loss 0.141512, acc 0.94
2016-09-07T04:01:56.517778: step 1853, loss 0.0777027, acc 0.98
2016-09-07T04:01:57.203817: step 1854, loss 0.0333675, acc 1
2016-09-07T04:01:57.928801: step 1855, loss 0.0554198, acc 0.98
2016-09-07T04:01:58.624725: step 1856, loss 0.00842214, acc 1
2016-09-07T04:01:59.312570: step 1857, loss 0.0146755, acc 1
2016-09-07T04:02:00.010714: step 1858, loss 0.216523, acc 0.9
2016-09-07T04:02:00.736706: step 1859, loss 0.0847533, acc 0.94
2016-09-07T04:02:01.443249: step 1860, loss 0.0408995, acc 0.98
2016-09-07T04:02:02.102891: step 1861, loss 0.0291548, acc 0.98
2016-09-07T04:02:02.830879: step 1862, loss 0.0438414, acc 0.96
2016-09-07T04:02:03.533567: step 1863, loss 0.0577206, acc 0.98
2016-09-07T04:02:04.223151: step 1864, loss 0.147959, acc 0.9
2016-09-07T04:02:04.918869: step 1865, loss 0.00325742, acc 1
2016-09-07T04:02:05.604038: step 1866, loss 0.0474019, acc 0.96
2016-09-07T04:02:06.291883: step 1867, loss 0.0209031, acc 1
2016-09-07T04:02:06.946472: step 1868, loss 0.0671833, acc 0.98
2016-09-07T04:02:07.654719: step 1869, loss 0.0506824, acc 1
2016-09-07T04:02:08.363149: step 1870, loss 0.13724, acc 0.92
2016-09-07T04:02:09.059325: step 1871, loss 0.144565, acc 0.94
2016-09-07T04:02:09.751054: step 1872, loss 0.0226178, acc 1
2016-09-07T04:02:10.434776: step 1873, loss 0.107179, acc 0.94
2016-09-07T04:02:11.147454: step 1874, loss 0.0495334, acc 1
2016-09-07T04:02:11.825947: step 1875, loss 0.0390895, acc 0.98
2016-09-07T04:02:12.511073: step 1876, loss 0.170898, acc 0.92
2016-09-07T04:02:13.207584: step 1877, loss 0.0268588, acc 1
2016-09-07T04:02:13.911887: step 1878, loss 0.136374, acc 0.96
2016-09-07T04:02:14.606022: step 1879, loss 0.181798, acc 0.96
2016-09-07T04:02:15.273751: step 1880, loss 0.0398627, acc 1
2016-09-07T04:02:15.974353: step 1881, loss 0.0885773, acc 0.96
2016-09-07T04:02:16.654243: step 1882, loss 0.0543734, acc 0.98
2016-09-07T04:02:17.327145: step 1883, loss 0.0501797, acc 0.98
2016-09-07T04:02:18.006320: step 1884, loss 0.0619455, acc 0.96
2016-09-07T04:02:18.676718: step 1885, loss 0.0236147, acc 0.98
2016-09-07T04:02:19.368503: step 1886, loss 0.0217709, acc 1
2016-09-07T04:02:20.048448: step 1887, loss 0.0237324, acc 1
2016-09-07T04:02:20.755277: step 1888, loss 0.0288533, acc 0.98
2016-09-07T04:02:21.437774: step 1889, loss 0.0693516, acc 0.98
2016-09-07T04:02:22.169191: step 1890, loss 0.068897, acc 0.96
2016-09-07T04:02:22.846955: step 1891, loss 0.0172964, acc 0.98
2016-09-07T04:02:23.530009: step 1892, loss 0.0201198, acc 1
2016-09-07T04:02:24.226547: step 1893, loss 0.0134138, acc 1
2016-09-07T04:02:24.899321: step 1894, loss 0.145196, acc 0.94
2016-09-07T04:02:25.613488: step 1895, loss 0.0465554, acc 0.96
2016-09-07T04:02:26.276488: step 1896, loss 0.0627042, acc 0.98
2016-09-07T04:02:26.959066: step 1897, loss 0.022973, acc 1
2016-09-07T04:02:27.649895: step 1898, loss 0.11651, acc 0.96
2016-09-07T04:02:28.337242: step 1899, loss 0.0130759, acc 1
2016-09-07T04:02:29.044857: step 1900, loss 0.117333, acc 0.94

Evaluation:
2016-09-07T04:02:32.193206: step 1900, loss 1.42274, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-1900

2016-09-07T04:02:33.980924: step 1901, loss 0.0298443, acc 0.98
2016-09-07T04:02:34.654989: step 1902, loss 0.101748, acc 0.94
2016-09-07T04:02:35.382045: step 1903, loss 0.0346474, acc 1
2016-09-07T04:02:36.083758: step 1904, loss 0.0923328, acc 0.98
2016-09-07T04:02:36.777482: step 1905, loss 0.00166926, acc 1
2016-09-07T04:02:37.477663: step 1906, loss 0.167997, acc 0.98
2016-09-07T04:02:38.155394: step 1907, loss 0.0747831, acc 0.94
2016-09-07T04:02:38.870604: step 1908, loss 0.0203287, acc 1
2016-09-07T04:02:39.568818: step 1909, loss 0.0801423, acc 0.96
2016-09-07T04:02:40.266996: step 1910, loss 0.0462762, acc 0.98
2016-09-07T04:02:41.006296: step 1911, loss 0.0358759, acc 0.98
2016-09-07T04:02:41.705591: step 1912, loss 0.103961, acc 0.94
2016-09-07T04:02:42.433307: step 1913, loss 0.0623714, acc 0.98
2016-09-07T04:02:43.121229: step 1914, loss 0.0105474, acc 1
2016-09-07T04:02:43.798407: step 1915, loss 0.0533337, acc 0.98
2016-09-07T04:02:44.496030: step 1916, loss 0.00739181, acc 1
2016-09-07T04:02:45.188017: step 1917, loss 0.0524057, acc 0.96
2016-09-07T04:02:45.869801: step 1918, loss 0.0868436, acc 0.98
2016-09-07T04:02:46.542493: step 1919, loss 0.0840206, acc 0.96
2016-09-07T04:02:47.200686: step 1920, loss 0.0223228, acc 0.977273
2016-09-07T04:02:47.897307: step 1921, loss 0.0994102, acc 0.96
2016-09-07T04:02:48.608623: step 1922, loss 0.034287, acc 1
2016-09-07T04:02:49.304900: step 1923, loss 0.0856258, acc 0.96
2016-09-07T04:02:50.004945: step 1924, loss 0.0672921, acc 0.96
2016-09-07T04:02:50.689959: step 1925, loss 0.0486574, acc 0.98
2016-09-07T04:02:51.377112: step 1926, loss 0.0574876, acc 0.98
2016-09-07T04:02:52.090823: step 1927, loss 0.0100512, acc 1
2016-09-07T04:02:52.775369: step 1928, loss 0.0303518, acc 0.98
2016-09-07T04:02:53.457843: step 1929, loss 0.029279, acc 1
2016-09-07T04:02:54.140923: step 1930, loss 0.0504806, acc 0.96
2016-09-07T04:02:54.824921: step 1931, loss 0.0267344, acc 1
2016-09-07T04:02:55.493789: step 1932, loss 0.0400346, acc 0.96
2016-09-07T04:02:56.181955: step 1933, loss 0.0434824, acc 1
2016-09-07T04:02:56.900913: step 1934, loss 0.128635, acc 0.94
2016-09-07T04:02:57.580407: step 1935, loss 0.00691705, acc 1
2016-09-07T04:02:58.282348: step 1936, loss 0.0138172, acc 1
2016-09-07T04:02:58.971379: step 1937, loss 0.0541263, acc 0.96
2016-09-07T04:02:59.660206: step 1938, loss 0.0212309, acc 1
2016-09-07T04:03:00.362797: step 1939, loss 0.054821, acc 0.98
2016-09-07T04:03:01.027067: step 1940, loss 0.019097, acc 0.98
2016-09-07T04:03:01.736718: step 1941, loss 0.0457679, acc 1
2016-09-07T04:03:02.435910: step 1942, loss 0.0583963, acc 0.96
2016-09-07T04:03:03.124127: step 1943, loss 0.0333447, acc 0.98
2016-09-07T04:03:03.814774: step 1944, loss 0.0340858, acc 1
2016-09-07T04:03:04.489131: step 1945, loss 0.0726102, acc 0.98
2016-09-07T04:03:05.197940: step 1946, loss 0.0268587, acc 0.98
2016-09-07T04:03:05.862766: step 1947, loss 0.0182721, acc 1
2016-09-07T04:03:06.565959: step 1948, loss 0.0255316, acc 1
2016-09-07T04:03:07.252537: step 1949, loss 0.0444664, acc 0.98
2016-09-07T04:03:07.940263: step 1950, loss 0.031194, acc 0.98
2016-09-07T04:03:08.653589: step 1951, loss 0.0246294, acc 1
2016-09-07T04:03:09.323929: step 1952, loss 0.0615179, acc 0.96
2016-09-07T04:03:10.029186: step 1953, loss 0.111928, acc 0.98
2016-09-07T04:03:10.697426: step 1954, loss 0.0543598, acc 0.96
2016-09-07T04:03:11.399180: step 1955, loss 0.0186455, acc 0.98
2016-09-07T04:03:12.109929: step 1956, loss 0.0675865, acc 0.94
2016-09-07T04:03:12.807212: step 1957, loss 0.00916864, acc 1
2016-09-07T04:03:13.497433: step 1958, loss 0.0349951, acc 0.98
2016-09-07T04:03:14.180220: step 1959, loss 0.00674637, acc 1
2016-09-07T04:03:14.895691: step 1960, loss 0.0171051, acc 0.98
2016-09-07T04:03:15.570343: step 1961, loss 0.0632214, acc 0.96
2016-09-07T04:03:16.267747: step 1962, loss 0.00656261, acc 1
2016-09-07T04:03:16.962414: step 1963, loss 0.113557, acc 0.96
2016-09-07T04:03:17.659531: step 1964, loss 0.0423353, acc 0.98
2016-09-07T04:03:18.338974: step 1965, loss 0.0513448, acc 0.96
2016-09-07T04:03:18.995661: step 1966, loss 0.0257546, acc 1
2016-09-07T04:03:19.691610: step 1967, loss 0.0314775, acc 0.98
2016-09-07T04:03:20.383722: step 1968, loss 0.0037958, acc 1
2016-09-07T04:03:21.091115: step 1969, loss 0.196681, acc 0.9
2016-09-07T04:03:21.777932: step 1970, loss 0.0637676, acc 0.98
2016-09-07T04:03:22.460994: step 1971, loss 0.0307378, acc 0.98
2016-09-07T04:03:23.180995: step 1972, loss 0.0593646, acc 0.98
2016-09-07T04:03:23.836837: step 1973, loss 0.0158401, acc 1
2016-09-07T04:03:24.535357: step 1974, loss 0.0575263, acc 0.98
2016-09-07T04:03:25.219187: step 1975, loss 0.0170312, acc 1
2016-09-07T04:03:25.934954: step 1976, loss 0.109643, acc 0.96
2016-09-07T04:03:26.633197: step 1977, loss 0.0226603, acc 1
2016-09-07T04:03:27.324225: step 1978, loss 0.00414286, acc 1
2016-09-07T04:03:28.035166: step 1979, loss 0.118737, acc 0.96
2016-09-07T04:03:28.716558: step 1980, loss 0.154851, acc 0.96
2016-09-07T04:03:29.414445: step 1981, loss 0.110724, acc 0.94
2016-09-07T04:03:30.094365: step 1982, loss 0.0408404, acc 0.98
2016-09-07T04:03:30.788913: step 1983, loss 0.133666, acc 0.94
2016-09-07T04:03:31.452431: step 1984, loss 0.0784585, acc 0.98
2016-09-07T04:03:32.149264: step 1985, loss 0.0319581, acc 0.98
2016-09-07T04:03:32.832082: step 1986, loss 0.0422668, acc 1
2016-09-07T04:03:33.477631: step 1987, loss 0.0544014, acc 0.94
2016-09-07T04:03:34.177115: step 1988, loss 0.0455778, acc 0.98
2016-09-07T04:03:34.844484: step 1989, loss 0.103653, acc 0.98
2016-09-07T04:03:35.524934: step 1990, loss 0.0882623, acc 0.96
2016-09-07T04:03:36.198368: step 1991, loss 0.0579061, acc 0.98
2016-09-07T04:03:36.881186: step 1992, loss 0.0730185, acc 0.94
2016-09-07T04:03:37.576869: step 1993, loss 0.0410428, acc 1
2016-09-07T04:03:38.247145: step 1994, loss 0.059901, acc 0.98
2016-09-07T04:03:38.950792: step 1995, loss 0.0274287, acc 0.98
2016-09-07T04:03:39.616298: step 1996, loss 0.035303, acc 1
2016-09-07T04:03:40.286438: step 1997, loss 0.0609827, acc 0.96
2016-09-07T04:03:40.951127: step 1998, loss 0.0235329, acc 1
2016-09-07T04:03:41.628114: step 1999, loss 0.0403035, acc 0.98
2016-09-07T04:03:42.341533: step 2000, loss 0.0749824, acc 0.98

Evaluation:
2016-09-07T04:03:45.506747: step 2000, loss 1.47053, acc 0.734522

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-2000

2016-09-07T04:03:47.289545: step 2001, loss 0.0203641, acc 1
2016-09-07T04:03:47.976664: step 2002, loss 0.0638157, acc 0.98
2016-09-07T04:03:48.672585: step 2003, loss 0.0334365, acc 0.98
2016-09-07T04:03:49.357288: step 2004, loss 0.0623638, acc 0.96
2016-09-07T04:03:50.047853: step 2005, loss 0.0903448, acc 0.94
2016-09-07T04:03:50.735796: step 2006, loss 0.0334651, acc 1
2016-09-07T04:03:51.423058: step 2007, loss 0.0150331, acc 1
2016-09-07T04:03:52.128047: step 2008, loss 0.0469723, acc 0.96
2016-09-07T04:03:52.793396: step 2009, loss 0.0181511, acc 1
2016-09-07T04:03:53.474494: step 2010, loss 0.0616876, acc 0.98
2016-09-07T04:03:54.166402: step 2011, loss 0.0921125, acc 0.98
2016-09-07T04:03:54.872004: step 2012, loss 0.0722678, acc 0.96
2016-09-07T04:03:55.549840: step 2013, loss 0.0480865, acc 1
2016-09-07T04:03:56.245512: step 2014, loss 0.0877359, acc 0.98
2016-09-07T04:03:56.964026: step 2015, loss 0.0392589, acc 0.98
2016-09-07T04:03:57.640472: step 2016, loss 0.0575299, acc 0.96
2016-09-07T04:03:58.340404: step 2017, loss 0.0552276, acc 0.98
2016-09-07T04:03:59.029306: step 2018, loss 0.0462095, acc 0.98
2016-09-07T04:03:59.731676: step 2019, loss 0.0733461, acc 0.98
2016-09-07T04:04:00.455361: step 2020, loss 0.109434, acc 0.96
2016-09-07T04:04:01.125631: step 2021, loss 0.0136606, acc 1
2016-09-07T04:04:01.824405: step 2022, loss 0.00409046, acc 1
2016-09-07T04:04:02.507293: step 2023, loss 0.161768, acc 0.96
2016-09-07T04:04:03.204391: step 2024, loss 0.0748156, acc 0.96
2016-09-07T04:04:03.915973: step 2025, loss 0.0734764, acc 0.96
2016-09-07T04:04:04.612033: step 2026, loss 0.0568947, acc 0.98
2016-09-07T04:04:05.303597: step 2027, loss 0.0883628, acc 0.96
2016-09-07T04:04:05.989248: step 2028, loss 0.0897507, acc 0.96
2016-09-07T04:04:06.697962: step 2029, loss 0.00692296, acc 1
2016-09-07T04:04:07.379102: step 2030, loss 0.11866, acc 0.98
2016-09-07T04:04:08.052622: step 2031, loss 0.0644201, acc 0.98
2016-09-07T04:04:08.730748: step 2032, loss 0.0899335, acc 0.94
2016-09-07T04:04:09.423782: step 2033, loss 0.0488795, acc 1
2016-09-07T04:04:10.115473: step 2034, loss 0.110321, acc 0.96
2016-09-07T04:04:10.777978: step 2035, loss 0.0182568, acc 1
2016-09-07T04:04:11.473239: step 2036, loss 0.0266551, acc 1
2016-09-07T04:04:12.145686: step 2037, loss 0.0685233, acc 0.98
2016-09-07T04:04:12.838429: step 2038, loss 0.0216044, acc 1
2016-09-07T04:04:13.558026: step 2039, loss 0.0786972, acc 0.96
2016-09-07T04:04:14.240095: step 2040, loss 0.0673053, acc 0.96
2016-09-07T04:04:14.939714: step 2041, loss 0.0391547, acc 0.96
2016-09-07T04:04:15.615787: step 2042, loss 0.0622511, acc 0.96
2016-09-07T04:04:16.303475: step 2043, loss 0.034551, acc 0.98
2016-09-07T04:04:16.984287: step 2044, loss 0.0551109, acc 0.98
2016-09-07T04:04:17.674174: step 2045, loss 0.241563, acc 0.96
2016-09-07T04:04:18.356704: step 2046, loss 0.0213399, acc 1
2016-09-07T04:04:19.049130: step 2047, loss 0.0664974, acc 0.98
2016-09-07T04:04:19.761867: step 2048, loss 0.0272513, acc 1
2016-09-07T04:04:20.424904: step 2049, loss 0.030309, acc 0.98
2016-09-07T04:04:21.112933: step 2050, loss 0.0421329, acc 0.98
2016-09-07T04:04:21.785888: step 2051, loss 0.0780275, acc 0.96
2016-09-07T04:04:22.467984: step 2052, loss 0.0722984, acc 0.96
2016-09-07T04:04:23.141034: step 2053, loss 0.0138033, acc 1
2016-09-07T04:04:23.830171: step 2054, loss 0.0799436, acc 0.98
2016-09-07T04:04:24.501479: step 2055, loss 0.0219232, acc 1
2016-09-07T04:04:25.167237: step 2056, loss 0.0546463, acc 0.98
2016-09-07T04:04:25.866538: step 2057, loss 0.0456769, acc 0.98
2016-09-07T04:04:26.539170: step 2058, loss 0.0344409, acc 0.98
2016-09-07T04:04:27.225965: step 2059, loss 0.0192026, acc 1
2016-09-07T04:04:27.925037: step 2060, loss 0.0273336, acc 1
2016-09-07T04:04:28.618281: step 2061, loss 0.102472, acc 0.94
2016-09-07T04:04:29.300574: step 2062, loss 0.0846245, acc 0.98
2016-09-07T04:04:29.966381: step 2063, loss 0.0637456, acc 0.98
2016-09-07T04:04:30.697474: step 2064, loss 0.0957324, acc 0.96
2016-09-07T04:04:31.382907: step 2065, loss 0.0322833, acc 0.98
2016-09-07T04:04:32.082352: step 2066, loss 0.0559844, acc 0.98
2016-09-07T04:04:32.765108: step 2067, loss 0.0680992, acc 0.98
2016-09-07T04:04:33.459893: step 2068, loss 0.0503834, acc 1
2016-09-07T04:04:34.136219: step 2069, loss 0.0285237, acc 0.98
2016-09-07T04:04:34.805791: step 2070, loss 0.0248639, acc 0.98
2016-09-07T04:04:35.511663: step 2071, loss 0.0193148, acc 1
2016-09-07T04:04:36.187131: step 2072, loss 0.0411484, acc 1
2016-09-07T04:04:36.870232: step 2073, loss 0.0404185, acc 0.98
2016-09-07T04:04:37.586341: step 2074, loss 0.0499833, acc 0.98
2016-09-07T04:04:38.286826: step 2075, loss 0.0363568, acc 0.96
2016-09-07T04:04:39.004917: step 2076, loss 0.0503465, acc 0.98
2016-09-07T04:04:39.666095: step 2077, loss 0.106258, acc 0.98
2016-09-07T04:04:40.374448: step 2078, loss 0.0539507, acc 0.98
2016-09-07T04:04:41.040852: step 2079, loss 0.0199243, acc 1
2016-09-07T04:04:41.706597: step 2080, loss 0.0339988, acc 0.98
2016-09-07T04:04:42.387697: step 2081, loss 0.0408241, acc 0.96
2016-09-07T04:04:43.056622: step 2082, loss 0.0355582, acc 1
2016-09-07T04:04:43.731372: step 2083, loss 0.0464933, acc 0.98
2016-09-07T04:04:44.393851: step 2084, loss 0.0572717, acc 0.98
2016-09-07T04:04:45.099292: step 2085, loss 0.0200564, acc 1
2016-09-07T04:04:45.786597: step 2086, loss 0.0731879, acc 0.96
2016-09-07T04:04:46.467674: step 2087, loss 0.0222499, acc 0.98
2016-09-07T04:04:47.172238: step 2088, loss 0.0337754, acc 0.98
2016-09-07T04:04:47.850381: step 2089, loss 0.0388657, acc 0.98
2016-09-07T04:04:48.536644: step 2090, loss 0.170117, acc 0.9
2016-09-07T04:04:49.200318: step 2091, loss 0.0672631, acc 0.98
2016-09-07T04:04:49.901380: step 2092, loss 0.148366, acc 0.96
2016-09-07T04:04:50.596413: step 2093, loss 0.183958, acc 0.94
2016-09-07T04:04:51.290736: step 2094, loss 0.0342231, acc 0.98
2016-09-07T04:04:51.983676: step 2095, loss 0.0284912, acc 0.98
2016-09-07T04:04:52.680081: step 2096, loss 0.0347248, acc 1
2016-09-07T04:04:53.363116: step 2097, loss 0.0704578, acc 0.98
2016-09-07T04:04:54.019572: step 2098, loss 0.0928331, acc 0.96
2016-09-07T04:04:54.721974: step 2099, loss 0.0877817, acc 0.94
2016-09-07T04:04:55.421412: step 2100, loss 0.0449638, acc 0.96

Evaluation:
2016-09-07T04:04:58.567609: step 2100, loss 1.71641, acc 0.728893

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-2100

2016-09-07T04:05:00.267985: step 2101, loss 0.0102277, acc 1
2016-09-07T04:05:00.953830: step 2102, loss 0.0952363, acc 0.96
2016-09-07T04:05:01.616286: step 2103, loss 0.0126408, acc 1
2016-09-07T04:05:02.287272: step 2104, loss 0.0349318, acc 0.98
2016-09-07T04:05:02.953926: step 2105, loss 0.0779463, acc 0.98
2016-09-07T04:05:03.612542: step 2106, loss 0.0745299, acc 0.96
2016-09-07T04:05:04.307115: step 2107, loss 0.0334059, acc 0.98
2016-09-07T04:05:04.988743: step 2108, loss 0.0214458, acc 1
2016-09-07T04:05:05.668602: step 2109, loss 0.154408, acc 0.94
2016-09-07T04:05:06.360904: step 2110, loss 0.134809, acc 0.96
2016-09-07T04:05:07.064511: step 2111, loss 0.0119509, acc 1
2016-09-07T04:05:07.707149: step 2112, loss 0.02555, acc 0.977273
2016-09-07T04:05:08.398928: step 2113, loss 0.0930145, acc 0.92
2016-09-07T04:05:09.098383: step 2114, loss 0.0524674, acc 0.96
2016-09-07T04:05:09.783671: step 2115, loss 0.0956851, acc 0.96
2016-09-07T04:05:10.472168: step 2116, loss 0.0143552, acc 1
2016-09-07T04:05:11.186627: step 2117, loss 0.0534203, acc 0.96
2016-09-07T04:05:11.866200: step 2118, loss 0.0643618, acc 0.98
2016-09-07T04:05:12.556940: step 2119, loss 0.0319034, acc 1
2016-09-07T04:05:13.214540: step 2120, loss 0.0150862, acc 1
2016-09-07T04:05:13.921819: step 2121, loss 0.0534098, acc 0.96
2016-09-07T04:05:14.619119: step 2122, loss 0.0550572, acc 0.98
2016-09-07T04:05:15.310819: step 2123, loss 0.152062, acc 0.94
2016-09-07T04:05:16.007296: step 2124, loss 0.0898595, acc 0.96
2016-09-07T04:05:16.704477: step 2125, loss 0.0956495, acc 0.94
2016-09-07T04:05:17.423483: step 2126, loss 0.0592159, acc 0.98
2016-09-07T04:05:18.105283: step 2127, loss 0.059032, acc 0.98
2016-09-07T04:05:18.820777: step 2128, loss 0.0509992, acc 0.96
2016-09-07T04:05:19.516958: step 2129, loss 0.0324555, acc 1
2016-09-07T04:05:20.212924: step 2130, loss 0.0490469, acc 0.96
2016-09-07T04:05:20.917102: step 2131, loss 0.0445788, acc 0.98
2016-09-07T04:05:21.598729: step 2132, loss 0.0152093, acc 1
2016-09-07T04:05:22.300132: step 2133, loss 0.101529, acc 0.96
2016-09-07T04:05:22.973867: step 2134, loss 0.0330715, acc 1
2016-09-07T04:05:23.643420: step 2135, loss 0.0466184, acc 1
2016-09-07T04:05:24.331907: step 2136, loss 0.0281047, acc 0.98
2016-09-07T04:05:25.008375: step 2137, loss 0.0448218, acc 0.96
2016-09-07T04:05:25.709909: step 2138, loss 0.0384308, acc 0.98
2016-09-07T04:05:26.397584: step 2139, loss 0.0273782, acc 0.98
2016-09-07T04:05:27.098477: step 2140, loss 0.00345335, acc 1
2016-09-07T04:05:27.766772: step 2141, loss 0.0470355, acc 0.96
2016-09-07T04:05:28.472869: step 2142, loss 0.0425341, acc 0.96
2016-09-07T04:05:29.161073: step 2143, loss 0.0188843, acc 0.98
2016-09-07T04:05:29.853931: step 2144, loss 0.0746763, acc 0.98
2016-09-07T04:05:30.545369: step 2145, loss 0.135373, acc 0.94
2016-09-07T04:05:31.237773: step 2146, loss 0.0379987, acc 1
2016-09-07T04:05:31.965274: step 2147, loss 0.0434276, acc 0.98
2016-09-07T04:05:32.637988: step 2148, loss 0.0714063, acc 0.98
2016-09-07T04:05:33.347638: step 2149, loss 0.0126876, acc 1
2016-09-07T04:05:34.029441: step 2150, loss 0.0218696, acc 0.98
2016-09-07T04:05:34.719543: step 2151, loss 0.0530955, acc 0.96
2016-09-07T04:05:35.403323: step 2152, loss 0.00162767, acc 1
2016-09-07T04:05:36.059854: step 2153, loss 0.129631, acc 0.96
2016-09-07T04:05:36.761882: step 2154, loss 0.0742868, acc 0.94
2016-09-07T04:05:37.443958: step 2155, loss 0.0223629, acc 1
2016-09-07T04:05:38.130886: step 2156, loss 0.0335199, acc 0.98
2016-09-07T04:05:38.834939: step 2157, loss 0.0469603, acc 0.96
2016-09-07T04:05:39.516457: step 2158, loss 0.0341712, acc 1
2016-09-07T04:05:40.220500: step 2159, loss 0.0874148, acc 0.96
2016-09-07T04:05:40.898090: step 2160, loss 0.0304427, acc 0.98
2016-09-07T04:05:41.598061: step 2161, loss 0.0231638, acc 1
2016-09-07T04:05:42.274958: step 2162, loss 0.0410733, acc 1
2016-09-07T04:05:42.961581: step 2163, loss 0.0144232, acc 1
2016-09-07T04:05:43.651009: step 2164, loss 0.0499026, acc 0.96
2016-09-07T04:05:44.332366: step 2165, loss 0.0190572, acc 0.98
2016-09-07T04:05:45.035642: step 2166, loss 0.0113647, acc 1
2016-09-07T04:05:45.722443: step 2167, loss 0.0322599, acc 0.98
2016-09-07T04:05:46.413058: step 2168, loss 0.0472573, acc 1
2016-09-07T04:05:47.076469: step 2169, loss 0.0835865, acc 0.96
2016-09-07T04:05:47.764995: step 2170, loss 0.00207091, acc 1
2016-09-07T04:05:48.455451: step 2171, loss 0.0794701, acc 0.98
2016-09-07T04:05:49.127976: step 2172, loss 0.0189869, acc 1
2016-09-07T04:05:49.814354: step 2173, loss 0.0825985, acc 0.96
2016-09-07T04:05:50.480605: step 2174, loss 0.045391, acc 0.98
2016-09-07T04:05:51.205723: step 2175, loss 0.0383963, acc 0.98
2016-09-07T04:05:51.870753: step 2176, loss 0.102704, acc 0.94
2016-09-07T04:05:52.545955: step 2177, loss 0.0345599, acc 1
2016-09-07T04:05:53.227224: step 2178, loss 0.0771193, acc 0.96
2016-09-07T04:05:53.916673: step 2179, loss 0.00552021, acc 1
2016-09-07T04:05:54.590021: step 2180, loss 0.0613098, acc 0.98
2016-09-07T04:05:55.289229: step 2181, loss 0.0915318, acc 0.96
2016-09-07T04:05:55.992821: step 2182, loss 0.0966732, acc 0.98
2016-09-07T04:05:56.669159: step 2183, loss 0.0172852, acc 0.98
2016-09-07T04:05:57.358066: step 2184, loss 0.0578215, acc 0.98
2016-09-07T04:05:58.044652: step 2185, loss 0.0848322, acc 0.94
2016-09-07T04:05:58.735272: step 2186, loss 0.0225998, acc 0.98
2016-09-07T04:05:59.449002: step 2187, loss 0.0383878, acc 1
2016-09-07T04:06:00.117518: step 2188, loss 0.0562593, acc 0.98
2016-09-07T04:06:00.851753: step 2189, loss 0.0263581, acc 1
2016-09-07T04:06:01.528949: step 2190, loss 0.00335617, acc 1
2016-09-07T04:06:02.221226: step 2191, loss 0.0452115, acc 0.98
2016-09-07T04:06:02.935440: step 2192, loss 0.0413918, acc 0.98
2016-09-07T04:06:03.638339: step 2193, loss 0.0192724, acc 1
2016-09-07T04:06:04.342643: step 2194, loss 0.198445, acc 0.94
2016-09-07T04:06:05.007372: step 2195, loss 0.01582, acc 1
2016-09-07T04:06:05.705505: step 2196, loss 0.0287563, acc 0.98
2016-09-07T04:06:06.393766: step 2197, loss 0.095894, acc 0.94
2016-09-07T04:06:07.085658: step 2198, loss 0.032587, acc 0.98
2016-09-07T04:06:07.800262: step 2199, loss 0.0270248, acc 0.98
2016-09-07T04:06:08.492313: step 2200, loss 0.093604, acc 0.94

Evaluation:
2016-09-07T04:06:11.678018: step 2200, loss 1.52663, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-2200

2016-09-07T04:06:13.404813: step 2201, loss 0.220218, acc 0.92
2016-09-07T04:06:14.122969: step 2202, loss 0.0187031, acc 1
2016-09-07T04:06:14.823203: step 2203, loss 0.0731216, acc 0.96
2016-09-07T04:06:15.508410: step 2204, loss 0.0218515, acc 0.98
2016-09-07T04:06:16.190578: step 2205, loss 0.101819, acc 0.94
2016-09-07T04:06:16.880920: step 2206, loss 0.046179, acc 0.98
2016-09-07T04:06:17.564555: step 2207, loss 0.0219218, acc 1
2016-09-07T04:06:18.260890: step 2208, loss 0.0160611, acc 1
2016-09-07T04:06:18.971495: step 2209, loss 0.0355768, acc 0.98
2016-09-07T04:06:19.652787: step 2210, loss 0.0361022, acc 1
2016-09-07T04:06:20.344935: step 2211, loss 0.0345509, acc 1
2016-09-07T04:06:21.027215: step 2212, loss 0.0830228, acc 0.96
2016-09-07T04:06:21.691263: step 2213, loss 0.0157869, acc 1
2016-09-07T04:06:22.358051: step 2214, loss 0.102527, acc 0.98
2016-09-07T04:06:23.027563: step 2215, loss 0.034925, acc 0.98
2016-09-07T04:06:23.733645: step 2216, loss 0.0328359, acc 0.98
2016-09-07T04:06:24.413777: step 2217, loss 0.0413298, acc 0.98
2016-09-07T04:06:25.113779: step 2218, loss 0.0514983, acc 0.96
2016-09-07T04:06:25.800745: step 2219, loss 0.0674936, acc 0.94
2016-09-07T04:06:26.485953: step 2220, loss 0.0502552, acc 0.98
2016-09-07T04:06:27.167326: step 2221, loss 0.190036, acc 0.94
2016-09-07T04:06:27.834026: step 2222, loss 0.0150027, acc 1
2016-09-07T04:06:28.534882: step 2223, loss 0.231737, acc 0.96
2016-09-07T04:06:29.200380: step 2224, loss 0.0415735, acc 0.98
2016-09-07T04:06:29.878287: step 2225, loss 0.0502666, acc 0.98
2016-09-07T04:06:30.554502: step 2226, loss 0.0255295, acc 1
2016-09-07T04:06:31.236421: step 2227, loss 0.0341166, acc 0.98
2016-09-07T04:06:31.960960: step 2228, loss 0.109481, acc 0.94
2016-09-07T04:06:32.662473: step 2229, loss 0.0135672, acc 1
2016-09-07T04:06:33.375280: step 2230, loss 0.0428627, acc 0.98
2016-09-07T04:06:34.058547: step 2231, loss 0.0418849, acc 0.98
2016-09-07T04:06:34.749934: step 2232, loss 0.0409609, acc 1
2016-09-07T04:06:35.427743: step 2233, loss 0.0402257, acc 0.98
2016-09-07T04:06:36.127918: step 2234, loss 0.074947, acc 0.98
2016-09-07T04:06:36.814623: step 2235, loss 0.0236051, acc 1
2016-09-07T04:06:37.494168: step 2236, loss 0.00706305, acc 1
2016-09-07T04:06:38.214862: step 2237, loss 0.0777969, acc 0.98
2016-09-07T04:06:38.907338: step 2238, loss 0.0335513, acc 0.98
2016-09-07T04:06:39.581538: step 2239, loss 0.0538389, acc 0.98
2016-09-07T04:06:40.271103: step 2240, loss 0.11903, acc 0.96
2016-09-07T04:06:40.952165: step 2241, loss 0.290102, acc 0.92
2016-09-07T04:06:41.652565: step 2242, loss 0.0129855, acc 1
2016-09-07T04:06:42.322646: step 2243, loss 0.0273342, acc 1
2016-09-07T04:06:43.037840: step 2244, loss 0.0639649, acc 0.96
2016-09-07T04:06:43.726192: step 2245, loss 0.0324286, acc 0.98
2016-09-07T04:06:44.408952: step 2246, loss 0.111386, acc 0.96
2016-09-07T04:06:45.092430: step 2247, loss 0.0488128, acc 0.98
2016-09-07T04:06:45.774739: step 2248, loss 0.0321059, acc 0.98
2016-09-07T04:06:46.490503: step 2249, loss 0.0706405, acc 0.98
2016-09-07T04:06:47.166636: step 2250, loss 0.00879265, acc 1
2016-09-07T04:06:47.855562: step 2251, loss 0.0157838, acc 1
2016-09-07T04:06:48.542190: step 2252, loss 0.0103594, acc 1
2016-09-07T04:06:49.221679: step 2253, loss 0.103461, acc 0.96
2016-09-07T04:06:49.905299: step 2254, loss 0.0239544, acc 1
2016-09-07T04:06:50.593060: step 2255, loss 0.0239109, acc 1
2016-09-07T04:06:51.288686: step 2256, loss 0.0177187, acc 0.98
2016-09-07T04:06:51.968986: step 2257, loss 0.0503713, acc 0.98
2016-09-07T04:06:52.670604: step 2258, loss 0.017976, acc 1
2016-09-07T04:06:53.351470: step 2259, loss 0.0367528, acc 1
2016-09-07T04:06:54.054077: step 2260, loss 0.0871744, acc 0.94
2016-09-07T04:06:54.751498: step 2261, loss 0.0299485, acc 0.98
2016-09-07T04:06:55.433148: step 2262, loss 0.0273857, acc 0.98
2016-09-07T04:06:56.151726: step 2263, loss 0.0264573, acc 0.98
2016-09-07T04:06:56.819310: step 2264, loss 0.125321, acc 0.94
2016-09-07T04:06:57.518606: step 2265, loss 0.0550577, acc 0.98
2016-09-07T04:06:58.206552: step 2266, loss 0.0760517, acc 0.98
2016-09-07T04:06:58.897510: step 2267, loss 0.0373764, acc 0.98
2016-09-07T04:06:59.592825: step 2268, loss 0.0604948, acc 0.98
2016-09-07T04:07:00.281163: step 2269, loss 0.0286231, acc 0.98
2016-09-07T04:07:00.987684: step 2270, loss 0.0280687, acc 1
2016-09-07T04:07:01.657558: step 2271, loss 0.0721283, acc 0.96
2016-09-07T04:07:02.339373: step 2272, loss 0.0590831, acc 0.96
2016-09-07T04:07:03.028221: step 2273, loss 0.0386961, acc 0.98
2016-09-07T04:07:03.724119: step 2274, loss 0.0855501, acc 0.98
2016-09-07T04:07:04.409138: step 2275, loss 0.088372, acc 0.98
2016-09-07T04:07:05.078693: step 2276, loss 0.0161753, acc 1
2016-09-07T04:07:05.780102: step 2277, loss 0.00342394, acc 1
2016-09-07T04:07:06.464781: step 2278, loss 0.139716, acc 0.96
2016-09-07T04:07:07.150128: step 2279, loss 0.044604, acc 0.98
2016-09-07T04:07:07.857251: step 2280, loss 0.00128822, acc 1
2016-09-07T04:07:08.557157: step 2281, loss 0.0297885, acc 1
2016-09-07T04:07:09.261391: step 2282, loss 0.0617542, acc 0.98
2016-09-07T04:07:09.926749: step 2283, loss 0.0132371, acc 1
2016-09-07T04:07:10.634681: step 2284, loss 0.102137, acc 0.96
2016-09-07T04:07:11.343111: step 2285, loss 0.0596537, acc 0.96
2016-09-07T04:07:12.048537: step 2286, loss 0.104337, acc 0.92
2016-09-07T04:07:12.765494: step 2287, loss 0.0432478, acc 0.98
2016-09-07T04:07:13.460301: step 2288, loss 0.0414723, acc 0.98
2016-09-07T04:07:14.160612: step 2289, loss 0.0183863, acc 1
2016-09-07T04:07:14.858778: step 2290, loss 0.0349878, acc 0.98
2016-09-07T04:07:15.581834: step 2291, loss 0.0168872, acc 1
2016-09-07T04:07:16.276617: step 2292, loss 0.0140444, acc 1
2016-09-07T04:07:17.004453: step 2293, loss 0.0298876, acc 1
2016-09-07T04:07:17.739249: step 2294, loss 0.02767, acc 0.98
2016-09-07T04:07:18.430405: step 2295, loss 0.0145723, acc 1
2016-09-07T04:07:19.109422: step 2296, loss 0.0356849, acc 1
2016-09-07T04:07:19.810692: step 2297, loss 0.043454, acc 0.98
2016-09-07T04:07:20.494075: step 2298, loss 0.128362, acc 0.98
2016-09-07T04:07:21.196072: step 2299, loss 0.0458697, acc 0.98
2016-09-07T04:07:21.872220: step 2300, loss 0.0133464, acc 1

Evaluation:
2016-09-07T04:07:25.078790: step 2300, loss 1.52725, acc 0.729831

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-2300

2016-09-07T04:07:26.721233: step 2301, loss 0.0930209, acc 0.94
2016-09-07T04:07:27.433348: step 2302, loss 0.0983156, acc 0.92
2016-09-07T04:07:28.115241: step 2303, loss 0.0458753, acc 0.98
2016-09-07T04:07:28.750970: step 2304, loss 0.0588621, acc 0.954545
2016-09-07T04:07:29.441856: step 2305, loss 0.00856956, acc 1
2016-09-07T04:07:30.132109: step 2306, loss 0.0205065, acc 1
2016-09-07T04:07:30.822905: step 2307, loss 0.17023, acc 0.94
2016-09-07T04:07:31.522988: step 2308, loss 0.0464348, acc 0.98
2016-09-07T04:07:32.220628: step 2309, loss 0.109637, acc 0.98
2016-09-07T04:07:32.891705: step 2310, loss 0.0321298, acc 0.98
2016-09-07T04:07:33.579160: step 2311, loss 0.0478739, acc 0.98
2016-09-07T04:07:34.262729: step 2312, loss 0.0446235, acc 0.98
2016-09-07T04:07:34.935362: step 2313, loss 0.0617534, acc 0.98
2016-09-07T04:07:35.612799: step 2314, loss 0.0382702, acc 0.96
2016-09-07T04:07:36.272568: step 2315, loss 0.100622, acc 0.96
2016-09-07T04:07:36.970683: step 2316, loss 0.0206108, acc 1
2016-09-07T04:07:37.647301: step 2317, loss 0.023576, acc 1
2016-09-07T04:07:38.364841: step 2318, loss 0.0548689, acc 0.98
2016-09-07T04:07:39.063634: step 2319, loss 0.0089041, acc 1
2016-09-07T04:07:39.754231: step 2320, loss 0.00658467, acc 1
2016-09-07T04:07:40.449967: step 2321, loss 0.0185352, acc 1
2016-09-07T04:07:41.139121: step 2322, loss 0.00260756, acc 1
2016-09-07T04:07:41.847503: step 2323, loss 0.0529459, acc 0.98
2016-09-07T04:07:42.531010: step 2324, loss 0.0968768, acc 0.96
2016-09-07T04:07:43.229725: step 2325, loss 0.0619893, acc 0.96
2016-09-07T04:07:43.907695: step 2326, loss 0.0446648, acc 0.98
2016-09-07T04:07:44.594651: step 2327, loss 0.0522284, acc 0.98
2016-09-07T04:07:45.279192: step 2328, loss 0.00922072, acc 1
2016-09-07T04:07:45.949357: step 2329, loss 0.0226791, acc 0.98
2016-09-07T04:07:46.655062: step 2330, loss 0.0231885, acc 0.98
2016-09-07T04:07:47.328372: step 2331, loss 0.0578236, acc 0.98
2016-09-07T04:07:48.011573: step 2332, loss 0.0438698, acc 0.98
2016-09-07T04:07:48.689040: step 2333, loss 0.0566903, acc 0.94
2016-09-07T04:07:49.387677: step 2334, loss 0.0268096, acc 0.98
2016-09-07T04:07:50.082593: step 2335, loss 0.030213, acc 1
2016-09-07T04:07:50.752598: step 2336, loss 0.067653, acc 0.96
2016-09-07T04:07:51.466978: step 2337, loss 0.0417355, acc 0.98
2016-09-07T04:07:52.151602: step 2338, loss 0.0827195, acc 0.94
2016-09-07T04:07:52.829681: step 2339, loss 0.0167338, acc 0.98
2016-09-07T04:07:53.506391: step 2340, loss 0.0396067, acc 0.98
2016-09-07T04:07:54.191116: step 2341, loss 0.0128009, acc 1
2016-09-07T04:07:54.884702: step 2342, loss 0.00692225, acc 1
2016-09-07T04:07:55.563846: step 2343, loss 0.00140067, acc 1
2016-09-07T04:07:56.289062: step 2344, loss 0.174211, acc 0.98
2016-09-07T04:07:56.974137: step 2345, loss 0.0655717, acc 0.98
2016-09-07T04:07:57.669266: step 2346, loss 0.0990104, acc 0.94
2016-09-07T04:07:58.372633: step 2347, loss 0.0185599, acc 1
2016-09-07T04:07:59.097407: step 2348, loss 0.0240175, acc 1
2016-09-07T04:07:59.811919: step 2349, loss 0.0788182, acc 0.98
2016-09-07T04:08:00.523305: step 2350, loss 0.0889005, acc 0.98
2016-09-07T04:08:01.213536: step 2351, loss 0.0314455, acc 0.98
2016-09-07T04:08:01.928592: step 2352, loss 0.0137729, acc 1
2016-09-07T04:08:02.651856: step 2353, loss 0.00683865, acc 1
2016-09-07T04:08:03.362877: step 2354, loss 0.0171039, acc 1
2016-09-07T04:08:04.048920: step 2355, loss 0.036557, acc 0.98
2016-09-07T04:08:04.747762: step 2356, loss 0.0215865, acc 1
2016-09-07T04:08:05.445966: step 2357, loss 0.0614924, acc 0.98
2016-09-07T04:08:06.133815: step 2358, loss 0.0209222, acc 1
2016-09-07T04:08:06.821974: step 2359, loss 0.00615314, acc 1
2016-09-07T04:08:07.505040: step 2360, loss 0.0544342, acc 0.98
2016-09-07T04:08:08.179463: step 2361, loss 0.0585108, acc 0.96
2016-09-07T04:08:08.839071: step 2362, loss 0.0077789, acc 1
2016-09-07T04:08:09.545693: step 2363, loss 0.00568121, acc 1
2016-09-07T04:08:10.250779: step 2364, loss 0.0396381, acc 0.98
2016-09-07T04:08:10.939844: step 2365, loss 0.161743, acc 0.9
2016-09-07T04:08:11.622348: step 2366, loss 0.0426427, acc 0.96
2016-09-07T04:08:12.306055: step 2367, loss 0.0582657, acc 0.98
2016-09-07T04:08:13.004268: step 2368, loss 0.0851044, acc 0.94
2016-09-07T04:08:13.657279: step 2369, loss 0.085989, acc 0.96
2016-09-07T04:08:14.349158: step 2370, loss 0.00837698, acc 1
2016-09-07T04:08:15.031251: step 2371, loss 0.0261605, acc 0.98
2016-09-07T04:08:15.728597: step 2372, loss 0.0246132, acc 1
2016-09-07T04:08:16.427111: step 2373, loss 0.0105737, acc 1
2016-09-07T04:08:17.116618: step 2374, loss 0.0573046, acc 0.96
2016-09-07T04:08:17.797878: step 2375, loss 0.00212999, acc 1
2016-09-07T04:08:18.474512: step 2376, loss 0.0218057, acc 1
2016-09-07T04:08:19.182533: step 2377, loss 0.0731664, acc 0.98
2016-09-07T04:08:19.882359: step 2378, loss 0.0310368, acc 0.98
2016-09-07T04:08:20.557300: step 2379, loss 0.0257905, acc 0.98
2016-09-07T04:08:21.250106: step 2380, loss 0.041199, acc 0.96
2016-09-07T04:08:21.948882: step 2381, loss 0.0364158, acc 0.98
2016-09-07T04:08:22.654249: step 2382, loss 0.021684, acc 0.98
2016-09-07T04:08:23.329358: step 2383, loss 0.00894009, acc 1
2016-09-07T04:08:24.040180: step 2384, loss 0.00475501, acc 1
2016-09-07T04:08:24.738253: step 2385, loss 0.0992351, acc 0.98
2016-09-07T04:08:25.420836: step 2386, loss 0.0649696, acc 0.98
2016-09-07T04:08:26.114063: step 2387, loss 0.124662, acc 0.92
2016-09-07T04:08:26.808528: step 2388, loss 0.0300844, acc 0.98
2016-09-07T04:08:27.527399: step 2389, loss 0.0154402, acc 1
2016-09-07T04:08:28.200932: step 2390, loss 0.0729311, acc 0.98
2016-09-07T04:08:28.885637: step 2391, loss 0.160902, acc 0.96
2016-09-07T04:08:29.588709: step 2392, loss 0.0694458, acc 0.98
2016-09-07T04:08:30.271444: step 2393, loss 0.0205514, acc 1
2016-09-07T04:08:30.944027: step 2394, loss 0.0460998, acc 0.98
2016-09-07T04:08:31.631249: step 2395, loss 0.0375111, acc 0.96
2016-09-07T04:08:32.341581: step 2396, loss 0.0706462, acc 0.96
2016-09-07T04:08:33.010780: step 2397, loss 0.00417071, acc 1
2016-09-07T04:08:33.700748: step 2398, loss 0.0635152, acc 0.94
2016-09-07T04:08:34.414050: step 2399, loss 0.020545, acc 1
2016-09-07T04:08:35.097686: step 2400, loss 0.0576341, acc 0.96

Evaluation:
2016-09-07T04:08:38.245825: step 2400, loss 1.64093, acc 0.719512

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-2400

2016-09-07T04:08:39.883395: step 2401, loss 0.0788399, acc 0.98
2016-09-07T04:08:40.605096: step 2402, loss 0.0410313, acc 1
2016-09-07T04:08:41.277990: step 2403, loss 0.0036674, acc 1
2016-09-07T04:08:42.000554: step 2404, loss 0.0460079, acc 0.98
2016-09-07T04:08:42.703336: step 2405, loss 0.00661724, acc 1
2016-09-07T04:08:43.413581: step 2406, loss 0.0467866, acc 0.98
2016-09-07T04:08:44.097673: step 2407, loss 0.0524324, acc 0.96
2016-09-07T04:08:44.777341: step 2408, loss 0.0170491, acc 1
2016-09-07T04:08:45.485866: step 2409, loss 0.00489816, acc 1
2016-09-07T04:08:46.157057: step 2410, loss 0.0482578, acc 0.96
2016-09-07T04:08:46.873739: step 2411, loss 0.0554524, acc 0.98
2016-09-07T04:08:47.552823: step 2412, loss 0.0168864, acc 1
2016-09-07T04:08:48.242463: step 2413, loss 0.0325208, acc 1
2016-09-07T04:08:48.934972: step 2414, loss 0.0130632, acc 1
2016-09-07T04:08:49.622397: step 2415, loss 0.0362447, acc 1
2016-09-07T04:08:50.321269: step 2416, loss 0.0245719, acc 0.98
2016-09-07T04:08:51.011836: step 2417, loss 0.0434846, acc 0.98
2016-09-07T04:08:51.721286: step 2418, loss 0.0171795, acc 1
2016-09-07T04:08:52.397251: step 2419, loss 0.0393206, acc 0.98
2016-09-07T04:08:53.085098: step 2420, loss 0.0480879, acc 0.98
2016-09-07T04:08:53.784284: step 2421, loss 0.0344553, acc 0.98
2016-09-07T04:08:54.488815: step 2422, loss 0.0176023, acc 0.98
2016-09-07T04:08:55.201384: step 2423, loss 0.0301148, acc 1
2016-09-07T04:08:55.883562: step 2424, loss 0.080061, acc 0.92
2016-09-07T04:08:56.557702: step 2425, loss 0.0443425, acc 0.98
2016-09-07T04:08:57.256140: step 2426, loss 0.0304262, acc 0.98
2016-09-07T04:08:57.951993: step 2427, loss 0.0117321, acc 1
2016-09-07T04:08:58.643972: step 2428, loss 0.0193543, acc 1
2016-09-07T04:08:59.303681: step 2429, loss 0.168291, acc 0.96
2016-09-07T04:09:00.025086: step 2430, loss 0.0061012, acc 1
2016-09-07T04:09:00.717210: step 2431, loss 0.202959, acc 0.96
2016-09-07T04:09:01.378696: step 2432, loss 0.114763, acc 0.96
2016-09-07T04:09:02.074149: step 2433, loss 0.024063, acc 0.98
2016-09-07T04:09:02.752430: step 2434, loss 0.0460963, acc 0.98
2016-09-07T04:09:03.453382: step 2435, loss 0.175167, acc 0.96
2016-09-07T04:09:04.135416: step 2436, loss 0.0553312, acc 0.98
2016-09-07T04:09:04.836831: step 2437, loss 0.0863196, acc 0.98
2016-09-07T04:09:05.529009: step 2438, loss 0.0272033, acc 0.98
2016-09-07T04:09:06.201911: step 2439, loss 0.0364154, acc 0.98
2016-09-07T04:09:06.896443: step 2440, loss 0.0194086, acc 1
2016-09-07T04:09:07.592667: step 2441, loss 0.0281614, acc 0.98
2016-09-07T04:09:08.281869: step 2442, loss 0.0331535, acc 0.98
2016-09-07T04:09:08.953085: step 2443, loss 0.000974022, acc 1
2016-09-07T04:09:09.652175: step 2444, loss 0.0659281, acc 0.96
2016-09-07T04:09:10.355400: step 2445, loss 0.0763205, acc 0.94
2016-09-07T04:09:11.052305: step 2446, loss 0.0675516, acc 0.96
2016-09-07T04:09:11.734771: step 2447, loss 0.0761001, acc 0.96
2016-09-07T04:09:12.415778: step 2448, loss 0.00755639, acc 1
2016-09-07T04:09:13.117478: step 2449, loss 0.085811, acc 0.98
2016-09-07T04:09:13.773409: step 2450, loss 0.1527, acc 0.92
2016-09-07T04:09:14.475143: step 2451, loss 0.112589, acc 0.98
2016-09-07T04:09:15.173950: step 2452, loss 0.0460647, acc 0.98
2016-09-07T04:09:15.859081: step 2453, loss 0.0370261, acc 0.98
2016-09-07T04:09:16.528001: step 2454, loss 0.00938502, acc 1
2016-09-07T04:09:17.208691: step 2455, loss 0.10289, acc 0.98
2016-09-07T04:09:17.903816: step 2456, loss 0.0641571, acc 0.96
2016-09-07T04:09:18.580550: step 2457, loss 0.033141, acc 0.98
2016-09-07T04:09:19.277424: step 2458, loss 0.111555, acc 0.96
2016-09-07T04:09:19.963372: step 2459, loss 0.14006, acc 0.96
2016-09-07T04:09:20.646823: step 2460, loss 0.0203273, acc 0.98
2016-09-07T04:09:21.344654: step 2461, loss 0.00524669, acc 1
2016-09-07T04:09:22.040922: step 2462, loss 0.0315785, acc 1
2016-09-07T04:09:22.733052: step 2463, loss 0.0119199, acc 1
2016-09-07T04:09:23.388931: step 2464, loss 0.0185241, acc 0.98
2016-09-07T04:09:24.091664: step 2465, loss 0.0636602, acc 0.98
2016-09-07T04:09:24.767943: step 2466, loss 0.0310074, acc 0.98
2016-09-07T04:09:25.442775: step 2467, loss 0.079161, acc 0.96
2016-09-07T04:09:26.137299: step 2468, loss 0.00419453, acc 1
2016-09-07T04:09:26.841641: step 2469, loss 0.0136446, acc 1
2016-09-07T04:09:27.533373: step 2470, loss 0.144614, acc 0.92
2016-09-07T04:09:28.217604: step 2471, loss 0.0324071, acc 0.98
2016-09-07T04:09:28.923658: step 2472, loss 0.0111498, acc 1
2016-09-07T04:09:29.597885: step 2473, loss 0.0262083, acc 1
2016-09-07T04:09:30.269682: step 2474, loss 0.121288, acc 0.92
2016-09-07T04:09:30.968900: step 2475, loss 0.0860451, acc 0.96
2016-09-07T04:09:31.650208: step 2476, loss 0.105209, acc 0.94
2016-09-07T04:09:32.362308: step 2477, loss 0.188201, acc 0.96
2016-09-07T04:09:33.041543: step 2478, loss 0.0444556, acc 1
2016-09-07T04:09:33.741203: step 2479, loss 0.0354061, acc 1
2016-09-07T04:09:34.455606: step 2480, loss 0.0380417, acc 0.98
2016-09-07T04:09:35.137376: step 2481, loss 0.111627, acc 0.94
2016-09-07T04:09:35.841812: step 2482, loss 0.0875905, acc 0.98
2016-09-07T04:09:36.547786: step 2483, loss 0.0510187, acc 0.96
2016-09-07T04:09:37.240559: step 2484, loss 0.0136717, acc 1
2016-09-07T04:09:37.926095: step 2485, loss 0.015534, acc 1
2016-09-07T04:09:38.640963: step 2486, loss 0.0516847, acc 0.98
2016-09-07T04:09:39.327771: step 2487, loss 0.0946425, acc 0.96
2016-09-07T04:09:40.002491: step 2488, loss 0.0690084, acc 0.98
2016-09-07T04:09:40.708907: step 2489, loss 0.0555189, acc 0.96
2016-09-07T04:09:41.403770: step 2490, loss 0.0846096, acc 0.94
2016-09-07T04:09:42.127008: step 2491, loss 0.0354505, acc 1
2016-09-07T04:09:42.798423: step 2492, loss 0.00813152, acc 1
2016-09-07T04:09:43.495646: step 2493, loss 0.0856271, acc 0.96
2016-09-07T04:09:44.169285: step 2494, loss 0.00932629, acc 1
2016-09-07T04:09:44.851995: step 2495, loss 0.0339525, acc 0.98
2016-09-07T04:09:45.481220: step 2496, loss 0.00618175, acc 1
2016-09-07T04:09:46.154302: step 2497, loss 0.0618326, acc 0.98
2016-09-07T04:09:46.826717: step 2498, loss 0.1092, acc 0.98
2016-09-07T04:09:47.496706: step 2499, loss 0.0318299, acc 0.98
2016-09-07T04:09:48.202310: step 2500, loss 0.0250533, acc 1

Evaluation:
2016-09-07T04:09:51.363686: step 2500, loss 1.59109, acc 0.725141

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-2500

2016-09-07T04:09:53.019244: step 2501, loss 0.0978502, acc 0.96
2016-09-07T04:09:53.687885: step 2502, loss 0.0288283, acc 1
2016-09-07T04:09:54.365196: step 2503, loss 0.0196243, acc 0.98
2016-09-07T04:09:55.038989: step 2504, loss 0.0664125, acc 0.98
2016-09-07T04:09:55.746195: step 2505, loss 0.00333561, acc 1
2016-09-07T04:09:56.414576: step 2506, loss 0.155839, acc 0.96
2016-09-07T04:09:57.073656: step 2507, loss 0.0336206, acc 0.96
2016-09-07T04:09:57.788729: step 2508, loss 0.00480199, acc 1
2016-09-07T04:09:58.477584: step 2509, loss 0.0580148, acc 0.96
2016-09-07T04:09:59.154550: step 2510, loss 0.0788145, acc 0.98
2016-09-07T04:09:59.839288: step 2511, loss 0.00760795, acc 1
2016-09-07T04:10:00.544901: step 2512, loss 0.0202903, acc 1
2016-09-07T04:10:01.241676: step 2513, loss 0.0536362, acc 0.96
2016-09-07T04:10:01.921434: step 2514, loss 0.0286444, acc 0.98
2016-09-07T04:10:02.613492: step 2515, loss 0.0496661, acc 0.98
2016-09-07T04:10:03.295151: step 2516, loss 0.0429003, acc 1
2016-09-07T04:10:03.983819: step 2517, loss 0.0397843, acc 0.98
2016-09-07T04:10:04.655925: step 2518, loss 0.0296174, acc 1
2016-09-07T04:10:05.337970: step 2519, loss 0.176892, acc 0.92
2016-09-07T04:10:06.025390: step 2520, loss 0.0147037, acc 1
2016-09-07T04:10:06.691253: step 2521, loss 0.0586503, acc 0.96
2016-09-07T04:10:07.403633: step 2522, loss 0.0382188, acc 0.98
2016-09-07T04:10:08.081971: step 2523, loss 0.0240664, acc 0.98
2016-09-07T04:10:08.776044: step 2524, loss 0.0328656, acc 0.98
2016-09-07T04:10:09.472395: step 2525, loss 0.01562, acc 1
2016-09-07T04:10:10.151822: step 2526, loss 0.0498528, acc 1
2016-09-07T04:10:10.833248: step 2527, loss 0.0107832, acc 1
2016-09-07T04:10:11.497233: step 2528, loss 0.0829525, acc 0.96
2016-09-07T04:10:12.201617: step 2529, loss 0.0305705, acc 1
2016-09-07T04:10:12.877530: step 2530, loss 0.0243164, acc 1
2016-09-07T04:10:13.548809: step 2531, loss 0.00271273, acc 1
2016-09-07T04:10:14.245329: step 2532, loss 0.131669, acc 0.96
2016-09-07T04:10:14.941258: step 2533, loss 0.0812317, acc 0.94
2016-09-07T04:10:15.636179: step 2534, loss 0.0347404, acc 0.98
2016-09-07T04:10:16.305687: step 2535, loss 0.00720206, acc 1
2016-09-07T04:10:17.014776: step 2536, loss 0.013624, acc 1
2016-09-07T04:10:17.694454: step 2537, loss 0.0194555, acc 0.98
2016-09-07T04:10:18.372600: step 2538, loss 0.0268655, acc 0.98
2016-09-07T04:10:19.051443: step 2539, loss 0.0511153, acc 0.98
2016-09-07T04:10:19.736809: step 2540, loss 0.0395612, acc 0.96
2016-09-07T04:10:20.450660: step 2541, loss 0.0100191, acc 1
2016-09-07T04:10:21.105855: step 2542, loss 0.0270595, acc 0.98
2016-09-07T04:10:21.824243: step 2543, loss 0.0138895, acc 1
2016-09-07T04:10:22.499977: step 2544, loss 0.0353044, acc 0.98
2016-09-07T04:10:23.206507: step 2545, loss 0.130189, acc 0.94
2016-09-07T04:10:23.893028: step 2546, loss 0.0430242, acc 0.98
2016-09-07T04:10:24.571042: step 2547, loss 0.036243, acc 0.98
2016-09-07T04:10:25.274270: step 2548, loss 0.0297377, acc 0.98
2016-09-07T04:10:25.965929: step 2549, loss 0.00639525, acc 1
2016-09-07T04:10:26.669916: step 2550, loss 0.0282827, acc 0.98
2016-09-07T04:10:27.361289: step 2551, loss 0.0201896, acc 0.98
2016-09-07T04:10:28.039700: step 2552, loss 0.0292365, acc 1
2016-09-07T04:10:28.736703: step 2553, loss 0.0375711, acc 0.96
2016-09-07T04:10:29.417307: step 2554, loss 0.0266168, acc 0.98
2016-09-07T04:10:30.110699: step 2555, loss 0.203429, acc 0.9
2016-09-07T04:10:30.778456: step 2556, loss 0.0321908, acc 0.98
2016-09-07T04:10:31.468821: step 2557, loss 0.045267, acc 0.98
2016-09-07T04:10:32.146527: step 2558, loss 0.0683117, acc 0.98
2016-09-07T04:10:32.817384: step 2559, loss 0.092438, acc 0.96
2016-09-07T04:10:33.508254: step 2560, loss 0.0196787, acc 0.98
2016-09-07T04:10:34.205766: step 2561, loss 0.146741, acc 0.94
2016-09-07T04:10:34.895414: step 2562, loss 0.113295, acc 0.96
2016-09-07T04:10:35.566304: step 2563, loss 0.0450466, acc 0.96
2016-09-07T04:10:36.267941: step 2564, loss 0.036306, acc 0.98
2016-09-07T04:10:36.969394: step 2565, loss 0.0144997, acc 1
2016-09-07T04:10:37.656432: step 2566, loss 0.0411204, acc 1
2016-09-07T04:10:38.369437: step 2567, loss 0.0382376, acc 0.98
2016-09-07T04:10:39.060158: step 2568, loss 0.0615953, acc 0.94
2016-09-07T04:10:39.762713: step 2569, loss 0.107627, acc 0.98
2016-09-07T04:10:40.444989: step 2570, loss 0.0768963, acc 0.96
2016-09-07T04:10:41.145671: step 2571, loss 0.0347379, acc 0.98
2016-09-07T04:10:41.846486: step 2572, loss 0.0600712, acc 0.96
2016-09-07T04:10:42.551031: step 2573, loss 0.013673, acc 1
2016-09-07T04:10:43.238569: step 2574, loss 0.0180466, acc 1
2016-09-07T04:10:43.910773: step 2575, loss 0.0589079, acc 0.98
2016-09-07T04:10:44.610617: step 2576, loss 0.0388204, acc 0.98
2016-09-07T04:10:45.307534: step 2577, loss 0.00944768, acc 1
2016-09-07T04:10:46.003049: step 2578, loss 0.0193104, acc 1
2016-09-07T04:10:46.684440: step 2579, loss 0.0443404, acc 0.98
2016-09-07T04:10:47.365635: step 2580, loss 0.0824371, acc 0.94
2016-09-07T04:10:48.039443: step 2581, loss 0.0111514, acc 1
2016-09-07T04:10:48.744384: step 2582, loss 0.0621934, acc 0.96
2016-09-07T04:10:49.471447: step 2583, loss 0.0605278, acc 0.96
2016-09-07T04:10:50.138390: step 2584, loss 0.0284115, acc 0.98
2016-09-07T04:10:50.830398: step 2585, loss 0.0123764, acc 1
2016-09-07T04:10:51.524605: step 2586, loss 0.106375, acc 0.94
2016-09-07T04:10:52.217478: step 2587, loss 0.032864, acc 0.98
2016-09-07T04:10:52.901082: step 2588, loss 0.0431428, acc 0.96
2016-09-07T04:10:53.564135: step 2589, loss 0.00488697, acc 1
2016-09-07T04:10:54.250093: step 2590, loss 0.0209731, acc 1
2016-09-07T04:10:54.915915: step 2591, loss 0.0273549, acc 1
2016-09-07T04:10:55.591963: step 2592, loss 0.064418, acc 0.98
2016-09-07T04:10:56.272881: step 2593, loss 0.0133328, acc 1
2016-09-07T04:10:56.959390: step 2594, loss 0.0744355, acc 0.96
2016-09-07T04:10:57.640455: step 2595, loss 0.0245219, acc 1
2016-09-07T04:10:58.351467: step 2596, loss 0.0317719, acc 0.98
2016-09-07T04:10:59.039970: step 2597, loss 0.176051, acc 0.94
2016-09-07T04:10:59.701988: step 2598, loss 0.0690864, acc 0.94
2016-09-07T04:11:00.430196: step 2599, loss 0.0191062, acc 0.98
2016-09-07T04:11:01.121753: step 2600, loss 0.0380284, acc 1

Evaluation:
2016-09-07T04:11:04.301154: step 2600, loss 1.70459, acc 0.743902

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-2600

2016-09-07T04:11:06.036040: step 2601, loss 0.0338609, acc 1
2016-09-07T04:11:06.722304: step 2602, loss 0.00179159, acc 1
2016-09-07T04:11:07.421654: step 2603, loss 0.00273379, acc 1
2016-09-07T04:11:08.107417: step 2604, loss 0.0177438, acc 1
2016-09-07T04:11:08.818432: step 2605, loss 0.0729676, acc 0.96
2016-09-07T04:11:09.509256: step 2606, loss 0.0225152, acc 1
2016-09-07T04:11:10.230988: step 2607, loss 0.0230682, acc 1
2016-09-07T04:11:10.912575: step 2608, loss 0.068178, acc 0.98
2016-09-07T04:11:11.597674: step 2609, loss 0.048526, acc 0.98
2016-09-07T04:11:12.301412: step 2610, loss 0.025979, acc 1
2016-09-07T04:11:12.989980: step 2611, loss 0.0190644, acc 0.98
2016-09-07T04:11:13.670896: step 2612, loss 0.0327851, acc 1
2016-09-07T04:11:14.341419: step 2613, loss 0.0366246, acc 0.98
2016-09-07T04:11:15.030940: step 2614, loss 0.0353353, acc 0.98
2016-09-07T04:11:15.736277: step 2615, loss 0.00843534, acc 1
2016-09-07T04:11:16.421673: step 2616, loss 0.198907, acc 0.92
2016-09-07T04:11:17.132865: step 2617, loss 0.0945614, acc 0.98
2016-09-07T04:11:17.823132: step 2618, loss 0.0126852, acc 1
2016-09-07T04:11:18.506734: step 2619, loss 0.0413836, acc 0.98
2016-09-07T04:11:19.206323: step 2620, loss 0.0244391, acc 0.98
2016-09-07T04:11:19.886563: step 2621, loss 0.0208109, acc 1
2016-09-07T04:11:20.597557: step 2622, loss 0.0534595, acc 1
2016-09-07T04:11:21.263520: step 2623, loss 0.0360605, acc 0.98
2016-09-07T04:11:21.960636: step 2624, loss 0.00470103, acc 1
2016-09-07T04:11:22.635183: step 2625, loss 0.0465176, acc 0.98
2016-09-07T04:11:23.322743: step 2626, loss 0.0275243, acc 0.98
2016-09-07T04:11:24.006521: step 2627, loss 0.0619208, acc 0.96
2016-09-07T04:11:24.700804: step 2628, loss 0.0357594, acc 0.98
2016-09-07T04:11:25.401524: step 2629, loss 0.0154206, acc 0.98
2016-09-07T04:11:26.082445: step 2630, loss 0.0184431, acc 0.98
2016-09-07T04:11:26.777335: step 2631, loss 0.056982, acc 0.98
2016-09-07T04:11:27.485496: step 2632, loss 0.0447663, acc 0.98
2016-09-07T04:11:28.204259: step 2633, loss 0.0501988, acc 0.96
2016-09-07T04:11:28.892950: step 2634, loss 0.0334546, acc 0.98
2016-09-07T04:11:29.594197: step 2635, loss 0.03798, acc 0.98
2016-09-07T04:11:30.304509: step 2636, loss 0.0329082, acc 0.98
2016-09-07T04:11:30.992707: step 2637, loss 0.0571939, acc 0.96
2016-09-07T04:11:31.695473: step 2638, loss 0.0600831, acc 0.98
2016-09-07T04:11:32.399063: step 2639, loss 0.0259902, acc 1
2016-09-07T04:11:33.076947: step 2640, loss 0.0694458, acc 0.96
2016-09-07T04:11:33.784646: step 2641, loss 0.0709027, acc 0.96
2016-09-07T04:11:34.442669: step 2642, loss 0.0709661, acc 0.98
2016-09-07T04:11:35.154141: step 2643, loss 0.176506, acc 0.96
2016-09-07T04:11:35.820735: step 2644, loss 0.182064, acc 0.92
2016-09-07T04:11:36.487686: step 2645, loss 0.045708, acc 0.98
2016-09-07T04:11:37.166994: step 2646, loss 0.0544892, acc 0.96
2016-09-07T04:11:37.871172: step 2647, loss 0.0511223, acc 0.96
2016-09-07T04:11:38.552796: step 2648, loss 0.0738363, acc 0.96
2016-09-07T04:11:39.234181: step 2649, loss 0.0453331, acc 0.98
2016-09-07T04:11:39.934594: step 2650, loss 0.0551089, acc 0.98
2016-09-07T04:11:40.625096: step 2651, loss 0.163707, acc 0.9
2016-09-07T04:11:41.313935: step 2652, loss 0.0278113, acc 0.98
2016-09-07T04:11:41.994480: step 2653, loss 0.0179665, acc 1
2016-09-07T04:11:42.691411: step 2654, loss 0.0445439, acc 0.96
2016-09-07T04:11:43.394752: step 2655, loss 0.0327111, acc 0.98
2016-09-07T04:11:44.079695: step 2656, loss 0.0300672, acc 0.98
2016-09-07T04:11:44.781637: step 2657, loss 0.053184, acc 1
2016-09-07T04:11:45.443089: step 2658, loss 0.00754315, acc 1
2016-09-07T04:11:46.115907: step 2659, loss 0.0118384, acc 1
2016-09-07T04:11:46.807287: step 2660, loss 0.0266842, acc 1
2016-09-07T04:11:47.490533: step 2661, loss 0.0291196, acc 1
2016-09-07T04:11:48.187298: step 2662, loss 0.0122788, acc 1
2016-09-07T04:11:48.869989: step 2663, loss 0.0125104, acc 1
2016-09-07T04:11:49.598736: step 2664, loss 0.0108554, acc 1
2016-09-07T04:11:50.288327: step 2665, loss 0.03855, acc 0.96
2016-09-07T04:11:50.991037: step 2666, loss 0.0378495, acc 0.98
2016-09-07T04:11:51.689699: step 2667, loss 0.0377852, acc 0.98
2016-09-07T04:11:52.374119: step 2668, loss 0.026891, acc 0.98
2016-09-07T04:11:53.083520: step 2669, loss 0.0628452, acc 0.98
2016-09-07T04:11:53.750052: step 2670, loss 0.0320248, acc 0.98
2016-09-07T04:11:54.459202: step 2671, loss 0.00168992, acc 1
2016-09-07T04:11:55.160986: step 2672, loss 0.0456028, acc 0.98
2016-09-07T04:11:55.870602: step 2673, loss 0.00580586, acc 1
2016-09-07T04:11:56.580677: step 2674, loss 0.0740379, acc 0.98
2016-09-07T04:11:57.263889: step 2675, loss 0.0303354, acc 0.98
2016-09-07T04:11:57.988261: step 2676, loss 0.0190581, acc 1
2016-09-07T04:11:58.661768: step 2677, loss 0.0139754, acc 1
2016-09-07T04:11:59.363026: step 2678, loss 0.00130824, acc 1
2016-09-07T04:12:00.050451: step 2679, loss 0.0266934, acc 0.98
2016-09-07T04:12:00.762742: step 2680, loss 0.09807, acc 0.98
2016-09-07T04:12:01.434527: step 2681, loss 0.014837, acc 1
2016-09-07T04:12:02.129610: step 2682, loss 0.0174163, acc 0.98
2016-09-07T04:12:02.848049: step 2683, loss 0.026008, acc 1
2016-09-07T04:12:03.541002: step 2684, loss 0.00924268, acc 1
2016-09-07T04:12:04.236886: step 2685, loss 0.0478319, acc 0.98
2016-09-07T04:12:04.942767: step 2686, loss 0.109276, acc 0.98
2016-09-07T04:12:05.623427: step 2687, loss 0.00766677, acc 1
2016-09-07T04:12:06.256173: step 2688, loss 0.15769, acc 0.954545
2016-09-07T04:12:06.953735: step 2689, loss 0.0166335, acc 1
2016-09-07T04:12:07.671312: step 2690, loss 0.0687949, acc 0.96
2016-09-07T04:12:08.378928: step 2691, loss 0.0801359, acc 0.92
2016-09-07T04:12:09.059708: step 2692, loss 0.0394579, acc 1
2016-09-07T04:12:09.779544: step 2693, loss 0.0118236, acc 1
2016-09-07T04:12:10.451436: step 2694, loss 0.00807289, acc 1
2016-09-07T04:12:11.142334: step 2695, loss 0.0153443, acc 1
2016-09-07T04:12:11.818939: step 2696, loss 0.259716, acc 0.94
2016-09-07T04:12:12.528241: step 2697, loss 0.0259938, acc 0.98
2016-09-07T04:12:13.213621: step 2698, loss 0.0673424, acc 0.96
2016-09-07T04:12:13.893633: step 2699, loss 0.000600774, acc 1
2016-09-07T04:12:14.557105: step 2700, loss 0.0227757, acc 1

Evaluation:
2016-09-07T04:12:17.729794: step 2700, loss 1.59977, acc 0.719512

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-2700

2016-09-07T04:12:19.411711: step 2701, loss 0.0413474, acc 0.98
2016-09-07T04:12:20.090463: step 2702, loss 0.0733177, acc 0.98
2016-09-07T04:12:20.783155: step 2703, loss 0.00996238, acc 1
2016-09-07T04:12:21.474575: step 2704, loss 0.00885608, acc 1
2016-09-07T04:12:22.174076: step 2705, loss 0.0088769, acc 1
2016-09-07T04:12:22.859519: step 2706, loss 0.0100825, acc 1
2016-09-07T04:12:23.566644: step 2707, loss 0.0330112, acc 1
2016-09-07T04:12:24.261293: step 2708, loss 0.0357744, acc 0.98
2016-09-07T04:12:24.960302: step 2709, loss 0.100964, acc 0.94
2016-09-07T04:12:25.680997: step 2710, loss 0.0292671, acc 0.98
2016-09-07T04:12:26.355659: step 2711, loss 0.0329208, acc 0.98
2016-09-07T04:12:27.028698: step 2712, loss 0.00796167, acc 1
2016-09-07T04:12:27.718395: step 2713, loss 0.0846038, acc 0.96
2016-09-07T04:12:28.411289: step 2714, loss 0.0303844, acc 0.98
2016-09-07T04:12:29.102709: step 2715, loss 0.00152879, acc 1
2016-09-07T04:12:29.794685: step 2716, loss 0.0546175, acc 0.98
2016-09-07T04:12:30.509992: step 2717, loss 0.0128163, acc 1
2016-09-07T04:12:31.210369: step 2718, loss 0.0556514, acc 0.96
2016-09-07T04:12:31.889222: step 2719, loss 0.0514437, acc 0.96
2016-09-07T04:12:32.573044: step 2720, loss 0.0968008, acc 0.96
2016-09-07T04:12:33.265716: step 2721, loss 0.0231241, acc 0.98
2016-09-07T04:12:33.944503: step 2722, loss 0.153141, acc 0.98
2016-09-07T04:12:34.612013: step 2723, loss 0.0978068, acc 0.96
2016-09-07T04:12:35.313216: step 2724, loss 0.0559289, acc 0.94
2016-09-07T04:12:35.985719: step 2725, loss 0.0134717, acc 1
2016-09-07T04:12:36.659504: step 2726, loss 0.0148687, acc 1
2016-09-07T04:12:37.345422: step 2727, loss 0.018135, acc 1
2016-09-07T04:12:38.031441: step 2728, loss 0.0142061, acc 1
2016-09-07T04:12:38.712497: step 2729, loss 0.0568056, acc 0.96
2016-09-07T04:12:39.389815: step 2730, loss 0.0284599, acc 1
2016-09-07T04:12:40.087668: step 2731, loss 0.00461219, acc 1
2016-09-07T04:12:40.771236: step 2732, loss 0.0147581, acc 1
2016-09-07T04:12:41.475776: step 2733, loss 0.0193191, acc 1
2016-09-07T04:12:42.166033: step 2734, loss 0.0537281, acc 0.98
2016-09-07T04:12:42.894525: step 2735, loss 0.0195985, acc 1
2016-09-07T04:12:43.601733: step 2736, loss 0.0246118, acc 1
2016-09-07T04:12:44.285769: step 2737, loss 0.0212401, acc 1
2016-09-07T04:12:44.990141: step 2738, loss 0.0265362, acc 1
2016-09-07T04:12:45.671078: step 2739, loss 0.0374184, acc 0.98
2016-09-07T04:12:46.348790: step 2740, loss 0.00556523, acc 1
2016-09-07T04:12:47.037788: step 2741, loss 0.0775875, acc 0.98
2016-09-07T04:12:47.705218: step 2742, loss 0.049833, acc 0.96
2016-09-07T04:12:48.406524: step 2743, loss 0.0389877, acc 0.98
2016-09-07T04:12:49.084481: step 2744, loss 0.027383, acc 0.98
2016-09-07T04:12:49.793436: step 2745, loss 0.0942899, acc 0.96
2016-09-07T04:12:50.508750: step 2746, loss 0.0257804, acc 0.98
2016-09-07T04:12:51.194861: step 2747, loss 0.0406079, acc 0.98
2016-09-07T04:12:51.884929: step 2748, loss 0.00647801, acc 1
2016-09-07T04:12:52.560410: step 2749, loss 0.138814, acc 0.96
2016-09-07T04:12:53.272537: step 2750, loss 0.00942527, acc 1
2016-09-07T04:12:53.947539: step 2751, loss 0.0480671, acc 0.98
2016-09-07T04:12:54.651307: step 2752, loss 0.0909433, acc 0.96
2016-09-07T04:12:55.332839: step 2753, loss 0.0352002, acc 0.98
2016-09-07T04:12:56.010302: step 2754, loss 0.0816586, acc 0.96
2016-09-07T04:12:56.711903: step 2755, loss 0.243815, acc 0.96
2016-09-07T04:12:57.400809: step 2756, loss 0.0133675, acc 1
2016-09-07T04:12:58.097563: step 2757, loss 0.0567916, acc 0.96
2016-09-07T04:12:58.769623: step 2758, loss 0.0690526, acc 0.96
2016-09-07T04:12:59.455626: step 2759, loss 0.0241841, acc 1
2016-09-07T04:13:00.139415: step 2760, loss 0.0943198, acc 0.98
2016-09-07T04:13:00.846516: step 2761, loss 0.0791124, acc 0.96
2016-09-07T04:13:01.531031: step 2762, loss 0.027212, acc 0.98
2016-09-07T04:13:02.210913: step 2763, loss 0.107651, acc 0.94
2016-09-07T04:13:02.920161: step 2764, loss 0.0583292, acc 0.96
2016-09-07T04:13:03.580172: step 2765, loss 0.00707422, acc 1
2016-09-07T04:13:04.263296: step 2766, loss 0.00302906, acc 1
2016-09-07T04:13:04.953219: step 2767, loss 0.0471429, acc 0.98
2016-09-07T04:13:05.632519: step 2768, loss 0.0345513, acc 0.98
2016-09-07T04:13:06.327686: step 2769, loss 0.0533767, acc 0.98
2016-09-07T04:13:07.018212: step 2770, loss 0.0418337, acc 0.98
2016-09-07T04:13:07.718636: step 2771, loss 0.0187073, acc 1
2016-09-07T04:13:08.393864: step 2772, loss 0.0346993, acc 0.98
2016-09-07T04:13:09.080404: step 2773, loss 0.0215937, acc 0.98
2016-09-07T04:13:09.774642: step 2774, loss 0.0796808, acc 0.96
2016-09-07T04:13:10.465508: step 2775, loss 0.0313136, acc 1
2016-09-07T04:13:11.156567: step 2776, loss 0.0522423, acc 0.98
2016-09-07T04:13:11.851876: step 2777, loss 0.0190359, acc 0.98
2016-09-07T04:13:12.561825: step 2778, loss 0.108011, acc 0.92
2016-09-07T04:13:13.236149: step 2779, loss 0.036628, acc 0.98
2016-09-07T04:13:13.923505: step 2780, loss 0.0191078, acc 1
2016-09-07T04:13:14.618274: step 2781, loss 0.0142151, acc 1
2016-09-07T04:13:15.309547: step 2782, loss 0.0199642, acc 1
2016-09-07T04:13:16.019646: step 2783, loss 0.0335226, acc 0.98
2016-09-07T04:13:16.698602: step 2784, loss 0.0418621, acc 1
2016-09-07T04:13:17.407899: step 2785, loss 0.0508032, acc 0.96
2016-09-07T04:13:18.105224: step 2786, loss 0.0816298, acc 0.92
2016-09-07T04:13:18.802560: step 2787, loss 0.0631929, acc 0.98
2016-09-07T04:13:19.499522: step 2788, loss 0.0181253, acc 0.98
2016-09-07T04:13:20.203909: step 2789, loss 0.0325995, acc 0.98
2016-09-07T04:13:20.904236: step 2790, loss 0.0219975, acc 1
2016-09-07T04:13:21.603514: step 2791, loss 0.0400777, acc 1
2016-09-07T04:13:22.289189: step 2792, loss 0.0547154, acc 0.98
2016-09-07T04:13:22.978807: step 2793, loss 0.00619041, acc 1
2016-09-07T04:13:23.666870: step 2794, loss 0.00890184, acc 1
2016-09-07T04:13:24.360267: step 2795, loss 0.0231409, acc 1
2016-09-07T04:13:25.040324: step 2796, loss 0.0283068, acc 1
2016-09-07T04:13:25.759464: step 2797, loss 0.00511477, acc 1
2016-09-07T04:13:26.442759: step 2798, loss 0.125408, acc 0.98
2016-09-07T04:13:27.143893: step 2799, loss 0.0651392, acc 0.96
2016-09-07T04:13:27.883113: step 2800, loss 0.0176889, acc 1

Evaluation:
2016-09-07T04:13:31.047320: step 2800, loss 1.91053, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-2800

2016-09-07T04:13:32.720557: step 2801, loss 0.0719646, acc 0.98
2016-09-07T04:13:33.410859: step 2802, loss 0.0151354, acc 1
2016-09-07T04:13:34.085554: step 2803, loss 0.0421319, acc 0.98
2016-09-07T04:13:34.759303: step 2804, loss 0.0434032, acc 0.98
2016-09-07T04:13:35.480541: step 2805, loss 0.0257984, acc 1
2016-09-07T04:13:36.161681: step 2806, loss 0.0413982, acc 0.98
2016-09-07T04:13:36.839115: step 2807, loss 0.051867, acc 0.98
2016-09-07T04:13:37.518596: step 2808, loss 0.130652, acc 0.98
2016-09-07T04:13:38.209187: step 2809, loss 0.0403208, acc 0.98
2016-09-07T04:13:38.900860: step 2810, loss 0.128626, acc 0.92
2016-09-07T04:13:39.573565: step 2811, loss 0.0300587, acc 0.98
2016-09-07T04:13:40.273393: step 2812, loss 0.0496395, acc 0.96
2016-09-07T04:13:40.963666: step 2813, loss 0.0347963, acc 1
2016-09-07T04:13:41.673630: step 2814, loss 0.0235788, acc 1
2016-09-07T04:13:42.383278: step 2815, loss 0.0325654, acc 1
2016-09-07T04:13:43.059374: step 2816, loss 0.0266094, acc 0.98
2016-09-07T04:13:43.762410: step 2817, loss 0.00372631, acc 1
2016-09-07T04:13:44.436504: step 2818, loss 0.0283456, acc 0.98
2016-09-07T04:13:45.135105: step 2819, loss 0.0606922, acc 0.96
2016-09-07T04:13:45.815085: step 2820, loss 0.0338864, acc 0.98
2016-09-07T04:13:46.529235: step 2821, loss 0.0219161, acc 0.98
2016-09-07T04:13:47.204951: step 2822, loss 0.0199905, acc 1
2016-09-07T04:13:47.897702: step 2823, loss 0.0790756, acc 0.98
2016-09-07T04:13:48.594950: step 2824, loss 0.0320374, acc 0.98
2016-09-07T04:13:49.287622: step 2825, loss 0.0248526, acc 1
2016-09-07T04:13:49.953336: step 2826, loss 0.00881052, acc 1
2016-09-07T04:13:50.630045: step 2827, loss 0.0405941, acc 0.98
2016-09-07T04:13:51.329259: step 2828, loss 0.102804, acc 0.98
2016-09-07T04:13:52.011563: step 2829, loss 0.00967287, acc 1
2016-09-07T04:13:52.714070: step 2830, loss 0.0776713, acc 0.98
2016-09-07T04:13:53.417972: step 2831, loss 0.04005, acc 0.96
2016-09-07T04:13:54.079663: step 2832, loss 0.0184466, acc 0.98
2016-09-07T04:13:54.773598: step 2833, loss 0.0628257, acc 0.96
2016-09-07T04:13:55.466794: step 2834, loss 0.0151, acc 1
2016-09-07T04:13:56.144623: step 2835, loss 0.0526923, acc 0.98
2016-09-07T04:13:56.847748: step 2836, loss 0.0471515, acc 1
2016-09-07T04:13:57.528013: step 2837, loss 0.0180988, acc 1
2016-09-07T04:13:58.220608: step 2838, loss 0.0864792, acc 0.96
2016-09-07T04:13:58.908611: step 2839, loss 0.0554182, acc 0.96
2016-09-07T04:13:59.633318: step 2840, loss 0.0404057, acc 0.96
2016-09-07T04:14:00.370531: step 2841, loss 0.0463412, acc 0.96
2016-09-07T04:14:01.076173: step 2842, loss 0.027694, acc 0.98
2016-09-07T04:14:01.769775: step 2843, loss 0.0452633, acc 1
2016-09-07T04:14:02.428596: step 2844, loss 0.0984442, acc 0.98
2016-09-07T04:14:03.156420: step 2845, loss 0.00293849, acc 1
2016-09-07T04:14:03.856951: step 2846, loss 0.0471716, acc 0.96
2016-09-07T04:14:04.548823: step 2847, loss 0.00105707, acc 1
2016-09-07T04:14:05.210107: step 2848, loss 0.00214833, acc 1
2016-09-07T04:14:05.905861: step 2849, loss 0.0248805, acc 0.98
2016-09-07T04:14:06.605149: step 2850, loss 0.0374431, acc 1
2016-09-07T04:14:07.267701: step 2851, loss 0.00442561, acc 1
2016-09-07T04:14:07.964275: step 2852, loss 0.00561599, acc 1
2016-09-07T04:14:08.646362: step 2853, loss 0.000553466, acc 1
2016-09-07T04:14:09.355135: step 2854, loss 0.0263753, acc 1
2016-09-07T04:14:10.042007: step 2855, loss 0.0181435, acc 1
2016-09-07T04:14:10.746648: step 2856, loss 0.00734385, acc 1
2016-09-07T04:14:11.462162: step 2857, loss 0.0607463, acc 0.96
2016-09-07T04:14:12.138376: step 2858, loss 0.079887, acc 0.96
2016-09-07T04:14:12.838205: step 2859, loss 0.0258933, acc 1
2016-09-07T04:14:13.537844: step 2860, loss 0.00892423, acc 1
2016-09-07T04:14:14.213029: step 2861, loss 0.0454313, acc 0.98
2016-09-07T04:14:14.906084: step 2862, loss 0.0641974, acc 0.98
2016-09-07T04:14:15.606763: step 2863, loss 0.00178984, acc 1
2016-09-07T04:14:16.308687: step 2864, loss 0.0981224, acc 0.96
2016-09-07T04:14:16.994833: step 2865, loss 0.0371792, acc 0.98
2016-09-07T04:14:17.703292: step 2866, loss 0.112529, acc 0.94
2016-09-07T04:14:18.384957: step 2867, loss 0.0086524, acc 1
2016-09-07T04:14:19.080170: step 2868, loss 0.0985363, acc 0.98
2016-09-07T04:14:19.783786: step 2869, loss 0.0186951, acc 1
2016-09-07T04:14:20.461861: step 2870, loss 0.0114994, acc 1
2016-09-07T04:14:21.158056: step 2871, loss 0.0122864, acc 1
2016-09-07T04:14:21.835159: step 2872, loss 0.0324184, acc 0.98
2016-09-07T04:14:22.511191: step 2873, loss 0.0564682, acc 0.96
2016-09-07T04:14:23.190320: step 2874, loss 0.0262495, acc 0.98
2016-09-07T04:14:23.870822: step 2875, loss 0.0380984, acc 1
2016-09-07T04:14:24.562566: step 2876, loss 0.00202768, acc 1
2016-09-07T04:14:25.230511: step 2877, loss 0.105801, acc 0.96
2016-09-07T04:14:25.944896: step 2878, loss 0.0117725, acc 1
2016-09-07T04:14:26.627170: step 2879, loss 0.0106185, acc 1
2016-09-07T04:14:27.280287: step 2880, loss 0.00215635, acc 1
2016-09-07T04:14:27.971260: step 2881, loss 0.0207501, acc 0.98
2016-09-07T04:14:28.637675: step 2882, loss 0.050529, acc 0.98
2016-09-07T04:14:29.324503: step 2883, loss 0.0402858, acc 1
2016-09-07T04:14:29.988280: step 2884, loss 0.0474974, acc 0.98
2016-09-07T04:14:30.691426: step 2885, loss 0.026594, acc 1
2016-09-07T04:14:31.385771: step 2886, loss 0.0619581, acc 0.94
2016-09-07T04:14:32.086809: step 2887, loss 0.0329567, acc 0.98
2016-09-07T04:14:32.766701: step 2888, loss 0.009544, acc 1
2016-09-07T04:14:33.466422: step 2889, loss 0.00101057, acc 1
2016-09-07T04:14:34.157409: step 2890, loss 0.0652005, acc 0.96
2016-09-07T04:14:34.830923: step 2891, loss 0.102, acc 0.98
2016-09-07T04:14:35.515067: step 2892, loss 0.342328, acc 0.94
2016-09-07T04:14:36.185792: step 2893, loss 0.0583839, acc 0.98
2016-09-07T04:14:36.852560: step 2894, loss 0.0695691, acc 0.98
2016-09-07T04:14:37.533335: step 2895, loss 0.00122093, acc 1
2016-09-07T04:14:38.234891: step 2896, loss 0.0136491, acc 1
2016-09-07T04:14:38.919121: step 2897, loss 0.019282, acc 1
2016-09-07T04:14:39.624868: step 2898, loss 0.0551897, acc 0.98
2016-09-07T04:14:40.365080: step 2899, loss 0.00794544, acc 1
2016-09-07T04:14:41.046925: step 2900, loss 0.00696957, acc 1

Evaluation:
2016-09-07T04:14:44.230443: step 2900, loss 1.54596, acc 0.724203

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-2900

2016-09-07T04:14:46.051642: step 2901, loss 0.0807525, acc 0.92
2016-09-07T04:14:46.727105: step 2902, loss 0.0107619, acc 1
2016-09-07T04:14:47.400335: step 2903, loss 0.0803084, acc 0.98
2016-09-07T04:14:48.073939: step 2904, loss 0.0307406, acc 0.98
2016-09-07T04:14:48.770518: step 2905, loss 0.0103544, acc 1
2016-09-07T04:14:49.461108: step 2906, loss 0.0188914, acc 1
2016-09-07T04:14:50.155420: step 2907, loss 0.0471136, acc 0.98
2016-09-07T04:14:50.838991: step 2908, loss 0.0176072, acc 1
2016-09-07T04:14:51.512315: step 2909, loss 0.0711314, acc 0.98
2016-09-07T04:14:52.207627: step 2910, loss 0.0387347, acc 0.98
2016-09-07T04:14:52.890411: step 2911, loss 0.108669, acc 0.94
2016-09-07T04:14:53.592619: step 2912, loss 0.104662, acc 0.96
2016-09-07T04:14:54.278526: step 2913, loss 0.0970733, acc 0.94
2016-09-07T04:14:54.973961: step 2914, loss 0.00437124, acc 1
2016-09-07T04:14:55.667094: step 2915, loss 0.0201146, acc 1
2016-09-07T04:14:56.358051: step 2916, loss 0.013018, acc 1
2016-09-07T04:14:57.056207: step 2917, loss 0.00853632, acc 1
2016-09-07T04:14:57.728879: step 2918, loss 0.0872305, acc 0.94
2016-09-07T04:14:58.426663: step 2919, loss 0.00724881, acc 1
2016-09-07T04:14:59.117657: step 2920, loss 0.0407877, acc 0.98
2016-09-07T04:14:59.802078: step 2921, loss 0.0544539, acc 1
2016-09-07T04:15:00.533723: step 2922, loss 0.0621239, acc 0.98
2016-09-07T04:15:01.238205: step 2923, loss 0.0611592, acc 0.98
2016-09-07T04:15:01.932788: step 2924, loss 0.0150996, acc 1
2016-09-07T04:15:02.622037: step 2925, loss 0.0461626, acc 1
2016-09-07T04:15:03.306054: step 2926, loss 0.0450741, acc 0.98
2016-09-07T04:15:04.011469: step 2927, loss 0.0158706, acc 1
2016-09-07T04:15:04.712605: step 2928, loss 0.0424971, acc 0.96
2016-09-07T04:15:05.390920: step 2929, loss 0.0464742, acc 0.98
2016-09-07T04:15:06.075312: step 2930, loss 0.0401005, acc 0.98
2016-09-07T04:15:06.773462: step 2931, loss 0.0456349, acc 0.98
2016-09-07T04:15:07.441834: step 2932, loss 0.0285715, acc 0.98
2016-09-07T04:15:08.136413: step 2933, loss 0.0116511, acc 1
2016-09-07T04:15:08.817710: step 2934, loss 0.0090045, acc 1
2016-09-07T04:15:09.482703: step 2935, loss 0.0474398, acc 0.98
2016-09-07T04:15:10.166285: step 2936, loss 0.0612908, acc 0.98
2016-09-07T04:15:10.904437: step 2937, loss 0.0179872, acc 1
2016-09-07T04:15:11.622826: step 2938, loss 0.00265892, acc 1
2016-09-07T04:15:12.299161: step 2939, loss 0.00649422, acc 1
2016-09-07T04:15:12.969519: step 2940, loss 0.00557716, acc 1
2016-09-07T04:15:13.663190: step 2941, loss 0.00944004, acc 1
2016-09-07T04:15:14.345872: step 2942, loss 0.0522481, acc 0.98
2016-09-07T04:15:15.024358: step 2943, loss 0.0308568, acc 0.98
2016-09-07T04:15:15.696311: step 2944, loss 0.0477549, acc 0.98
2016-09-07T04:15:16.408641: step 2945, loss 0.0699206, acc 0.98
2016-09-07T04:15:17.092489: step 2946, loss 0.0225895, acc 1
2016-09-07T04:15:17.780096: step 2947, loss 0.0140545, acc 1
2016-09-07T04:15:18.467481: step 2948, loss 0.0230677, acc 0.98
2016-09-07T04:15:19.170333: step 2949, loss 0.063129, acc 0.96
2016-09-07T04:15:19.869554: step 2950, loss 0.0539946, acc 0.96
2016-09-07T04:15:20.530344: step 2951, loss 0.12059, acc 0.98
2016-09-07T04:15:21.230100: step 2952, loss 0.0318129, acc 0.96
2016-09-07T04:15:21.897378: step 2953, loss 0.00157073, acc 1
2016-09-07T04:15:22.584439: step 2954, loss 0.0187689, acc 1
2016-09-07T04:15:23.267553: step 2955, loss 0.0899442, acc 0.96
2016-09-07T04:15:23.964828: step 2956, loss 0.0256568, acc 1
2016-09-07T04:15:24.674742: step 2957, loss 0.0150398, acc 1
2016-09-07T04:15:25.336307: step 2958, loss 0.00286549, acc 1
2016-09-07T04:15:26.030986: step 2959, loss 0.0253392, acc 0.98
2016-09-07T04:15:26.704706: step 2960, loss 0.0314435, acc 0.98
2016-09-07T04:15:27.366994: step 2961, loss 0.0113017, acc 1
2016-09-07T04:15:28.046346: step 2962, loss 0.00548944, acc 1
2016-09-07T04:15:28.734544: step 2963, loss 0.0212995, acc 1
2016-09-07T04:15:29.416379: step 2964, loss 0.014086, acc 1
2016-09-07T04:15:30.100756: step 2965, loss 0.016807, acc 1
2016-09-07T04:15:30.797306: step 2966, loss 0.0347531, acc 0.98
2016-09-07T04:15:31.480172: step 2967, loss 0.0381689, acc 0.98
2016-09-07T04:15:32.167347: step 2968, loss 0.0195662, acc 1
2016-09-07T04:15:32.852356: step 2969, loss 0.0115435, acc 1
2016-09-07T04:15:33.540398: step 2970, loss 0.0071951, acc 1
2016-09-07T04:15:34.212671: step 2971, loss 0.0477602, acc 0.98
2016-09-07T04:15:34.894342: step 2972, loss 0.0385105, acc 0.98
2016-09-07T04:15:35.603912: step 2973, loss 0.0720488, acc 0.96
2016-09-07T04:15:36.275972: step 2974, loss 0.0283964, acc 0.98
2016-09-07T04:15:36.960074: step 2975, loss 0.00829902, acc 1
2016-09-07T04:15:37.677437: step 2976, loss 0.0137838, acc 1
2016-09-07T04:15:38.365522: step 2977, loss 0.0489273, acc 0.98
2016-09-07T04:15:39.041455: step 2978, loss 0.0496376, acc 1
2016-09-07T04:15:39.732898: step 2979, loss 0.117716, acc 0.98
2016-09-07T04:15:40.427643: step 2980, loss 0.0251898, acc 0.98
2016-09-07T04:15:41.113212: step 2981, loss 0.0330038, acc 0.98
2016-09-07T04:15:41.797358: step 2982, loss 0.0779067, acc 0.98
2016-09-07T04:15:42.483170: step 2983, loss 0.00200471, acc 1
2016-09-07T04:15:43.151739: step 2984, loss 0.0879705, acc 0.94
2016-09-07T04:15:43.836205: step 2985, loss 0.026287, acc 1
2016-09-07T04:15:44.520275: step 2986, loss 0.0227764, acc 0.98
2016-09-07T04:15:45.218595: step 2987, loss 0.0916876, acc 0.96
2016-09-07T04:15:45.889447: step 2988, loss 0.00607893, acc 1
2016-09-07T04:15:46.571422: step 2989, loss 0.0213925, acc 1
2016-09-07T04:15:47.264022: step 2990, loss 0.0121646, acc 1
2016-09-07T04:15:47.990306: step 2991, loss 0.0774896, acc 0.98
2016-09-07T04:15:48.709425: step 2992, loss 0.0943733, acc 0.96
2016-09-07T04:15:49.413816: step 2993, loss 0.0154323, acc 1
2016-09-07T04:15:50.111107: step 2994, loss 0.022084, acc 0.98
2016-09-07T04:15:50.801443: step 2995, loss 0.0794288, acc 0.96
2016-09-07T04:15:51.489812: step 2996, loss 0.0912151, acc 0.96
2016-09-07T04:15:52.191516: step 2997, loss 0.0161148, acc 0.98
2016-09-07T04:15:52.895803: step 2998, loss 0.00475568, acc 1
2016-09-07T04:15:53.568750: step 2999, loss 0.00778748, acc 1
2016-09-07T04:15:54.225255: step 3000, loss 0.0516897, acc 0.96

Evaluation:
2016-09-07T04:15:57.432442: step 3000, loss 1.60464, acc 0.727017

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-3000

2016-09-07T04:15:59.088797: step 3001, loss 0.106973, acc 0.96
2016-09-07T04:15:59.787561: step 3002, loss 0.020199, acc 0.98
2016-09-07T04:16:00.523053: step 3003, loss 0.0255055, acc 0.98
2016-09-07T04:16:01.211642: step 3004, loss 0.0038367, acc 1
2016-09-07T04:16:01.881085: step 3005, loss 0.0356472, acc 1
2016-09-07T04:16:02.554656: step 3006, loss 0.0349545, acc 1
2016-09-07T04:16:03.262597: step 3007, loss 0.00427429, acc 1
2016-09-07T04:16:03.947245: step 3008, loss 0.0161218, acc 1
2016-09-07T04:16:04.641783: step 3009, loss 0.0196385, acc 0.98
2016-09-07T04:16:05.319892: step 3010, loss 0.0311721, acc 0.98
2016-09-07T04:16:06.022575: step 3011, loss 0.0351081, acc 0.98
2016-09-07T04:16:06.717368: step 3012, loss 0.0698454, acc 0.96
2016-09-07T04:16:07.403381: step 3013, loss 0.0563564, acc 0.96
2016-09-07T04:16:08.105943: step 3014, loss 0.0148071, acc 1
2016-09-07T04:16:08.762309: step 3015, loss 0.0186708, acc 1
2016-09-07T04:16:09.420741: step 3016, loss 0.0845067, acc 0.94
2016-09-07T04:16:10.112856: step 3017, loss 0.0633706, acc 0.96
2016-09-07T04:16:10.797918: step 3018, loss 0.0100747, acc 1
2016-09-07T04:16:11.482998: step 3019, loss 0.0640431, acc 0.96
2016-09-07T04:16:12.168309: step 3020, loss 0.0818224, acc 0.96
2016-09-07T04:16:12.870927: step 3021, loss 0.0335978, acc 0.98
2016-09-07T04:16:13.551778: step 3022, loss 0.0113472, acc 1
2016-09-07T04:16:14.239120: step 3023, loss 0.0248134, acc 1
2016-09-07T04:16:14.928852: step 3024, loss 0.0539745, acc 0.98
2016-09-07T04:16:15.612453: step 3025, loss 0.00433554, acc 1
2016-09-07T04:16:16.309689: step 3026, loss 0.0776945, acc 0.96
2016-09-07T04:16:16.997413: step 3027, loss 0.0121168, acc 1
2016-09-07T04:16:17.697728: step 3028, loss 0.0511644, acc 0.98
2016-09-07T04:16:18.375065: step 3029, loss 0.0104468, acc 1
2016-09-07T04:16:19.055982: step 3030, loss 0.00322643, acc 1
2016-09-07T04:16:19.734546: step 3031, loss 0.0471423, acc 0.96
2016-09-07T04:16:20.445184: step 3032, loss 0.0616269, acc 0.98
2016-09-07T04:16:21.128504: step 3033, loss 0.0443913, acc 0.96
2016-09-07T04:16:21.817146: step 3034, loss 0.0598834, acc 0.98
2016-09-07T04:16:22.525148: step 3035, loss 0.0690232, acc 0.96
2016-09-07T04:16:23.196872: step 3036, loss 0.0136098, acc 1
2016-09-07T04:16:23.889666: step 3037, loss 0.0419465, acc 0.98
2016-09-07T04:16:24.573647: step 3038, loss 0.00887685, acc 1
2016-09-07T04:16:25.266164: step 3039, loss 0.0401891, acc 0.98
2016-09-07T04:16:25.957004: step 3040, loss 0.0242516, acc 0.98
2016-09-07T04:16:26.637925: step 3041, loss 0.0103093, acc 1
2016-09-07T04:16:27.331507: step 3042, loss 0.0200499, acc 1
2016-09-07T04:16:27.999660: step 3043, loss 0.00699051, acc 1
2016-09-07T04:16:28.675719: step 3044, loss 0.126184, acc 0.96
2016-09-07T04:16:29.374946: step 3045, loss 0.00471867, acc 1
2016-09-07T04:16:30.044996: step 3046, loss 0.0650812, acc 0.96
2016-09-07T04:16:30.713005: step 3047, loss 0.00884114, acc 1
2016-09-07T04:16:31.391204: step 3048, loss 0.0133839, acc 1
2016-09-07T04:16:32.094581: step 3049, loss 0.0492872, acc 0.98
2016-09-07T04:16:32.782963: step 3050, loss 0.114117, acc 0.94
2016-09-07T04:16:33.473539: step 3051, loss 0.0523112, acc 0.96
2016-09-07T04:16:34.175784: step 3052, loss 0.0122763, acc 1
2016-09-07T04:16:34.896484: step 3053, loss 0.0186781, acc 1
2016-09-07T04:16:35.590006: step 3054, loss 0.0233809, acc 1
2016-09-07T04:16:36.260654: step 3055, loss 0.0177306, acc 1
2016-09-07T04:16:36.976231: step 3056, loss 0.0376436, acc 0.98
2016-09-07T04:16:37.648731: step 3057, loss 0.00225254, acc 1
2016-09-07T04:16:38.325383: step 3058, loss 0.0222498, acc 0.98
2016-09-07T04:16:39.009943: step 3059, loss 0.00497828, acc 1
2016-09-07T04:16:39.703556: step 3060, loss 0.0608914, acc 0.98
2016-09-07T04:16:40.404211: step 3061, loss 0.0107585, acc 1
2016-09-07T04:16:41.094898: step 3062, loss 0.0210153, acc 0.98
2016-09-07T04:16:41.787717: step 3063, loss 0.00447699, acc 1
2016-09-07T04:16:42.479349: step 3064, loss 0.0676804, acc 0.96
2016-09-07T04:16:43.151509: step 3065, loss 0.0874485, acc 0.98
2016-09-07T04:16:43.842063: step 3066, loss 0.0461524, acc 0.98
2016-09-07T04:16:44.526999: step 3067, loss 0.0441232, acc 0.98
2016-09-07T04:16:45.218744: step 3068, loss 0.00547133, acc 1
2016-09-07T04:16:45.899218: step 3069, loss 0.0217408, acc 0.98
2016-09-07T04:16:46.586570: step 3070, loss 0.0198896, acc 0.98
2016-09-07T04:16:47.264241: step 3071, loss 0.0169627, acc 1
2016-09-07T04:16:47.899894: step 3072, loss 0.00338444, acc 1
2016-09-07T04:16:48.597581: step 3073, loss 0.038883, acc 0.98
2016-09-07T04:16:49.286292: step 3074, loss 0.029387, acc 0.98
2016-09-07T04:16:49.979565: step 3075, loss 0.0649464, acc 0.98
2016-09-07T04:16:50.665727: step 3076, loss 0.0280601, acc 0.98
2016-09-07T04:16:51.382327: step 3077, loss 0.0495617, acc 0.96
2016-09-07T04:16:52.060880: step 3078, loss 0.0256894, acc 1
2016-09-07T04:16:52.737407: step 3079, loss 0.0153193, acc 1
2016-09-07T04:16:53.399512: step 3080, loss 0.0165221, acc 1
2016-09-07T04:16:54.089470: step 3081, loss 0.00740494, acc 1
2016-09-07T04:16:54.769021: step 3082, loss 0.0161922, acc 1
2016-09-07T04:16:55.452211: step 3083, loss 0.013682, acc 1
2016-09-07T04:16:56.142530: step 3084, loss 0.00515409, acc 1
2016-09-07T04:16:56.793471: step 3085, loss 0.0855745, acc 0.94
2016-09-07T04:16:57.502279: step 3086, loss 0.0669651, acc 0.98
2016-09-07T04:16:58.200009: step 3087, loss 0.00419023, acc 1
2016-09-07T04:16:58.881137: step 3088, loss 0.0156271, acc 1
2016-09-07T04:16:59.555621: step 3089, loss 0.00657985, acc 1
2016-09-07T04:17:00.226615: step 3090, loss 0.00337688, acc 1
2016-09-07T04:17:00.931684: step 3091, loss 0.017928, acc 0.98
2016-09-07T04:17:01.609161: step 3092, loss 0.032658, acc 0.98
2016-09-07T04:17:02.320651: step 3093, loss 0.017627, acc 1
2016-09-07T04:17:03.019457: step 3094, loss 0.0364958, acc 0.98
2016-09-07T04:17:03.685968: step 3095, loss 0.00069305, acc 1
2016-09-07T04:17:04.364921: step 3096, loss 0.00709591, acc 1
2016-09-07T04:17:05.050843: step 3097, loss 0.0281367, acc 0.98
2016-09-07T04:17:05.731063: step 3098, loss 0.0334351, acc 0.98
2016-09-07T04:17:06.408261: step 3099, loss 0.000972141, acc 1
2016-09-07T04:17:07.115513: step 3100, loss 0.0365873, acc 0.98

Evaluation:
2016-09-07T04:17:10.282157: step 3100, loss 2.04619, acc 0.724203

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-3100

2016-09-07T04:17:11.945676: step 3101, loss 0.0302376, acc 0.98
2016-09-07T04:17:12.634554: step 3102, loss 0.0291377, acc 1
2016-09-07T04:17:13.318081: step 3103, loss 0.0164442, acc 1
2016-09-07T04:17:14.028638: step 3104, loss 0.0946816, acc 0.96
2016-09-07T04:17:14.720091: step 3105, loss 0.0210005, acc 1
2016-09-07T04:17:15.403464: step 3106, loss 0.00617619, acc 1
2016-09-07T04:17:16.089715: step 3107, loss 0.025722, acc 0.98
2016-09-07T04:17:16.761070: step 3108, loss 0.030728, acc 0.98
2016-09-07T04:17:17.440312: step 3109, loss 0.000278457, acc 1
2016-09-07T04:17:18.132669: step 3110, loss 0.0316027, acc 0.98
2016-09-07T04:17:18.835560: step 3111, loss 0.0301487, acc 0.98
2016-09-07T04:17:19.524901: step 3112, loss 0.0074878, acc 1
2016-09-07T04:17:20.244522: step 3113, loss 0.133056, acc 0.92
2016-09-07T04:17:20.931066: step 3114, loss 0.0162409, acc 1
2016-09-07T04:17:21.617680: step 3115, loss 0.00613771, acc 1
2016-09-07T04:17:22.333805: step 3116, loss 0.00261752, acc 1
2016-09-07T04:17:23.017549: step 3117, loss 0.0255622, acc 0.98
2016-09-07T04:17:23.703952: step 3118, loss 0.0155475, acc 0.98
2016-09-07T04:17:24.380348: step 3119, loss 0.0417972, acc 0.98
2016-09-07T04:17:25.078674: step 3120, loss 0.0251057, acc 1
2016-09-07T04:17:25.772054: step 3121, loss 0.00524363, acc 1
2016-09-07T04:17:26.457663: step 3122, loss 0.061103, acc 0.96
2016-09-07T04:17:27.160071: step 3123, loss 0.0191879, acc 1
2016-09-07T04:17:27.847083: step 3124, loss 0.00773259, acc 1
2016-09-07T04:17:28.538292: step 3125, loss 0.0203364, acc 0.98
2016-09-07T04:17:29.198990: step 3126, loss 0.0692175, acc 0.96
2016-09-07T04:17:29.901437: step 3127, loss 0.0142626, acc 1
2016-09-07T04:17:30.591436: step 3128, loss 0.190692, acc 0.96
2016-09-07T04:17:31.273340: step 3129, loss 0.0947927, acc 0.96
2016-09-07T04:17:31.968224: step 3130, loss 0.00462981, acc 1
2016-09-07T04:17:32.656695: step 3131, loss 0.000300324, acc 1
2016-09-07T04:17:33.354667: step 3132, loss 0.0534366, acc 0.98
2016-09-07T04:17:34.015289: step 3133, loss 0.0746394, acc 0.98
2016-09-07T04:17:34.723787: step 3134, loss 0.0452102, acc 0.96
2016-09-07T04:17:35.428466: step 3135, loss 0.0692325, acc 0.98
2016-09-07T04:17:36.121948: step 3136, loss 0.107099, acc 0.98
2016-09-07T04:17:36.813233: step 3137, loss 0.0759378, acc 0.98
2016-09-07T04:17:37.500496: step 3138, loss 0.193369, acc 0.94
2016-09-07T04:17:38.204122: step 3139, loss 0.0265711, acc 0.98
2016-09-07T04:17:38.864342: step 3140, loss 0.00105323, acc 1
2016-09-07T04:17:39.570507: step 3141, loss 0.019455, acc 0.98
2016-09-07T04:17:40.236840: step 3142, loss 0.226481, acc 0.92
2016-09-07T04:17:40.927171: step 3143, loss 0.024913, acc 0.98
2016-09-07T04:17:41.617274: step 3144, loss 0.045192, acc 0.98
2016-09-07T04:17:42.289229: step 3145, loss 0.0285202, acc 0.98
2016-09-07T04:17:42.993334: step 3146, loss 0.0366187, acc 0.98
2016-09-07T04:17:43.662143: step 3147, loss 0.0759167, acc 0.98
2016-09-07T04:17:44.356921: step 3148, loss 0.0302798, acc 0.98
2016-09-07T04:17:45.047993: step 3149, loss 0.0109515, acc 1
2016-09-07T04:17:45.738106: step 3150, loss 0.00987717, acc 1
2016-09-07T04:17:46.432608: step 3151, loss 0.0695331, acc 0.96
2016-09-07T04:17:47.142506: step 3152, loss 0.0184955, acc 1
2016-09-07T04:17:47.842182: step 3153, loss 0.0147345, acc 1
2016-09-07T04:17:48.517892: step 3154, loss 0.0234689, acc 1
2016-09-07T04:17:49.217816: step 3155, loss 0.0836157, acc 0.96
2016-09-07T04:17:49.900740: step 3156, loss 0.0334235, acc 0.98
2016-09-07T04:17:50.596236: step 3157, loss 0.0739038, acc 0.98
2016-09-07T04:17:51.306546: step 3158, loss 0.0499995, acc 0.96
2016-09-07T04:17:51.996296: step 3159, loss 0.010344, acc 1
2016-09-07T04:17:52.698370: step 3160, loss 0.0345403, acc 1
2016-09-07T04:17:53.384867: step 3161, loss 0.0256704, acc 1
2016-09-07T04:17:54.112273: step 3162, loss 0.0796338, acc 0.96
2016-09-07T04:17:54.805922: step 3163, loss 0.0290031, acc 1
2016-09-07T04:17:55.503076: step 3164, loss 0.0307864, acc 1
2016-09-07T04:17:56.196858: step 3165, loss 0.00424644, acc 1
2016-09-07T04:17:56.865312: step 3166, loss 0.00342366, acc 1
2016-09-07T04:17:57.584043: step 3167, loss 0.0300225, acc 0.98
2016-09-07T04:17:58.258593: step 3168, loss 0.0251717, acc 1
2016-09-07T04:17:58.944723: step 3169, loss 0.0232682, acc 1
2016-09-07T04:17:59.620147: step 3170, loss 0.0797529, acc 0.96
2016-09-07T04:18:00.339909: step 3171, loss 0.0255213, acc 0.98
2016-09-07T04:18:01.030797: step 3172, loss 0.0474739, acc 0.96
2016-09-07T04:18:01.695860: step 3173, loss 0.0238366, acc 1
2016-09-07T04:18:02.401844: step 3174, loss 0.0268423, acc 0.98
2016-09-07T04:18:03.098336: step 3175, loss 0.0148974, acc 1
2016-09-07T04:18:03.797910: step 3176, loss 0.0231014, acc 1
2016-09-07T04:18:04.495571: step 3177, loss 0.0333571, acc 1
2016-09-07T04:18:05.168619: step 3178, loss 0.16817, acc 0.94
2016-09-07T04:18:05.860587: step 3179, loss 0.00265079, acc 1
2016-09-07T04:18:06.514355: step 3180, loss 0.249106, acc 0.9
2016-09-07T04:18:07.236052: step 3181, loss 0.0630335, acc 0.98
2016-09-07T04:18:07.910723: step 3182, loss 0.0437407, acc 0.98
2016-09-07T04:18:08.605317: step 3183, loss 0.0366797, acc 0.98
2016-09-07T04:18:09.318317: step 3184, loss 0.000574563, acc 1
2016-09-07T04:18:10.013698: step 3185, loss 0.0230566, acc 1
2016-09-07T04:18:10.723238: step 3186, loss 0.12868, acc 0.96
2016-09-07T04:18:11.383028: step 3187, loss 0.0196459, acc 1
2016-09-07T04:18:12.098628: step 3188, loss 0.0352968, acc 0.96
2016-09-07T04:18:12.784804: step 3189, loss 0.0530321, acc 0.98
2016-09-07T04:18:13.466431: step 3190, loss 0.136724, acc 0.94
2016-09-07T04:18:14.143333: step 3191, loss 0.0103994, acc 1
2016-09-07T04:18:14.828534: step 3192, loss 0.153985, acc 0.96
2016-09-07T04:18:15.514458: step 3193, loss 0.0360734, acc 0.98
2016-09-07T04:18:16.179306: step 3194, loss 0.0260848, acc 0.98
2016-09-07T04:18:16.861946: step 3195, loss 0.00219846, acc 1
2016-09-07T04:18:17.552838: step 3196, loss 0.015543, acc 1
2016-09-07T04:18:18.249342: step 3197, loss 0.0424321, acc 0.98
2016-09-07T04:18:18.936706: step 3198, loss 0.0886245, acc 0.96
2016-09-07T04:18:19.626012: step 3199, loss 0.0157836, acc 1
2016-09-07T04:18:20.348609: step 3200, loss 0.0618434, acc 0.96

Evaluation:
2016-09-07T04:18:23.523844: step 3200, loss 1.55501, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-3200

2016-09-07T04:18:25.258606: step 3201, loss 0.0825143, acc 0.94
2016-09-07T04:18:25.947567: step 3202, loss 0.0663115, acc 0.94
2016-09-07T04:18:26.630624: step 3203, loss 0.060358, acc 0.96
2016-09-07T04:18:27.340314: step 3204, loss 0.126802, acc 0.94
2016-09-07T04:18:28.024452: step 3205, loss 0.0217814, acc 1
2016-09-07T04:18:28.716149: step 3206, loss 0.0210817, acc 1
2016-09-07T04:18:29.377612: step 3207, loss 0.00968335, acc 1
2016-09-07T04:18:30.078946: step 3208, loss 0.108693, acc 0.98
2016-09-07T04:18:30.752544: step 3209, loss 0.169443, acc 0.96
2016-09-07T04:18:31.429419: step 3210, loss 0.0144206, acc 1
2016-09-07T04:18:32.114216: step 3211, loss 0.0200716, acc 1
2016-09-07T04:18:32.796933: step 3212, loss 0.0407201, acc 0.98
2016-09-07T04:18:33.493067: step 3213, loss 0.03685, acc 1
2016-09-07T04:18:34.162337: step 3214, loss 0.0722022, acc 0.98
2016-09-07T04:18:34.875164: step 3215, loss 0.0347729, acc 0.96
2016-09-07T04:18:35.561997: step 3216, loss 0.0102947, acc 1
2016-09-07T04:18:36.237430: step 3217, loss 0.0126713, acc 1
2016-09-07T04:18:36.939006: step 3218, loss 0.0240516, acc 0.98
2016-09-07T04:18:37.625916: step 3219, loss 0.063433, acc 0.96
2016-09-07T04:18:38.323580: step 3220, loss 0.0214708, acc 0.98
2016-09-07T04:18:38.996359: step 3221, loss 0.0645811, acc 0.96
2016-09-07T04:18:39.694490: step 3222, loss 0.00476715, acc 1
2016-09-07T04:18:40.351163: step 3223, loss 0.0486466, acc 0.98
2016-09-07T04:18:41.024231: step 3224, loss 0.0465526, acc 0.98
2016-09-07T04:18:41.689724: step 3225, loss 0.0432775, acc 0.96
2016-09-07T04:18:42.384109: step 3226, loss 0.0470559, acc 0.98
2016-09-07T04:18:43.091900: step 3227, loss 0.0211861, acc 1
2016-09-07T04:18:43.762045: step 3228, loss 0.00256212, acc 1
2016-09-07T04:18:44.457493: step 3229, loss 0.00567877, acc 1
2016-09-07T04:18:45.126750: step 3230, loss 0.015594, acc 1
2016-09-07T04:18:45.801985: step 3231, loss 0.0934732, acc 0.96
2016-09-07T04:18:46.501530: step 3232, loss 0.0219452, acc 0.98
2016-09-07T04:18:47.173883: step 3233, loss 0.0675222, acc 0.96
2016-09-07T04:18:47.844786: step 3234, loss 0.0411082, acc 0.98
2016-09-07T04:18:48.531121: step 3235, loss 0.0186152, acc 1
2016-09-07T04:18:49.227158: step 3236, loss 0.0376084, acc 0.98
2016-09-07T04:18:49.890623: step 3237, loss 0.0220989, acc 1
2016-09-07T04:18:50.578824: step 3238, loss 0.0791284, acc 0.94
2016-09-07T04:18:51.269539: step 3239, loss 0.102156, acc 0.96
2016-09-07T04:18:51.966907: step 3240, loss 0.0019686, acc 1
2016-09-07T04:18:52.616047: step 3241, loss 0.017696, acc 1
2016-09-07T04:18:53.285486: step 3242, loss 0.0623305, acc 0.98
2016-09-07T04:18:53.965097: step 3243, loss 0.0258629, acc 1
2016-09-07T04:18:54.654891: step 3244, loss 0.00236642, acc 1
2016-09-07T04:18:55.371064: step 3245, loss 0.0177957, acc 1
2016-09-07T04:18:56.040731: step 3246, loss 0.0364312, acc 0.96
2016-09-07T04:18:56.736157: step 3247, loss 0.0466253, acc 0.98
2016-09-07T04:18:57.417047: step 3248, loss 0.0273071, acc 0.98
2016-09-07T04:18:58.098571: step 3249, loss 0.0383376, acc 0.98
2016-09-07T04:18:58.818529: step 3250, loss 0.0828646, acc 0.94
2016-09-07T04:18:59.500755: step 3251, loss 0.0120426, acc 1
2016-09-07T04:19:00.210365: step 3252, loss 0.00588541, acc 1
2016-09-07T04:19:00.905719: step 3253, loss 0.0231835, acc 0.98
2016-09-07T04:19:01.596553: step 3254, loss 0.0272298, acc 0.98
2016-09-07T04:19:02.273609: step 3255, loss 0.0611698, acc 0.98
2016-09-07T04:19:02.949622: step 3256, loss 0.0292724, acc 0.98
2016-09-07T04:19:03.659797: step 3257, loss 0.0345817, acc 0.98
2016-09-07T04:19:04.339010: step 3258, loss 0.0222477, acc 1
2016-09-07T04:19:05.025298: step 3259, loss 0.0213579, acc 0.98
2016-09-07T04:19:05.725700: step 3260, loss 0.0391404, acc 0.98
2016-09-07T04:19:06.407463: step 3261, loss 0.0127446, acc 1
2016-09-07T04:19:07.087661: step 3262, loss 0.0237974, acc 0.98
2016-09-07T04:19:07.783334: step 3263, loss 0.000970602, acc 1
2016-09-07T04:19:08.449899: step 3264, loss 0.0159444, acc 0.977273
2016-09-07T04:19:09.137448: step 3265, loss 0.0489234, acc 0.98
2016-09-07T04:19:09.838553: step 3266, loss 0.128837, acc 0.96
2016-09-07T04:19:10.533526: step 3267, loss 0.0621588, acc 0.98
2016-09-07T04:19:11.210449: step 3268, loss 0.0313978, acc 0.98
2016-09-07T04:19:11.878556: step 3269, loss 0.0240911, acc 0.98
2016-09-07T04:19:12.581506: step 3270, loss 0.076272, acc 0.96
2016-09-07T04:19:13.291125: step 3271, loss 0.0132599, acc 1
2016-09-07T04:19:13.946557: step 3272, loss 0.0428082, acc 0.98
2016-09-07T04:19:14.644802: step 3273, loss 0.0071337, acc 1
2016-09-07T04:19:15.334136: step 3274, loss 0.00100105, acc 1
2016-09-07T04:19:16.011422: step 3275, loss 0.0224842, acc 1
2016-09-07T04:19:16.696491: step 3276, loss 0.0923802, acc 0.98
2016-09-07T04:19:17.403864: step 3277, loss 0.100914, acc 0.92
2016-09-07T04:19:18.101331: step 3278, loss 0.0119505, acc 1
2016-09-07T04:19:18.788748: step 3279, loss 0.0176185, acc 0.98
2016-09-07T04:19:19.469258: step 3280, loss 0.0297389, acc 0.98
2016-09-07T04:19:20.183466: step 3281, loss 0.121921, acc 0.94
2016-09-07T04:19:20.871985: step 3282, loss 0.026659, acc 1
2016-09-07T04:19:21.570969: step 3283, loss 0.0438255, acc 0.96
2016-09-07T04:19:22.234131: step 3284, loss 0.0170165, acc 1
2016-09-07T04:19:22.949529: step 3285, loss 0.0215575, acc 1
2016-09-07T04:19:23.626440: step 3286, loss 0.00329868, acc 1
2016-09-07T04:19:24.312884: step 3287, loss 0.0273698, acc 1
2016-09-07T04:19:25.033887: step 3288, loss 0.0310591, acc 1
2016-09-07T04:19:25.723043: step 3289, loss 0.000883952, acc 1
2016-09-07T04:19:26.411067: step 3290, loss 0.000494073, acc 1
2016-09-07T04:19:27.092084: step 3291, loss 0.0283633, acc 0.98
2016-09-07T04:19:27.790886: step 3292, loss 0.0114193, acc 1
2016-09-07T04:19:28.467369: step 3293, loss 0.0276336, acc 1
2016-09-07T04:19:29.144488: step 3294, loss 0.0315271, acc 0.98
2016-09-07T04:19:29.845517: step 3295, loss 0.00321989, acc 1
2016-09-07T04:19:30.539908: step 3296, loss 0.0425544, acc 0.98
2016-09-07T04:19:31.212442: step 3297, loss 0.00113639, acc 1
2016-09-07T04:19:31.873484: step 3298, loss 0.0231599, acc 0.98
2016-09-07T04:19:32.597834: step 3299, loss 0.0293511, acc 0.98
2016-09-07T04:19:33.272055: step 3300, loss 0.024205, acc 1

Evaluation:
2016-09-07T04:19:36.451414: step 3300, loss 1.82302, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-3300

2016-09-07T04:19:38.237246: step 3301, loss 0.0522447, acc 0.96
2016-09-07T04:19:38.932034: step 3302, loss 0.0111004, acc 1
2016-09-07T04:19:39.627845: step 3303, loss 0.00229789, acc 1
2016-09-07T04:19:40.299336: step 3304, loss 0.00120341, acc 1
2016-09-07T04:19:40.999202: step 3305, loss 0.0556044, acc 0.98
2016-09-07T04:19:41.710017: step 3306, loss 0.00615158, acc 1
2016-09-07T04:19:42.386505: step 3307, loss 0.0647328, acc 0.98
2016-09-07T04:19:43.071113: step 3308, loss 0.0365059, acc 0.98
2016-09-07T04:19:43.748632: step 3309, loss 0.011517, acc 1
2016-09-07T04:19:44.427906: step 3310, loss 0.0488996, acc 0.98
2016-09-07T04:19:45.093090: step 3311, loss 0.00729168, acc 1
2016-09-07T04:19:45.803418: step 3312, loss 0.0665301, acc 0.96
2016-09-07T04:19:46.484969: step 3313, loss 0.0228448, acc 1
2016-09-07T04:19:47.155549: step 3314, loss 0.0522221, acc 0.96
2016-09-07T04:19:47.831747: step 3315, loss 0.0468189, acc 0.98
2016-09-07T04:19:48.512429: step 3316, loss 0.00434258, acc 1
2016-09-07T04:19:49.195485: step 3317, loss 0.0362651, acc 0.96
2016-09-07T04:19:49.875756: step 3318, loss 0.00646724, acc 1
2016-09-07T04:19:50.607986: step 3319, loss 0.0181172, acc 1
2016-09-07T04:19:51.299793: step 3320, loss 0.0399432, acc 0.96
2016-09-07T04:19:51.992937: step 3321, loss 0.0496065, acc 0.98
2016-09-07T04:19:52.680111: step 3322, loss 0.00773537, acc 1
2016-09-07T04:19:53.371072: step 3323, loss 0.0276761, acc 0.98
2016-09-07T04:19:54.083599: step 3324, loss 0.0237119, acc 0.98
2016-09-07T04:19:54.745351: step 3325, loss 0.0322251, acc 0.98
2016-09-07T04:19:55.438371: step 3326, loss 0.00816077, acc 1
2016-09-07T04:19:56.146374: step 3327, loss 0.02339, acc 0.98
2016-09-07T04:19:56.822992: step 3328, loss 0.0478014, acc 0.98
2016-09-07T04:19:57.502707: step 3329, loss 0.0432696, acc 1
2016-09-07T04:19:58.195461: step 3330, loss 0.0337288, acc 0.98
2016-09-07T04:19:58.894485: step 3331, loss 0.0373925, acc 0.98
2016-09-07T04:19:59.562787: step 3332, loss 0.0437584, acc 0.98
2016-09-07T04:20:00.269617: step 3333, loss 0.0655797, acc 0.98
2016-09-07T04:20:00.953201: step 3334, loss 0.00217571, acc 1
2016-09-07T04:20:01.635220: step 3335, loss 0.0288356, acc 1
2016-09-07T04:20:02.318127: step 3336, loss 0.0474855, acc 0.98
2016-09-07T04:20:02.986660: step 3337, loss 0.00845601, acc 1
2016-09-07T04:20:03.679985: step 3338, loss 0.0213513, acc 1
2016-09-07T04:20:04.339369: step 3339, loss 0.0698051, acc 0.98
2016-09-07T04:20:05.056977: step 3340, loss 0.0641576, acc 0.96
2016-09-07T04:20:05.746424: step 3341, loss 0.0376715, acc 1
2016-09-07T04:20:06.430756: step 3342, loss 0.000896586, acc 1
2016-09-07T04:20:07.103891: step 3343, loss 0.107218, acc 0.94
2016-09-07T04:20:07.793582: step 3344, loss 0.0713493, acc 0.98
2016-09-07T04:20:08.477800: step 3345, loss 0.00580117, acc 1
2016-09-07T04:20:09.165151: step 3346, loss 0.0213595, acc 1
2016-09-07T04:20:09.855811: step 3347, loss 0.0269243, acc 0.98
2016-09-07T04:20:10.524679: step 3348, loss 0.00691944, acc 1
2016-09-07T04:20:11.207052: step 3349, loss 0.0191353, acc 1
2016-09-07T04:20:11.902027: step 3350, loss 0.13636, acc 0.96
2016-09-07T04:20:12.604427: step 3351, loss 0.00271517, acc 1
2016-09-07T04:20:13.291543: step 3352, loss 0.0302344, acc 0.98
2016-09-07T04:20:13.954345: step 3353, loss 0.0218913, acc 0.98
2016-09-07T04:20:14.667317: step 3354, loss 0.0417519, acc 0.98
2016-09-07T04:20:15.371162: step 3355, loss 0.02514, acc 1
2016-09-07T04:20:16.066315: step 3356, loss 0.0315476, acc 1
2016-09-07T04:20:16.781204: step 3357, loss 0.00183994, acc 1
2016-09-07T04:20:17.449540: step 3358, loss 0.0178333, acc 1
2016-09-07T04:20:18.117978: step 3359, loss 0.0186883, acc 1
2016-09-07T04:20:18.797454: step 3360, loss 0.005453, acc 1
2016-09-07T04:20:19.499823: step 3361, loss 0.0425075, acc 0.98
2016-09-07T04:20:20.191772: step 3362, loss 0.0382009, acc 0.98
2016-09-07T04:20:20.866181: step 3363, loss 0.0114963, acc 1
2016-09-07T04:20:21.554999: step 3364, loss 0.0167725, acc 1
2016-09-07T04:20:22.236924: step 3365, loss 0.0245317, acc 1
2016-09-07T04:20:22.950650: step 3366, loss 0.0654614, acc 0.96
2016-09-07T04:20:23.614262: step 3367, loss 0.0714377, acc 0.98
2016-09-07T04:20:24.311190: step 3368, loss 0.0115805, acc 1
2016-09-07T04:20:24.992866: step 3369, loss 0.0221382, acc 1
2016-09-07T04:20:25.664602: step 3370, loss 0.0129222, acc 1
2016-09-07T04:20:26.372925: step 3371, loss 0.00858318, acc 1
2016-09-07T04:20:27.054341: step 3372, loss 0.0119012, acc 1
2016-09-07T04:20:27.715056: step 3373, loss 0.0758547, acc 0.98
2016-09-07T04:20:28.381123: step 3374, loss 0.0388562, acc 0.98
2016-09-07T04:20:29.085415: step 3375, loss 0.0191007, acc 1
2016-09-07T04:20:29.779113: step 3376, loss 0.0539001, acc 0.98
2016-09-07T04:20:30.467660: step 3377, loss 0.00904201, acc 1
2016-09-07T04:20:31.140333: step 3378, loss 0.0149849, acc 1
2016-09-07T04:20:31.833578: step 3379, loss 0.0498246, acc 0.98
2016-09-07T04:20:32.525075: step 3380, loss 0.0525196, acc 0.96
2016-09-07T04:20:33.199550: step 3381, loss 0.00311831, acc 1
2016-09-07T04:20:33.885693: step 3382, loss 0.0299526, acc 0.98
2016-09-07T04:20:34.559931: step 3383, loss 0.000621188, acc 1
2016-09-07T04:20:35.234874: step 3384, loss 0.00320815, acc 1
2016-09-07T04:20:35.921312: step 3385, loss 0.0101535, acc 1
2016-09-07T04:20:36.619888: step 3386, loss 0.00418224, acc 1
2016-09-07T04:20:37.296437: step 3387, loss 2.30226e-05, acc 1
2016-09-07T04:20:37.982819: step 3388, loss 0.00357609, acc 1
2016-09-07T04:20:38.697513: step 3389, loss 0.0146527, acc 1
2016-09-07T04:20:39.390711: step 3390, loss 0.184352, acc 0.94
2016-09-07T04:20:40.083443: step 3391, loss 0.022131, acc 0.98
2016-09-07T04:20:40.764771: step 3392, loss 0.134877, acc 0.98
2016-09-07T04:20:41.460860: step 3393, loss 0.0565998, acc 0.98
2016-09-07T04:20:42.154463: step 3394, loss 0.00281937, acc 1
2016-09-07T04:20:42.828911: step 3395, loss 0.0398166, acc 0.98
2016-09-07T04:20:43.524032: step 3396, loss 0.158454, acc 0.96
2016-09-07T04:20:44.210620: step 3397, loss 0.0126347, acc 1
2016-09-07T04:20:44.898154: step 3398, loss 0.0307463, acc 1
2016-09-07T04:20:45.587974: step 3399, loss 0.0285328, acc 0.98
2016-09-07T04:20:46.268921: step 3400, loss 0.175887, acc 0.96

Evaluation:
2016-09-07T04:20:49.468993: step 3400, loss 2.08436, acc 0.713884

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-3400

2016-09-07T04:20:51.178914: step 3401, loss 0.056653, acc 0.98
2016-09-07T04:20:51.901637: step 3402, loss 0.000120236, acc 1
2016-09-07T04:20:52.580960: step 3403, loss 0.0690315, acc 0.94
2016-09-07T04:20:53.260978: step 3404, loss 0.0172224, acc 0.98
2016-09-07T04:20:53.941233: step 3405, loss 0.0212236, acc 0.98
2016-09-07T04:20:54.621558: step 3406, loss 0.0421585, acc 0.98
2016-09-07T04:20:55.310293: step 3407, loss 0.0226498, acc 1
2016-09-07T04:20:56.013433: step 3408, loss 0.0434171, acc 0.98
2016-09-07T04:20:56.705164: step 3409, loss 0.0472572, acc 0.94
2016-09-07T04:20:57.372301: step 3410, loss 0.0743468, acc 0.98
2016-09-07T04:20:58.069519: step 3411, loss 0.0420863, acc 0.98
2016-09-07T04:20:58.766083: step 3412, loss 0.0108768, acc 1
2016-09-07T04:20:59.437184: step 3413, loss 0.000253438, acc 1
2016-09-07T04:21:00.107503: step 3414, loss 0.0148144, acc 1
2016-09-07T04:21:00.823227: step 3415, loss 0.0337105, acc 0.98
2016-09-07T04:21:01.521000: step 3416, loss 0.0328528, acc 0.96
2016-09-07T04:21:02.181328: step 3417, loss 0.0906989, acc 0.98
2016-09-07T04:21:02.862166: step 3418, loss 0.056439, acc 0.98
2016-09-07T04:21:03.537630: step 3419, loss 0.0761124, acc 0.94
2016-09-07T04:21:04.203127: step 3420, loss 0.056576, acc 0.98
2016-09-07T04:21:04.890209: step 3421, loss 0.0332639, acc 0.98
2016-09-07T04:21:05.573196: step 3422, loss 0.0200805, acc 0.98
2016-09-07T04:21:06.270237: step 3423, loss 0.0311623, acc 0.98
2016-09-07T04:21:06.952376: step 3424, loss 0.0256535, acc 0.98
2016-09-07T04:21:07.646220: step 3425, loss 0.0440482, acc 0.96
2016-09-07T04:21:08.350609: step 3426, loss 0.0212086, acc 1
2016-09-07T04:21:09.025389: step 3427, loss 0.0593544, acc 0.96
2016-09-07T04:21:09.701647: step 3428, loss 0.0332094, acc 0.98
2016-09-07T04:21:10.366676: step 3429, loss 0.00442898, acc 1
2016-09-07T04:21:11.058413: step 3430, loss 0.0533955, acc 0.98
2016-09-07T04:21:11.727311: step 3431, loss 0.00117896, acc 1
2016-09-07T04:21:12.427052: step 3432, loss 0.0173799, acc 1
2016-09-07T04:21:13.111131: step 3433, loss 0.0222905, acc 0.98
2016-09-07T04:21:13.846695: step 3434, loss 0.030161, acc 0.98
2016-09-07T04:21:14.530205: step 3435, loss 0.0504289, acc 0.96
2016-09-07T04:21:15.213613: step 3436, loss 0.0232398, acc 0.98
2016-09-07T04:21:15.915643: step 3437, loss 0.0528107, acc 0.98
2016-09-07T04:21:16.602685: step 3438, loss 0.125269, acc 0.98
2016-09-07T04:21:17.290515: step 3439, loss 0.04197, acc 0.98
2016-09-07T04:21:17.986287: step 3440, loss 0.0455628, acc 0.96
2016-09-07T04:21:18.676938: step 3441, loss 0.00423504, acc 1
2016-09-07T04:21:19.358991: step 3442, loss 0.108621, acc 0.96
2016-09-07T04:21:20.046203: step 3443, loss 0.020743, acc 1
2016-09-07T04:21:20.743865: step 3444, loss 0.034255, acc 0.98
2016-09-07T04:21:21.418742: step 3445, loss 0.0218586, acc 0.98
2016-09-07T04:21:22.106469: step 3446, loss 0.00668471, acc 1
2016-09-07T04:21:22.801730: step 3447, loss 0.00247576, acc 1
2016-09-07T04:21:23.501901: step 3448, loss 0.0318592, acc 1
2016-09-07T04:21:24.200690: step 3449, loss 0.228337, acc 0.98
2016-09-07T04:21:24.877806: step 3450, loss 0.0338164, acc 0.98
2016-09-07T04:21:25.592980: step 3451, loss 0.0283206, acc 0.98
2016-09-07T04:21:26.277201: step 3452, loss 0.00467526, acc 1
2016-09-07T04:21:26.981031: step 3453, loss 0.0430319, acc 0.96
2016-09-07T04:21:27.670222: step 3454, loss 0.0254171, acc 1
2016-09-07T04:21:28.356469: step 3455, loss 0.0319092, acc 1
2016-09-07T04:21:29.019746: step 3456, loss 0.0158313, acc 1
2016-09-07T04:21:29.709302: step 3457, loss 0.0127963, acc 1
2016-09-07T04:21:30.399685: step 3458, loss 0.155515, acc 0.96
2016-09-07T04:21:31.075277: step 3459, loss 0.0419362, acc 0.96
2016-09-07T04:21:31.751925: step 3460, loss 0.0186847, acc 1
2016-09-07T04:21:32.446186: step 3461, loss 0.00328909, acc 1
2016-09-07T04:21:33.116330: step 3462, loss 0.0414292, acc 0.98
2016-09-07T04:21:33.799222: step 3463, loss 0.0183953, acc 0.98
2016-09-07T04:21:34.488685: step 3464, loss 0.0193954, acc 1
2016-09-07T04:21:35.194124: step 3465, loss 0.0118605, acc 1
2016-09-07T04:21:35.886752: step 3466, loss 0.00650587, acc 1
2016-09-07T04:21:36.569368: step 3467, loss 0.0594105, acc 0.98
2016-09-07T04:21:37.244832: step 3468, loss 0.0249876, acc 1
2016-09-07T04:21:37.931480: step 3469, loss 0.0216724, acc 1
2016-09-07T04:21:38.617268: step 3470, loss 0.018344, acc 1
2016-09-07T04:21:39.292438: step 3471, loss 0.017095, acc 1
2016-09-07T04:21:40.007366: step 3472, loss 0.206284, acc 0.98
2016-09-07T04:21:40.712205: step 3473, loss 0.0332633, acc 0.98
2016-09-07T04:21:41.451623: step 3474, loss 0.00284857, acc 1
2016-09-07T04:21:42.145009: step 3475, loss 0.0112591, acc 1
2016-09-07T04:21:42.841263: step 3476, loss 0.00301996, acc 1
2016-09-07T04:21:43.523333: step 3477, loss 0.0342433, acc 0.98
2016-09-07T04:21:44.187402: step 3478, loss 0.00688026, acc 1
2016-09-07T04:21:44.886822: step 3479, loss 0.00813078, acc 1
2016-09-07T04:21:45.578963: step 3480, loss 0.0388684, acc 1
2016-09-07T04:21:46.281755: step 3481, loss 0.0471697, acc 0.98
2016-09-07T04:21:46.970036: step 3482, loss 0.00213256, acc 1
2016-09-07T04:21:47.652319: step 3483, loss 0.00759382, acc 1
2016-09-07T04:21:48.353414: step 3484, loss 0.00164395, acc 1
2016-09-07T04:21:49.011793: step 3485, loss 0.015589, acc 1
2016-09-07T04:21:49.726000: step 3486, loss 0.0164328, acc 0.98
2016-09-07T04:21:50.410332: step 3487, loss 0.0187041, acc 1
2016-09-07T04:21:51.094330: step 3488, loss 0.0423883, acc 0.98
2016-09-07T04:21:51.777784: step 3489, loss 0.00969676, acc 1
2016-09-07T04:21:52.471952: step 3490, loss 0.0573869, acc 0.96
2016-09-07T04:21:53.186146: step 3491, loss 0.0204678, acc 1
2016-09-07T04:21:53.873517: step 3492, loss 0.0340999, acc 0.98
2016-09-07T04:21:54.579834: step 3493, loss 0.0833693, acc 0.98
2016-09-07T04:21:55.256694: step 3494, loss 0.0290564, acc 0.98
2016-09-07T04:21:55.932428: step 3495, loss 0.000338217, acc 1
2016-09-07T04:21:56.615769: step 3496, loss 0.000768118, acc 1
2016-09-07T04:21:57.295602: step 3497, loss 0.0721704, acc 0.94
2016-09-07T04:21:58.003822: step 3498, loss 0.000394983, acc 1
2016-09-07T04:21:58.728909: step 3499, loss 0.0203645, acc 1
2016-09-07T04:21:59.401904: step 3500, loss 0.00462547, acc 1

Evaluation:
2016-09-07T04:22:02.628294: step 3500, loss 1.91586, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-3500

2016-09-07T04:22:04.297605: step 3501, loss 0.0360178, acc 0.98
2016-09-07T04:22:04.988470: step 3502, loss 0.00841322, acc 1
2016-09-07T04:22:05.686016: step 3503, loss 0.00434647, acc 1
2016-09-07T04:22:06.381283: step 3504, loss 0.0137752, acc 1
2016-09-07T04:22:07.041866: step 3505, loss 0.0136693, acc 1
2016-09-07T04:22:07.755980: step 3506, loss 0.0027175, acc 1
2016-09-07T04:22:08.450206: step 3507, loss 0.036736, acc 1
2016-09-07T04:22:09.152745: step 3508, loss 0.0153089, acc 1
2016-09-07T04:22:09.846961: step 3509, loss 0.00240416, acc 1
2016-09-07T04:22:10.523064: step 3510, loss 0.0125064, acc 1
2016-09-07T04:22:11.219756: step 3511, loss 0.00524003, acc 1
2016-09-07T04:22:11.901777: step 3512, loss 0.0392958, acc 0.98
2016-09-07T04:22:12.630618: step 3513, loss 0.0258268, acc 0.98
2016-09-07T04:22:13.327139: step 3514, loss 0.0562239, acc 0.98
2016-09-07T04:22:14.019382: step 3515, loss 0.059592, acc 0.96
2016-09-07T04:22:14.701048: step 3516, loss 0.0128126, acc 1
2016-09-07T04:22:15.361993: step 3517, loss 0.0533865, acc 0.98
2016-09-07T04:22:16.085891: step 3518, loss 0.00218924, acc 1
2016-09-07T04:22:16.767760: step 3519, loss 0.0193419, acc 1
2016-09-07T04:22:17.452874: step 3520, loss 0.0509144, acc 0.96
2016-09-07T04:22:18.152886: step 3521, loss 0.012701, acc 1
2016-09-07T04:22:18.859530: step 3522, loss 0.081934, acc 0.96
2016-09-07T04:22:19.556641: step 3523, loss 0.00306278, acc 1
2016-09-07T04:22:20.227015: step 3524, loss 0.0302481, acc 0.98
2016-09-07T04:22:20.923238: step 3525, loss 0.170983, acc 0.98
2016-09-07T04:22:21.616082: step 3526, loss 0.00602077, acc 1
2016-09-07T04:22:22.312544: step 3527, loss 0.0925349, acc 0.98
2016-09-07T04:22:22.999190: step 3528, loss 0.012947, acc 1
2016-09-07T04:22:23.700402: step 3529, loss 4.86441e-05, acc 1
2016-09-07T04:22:24.391050: step 3530, loss 0.0732858, acc 0.96
2016-09-07T04:22:25.056512: step 3531, loss 0.0136729, acc 1
2016-09-07T04:22:25.750402: step 3532, loss 0.102558, acc 0.96
2016-09-07T04:22:26.425744: step 3533, loss 0.0144913, acc 1
2016-09-07T04:22:27.113316: step 3534, loss 0.0503618, acc 0.98
2016-09-07T04:22:27.805862: step 3535, loss 0.0115856, acc 1
2016-09-07T04:22:28.495556: step 3536, loss 0.0832423, acc 0.98
2016-09-07T04:22:29.193184: step 3537, loss 0.00803379, acc 1
2016-09-07T04:22:29.870826: step 3538, loss 0.00126692, acc 1
2016-09-07T04:22:30.574842: step 3539, loss 0.00668667, acc 1
2016-09-07T04:22:31.270674: step 3540, loss 0.0135278, acc 1
2016-09-07T04:22:31.956247: step 3541, loss 0.0444879, acc 0.98
2016-09-07T04:22:32.656282: step 3542, loss 0.00648287, acc 1
2016-09-07T04:22:33.347423: step 3543, loss 0.00620875, acc 1
2016-09-07T04:22:34.032625: step 3544, loss 0.0363011, acc 1
2016-09-07T04:22:34.689144: step 3545, loss 0.00795909, acc 1
2016-09-07T04:22:35.377057: step 3546, loss 0.0357751, acc 0.98
2016-09-07T04:22:36.083693: step 3547, loss 0.0116473, acc 1
2016-09-07T04:22:36.763718: step 3548, loss 0.00952725, acc 1
2016-09-07T04:22:37.460703: step 3549, loss 0.0162147, acc 0.98
2016-09-07T04:22:38.152679: step 3550, loss 0.00754638, acc 1
2016-09-07T04:22:38.860512: step 3551, loss 0.0142402, acc 1
2016-09-07T04:22:39.541466: step 3552, loss 0.0339249, acc 0.98
2016-09-07T04:22:40.228939: step 3553, loss 0.0131026, acc 1
2016-09-07T04:22:40.912647: step 3554, loss 0.063212, acc 0.98
2016-09-07T04:22:41.600307: step 3555, loss 0.135918, acc 0.96
2016-09-07T04:22:42.276963: step 3556, loss 0.00445635, acc 1
2016-09-07T04:22:42.944090: step 3557, loss 0.0576552, acc 0.98
2016-09-07T04:22:43.641836: step 3558, loss 0.0274852, acc 0.98
2016-09-07T04:22:44.309248: step 3559, loss 0.0905094, acc 0.94
2016-09-07T04:22:44.999459: step 3560, loss 0.0218461, acc 0.98
2016-09-07T04:22:45.695100: step 3561, loss 0.0810747, acc 0.98
2016-09-07T04:22:46.408853: step 3562, loss 0.0248625, acc 1
2016-09-07T04:22:47.091341: step 3563, loss 0.0401817, acc 0.98
2016-09-07T04:22:47.771743: step 3564, loss 0.0326745, acc 0.98
2016-09-07T04:22:48.475170: step 3565, loss 0.070292, acc 0.96
2016-09-07T04:22:49.135842: step 3566, loss 0.0165527, acc 1
2016-09-07T04:22:49.840047: step 3567, loss 0.026684, acc 1
2016-09-07T04:22:50.542284: step 3568, loss 0.0555843, acc 0.96
2016-09-07T04:22:51.215278: step 3569, loss 0.0914692, acc 0.98
2016-09-07T04:22:51.907475: step 3570, loss 0.00972887, acc 1
2016-09-07T04:22:52.573136: step 3571, loss 0.0371127, acc 0.98
2016-09-07T04:22:53.294290: step 3572, loss 0.0908534, acc 0.92
2016-09-07T04:22:53.976344: step 3573, loss 0.0110141, acc 1
2016-09-07T04:22:54.651799: step 3574, loss 0.0175957, acc 1
2016-09-07T04:22:55.361135: step 3575, loss 0.0175634, acc 1
2016-09-07T04:22:56.045869: step 3576, loss 0.0515827, acc 0.98
2016-09-07T04:22:56.715826: step 3577, loss 0.0292709, acc 1
2016-09-07T04:22:57.406911: step 3578, loss 0.079206, acc 0.98
2016-09-07T04:22:58.117145: step 3579, loss 0.0303822, acc 0.98
2016-09-07T04:22:58.810125: step 3580, loss 0.00927739, acc 1
2016-09-07T04:22:59.505405: step 3581, loss 0.0497256, acc 0.98
2016-09-07T04:23:00.207677: step 3582, loss 0.0238666, acc 1
2016-09-07T04:23:00.931286: step 3583, loss 0.0129253, acc 1
2016-09-07T04:23:01.632387: step 3584, loss 0.00899449, acc 1
2016-09-07T04:23:02.305840: step 3585, loss 0.0295701, acc 1
2016-09-07T04:23:02.987430: step 3586, loss 0.0190133, acc 1
2016-09-07T04:23:03.664972: step 3587, loss 0.0749726, acc 0.98
2016-09-07T04:23:04.349465: step 3588, loss 0.0638414, acc 0.96
2016-09-07T04:23:05.057886: step 3589, loss 0.00303592, acc 1
2016-09-07T04:23:05.722722: step 3590, loss 0.0125653, acc 1
2016-09-07T04:23:06.433348: step 3591, loss 0.0244974, acc 0.98
2016-09-07T04:23:07.116336: step 3592, loss 0.10006, acc 0.96
2016-09-07T04:23:07.809788: step 3593, loss 0.0629875, acc 0.96
2016-09-07T04:23:08.495858: step 3594, loss 0.00666609, acc 1
2016-09-07T04:23:09.189121: step 3595, loss 0.00832756, acc 1
2016-09-07T04:23:09.872559: step 3596, loss 0.0314665, acc 0.98
2016-09-07T04:23:10.566752: step 3597, loss 0.0236333, acc 0.98
2016-09-07T04:23:11.264246: step 3598, loss 0.00527963, acc 1
2016-09-07T04:23:11.922076: step 3599, loss 0.0304998, acc 1
2016-09-07T04:23:12.627066: step 3600, loss 0.0327825, acc 0.98

Evaluation:
2016-09-07T04:23:15.858907: step 3600, loss 1.74981, acc 0.731707

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-3600

2016-09-07T04:23:17.514881: step 3601, loss 0.0452441, acc 0.96
2016-09-07T04:23:18.216847: step 3602, loss 0.0295235, acc 0.98
2016-09-07T04:23:18.896418: step 3603, loss 0.0890167, acc 0.96
2016-09-07T04:23:19.584822: step 3604, loss 0.0773475, acc 0.98
2016-09-07T04:23:20.276214: step 3605, loss 0.0846887, acc 0.94
2016-09-07T04:23:21.023930: step 3606, loss 0.000537333, acc 1
2016-09-07T04:23:21.713115: step 3607, loss 0.0383979, acc 0.98
2016-09-07T04:23:22.399638: step 3608, loss 0.0251251, acc 1
2016-09-07T04:23:23.075497: step 3609, loss 0.0180391, acc 1
2016-09-07T04:23:23.763902: step 3610, loss 0.0756, acc 0.94
2016-09-07T04:23:24.453789: step 3611, loss 0.0579941, acc 0.98
2016-09-07T04:23:25.143903: step 3612, loss 0.000562768, acc 1
2016-09-07T04:23:25.846649: step 3613, loss 0.00233068, acc 1
2016-09-07T04:23:26.530629: step 3614, loss 0.0245349, acc 1
2016-09-07T04:23:27.223174: step 3615, loss 0.0268577, acc 0.98
2016-09-07T04:23:27.905943: step 3616, loss 0.116856, acc 0.96
2016-09-07T04:23:28.590480: step 3617, loss 0.0455016, acc 0.96
2016-09-07T04:23:29.293714: step 3618, loss 0.0194696, acc 0.98
2016-09-07T04:23:29.963413: step 3619, loss 0.000615829, acc 1
2016-09-07T04:23:30.673625: step 3620, loss 0.0114739, acc 1
2016-09-07T04:23:31.358161: step 3621, loss 0.0150925, acc 1
2016-09-07T04:23:32.038040: step 3622, loss 0.0337639, acc 0.98
2016-09-07T04:23:32.720841: step 3623, loss 0.000862431, acc 1
2016-09-07T04:23:33.420042: step 3624, loss 0.000780103, acc 1
2016-09-07T04:23:34.123767: step 3625, loss 0.0404997, acc 0.98
2016-09-07T04:23:34.807395: step 3626, loss 0.116417, acc 0.94
2016-09-07T04:23:35.514262: step 3627, loss 0.0296874, acc 0.98
2016-09-07T04:23:36.201960: step 3628, loss 0.119953, acc 0.98
2016-09-07T04:23:36.891311: step 3629, loss 0.00624114, acc 1
2016-09-07T04:23:37.591755: step 3630, loss 0.0162329, acc 1
2016-09-07T04:23:38.290862: step 3631, loss 0.00737145, acc 1
2016-09-07T04:23:38.985535: step 3632, loss 0.0487613, acc 0.98
2016-09-07T04:23:39.657539: step 3633, loss 0.0124648, acc 1
2016-09-07T04:23:40.344781: step 3634, loss 0.0357435, acc 0.98
2016-09-07T04:23:41.043777: step 3635, loss 0.0319261, acc 0.98
2016-09-07T04:23:41.728035: step 3636, loss 0.0333581, acc 0.98
2016-09-07T04:23:42.406071: step 3637, loss 0.00570207, acc 1
2016-09-07T04:23:43.106241: step 3638, loss 0.0154308, acc 1
2016-09-07T04:23:43.811884: step 3639, loss 0.00516235, acc 1
2016-09-07T04:23:44.500909: step 3640, loss 0.039621, acc 0.98
2016-09-07T04:23:45.180359: step 3641, loss 0.0811816, acc 0.96
2016-09-07T04:23:45.875742: step 3642, loss 0.0642686, acc 0.98
2016-09-07T04:23:46.552910: step 3643, loss 0.171456, acc 0.96
2016-09-07T04:23:47.245254: step 3644, loss 0.0394855, acc 0.98
2016-09-07T04:23:47.935979: step 3645, loss 0.0344176, acc 0.98
2016-09-07T04:23:48.642611: step 3646, loss 0.0313519, acc 1
2016-09-07T04:23:49.315154: step 3647, loss 0.0831496, acc 0.94
2016-09-07T04:23:49.962982: step 3648, loss 0.00487406, acc 1
2016-09-07T04:23:50.643101: step 3649, loss 0.0373149, acc 0.98
2016-09-07T04:23:51.328332: step 3650, loss 0.00990213, acc 1
2016-09-07T04:23:52.019446: step 3651, loss 0.031087, acc 1
2016-09-07T04:23:52.704166: step 3652, loss 0.0106625, acc 1
2016-09-07T04:23:53.422668: step 3653, loss 0.0401924, acc 0.98
2016-09-07T04:23:54.107366: step 3654, loss 0.00615243, acc 1
2016-09-07T04:23:54.782480: step 3655, loss 0.0102079, acc 1
2016-09-07T04:23:55.457796: step 3656, loss 0.0263371, acc 0.98
2016-09-07T04:23:56.145305: step 3657, loss 0.0442738, acc 0.96
2016-09-07T04:23:56.841237: step 3658, loss 0.120268, acc 0.96
2016-09-07T04:23:57.510973: step 3659, loss 0.00996922, acc 1
2016-09-07T04:23:58.210250: step 3660, loss 0.00620735, acc 1
2016-09-07T04:23:58.866117: step 3661, loss 0.04815, acc 0.98
2016-09-07T04:23:59.577906: step 3662, loss 0.038187, acc 0.98
2016-09-07T04:24:00.271112: step 3663, loss 0.00324746, acc 1
2016-09-07T04:24:00.968035: step 3664, loss 0.00975331, acc 1
2016-09-07T04:24:01.658699: step 3665, loss 0.0751968, acc 0.96
2016-09-07T04:24:02.343644: step 3666, loss 0.02885, acc 0.98
2016-09-07T04:24:03.050382: step 3667, loss 0.013272, acc 1
2016-09-07T04:24:03.740143: step 3668, loss 0.00188201, acc 1
2016-09-07T04:24:04.417591: step 3669, loss 0.0473777, acc 0.96
2016-09-07T04:24:05.116208: step 3670, loss 0.0389391, acc 1
2016-09-07T04:24:05.804468: step 3671, loss 0.0314462, acc 0.98
2016-09-07T04:24:06.510250: step 3672, loss 0.0088607, acc 1
2016-09-07T04:24:07.168449: step 3673, loss 0.080067, acc 0.94
2016-09-07T04:24:07.874551: step 3674, loss 0.00364052, acc 1
2016-09-07T04:24:08.551280: step 3675, loss 0.0125552, acc 1
2016-09-07T04:24:09.222936: step 3676, loss 0.00281616, acc 1
2016-09-07T04:24:09.916338: step 3677, loss 0.0495206, acc 0.96
2016-09-07T04:24:10.622775: step 3678, loss 0.0160025, acc 0.98
2016-09-07T04:24:11.381907: step 3679, loss 0.0400227, acc 0.98
2016-09-07T04:24:12.046740: step 3680, loss 0.225526, acc 0.96
2016-09-07T04:24:12.751744: step 3681, loss 0.0119752, acc 1
2016-09-07T04:24:13.427615: step 3682, loss 0.104363, acc 0.96
2016-09-07T04:24:14.114155: step 3683, loss 0.00122236, acc 1
2016-09-07T04:24:14.809486: step 3684, loss 0.0158533, acc 0.98
2016-09-07T04:24:15.489044: step 3685, loss 0.0269929, acc 0.98
2016-09-07T04:24:16.225566: step 3686, loss 0.0571481, acc 0.98
2016-09-07T04:24:16.917932: step 3687, loss 0.0017206, acc 1
2016-09-07T04:24:17.609482: step 3688, loss 0.0158157, acc 1
2016-09-07T04:24:18.328295: step 3689, loss 0.0163611, acc 1
2016-09-07T04:24:19.037392: step 3690, loss 0.00184134, acc 1
2016-09-07T04:24:19.721190: step 3691, loss 0.023785, acc 1
2016-09-07T04:24:20.392911: step 3692, loss 0.000994119, acc 1
2016-09-07T04:24:21.118118: step 3693, loss 0.00111508, acc 1
2016-09-07T04:24:21.790915: step 3694, loss 0.0132451, acc 1
2016-09-07T04:24:22.488226: step 3695, loss 0.0229684, acc 0.98
2016-09-07T04:24:23.169697: step 3696, loss 0.018055, acc 1
2016-09-07T04:24:23.867346: step 3697, loss 0.0127602, acc 1
2016-09-07T04:24:24.556782: step 3698, loss 0.0573322, acc 0.98
2016-09-07T04:24:25.220876: step 3699, loss 0.0757459, acc 0.96
2016-09-07T04:24:25.933501: step 3700, loss 0.0155632, acc 0.98

Evaluation:
2016-09-07T04:24:29.109127: step 3700, loss 1.8047, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-3700

2016-09-07T04:24:30.833989: step 3701, loss 0.176417, acc 0.96
2016-09-07T04:24:31.522986: step 3702, loss 0.0161271, acc 1
2016-09-07T04:24:32.202751: step 3703, loss 0.016263, acc 0.98
2016-09-07T04:24:32.901976: step 3704, loss 0.0749271, acc 0.96
2016-09-07T04:24:33.571179: step 3705, loss 0.0224539, acc 0.98
2016-09-07T04:24:34.261060: step 3706, loss 0.00513929, acc 1
2016-09-07T04:24:34.935973: step 3707, loss 0.00596291, acc 1
2016-09-07T04:24:35.638822: step 3708, loss 0.022703, acc 0.98
2016-09-07T04:24:36.325641: step 3709, loss 0.0330471, acc 1
2016-09-07T04:24:37.010065: step 3710, loss 0.00745763, acc 1
2016-09-07T04:24:37.686430: step 3711, loss 0.100375, acc 0.96
2016-09-07T04:24:38.362305: step 3712, loss 0.0697536, acc 0.98
2016-09-07T04:24:39.062758: step 3713, loss 0.0824377, acc 0.94
2016-09-07T04:24:39.738168: step 3714, loss 0.0263525, acc 0.98
2016-09-07T04:24:40.432530: step 3715, loss 0.015628, acc 1
2016-09-07T04:24:41.126358: step 3716, loss 0.0191101, acc 1
2016-09-07T04:24:41.808010: step 3717, loss 0.0132237, acc 1
2016-09-07T04:24:42.501701: step 3718, loss 0.104005, acc 0.98
2016-09-07T04:24:43.199152: step 3719, loss 0.0530557, acc 0.98
2016-09-07T04:24:43.907315: step 3720, loss 0.0138316, acc 1
2016-09-07T04:24:44.591176: step 3721, loss 0.00033473, acc 1
2016-09-07T04:24:45.262944: step 3722, loss 0.00952087, acc 1
2016-09-07T04:24:45.947513: step 3723, loss 0.0556559, acc 0.98
2016-09-07T04:24:46.649789: step 3724, loss 0.033988, acc 0.98
2016-09-07T04:24:47.338657: step 3725, loss 0.0240474, acc 1
2016-09-07T04:24:48.026929: step 3726, loss 0.0012081, acc 1
2016-09-07T04:24:48.724729: step 3727, loss 0.0240964, acc 1
2016-09-07T04:24:49.406160: step 3728, loss 0.0238879, acc 0.98
2016-09-07T04:24:50.113486: step 3729, loss 0.02153, acc 0.98
2016-09-07T04:24:50.787721: step 3730, loss 0.0236175, acc 0.98
2016-09-07T04:24:51.478137: step 3731, loss 0.0494569, acc 0.96
2016-09-07T04:24:52.173378: step 3732, loss 0.0150359, acc 1
2016-09-07T04:24:52.866371: step 3733, loss 0.00352412, acc 1
2016-09-07T04:24:53.560560: step 3734, loss 0.0111237, acc 1
2016-09-07T04:24:54.237094: step 3735, loss 0.0155469, acc 1
2016-09-07T04:24:54.935910: step 3736, loss 0.00225699, acc 1
2016-09-07T04:24:55.633691: step 3737, loss 0.00951711, acc 1
2016-09-07T04:24:56.323265: step 3738, loss 0.0476715, acc 0.98
2016-09-07T04:24:56.989568: step 3739, loss 0.0147122, acc 1
2016-09-07T04:24:57.664784: step 3740, loss 0.00893995, acc 1
2016-09-07T04:24:58.355772: step 3741, loss 0.0679423, acc 0.98
2016-09-07T04:24:59.033376: step 3742, loss 0.0307234, acc 0.98
2016-09-07T04:24:59.728604: step 3743, loss 0.0955453, acc 0.96
2016-09-07T04:25:00.455998: step 3744, loss 0.00469365, acc 1
2016-09-07T04:25:01.150013: step 3745, loss 0.0131257, acc 1
2016-09-07T04:25:01.831338: step 3746, loss 0.0876083, acc 0.96
2016-09-07T04:25:02.487907: step 3747, loss 0.00138837, acc 1
2016-09-07T04:25:03.182494: step 3748, loss 0.0314396, acc 0.98
2016-09-07T04:25:03.851660: step 3749, loss 0.0182002, acc 0.98
2016-09-07T04:25:04.527425: step 3750, loss 0.0503791, acc 0.98
2016-09-07T04:25:05.200715: step 3751, loss 0.0634993, acc 0.98
2016-09-07T04:25:05.912417: step 3752, loss 0.00398819, acc 1
2016-09-07T04:25:06.605706: step 3753, loss 0.0183833, acc 1
2016-09-07T04:25:07.285599: step 3754, loss 0.0117059, acc 1
2016-09-07T04:25:08.005501: step 3755, loss 0.111132, acc 0.96
2016-09-07T04:25:08.676620: step 3756, loss 0.0534019, acc 0.96
2016-09-07T04:25:09.356978: step 3757, loss 0.0455306, acc 0.98
2016-09-07T04:25:10.060186: step 3758, loss 0.0142637, acc 1
2016-09-07T04:25:10.755732: step 3759, loss 0.0649073, acc 0.94
2016-09-07T04:25:11.445190: step 3760, loss 0.00980109, acc 1
2016-09-07T04:25:12.107684: step 3761, loss 0.0413602, acc 0.98
2016-09-07T04:25:12.819539: step 3762, loss 0.0163778, acc 1
2016-09-07T04:25:13.497589: step 3763, loss 0.0123362, acc 1
2016-09-07T04:25:14.190919: step 3764, loss 0.0180111, acc 0.98
2016-09-07T04:25:14.871128: step 3765, loss 0.0426809, acc 0.98
2016-09-07T04:25:15.560520: step 3766, loss 0.0350941, acc 0.96
2016-09-07T04:25:16.258749: step 3767, loss 0.0409917, acc 0.98
2016-09-07T04:25:16.945387: step 3768, loss 0.0462179, acc 0.98
2016-09-07T04:25:17.655173: step 3769, loss 0.00765878, acc 1
2016-09-07T04:25:18.340444: step 3770, loss 0.0561002, acc 0.96
2016-09-07T04:25:19.035782: step 3771, loss 0.012852, acc 1
2016-09-07T04:25:19.733190: step 3772, loss 0.0177005, acc 0.98
2016-09-07T04:25:20.421443: step 3773, loss 0.0315928, acc 0.98
2016-09-07T04:25:21.130797: step 3774, loss 0.0350139, acc 0.98
2016-09-07T04:25:21.794054: step 3775, loss 0.0218107, acc 0.98
2016-09-07T04:25:22.487534: step 3776, loss 0.0347693, acc 0.98
2016-09-07T04:25:23.179223: step 3777, loss 0.0622203, acc 0.96
2016-09-07T04:25:23.856625: step 3778, loss 0.0521176, acc 0.98
2016-09-07T04:25:24.546736: step 3779, loss 0.0425153, acc 0.98
2016-09-07T04:25:25.247368: step 3780, loss 0.0998222, acc 0.98
2016-09-07T04:25:25.971927: step 3781, loss 0.00724549, acc 1
2016-09-07T04:25:26.666511: step 3782, loss 0.0106154, acc 1
2016-09-07T04:25:27.353074: step 3783, loss 0.0511597, acc 0.96
2016-09-07T04:25:28.029347: step 3784, loss 0.0309833, acc 0.98
2016-09-07T04:25:28.726346: step 3785, loss 0.0222184, acc 0.98
2016-09-07T04:25:29.413552: step 3786, loss 0.00822206, acc 1
2016-09-07T04:25:30.107033: step 3787, loss 0.00037349, acc 1
2016-09-07T04:25:30.816218: step 3788, loss 0.0472444, acc 0.96
2016-09-07T04:25:31.493349: step 3789, loss 0.0117453, acc 1
2016-09-07T04:25:32.152132: step 3790, loss 0.0313205, acc 1
2016-09-07T04:25:32.841361: step 3791, loss 0.013986, acc 1
2016-09-07T04:25:33.517426: step 3792, loss 0.00280342, acc 1
2016-09-07T04:25:34.192434: step 3793, loss 0.0209786, acc 0.98
2016-09-07T04:25:34.880879: step 3794, loss 0.00253182, acc 1
2016-09-07T04:25:35.597715: step 3795, loss 0.00525509, acc 1
2016-09-07T04:25:36.253831: step 3796, loss 0.0459934, acc 0.98
2016-09-07T04:25:36.949791: step 3797, loss 0.0224492, acc 1
2016-09-07T04:25:37.636677: step 3798, loss 0.0527807, acc 0.98
2016-09-07T04:25:38.333584: step 3799, loss 0.0491281, acc 0.98
2016-09-07T04:25:39.020348: step 3800, loss 0.123569, acc 0.96

Evaluation:
2016-09-07T04:25:42.233888: step 3800, loss 2.16049, acc 0.736398

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-3800

2016-09-07T04:25:43.909577: step 3801, loss 0.0379821, acc 0.98
2016-09-07T04:25:44.601941: step 3802, loss 0.0457283, acc 0.98
2016-09-07T04:25:45.311329: step 3803, loss 0.0864901, acc 0.96
2016-09-07T04:25:45.995518: step 3804, loss 0.0737875, acc 0.98
2016-09-07T04:25:46.685810: step 3805, loss 0.007091, acc 1
2016-09-07T04:25:47.369054: step 3806, loss 0.0256044, acc 0.98
2016-09-07T04:25:48.050364: step 3807, loss 0.0128629, acc 1
2016-09-07T04:25:48.743799: step 3808, loss 0.015536, acc 1
2016-09-07T04:25:49.427730: step 3809, loss 0.00936431, acc 1
2016-09-07T04:25:50.142641: step 3810, loss 0.0701795, acc 0.98
2016-09-07T04:25:50.826201: step 3811, loss 0.0663158, acc 0.96
2016-09-07T04:25:51.510551: step 3812, loss 0.0106437, acc 1
2016-09-07T04:25:52.201818: step 3813, loss 0.0380826, acc 0.98
2016-09-07T04:25:52.882140: step 3814, loss 0.115469, acc 0.96
2016-09-07T04:25:53.617409: step 3815, loss 0.0453476, acc 0.98
2016-09-07T04:25:54.281232: step 3816, loss 0.0120733, acc 1
2016-09-07T04:25:54.987616: step 3817, loss 0.076615, acc 0.98
2016-09-07T04:25:55.686352: step 3818, loss 0.0817626, acc 0.98
2016-09-07T04:25:56.364963: step 3819, loss 0.0157048, acc 1
2016-09-07T04:25:57.047414: step 3820, loss 0.0187681, acc 0.98
2016-09-07T04:25:57.787303: step 3821, loss 0.0354063, acc 0.98
2016-09-07T04:25:58.525436: step 3822, loss 0.0894953, acc 0.94
2016-09-07T04:25:59.209299: step 3823, loss 0.00503423, acc 1
2016-09-07T04:25:59.887902: step 3824, loss 0.0275809, acc 1
2016-09-07T04:26:00.604129: step 3825, loss 0.0120783, acc 1
2016-09-07T04:26:01.282560: step 3826, loss 0.166636, acc 0.9
2016-09-07T04:26:01.990544: step 3827, loss 0.0115928, acc 1
2016-09-07T04:26:02.645513: step 3828, loss 0.0429068, acc 0.98
2016-09-07T04:26:03.329298: step 3829, loss 0.0242716, acc 1
2016-09-07T04:26:04.005287: step 3830, loss 0.000761559, acc 1
2016-09-07T04:26:04.695299: step 3831, loss 0.00204833, acc 1
2016-09-07T04:26:05.394678: step 3832, loss 0.269557, acc 0.98
2016-09-07T04:26:06.093790: step 3833, loss 0.068433, acc 0.96
2016-09-07T04:26:06.781785: step 3834, loss 0.0150683, acc 1
2016-09-07T04:26:07.459146: step 3835, loss 0.0612054, acc 0.96
2016-09-07T04:26:08.195949: step 3836, loss 0.00260835, acc 1
2016-09-07T04:26:08.902849: step 3837, loss 0.0163077, acc 0.98
2016-09-07T04:26:09.586086: step 3838, loss 0.00665096, acc 1
2016-09-07T04:26:10.269302: step 3839, loss 0.0566737, acc 0.98
2016-09-07T04:26:10.919089: step 3840, loss 0.00373871, acc 1
2016-09-07T04:26:11.629917: step 3841, loss 0.0342946, acc 0.96
2016-09-07T04:26:12.292435: step 3842, loss 0.0956613, acc 0.96
2016-09-07T04:26:12.990696: step 3843, loss 0.0376409, acc 0.96
2016-09-07T04:26:13.687743: step 3844, loss 0.0341026, acc 0.98
2016-09-07T04:26:14.378083: step 3845, loss 0.0395453, acc 1
2016-09-07T04:26:15.084340: step 3846, loss 0.0270414, acc 0.98
2016-09-07T04:26:15.774088: step 3847, loss 0.0172528, acc 0.98
2016-09-07T04:26:16.466515: step 3848, loss 0.0166283, acc 0.98
2016-09-07T04:26:17.165365: step 3849, loss 0.000279563, acc 1
2016-09-07T04:26:17.854679: step 3850, loss 0.0146944, acc 1
2016-09-07T04:26:18.530374: step 3851, loss 0.0288017, acc 1
2016-09-07T04:26:19.226543: step 3852, loss 0.0925711, acc 0.96
2016-09-07T04:26:19.924151: step 3853, loss 0.025759, acc 1
2016-09-07T04:26:20.612606: step 3854, loss 0.00989592, acc 1
2016-09-07T04:26:21.310950: step 3855, loss 0.0220836, acc 0.98
2016-09-07T04:26:21.974701: step 3856, loss 0.00410789, acc 1
2016-09-07T04:26:22.671340: step 3857, loss 0.0417137, acc 0.96
2016-09-07T04:26:23.366630: step 3858, loss 0.0126989, acc 1
2016-09-07T04:26:24.076626: step 3859, loss 0.0139527, acc 1
2016-09-07T04:26:24.786799: step 3860, loss 0.0176851, acc 1
2016-09-07T04:26:25.443282: step 3861, loss 0.00972469, acc 1
2016-09-07T04:26:26.157480: step 3862, loss 0.0148144, acc 1
2016-09-07T04:26:26.821532: step 3863, loss 0.0130924, acc 1
2016-09-07T04:26:27.503742: step 3864, loss 0.0243357, acc 1
2016-09-07T04:26:28.203164: step 3865, loss 0.0162392, acc 1
2016-09-07T04:26:28.904709: step 3866, loss 0.0107857, acc 1
2016-09-07T04:26:29.599996: step 3867, loss 0.00664765, acc 1
2016-09-07T04:26:30.266260: step 3868, loss 0.0148833, acc 1
2016-09-07T04:26:30.952871: step 3869, loss 0.0327909, acc 0.98
2016-09-07T04:26:31.611329: step 3870, loss 0.0177479, acc 1
2016-09-07T04:26:32.284570: step 3871, loss 0.0137691, acc 1
2016-09-07T04:26:33.005756: step 3872, loss 0.0921919, acc 0.94
2016-09-07T04:26:33.705857: step 3873, loss 0.0594399, acc 0.98
2016-09-07T04:26:34.415307: step 3874, loss 0.00258853, acc 1
2016-09-07T04:26:35.127815: step 3875, loss 0.00307519, acc 1
2016-09-07T04:26:35.841407: step 3876, loss 0.0332492, acc 0.98
2016-09-07T04:26:36.531541: step 3877, loss 0.00749818, acc 1
2016-09-07T04:26:37.212101: step 3878, loss 0.0423045, acc 0.98
2016-09-07T04:26:37.887207: step 3879, loss 0.00967437, acc 1
2016-09-07T04:26:38.601618: step 3880, loss 0.0227222, acc 0.98
2016-09-07T04:26:39.316750: step 3881, loss 0.0554901, acc 0.96
2016-09-07T04:26:40.025302: step 3882, loss 0.050819, acc 0.98
2016-09-07T04:26:40.711650: step 3883, loss 0.00353901, acc 1
2016-09-07T04:26:41.396821: step 3884, loss 0.0014151, acc 1
2016-09-07T04:26:42.056904: step 3885, loss 0.00507142, acc 1
2016-09-07T04:26:42.740918: step 3886, loss 0.0309574, acc 0.98
2016-09-07T04:26:43.432664: step 3887, loss 0.050486, acc 0.96
2016-09-07T04:26:44.125768: step 3888, loss 0.0816467, acc 0.94
2016-09-07T04:26:44.780487: step 3889, loss 0.00203785, acc 1
2016-09-07T04:26:45.479863: step 3890, loss 0.128573, acc 0.96
2016-09-07T04:26:46.171973: step 3891, loss 0.0182103, acc 1
2016-09-07T04:26:46.852063: step 3892, loss 0.0022191, acc 1
2016-09-07T04:26:47.547921: step 3893, loss 0.0111689, acc 1
2016-09-07T04:26:48.243431: step 3894, loss 0.0289797, acc 0.98
2016-09-07T04:26:48.953008: step 3895, loss 0.00191345, acc 1
2016-09-07T04:26:49.617772: step 3896, loss 0.18169, acc 0.98
2016-09-07T04:26:50.314263: step 3897, loss 0.11012, acc 0.94
2016-09-07T04:26:51.001808: step 3898, loss 0.0323257, acc 0.98
2016-09-07T04:26:51.699047: step 3899, loss 0.138802, acc 0.96
2016-09-07T04:26:52.409576: step 3900, loss 0.00965275, acc 1

Evaluation:
2016-09-07T04:26:55.645380: step 3900, loss 1.92201, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-3900

2016-09-07T04:26:57.345476: step 3901, loss 0.0556693, acc 0.98
2016-09-07T04:26:58.015747: step 3902, loss 0.0215285, acc 0.98
2016-09-07T04:26:58.720164: step 3903, loss 0.0912197, acc 0.94
2016-09-07T04:26:59.404927: step 3904, loss 0.048375, acc 0.96
2016-09-07T04:27:00.099753: step 3905, loss 0.0837851, acc 0.94
2016-09-07T04:27:00.837261: step 3906, loss 0.0489064, acc 0.96
2016-09-07T04:27:01.498608: step 3907, loss 0.0274882, acc 0.98
2016-09-07T04:27:02.206370: step 3908, loss 0.0507581, acc 0.98
2016-09-07T04:27:02.897038: step 3909, loss 0.0130577, acc 1
2016-09-07T04:27:03.568616: step 3910, loss 0.0391922, acc 0.96
2016-09-07T04:27:04.250641: step 3911, loss 0.0272454, acc 1
2016-09-07T04:27:04.940706: step 3912, loss 0.106153, acc 0.96
2016-09-07T04:27:05.647834: step 3913, loss 0.0118563, acc 1
2016-09-07T04:27:06.318230: step 3914, loss 0.00967568, acc 1
2016-09-07T04:27:07.033418: step 3915, loss 0.0123039, acc 1
2016-09-07T04:27:07.708756: step 3916, loss 0.100322, acc 0.96
2016-09-07T04:27:08.406507: step 3917, loss 0.045096, acc 0.98
2016-09-07T04:27:09.089354: step 3918, loss 0.0106481, acc 1
2016-09-07T04:27:09.784496: step 3919, loss 0.0460134, acc 0.98
2016-09-07T04:27:10.488354: step 3920, loss 0.040942, acc 0.96
2016-09-07T04:27:11.157566: step 3921, loss 0.00446195, acc 1
2016-09-07T04:27:11.862039: step 3922, loss 0.0142125, acc 1
2016-09-07T04:27:12.524197: step 3923, loss 0.18212, acc 0.94
2016-09-07T04:27:13.207763: step 3924, loss 0.00497558, acc 1
2016-09-07T04:27:13.902730: step 3925, loss 0.0124239, acc 1
2016-09-07T04:27:14.592054: step 3926, loss 0.0609354, acc 0.96
2016-09-07T04:27:15.296621: step 3927, loss 0.0121921, acc 1
2016-09-07T04:27:15.951025: step 3928, loss 0.0329341, acc 0.98
2016-09-07T04:27:16.656809: step 3929, loss 0.0129516, acc 1
2016-09-07T04:27:17.363548: step 3930, loss 0.0124495, acc 1
2016-09-07T04:27:18.063615: step 3931, loss 0.0321861, acc 0.98
2016-09-07T04:27:18.758622: step 3932, loss 0.0503869, acc 0.96
2016-09-07T04:27:19.435209: step 3933, loss 0.00345144, acc 1
2016-09-07T04:27:20.116827: step 3934, loss 0.0134822, acc 1
2016-09-07T04:27:20.773000: step 3935, loss 0.0370405, acc 0.96
2016-09-07T04:27:21.477590: step 3936, loss 0.0181974, acc 0.98
2016-09-07T04:27:22.146513: step 3937, loss 0.0548142, acc 0.98
2016-09-07T04:27:22.826676: step 3938, loss 0.0389484, acc 1
2016-09-07T04:27:23.499673: step 3939, loss 0.0426119, acc 0.98
2016-09-07T04:27:24.182571: step 3940, loss 0.0189669, acc 0.98
2016-09-07T04:27:24.861220: step 3941, loss 0.0115261, acc 1
2016-09-07T04:27:25.546726: step 3942, loss 0.0449529, acc 0.98
2016-09-07T04:27:26.273609: step 3943, loss 0.0924454, acc 0.98
2016-09-07T04:27:26.951089: step 3944, loss 0.0518576, acc 0.98
2016-09-07T04:27:27.629036: step 3945, loss 0.0933195, acc 0.94
2016-09-07T04:27:28.317295: step 3946, loss 0.0051398, acc 1
2016-09-07T04:27:29.014729: step 3947, loss 0.0078843, acc 1
2016-09-07T04:27:29.719854: step 3948, loss 0.0103365, acc 1
2016-09-07T04:27:30.396597: step 3949, loss 0.0207784, acc 1
2016-09-07T04:27:31.102198: step 3950, loss 0.0441698, acc 0.98
2016-09-07T04:27:31.790442: step 3951, loss 0.0131829, acc 1
2016-09-07T04:27:32.467756: step 3952, loss 0.00518907, acc 1
2016-09-07T04:27:33.165134: step 3953, loss 0.0124557, acc 1
2016-09-07T04:27:33.835035: step 3954, loss 0.0738708, acc 0.96
2016-09-07T04:27:34.536211: step 3955, loss 0.0179724, acc 0.98
2016-09-07T04:27:35.201790: step 3956, loss 0.028028, acc 0.98
2016-09-07T04:27:35.910062: step 3957, loss 0.018165, acc 1
2016-09-07T04:27:36.612465: step 3958, loss 0.0325494, acc 0.98
2016-09-07T04:27:37.294367: step 3959, loss 0.0249549, acc 1
2016-09-07T04:27:37.982816: step 3960, loss 0.060058, acc 0.98
2016-09-07T04:27:38.658531: step 3961, loss 0.135705, acc 0.98
2016-09-07T04:27:39.344943: step 3962, loss 0.00868026, acc 1
2016-09-07T04:27:40.029400: step 3963, loss 0.000448843, acc 1
2016-09-07T04:27:40.735411: step 3964, loss 0.00155707, acc 1
2016-09-07T04:27:41.410109: step 3965, loss 0.0149321, acc 1
2016-09-07T04:27:42.081933: step 3966, loss 0.0151795, acc 0.98
2016-09-07T04:27:42.755165: step 3967, loss 0.106869, acc 0.94
2016-09-07T04:27:43.440788: step 3968, loss 0.0017866, acc 1
2016-09-07T04:27:44.121418: step 3969, loss 0.0167609, acc 1
2016-09-07T04:27:44.793260: step 3970, loss 0.0106964, acc 1
2016-09-07T04:27:45.505021: step 3971, loss 0.0784394, acc 0.96
2016-09-07T04:27:46.186798: step 3972, loss 0.0465708, acc 0.98
2016-09-07T04:27:46.855701: step 3973, loss 0.0833343, acc 0.98
2016-09-07T04:27:47.531596: step 3974, loss 0.0197483, acc 1
2016-09-07T04:27:48.218329: step 3975, loss 0.0170629, acc 1
2016-09-07T04:27:48.911114: step 3976, loss 0.07801, acc 0.98
2016-09-07T04:27:49.612335: step 3977, loss 0.026497, acc 0.98
2016-09-07T04:27:50.331126: step 3978, loss 0.027794, acc 0.98
2016-09-07T04:27:51.009210: step 3979, loss 0.00747066, acc 1
2016-09-07T04:27:51.691140: step 3980, loss 0.0240838, acc 1
2016-09-07T04:27:52.370019: step 3981, loss 0.174726, acc 0.96
2016-09-07T04:27:53.053922: step 3982, loss 0.094722, acc 0.96
2016-09-07T04:27:53.724992: step 3983, loss 0.0366198, acc 1
2016-09-07T04:27:54.396937: step 3984, loss 0.0119069, acc 1
2016-09-07T04:27:55.107238: step 3985, loss 0.0174216, acc 0.98
2016-09-07T04:27:55.785486: step 3986, loss 0.0187371, acc 1
2016-09-07T04:27:56.490583: step 3987, loss 0.0163233, acc 1
2016-09-07T04:27:57.195441: step 3988, loss 0.0638874, acc 0.98
2016-09-07T04:27:57.881669: step 3989, loss 0.041402, acc 0.98
2016-09-07T04:27:58.573621: step 3990, loss 0.0100133, acc 1
2016-09-07T04:27:59.263860: step 3991, loss 0.0629614, acc 0.96
2016-09-07T04:27:59.972372: step 3992, loss 0.123482, acc 0.94
2016-09-07T04:28:00.696737: step 3993, loss 0.0249397, acc 1
2016-09-07T04:28:01.391921: step 3994, loss 0.0212055, acc 1
2016-09-07T04:28:02.073457: step 3995, loss 0.0366696, acc 0.98
2016-09-07T04:28:02.755585: step 3996, loss 0.00474975, acc 1
2016-09-07T04:28:03.454075: step 3997, loss 0.0190726, acc 0.98
2016-09-07T04:28:04.144308: step 3998, loss 0.0443544, acc 1
2016-09-07T04:28:04.844181: step 3999, loss 0.0147515, acc 1
2016-09-07T04:28:05.536136: step 4000, loss 0.0464583, acc 0.98

Evaluation:
2016-09-07T04:28:08.737500: step 4000, loss 1.68003, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-4000

2016-09-07T04:28:10.474262: step 4001, loss 0.0266826, acc 0.98
2016-09-07T04:28:11.163351: step 4002, loss 0.110067, acc 0.96
2016-09-07T04:28:11.848569: step 4003, loss 0.00564442, acc 1
2016-09-07T04:28:12.524566: step 4004, loss 0.0244391, acc 0.98
2016-09-07T04:28:13.245939: step 4005, loss 0.00944001, acc 1
2016-09-07T04:28:13.956657: step 4006, loss 0.0530774, acc 0.96
2016-09-07T04:28:14.651023: step 4007, loss 0.00433874, acc 1
2016-09-07T04:28:15.331493: step 4008, loss 0.0101299, acc 1
2016-09-07T04:28:16.037973: step 4009, loss 0.0143921, acc 1
2016-09-07T04:28:16.748637: step 4010, loss 0.105417, acc 0.94
2016-09-07T04:28:17.424548: step 4011, loss 0.0731397, acc 0.92
2016-09-07T04:28:18.154398: step 4012, loss 0.0289932, acc 0.98
2016-09-07T04:28:18.853401: step 4013, loss 0.0182338, acc 0.98
2016-09-07T04:28:19.536189: step 4014, loss 0.0156646, acc 1
2016-09-07T04:28:20.210489: step 4015, loss 0.00950777, acc 1
2016-09-07T04:28:20.876912: step 4016, loss 0.0478946, acc 0.96
2016-09-07T04:28:21.589416: step 4017, loss 0.00189808, acc 1
2016-09-07T04:28:22.278966: step 4018, loss 0.120492, acc 0.94
2016-09-07T04:28:22.964560: step 4019, loss 0.0830716, acc 0.98
2016-09-07T04:28:23.650875: step 4020, loss 0.00666188, acc 1
2016-09-07T04:28:24.337059: step 4021, loss 0.010724, acc 1
2016-09-07T04:28:25.016096: step 4022, loss 0.000234074, acc 1
2016-09-07T04:28:25.691339: step 4023, loss 0.000545482, acc 1
2016-09-07T04:28:26.400233: step 4024, loss 0.0523391, acc 0.98
2016-09-07T04:28:27.106476: step 4025, loss 0.0487507, acc 0.96
2016-09-07T04:28:27.802375: step 4026, loss 0.113159, acc 0.98
2016-09-07T04:28:28.489646: step 4027, loss 0.0419977, acc 0.98
2016-09-07T04:28:29.172636: step 4028, loss 0.0177309, acc 0.98
2016-09-07T04:28:29.896490: step 4029, loss 0.0227954, acc 0.98
2016-09-07T04:28:30.599692: step 4030, loss 0.045676, acc 0.98
2016-09-07T04:28:31.299172: step 4031, loss 0.00501295, acc 1
2016-09-07T04:28:31.943994: step 4032, loss 0.0513242, acc 0.954545
2016-09-07T04:28:32.636949: step 4033, loss 0.04378, acc 0.98
2016-09-07T04:28:33.332806: step 4034, loss 0.00788768, acc 1
2016-09-07T04:28:34.027348: step 4035, loss 0.0729273, acc 0.96
2016-09-07T04:28:34.713911: step 4036, loss 0.00496044, acc 1
2016-09-07T04:28:35.379204: step 4037, loss 0.00323691, acc 1
2016-09-07T04:28:36.073240: step 4038, loss 0.0242124, acc 1
2016-09-07T04:28:36.764618: step 4039, loss 0.0248015, acc 0.98
2016-09-07T04:28:37.435887: step 4040, loss 0.0516041, acc 0.98
2016-09-07T04:28:38.132247: step 4041, loss 0.00204094, acc 1
2016-09-07T04:28:38.816633: step 4042, loss 0.0407468, acc 0.96
2016-09-07T04:28:39.537563: step 4043, loss 0.0111929, acc 1
2016-09-07T04:28:40.225116: step 4044, loss 0.109451, acc 0.98
2016-09-07T04:28:40.936823: step 4045, loss 0.0596703, acc 0.98
2016-09-07T04:28:41.660411: step 4046, loss 0.0246246, acc 0.98
2016-09-07T04:28:42.347121: step 4047, loss 0.0528102, acc 0.98
2016-09-07T04:28:43.020196: step 4048, loss 0.0168164, acc 1
2016-09-07T04:28:43.691292: step 4049, loss 0.00242727, acc 1
2016-09-07T04:28:44.385231: step 4050, loss 0.0033279, acc 1
2016-09-07T04:28:45.059449: step 4051, loss 0.0590588, acc 0.96
2016-09-07T04:28:45.757045: step 4052, loss 0.0990274, acc 0.98
2016-09-07T04:28:46.456250: step 4053, loss 0.0631423, acc 0.98
2016-09-07T04:28:47.147594: step 4054, loss 0.137752, acc 0.98
2016-09-07T04:28:47.845698: step 4055, loss 0.0128102, acc 1
2016-09-07T04:28:48.532566: step 4056, loss 0.0351944, acc 0.98
2016-09-07T04:28:49.248518: step 4057, loss 0.0163134, acc 1
2016-09-07T04:28:49.943166: step 4058, loss 0.00671972, acc 1
2016-09-07T04:28:50.635775: step 4059, loss 0.00814117, acc 1
2016-09-07T04:28:51.313848: step 4060, loss 0.0630621, acc 0.96
2016-09-07T04:28:51.990131: step 4061, loss 0.0398585, acc 0.98
2016-09-07T04:28:52.686545: step 4062, loss 0.00126453, acc 1
2016-09-07T04:28:53.342472: step 4063, loss 0.00283666, acc 1
2016-09-07T04:28:54.054687: step 4064, loss 0.0911313, acc 0.96
2016-09-07T04:28:54.714658: step 4065, loss 0.0049305, acc 1
2016-09-07T04:28:55.392354: step 4066, loss 0.0621406, acc 0.98
2016-09-07T04:28:56.088958: step 4067, loss 0.00748266, acc 1
2016-09-07T04:28:56.775137: step 4068, loss 0.0890245, acc 0.98
2016-09-07T04:28:57.449331: step 4069, loss 0.0117218, acc 1
2016-09-07T04:28:58.129226: step 4070, loss 0.0225439, acc 1
2016-09-07T04:28:58.872607: step 4071, loss 0.00384108, acc 1
2016-09-07T04:28:59.527065: step 4072, loss 0.0117407, acc 1
2016-09-07T04:29:00.214926: step 4073, loss 0.0669421, acc 0.96
2016-09-07T04:29:00.920870: step 4074, loss 0.0102106, acc 1
2016-09-07T04:29:01.603926: step 4075, loss 0.0395514, acc 0.98
2016-09-07T04:29:02.295735: step 4076, loss 0.0012361, acc 1
2016-09-07T04:29:02.973415: step 4077, loss 0.00821324, acc 1
2016-09-07T04:29:03.671546: step 4078, loss 0.041005, acc 0.98
2016-09-07T04:29:04.335237: step 4079, loss 0.0389989, acc 0.98
2016-09-07T04:29:05.011239: step 4080, loss 0.0404449, acc 0.98
2016-09-07T04:29:05.704034: step 4081, loss 0.0112049, acc 1
2016-09-07T04:29:06.396779: step 4082, loss 0.0312069, acc 0.98
2016-09-07T04:29:07.079539: step 4083, loss 0.0514219, acc 0.98
2016-09-07T04:29:07.775582: step 4084, loss 0.00998612, acc 1
2016-09-07T04:29:08.485465: step 4085, loss 0.118958, acc 0.96
2016-09-07T04:29:09.162498: step 4086, loss 0.000828834, acc 1
2016-09-07T04:29:09.854320: step 4087, loss 0.0179157, acc 0.98
2016-09-07T04:29:10.541903: step 4088, loss 0.0160372, acc 1
2016-09-07T04:29:11.221637: step 4089, loss 0.0350049, acc 0.96
2016-09-07T04:29:11.903276: step 4090, loss 0.0359995, acc 0.98
2016-09-07T04:29:12.592894: step 4091, loss 0.0436387, acc 0.98
2016-09-07T04:29:13.272652: step 4092, loss 0.0492612, acc 0.98
2016-09-07T04:29:13.959102: step 4093, loss 0.0711627, acc 0.96
2016-09-07T04:29:14.632716: step 4094, loss 0.0931574, acc 0.94
2016-09-07T04:29:15.314374: step 4095, loss 0.0222147, acc 1
2016-09-07T04:29:16.006950: step 4096, loss 0.0217668, acc 0.98
2016-09-07T04:29:16.711165: step 4097, loss 0.0169886, acc 1
2016-09-07T04:29:17.402053: step 4098, loss 0.0674628, acc 0.96
2016-09-07T04:29:18.107907: step 4099, loss 0.0477351, acc 0.96
2016-09-07T04:29:18.792663: step 4100, loss 0.000946681, acc 1

Evaluation:
2016-09-07T04:29:21.951758: step 4100, loss 1.70721, acc 0.731707

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-4100

2016-09-07T04:29:23.661155: step 4101, loss 0.0236685, acc 1
2016-09-07T04:29:24.334103: step 4102, loss 0.0771636, acc 0.98
2016-09-07T04:29:25.012999: step 4103, loss 0.011517, acc 1
2016-09-07T04:29:25.691237: step 4104, loss 0.0290098, acc 0.98
2016-09-07T04:29:26.386389: step 4105, loss 0.032049, acc 0.98
2016-09-07T04:29:27.042214: step 4106, loss 0.0108665, acc 1
2016-09-07T04:29:27.750160: step 4107, loss 0.101948, acc 0.92
2016-09-07T04:29:28.419249: step 4108, loss 0.0331458, acc 0.98
2016-09-07T04:29:29.092730: step 4109, loss 0.00833459, acc 1
2016-09-07T04:29:29.788359: step 4110, loss 0.00395326, acc 1
2016-09-07T04:29:30.474160: step 4111, loss 0.0800377, acc 0.96
2016-09-07T04:29:31.163382: step 4112, loss 0.00844522, acc 1
2016-09-07T04:29:31.830292: step 4113, loss 0.0204572, acc 1
2016-09-07T04:29:32.536493: step 4114, loss 0.0115217, acc 1
2016-09-07T04:29:33.224093: step 4115, loss 0.0403169, acc 0.98
2016-09-07T04:29:33.913068: step 4116, loss 0.00627999, acc 1
2016-09-07T04:29:34.588162: step 4117, loss 0.0214989, acc 0.98
2016-09-07T04:29:35.282996: step 4118, loss 0.00516266, acc 1
2016-09-07T04:29:35.958959: step 4119, loss 0.0271026, acc 0.98
2016-09-07T04:29:36.649625: step 4120, loss 0.0348349, acc 0.98
2016-09-07T04:29:37.362354: step 4121, loss 0.0160827, acc 1
2016-09-07T04:29:38.035316: step 4122, loss 0.0406091, acc 0.96
2016-09-07T04:29:38.715727: step 4123, loss 0.0303958, acc 1
2016-09-07T04:29:39.435583: step 4124, loss 0.00749153, acc 1
2016-09-07T04:29:40.120382: step 4125, loss 0.0145455, acc 1
2016-09-07T04:29:40.824165: step 4126, loss 0.0417583, acc 0.98
2016-09-07T04:29:41.494507: step 4127, loss 0.0129454, acc 1
2016-09-07T04:29:42.199626: step 4128, loss 0.00448622, acc 1
2016-09-07T04:29:42.895292: step 4129, loss 0.078627, acc 0.96
2016-09-07T04:29:43.571960: step 4130, loss 0.0688187, acc 0.96
2016-09-07T04:29:44.247762: step 4131, loss 0.0381785, acc 0.96
2016-09-07T04:29:44.926507: step 4132, loss 0.00732118, acc 1
2016-09-07T04:29:45.667670: step 4133, loss 0.0184304, acc 0.98
2016-09-07T04:29:46.334878: step 4134, loss 0.00667998, acc 1
2016-09-07T04:29:47.034781: step 4135, loss 0.0653865, acc 0.96
2016-09-07T04:29:47.720857: step 4136, loss 0.154605, acc 0.94
2016-09-07T04:29:48.418900: step 4137, loss 0.0134993, acc 1
2016-09-07T04:29:49.099826: step 4138, loss 0.0860412, acc 0.96
2016-09-07T04:29:49.784415: step 4139, loss 0.0291437, acc 0.98
2016-09-07T04:29:50.494068: step 4140, loss 0.00994266, acc 1
2016-09-07T04:29:51.165472: step 4141, loss 0.111758, acc 0.98
2016-09-07T04:29:51.867764: step 4142, loss 0.0899373, acc 0.96
2016-09-07T04:29:52.577117: step 4143, loss 0.0469484, acc 0.96
2016-09-07T04:29:53.278948: step 4144, loss 0.0174309, acc 0.98
2016-09-07T04:29:53.959431: step 4145, loss 0.0612672, acc 0.96
2016-09-07T04:29:54.649664: step 4146, loss 0.0460617, acc 0.96
2016-09-07T04:29:55.356393: step 4147, loss 0.0106988, acc 1
2016-09-07T04:29:56.021404: step 4148, loss 0.0728873, acc 0.96
2016-09-07T04:29:56.710006: step 4149, loss 0.0166624, acc 1
2016-09-07T04:29:57.389792: step 4150, loss 0.0100345, acc 1
2016-09-07T04:29:58.080723: step 4151, loss 0.00464201, acc 1
2016-09-07T04:29:58.768210: step 4152, loss 0.0558866, acc 0.98
2016-09-07T04:29:59.452438: step 4153, loss 0.0426155, acc 0.98
2016-09-07T04:30:00.160089: step 4154, loss 0.0656884, acc 0.98
2016-09-07T04:30:00.860848: step 4155, loss 0.00872512, acc 1
2016-09-07T04:30:01.544322: step 4156, loss 0.0440618, acc 0.98
2016-09-07T04:30:02.225503: step 4157, loss 0.0469268, acc 1
2016-09-07T04:30:02.920518: step 4158, loss 0.0603434, acc 0.96
2016-09-07T04:30:03.618999: step 4159, loss 0.0176554, acc 0.98
2016-09-07T04:30:04.286523: step 4160, loss 0.0306804, acc 0.98
2016-09-07T04:30:05.001137: step 4161, loss 0.00270949, acc 1
2016-09-07T04:30:05.670778: step 4162, loss 0.0307056, acc 0.98
2016-09-07T04:30:06.346873: step 4163, loss 0.0954357, acc 0.96
2016-09-07T04:30:07.016684: step 4164, loss 0.0381415, acc 0.98
2016-09-07T04:30:07.702754: step 4165, loss 0.00594025, acc 1
2016-09-07T04:30:08.380508: step 4166, loss 0.00288369, acc 1
2016-09-07T04:30:09.056156: step 4167, loss 0.00256315, acc 1
2016-09-07T04:30:09.742082: step 4168, loss 0.0241514, acc 1
2016-09-07T04:30:10.413839: step 4169, loss 0.020066, acc 0.98
2016-09-07T04:30:11.089683: step 4170, loss 0.000412857, acc 1
2016-09-07T04:30:11.781265: step 4171, loss 0.0137214, acc 1
2016-09-07T04:30:12.471390: step 4172, loss 0.0986292, acc 0.94
2016-09-07T04:30:13.172395: step 4173, loss 0.0414636, acc 0.98
2016-09-07T04:30:13.870558: step 4174, loss 0.00681164, acc 1
2016-09-07T04:30:14.579855: step 4175, loss 0.0412219, acc 0.96
2016-09-07T04:30:15.270480: step 4176, loss 0.209328, acc 0.94
2016-09-07T04:30:15.954072: step 4177, loss 0.00735903, acc 1
2016-09-07T04:30:16.645599: step 4178, loss 0.00402797, acc 1
2016-09-07T04:30:17.331929: step 4179, loss 0.0367983, acc 0.98
2016-09-07T04:30:18.033651: step 4180, loss 0.0117161, acc 1
2016-09-07T04:30:18.724147: step 4181, loss 0.0161708, acc 0.98
2016-09-07T04:30:19.430051: step 4182, loss 0.0695715, acc 0.98
2016-09-07T04:30:20.110899: step 4183, loss 0.0300429, acc 0.98
2016-09-07T04:30:20.813009: step 4184, loss 0.0256701, acc 0.98
2016-09-07T04:30:21.497564: step 4185, loss 0.053488, acc 0.96
2016-09-07T04:30:22.196020: step 4186, loss 0.0760775, acc 0.98
2016-09-07T04:30:22.883988: step 4187, loss 0.0401264, acc 1
2016-09-07T04:30:23.547208: step 4188, loss 0.0685972, acc 0.96
2016-09-07T04:30:24.250345: step 4189, loss 0.00421507, acc 1
2016-09-07T04:30:24.938115: step 4190, loss 0.0146903, acc 1
2016-09-07T04:30:25.632143: step 4191, loss 0.00180972, acc 1
2016-09-07T04:30:26.344163: step 4192, loss 0.050813, acc 0.96
2016-09-07T04:30:27.031392: step 4193, loss 0.0465274, acc 0.96
2016-09-07T04:30:27.731242: step 4194, loss 0.0922507, acc 0.96
2016-09-07T04:30:28.400034: step 4195, loss 0.0266274, acc 1
2016-09-07T04:30:29.102974: step 4196, loss 0.0178908, acc 1
2016-09-07T04:30:29.822489: step 4197, loss 0.0544478, acc 0.96
2016-09-07T04:30:30.536667: step 4198, loss 0.0372134, acc 0.98
2016-09-07T04:30:31.219802: step 4199, loss 0.00223735, acc 1
2016-09-07T04:30:31.901405: step 4200, loss 0.0586253, acc 0.98

Evaluation:
2016-09-07T04:30:35.119095: step 4200, loss 1.7313, acc 0.713884

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-4200

2016-09-07T04:30:36.778031: step 4201, loss 0.0108724, acc 1
2016-09-07T04:30:37.474070: step 4202, loss 0.0431038, acc 0.96
2016-09-07T04:30:38.166210: step 4203, loss 0.0382697, acc 0.98
2016-09-07T04:30:38.866580: step 4204, loss 0.00193436, acc 1
2016-09-07T04:30:39.565698: step 4205, loss 0.0436093, acc 0.98
2016-09-07T04:30:40.277751: step 4206, loss 0.0631303, acc 0.96
2016-09-07T04:30:40.997769: step 4207, loss 0.00851668, acc 1
2016-09-07T04:30:41.689714: step 4208, loss 0.0364623, acc 0.98
2016-09-07T04:30:42.365856: step 4209, loss 0.0475673, acc 0.96
2016-09-07T04:30:43.063770: step 4210, loss 0.0270257, acc 0.98
2016-09-07T04:30:43.760673: step 4211, loss 0.0202083, acc 0.98
2016-09-07T04:30:44.445321: step 4212, loss 0.00768861, acc 1
2016-09-07T04:30:45.134780: step 4213, loss 0.000988274, acc 1
2016-09-07T04:30:45.830434: step 4214, loss 0.0176456, acc 0.98
2016-09-07T04:30:46.520326: step 4215, loss 0.0158265, acc 1
2016-09-07T04:30:47.207150: step 4216, loss 0.0357621, acc 1
2016-09-07T04:30:47.919276: step 4217, loss 0.0358176, acc 0.96
2016-09-07T04:30:48.614725: step 4218, loss 0.0116317, acc 1
2016-09-07T04:30:49.306592: step 4219, loss 0.0425068, acc 0.98
2016-09-07T04:30:49.973866: step 4220, loss 0.000585082, acc 1
2016-09-07T04:30:50.688068: step 4221, loss 0.0187824, acc 1
2016-09-07T04:30:51.360359: step 4222, loss 0.0555854, acc 0.98
2016-09-07T04:30:52.055511: step 4223, loss 0.0436342, acc 0.98
2016-09-07T04:30:52.695902: step 4224, loss 0.0329082, acc 0.977273
2016-09-07T04:30:53.386746: step 4225, loss 0.0185535, acc 1
2016-09-07T04:30:54.091924: step 4226, loss 0.0096996, acc 1
2016-09-07T04:30:54.778506: step 4227, loss 0.0417087, acc 0.96
2016-09-07T04:30:55.498460: step 4228, loss 0.0192817, acc 1
2016-09-07T04:30:56.174630: step 4229, loss 0.0139206, acc 1
2016-09-07T04:30:56.850298: step 4230, loss 0.119055, acc 0.96
2016-09-07T04:30:57.547771: step 4231, loss 0.00112708, acc 1
2016-09-07T04:30:58.231396: step 4232, loss 0.0181986, acc 0.98
2016-09-07T04:30:58.916839: step 4233, loss 0.00112249, acc 1
2016-09-07T04:30:59.598744: step 4234, loss 0.0127473, acc 1
2016-09-07T04:31:00.350101: step 4235, loss 0.0245038, acc 0.98
2016-09-07T04:31:01.029522: step 4236, loss 0.013409, acc 1
2016-09-07T04:31:01.717291: step 4237, loss 0.0370591, acc 0.98
2016-09-07T04:31:02.399779: step 4238, loss 0.0444687, acc 0.98
2016-09-07T04:31:03.093583: step 4239, loss 0.00619606, acc 1
2016-09-07T04:31:03.792761: step 4240, loss 0.0456123, acc 0.96
2016-09-07T04:31:04.458132: step 4241, loss 0.000978388, acc 1
2016-09-07T04:31:05.167047: step 4242, loss 0.0113467, acc 1
2016-09-07T04:31:05.845533: step 4243, loss 0.0674268, acc 0.96
2016-09-07T04:31:06.537501: step 4244, loss 0.0164857, acc 0.98
2016-09-07T04:31:07.235219: step 4245, loss 0.00175164, acc 1
2016-09-07T04:31:07.935616: step 4246, loss 0.0738303, acc 0.96
2016-09-07T04:31:08.639721: step 4247, loss 0.0743703, acc 0.98
2016-09-07T04:31:09.319155: step 4248, loss 0.00480143, acc 1
2016-09-07T04:31:10.016561: step 4249, loss 0.0942959, acc 0.98
2016-09-07T04:31:10.698115: step 4250, loss 0.0161147, acc 1
2016-09-07T04:31:11.364741: step 4251, loss 0.0216257, acc 0.98
2016-09-07T04:31:12.035324: step 4252, loss 0.0222243, acc 0.98
2016-09-07T04:31:12.714917: step 4253, loss 0.00368389, acc 1
2016-09-07T04:31:13.416358: step 4254, loss 0.187807, acc 0.96
2016-09-07T04:31:14.086329: step 4255, loss 0.0742818, acc 0.98
2016-09-07T04:31:14.796790: step 4256, loss 0.13879, acc 0.94
2016-09-07T04:31:15.464587: step 4257, loss 0.0171035, acc 0.98
2016-09-07T04:31:16.177746: step 4258, loss 0.0665759, acc 0.98
2016-09-07T04:31:16.855600: step 4259, loss 0.0306564, acc 0.98
2016-09-07T04:31:17.572714: step 4260, loss 0.0171408, acc 0.98
2016-09-07T04:31:18.265222: step 4261, loss 0.000148173, acc 1
2016-09-07T04:31:18.926628: step 4262, loss 0.0167948, acc 1
2016-09-07T04:31:19.631031: step 4263, loss 0.00339717, acc 1
2016-09-07T04:31:20.316693: step 4264, loss 0.00758959, acc 1
2016-09-07T04:31:20.996260: step 4265, loss 0.0401652, acc 1
2016-09-07T04:31:21.698094: step 4266, loss 0.0279009, acc 1
2016-09-07T04:31:22.389431: step 4267, loss 0.00676529, acc 1
2016-09-07T04:31:23.091189: step 4268, loss 0.0073306, acc 1
2016-09-07T04:31:23.771764: step 4269, loss 0.0397903, acc 0.98
2016-09-07T04:31:24.462425: step 4270, loss 0.0421857, acc 0.98
2016-09-07T04:31:25.161344: step 4271, loss 0.0154582, acc 1
2016-09-07T04:31:25.851371: step 4272, loss 0.0410223, acc 0.98
2016-09-07T04:31:26.549397: step 4273, loss 0.0109523, acc 1
2016-09-07T04:31:27.230965: step 4274, loss 0.0086689, acc 1
2016-09-07T04:31:27.937328: step 4275, loss 0.0591285, acc 0.98
2016-09-07T04:31:28.613765: step 4276, loss 0.00242707, acc 1
2016-09-07T04:31:29.288035: step 4277, loss 0.051532, acc 0.98
2016-09-07T04:31:29.972869: step 4278, loss 0.00108417, acc 1
2016-09-07T04:31:30.645129: step 4279, loss 0.0232541, acc 0.98
2016-09-07T04:31:31.347362: step 4280, loss 0.0102468, acc 1
2016-09-07T04:31:32.057780: step 4281, loss 0.0812878, acc 0.98
2016-09-07T04:31:32.780508: step 4282, loss 0.00125365, acc 1
2016-09-07T04:31:33.446073: step 4283, loss 0.0240781, acc 0.98
2016-09-07T04:31:34.103324: step 4284, loss 0.0222845, acc 0.98
2016-09-07T04:31:34.796074: step 4285, loss 0.131861, acc 0.96
2016-09-07T04:31:35.523518: step 4286, loss 0.0359263, acc 0.96
2016-09-07T04:31:36.231026: step 4287, loss 0.0230102, acc 1
2016-09-07T04:31:36.882434: step 4288, loss 0.0232039, acc 1
2016-09-07T04:31:37.584787: step 4289, loss 0.0778809, acc 0.96
2016-09-07T04:31:38.252374: step 4290, loss 0.0327047, acc 1
2016-09-07T04:31:38.953708: step 4291, loss 0.0375972, acc 0.98
2016-09-07T04:31:39.637571: step 4292, loss 0.0147311, acc 1
2016-09-07T04:31:40.321299: step 4293, loss 0.0662174, acc 0.98
2016-09-07T04:31:40.997214: step 4294, loss 0.0531746, acc 0.98
2016-09-07T04:31:41.682611: step 4295, loss 0.0103283, acc 1
2016-09-07T04:31:42.400561: step 4296, loss 0.00302622, acc 1
2016-09-07T04:31:43.088178: step 4297, loss 0.000180654, acc 1
2016-09-07T04:31:43.803151: step 4298, loss 0.0843315, acc 0.98
2016-09-07T04:31:44.497411: step 4299, loss 0.0390251, acc 0.98
2016-09-07T04:31:45.186654: step 4300, loss 0.0457886, acc 0.98

Evaluation:
2016-09-07T04:31:48.391782: step 4300, loss 1.97138, acc 0.729831

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-4300

2016-09-07T04:31:50.103881: step 4301, loss 0.0367077, acc 0.96
2016-09-07T04:31:50.815106: step 4302, loss 0.000191046, acc 1
2016-09-07T04:31:51.497034: step 4303, loss 0.00316208, acc 1
2016-09-07T04:31:52.187623: step 4304, loss 0.0534246, acc 0.98
2016-09-07T04:31:52.886406: step 4305, loss 0.0582734, acc 0.98
2016-09-07T04:31:53.577337: step 4306, loss 0.00943334, acc 1
2016-09-07T04:31:54.260071: step 4307, loss 0.0197886, acc 1
2016-09-07T04:31:54.918496: step 4308, loss 0.000126085, acc 1
2016-09-07T04:31:55.636876: step 4309, loss 0.0304816, acc 0.98
2016-09-07T04:31:56.317450: step 4310, loss 0.011542, acc 1
2016-09-07T04:31:57.010119: step 4311, loss 0.0450681, acc 0.96
2016-09-07T04:31:57.698613: step 4312, loss 0.00912487, acc 1
2016-09-07T04:31:58.367711: step 4313, loss 0.0806877, acc 0.96
2016-09-07T04:31:59.040978: step 4314, loss 0.0356051, acc 0.96
2016-09-07T04:31:59.714545: step 4315, loss 0.03954, acc 0.98
2016-09-07T04:32:00.442672: step 4316, loss 0.0370451, acc 0.98
2016-09-07T04:32:01.130918: step 4317, loss 0.0325973, acc 0.98
2016-09-07T04:32:01.818132: step 4318, loss 0.00142041, acc 1
2016-09-07T04:32:02.502623: step 4319, loss 0.021266, acc 0.98
2016-09-07T04:32:03.193667: step 4320, loss 0.0143283, acc 1
2016-09-07T04:32:03.890414: step 4321, loss 0.0129342, acc 1
2016-09-07T04:32:04.562755: step 4322, loss 0.0597053, acc 0.98
2016-09-07T04:32:05.288182: step 4323, loss 0.0174045, acc 1
2016-09-07T04:32:05.967384: step 4324, loss 0.0154423, acc 0.98
2016-09-07T04:32:06.650954: step 4325, loss 0.0382048, acc 0.98
2016-09-07T04:32:07.336324: step 4326, loss 0.00369569, acc 1
2016-09-07T04:32:08.015865: step 4327, loss 0.0310298, acc 0.96
2016-09-07T04:32:08.694697: step 4328, loss 0.0623023, acc 0.94
2016-09-07T04:32:09.391934: step 4329, loss 0.0202351, acc 1
2016-09-07T04:32:10.100342: step 4330, loss 0.040411, acc 0.98
2016-09-07T04:32:10.784945: step 4331, loss 0.0213372, acc 0.98
2016-09-07T04:32:11.465474: step 4332, loss 0.0213793, acc 1
2016-09-07T04:32:12.150532: step 4333, loss 0.0297947, acc 0.98
2016-09-07T04:32:12.838259: step 4334, loss 0.00861585, acc 1
2016-09-07T04:32:13.526622: step 4335, loss 0.041549, acc 0.98
2016-09-07T04:32:14.202993: step 4336, loss 0.0454701, acc 0.98
2016-09-07T04:32:14.930329: step 4337, loss 0.0127186, acc 1
2016-09-07T04:32:15.621933: step 4338, loss 0.0279998, acc 1
2016-09-07T04:32:16.311518: step 4339, loss 0.0205516, acc 0.98
2016-09-07T04:32:16.982343: step 4340, loss 0.133704, acc 0.94
2016-09-07T04:32:17.653113: step 4341, loss 0.0308388, acc 0.98
2016-09-07T04:32:18.348674: step 4342, loss 0.0170019, acc 1
2016-09-07T04:32:19.021274: step 4343, loss 0.0266568, acc 1
2016-09-07T04:32:19.713225: step 4344, loss 0.0210411, acc 1
2016-09-07T04:32:20.378621: step 4345, loss 0.0128243, acc 1
2016-09-07T04:32:21.065635: step 4346, loss 0.0166265, acc 0.98
2016-09-07T04:32:21.748747: step 4347, loss 0.000263519, acc 1
2016-09-07T04:32:22.454210: step 4348, loss 0.0995371, acc 0.98
2016-09-07T04:32:23.153529: step 4349, loss 0.0202229, acc 1
2016-09-07T04:32:23.832467: step 4350, loss 0.0156726, acc 1
2016-09-07T04:32:24.561392: step 4351, loss 0.0259231, acc 0.98
2016-09-07T04:32:25.244880: step 4352, loss 0.00116877, acc 1
2016-09-07T04:32:25.928864: step 4353, loss 0.0174049, acc 1
2016-09-07T04:32:26.637646: step 4354, loss 0.0147367, acc 1
2016-09-07T04:32:27.344160: step 4355, loss 0.00994209, acc 1
2016-09-07T04:32:28.030502: step 4356, loss 0.179182, acc 0.94
2016-09-07T04:32:28.694781: step 4357, loss 0.0111767, acc 1
2016-09-07T04:32:29.390318: step 4358, loss 0.0245117, acc 0.98
2016-09-07T04:32:30.068280: step 4359, loss 0.00504385, acc 1
2016-09-07T04:32:30.753537: step 4360, loss 0.0177929, acc 1
2016-09-07T04:32:31.424122: step 4361, loss 0.0222401, acc 0.98
2016-09-07T04:32:32.144697: step 4362, loss 0.0221437, acc 0.98
2016-09-07T04:32:32.857839: step 4363, loss 0.0184358, acc 0.98
2016-09-07T04:32:33.543542: step 4364, loss 0.0735656, acc 0.96
2016-09-07T04:32:34.244941: step 4365, loss 0.0328946, acc 0.98
2016-09-07T04:32:34.940440: step 4366, loss 0.0436044, acc 0.98
2016-09-07T04:32:35.620461: step 4367, loss 0.0646943, acc 0.94
2016-09-07T04:32:36.308782: step 4368, loss 0.00116012, acc 1
2016-09-07T04:32:36.989151: step 4369, loss 0.0410781, acc 0.98
2016-09-07T04:32:37.711780: step 4370, loss 0.0266244, acc 0.98
2016-09-07T04:32:38.381984: step 4371, loss 0.0125974, acc 1
2016-09-07T04:32:39.065931: step 4372, loss 0.00523596, acc 1
2016-09-07T04:32:39.745185: step 4373, loss 0.148712, acc 0.96
2016-09-07T04:32:40.440810: step 4374, loss 0.0275401, acc 0.98
2016-09-07T04:32:41.145189: step 4375, loss 0.0364435, acc 0.98
2016-09-07T04:32:41.841363: step 4376, loss 0.0297688, acc 0.98
2016-09-07T04:32:42.551540: step 4377, loss 0.0594947, acc 0.98
2016-09-07T04:32:43.236144: step 4378, loss 0.0882622, acc 0.94
2016-09-07T04:32:43.917706: step 4379, loss 0.0785007, acc 0.94
2016-09-07T04:32:44.604424: step 4380, loss 0.00676438, acc 1
2016-09-07T04:32:45.287879: step 4381, loss 0.00436228, acc 1
2016-09-07T04:32:45.993541: step 4382, loss 0.00146421, acc 1
2016-09-07T04:32:46.657639: step 4383, loss 0.0361518, acc 0.98
2016-09-07T04:32:47.354095: step 4384, loss 0.0163194, acc 1
2016-09-07T04:32:48.015144: step 4385, loss 0.0105236, acc 1
2016-09-07T04:32:48.707508: step 4386, loss 0.0590434, acc 0.98
2016-09-07T04:32:49.392655: step 4387, loss 0.00712013, acc 1
2016-09-07T04:32:50.105336: step 4388, loss 0.00228868, acc 1
2016-09-07T04:32:50.812517: step 4389, loss 0.00435154, acc 1
2016-09-07T04:32:51.466771: step 4390, loss 0.0470607, acc 0.98
2016-09-07T04:32:52.162674: step 4391, loss 0.000529837, acc 1
2016-09-07T04:32:52.838545: step 4392, loss 0.0565222, acc 0.98
2016-09-07T04:32:53.525369: step 4393, loss 0.0289493, acc 0.98
2016-09-07T04:32:54.200734: step 4394, loss 0.0504554, acc 1
2016-09-07T04:32:54.903062: step 4395, loss 0.015904, acc 1
2016-09-07T04:32:55.602569: step 4396, loss 0.00289477, acc 1
2016-09-07T04:32:56.274617: step 4397, loss 0.0080982, acc 1
2016-09-07T04:32:56.988821: step 4398, loss 0.0104739, acc 1
2016-09-07T04:32:57.662322: step 4399, loss 0.0163001, acc 1
2016-09-07T04:32:58.349806: step 4400, loss 0.0604297, acc 0.98

Evaluation:
2016-09-07T04:33:01.584921: step 4400, loss 1.92411, acc 0.727017

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-4400

2016-09-07T04:33:03.263160: step 4401, loss 0.0759007, acc 0.94
2016-09-07T04:33:03.946157: step 4402, loss 0.0457135, acc 0.98
2016-09-07T04:33:04.638333: step 4403, loss 0.0208681, acc 0.98
2016-09-07T04:33:05.344468: step 4404, loss 0.0301515, acc 0.98
2016-09-07T04:33:06.024363: step 4405, loss 0.00140264, acc 1
2016-09-07T04:33:06.718069: step 4406, loss 0.00898389, acc 1
2016-09-07T04:33:07.415129: step 4407, loss 0.0024868, acc 1
2016-09-07T04:33:08.102532: step 4408, loss 0.0984134, acc 0.92
2016-09-07T04:33:08.769234: step 4409, loss 0.0215092, acc 0.98
2016-09-07T04:33:09.454452: step 4410, loss 0.0627944, acc 0.94
2016-09-07T04:33:10.130818: step 4411, loss 0.0279863, acc 1
2016-09-07T04:33:10.791697: step 4412, loss 0.0110958, acc 1
2016-09-07T04:33:11.481643: step 4413, loss 0.0530346, acc 0.96
2016-09-07T04:33:12.168824: step 4414, loss 0.0451271, acc 0.96
2016-09-07T04:33:12.850889: step 4415, loss 0.0119991, acc 1
2016-09-07T04:33:13.482796: step 4416, loss 0.0107195, acc 1
2016-09-07T04:33:14.174954: step 4417, loss 0.0188902, acc 0.98
2016-09-07T04:33:14.879849: step 4418, loss 0.0184902, acc 1
2016-09-07T04:33:15.541576: step 4419, loss 0.0386933, acc 0.98
2016-09-07T04:33:16.238208: step 4420, loss 0.149938, acc 0.98
2016-09-07T04:33:16.940034: step 4421, loss 0.0326595, acc 0.98
2016-09-07T04:33:17.622159: step 4422, loss 0.0589283, acc 0.96
2016-09-07T04:33:18.325649: step 4423, loss 0.0262057, acc 1
2016-09-07T04:33:19.008846: step 4424, loss 0.0022705, acc 1
2016-09-07T04:33:19.689850: step 4425, loss 0.0299833, acc 0.98
2016-09-07T04:33:20.350795: step 4426, loss 0.0284567, acc 1
2016-09-07T04:33:21.056188: step 4427, loss 0.0341754, acc 0.98
2016-09-07T04:33:21.719559: step 4428, loss 0.00234617, acc 1
2016-09-07T04:33:22.420010: step 4429, loss 0.03019, acc 0.98
2016-09-07T04:33:23.112770: step 4430, loss 0.0219443, acc 1
2016-09-07T04:33:23.795405: step 4431, loss 0.00513754, acc 1
2016-09-07T04:33:24.479661: step 4432, loss 0.00623095, acc 1
2016-09-07T04:33:25.137891: step 4433, loss 0.0111199, acc 1
2016-09-07T04:33:25.846407: step 4434, loss 0.0367068, acc 0.98
2016-09-07T04:33:26.514877: step 4435, loss 0.0473553, acc 0.98
2016-09-07T04:33:27.198283: step 4436, loss 0.021357, acc 0.98
2016-09-07T04:33:27.883931: step 4437, loss 0.0148515, acc 0.98
2016-09-07T04:33:28.587528: step 4438, loss 0.0585155, acc 0.98
2016-09-07T04:33:29.293525: step 4439, loss 0.119776, acc 0.94
2016-09-07T04:33:29.972837: step 4440, loss 0.00376499, acc 1
2016-09-07T04:33:30.678079: step 4441, loss 0.0101772, acc 1
2016-09-07T04:33:31.359424: step 4442, loss 0.00238854, acc 1
2016-09-07T04:33:32.050746: step 4443, loss 0.0162562, acc 1
2016-09-07T04:33:32.745571: step 4444, loss 0.00476586, acc 1
2016-09-07T04:33:33.432720: step 4445, loss 0.0294961, acc 0.98
2016-09-07T04:33:34.124332: step 4446, loss 0.0206427, acc 0.98
2016-09-07T04:33:34.776334: step 4447, loss 0.0126696, acc 1
2016-09-07T04:33:35.488628: step 4448, loss 0.0746695, acc 0.96
2016-09-07T04:33:36.196120: step 4449, loss 0.054095, acc 0.96
2016-09-07T04:33:36.890284: step 4450, loss 0.0368775, acc 0.96
2016-09-07T04:33:37.613045: step 4451, loss 0.023375, acc 0.98
2016-09-07T04:33:38.295627: step 4452, loss 0.00223323, acc 1
2016-09-07T04:33:39.005797: step 4453, loss 0.00995572, acc 1
2016-09-07T04:33:39.698845: step 4454, loss 0.000783331, acc 1
2016-09-07T04:33:40.382067: step 4455, loss 0.00544903, acc 1
2016-09-07T04:33:41.067811: step 4456, loss 0.00260525, acc 1
2016-09-07T04:33:41.763918: step 4457, loss 0.0128767, acc 1
2016-09-07T04:33:42.462394: step 4458, loss 0.00122135, acc 1
2016-09-07T04:33:43.153540: step 4459, loss 0.02063, acc 0.98
2016-09-07T04:33:43.859759: step 4460, loss 0.0327666, acc 0.98
2016-09-07T04:33:44.547952: step 4461, loss 0.0438682, acc 0.98
2016-09-07T04:33:45.214635: step 4462, loss 0.0234948, acc 0.98
2016-09-07T04:33:45.883946: step 4463, loss 0.00837267, acc 1
2016-09-07T04:33:46.581164: step 4464, loss 0.0293079, acc 0.98
2016-09-07T04:33:47.261606: step 4465, loss 0.00117755, acc 1
2016-09-07T04:33:47.932710: step 4466, loss 0.0260881, acc 1
2016-09-07T04:33:48.623399: step 4467, loss 0.011071, acc 1
2016-09-07T04:33:49.306899: step 4468, loss 0.000722003, acc 1
2016-09-07T04:33:49.994872: step 4469, loss 0.00206352, acc 1
2016-09-07T04:33:50.671491: step 4470, loss 0.000179412, acc 1
2016-09-07T04:33:51.373391: step 4471, loss 0.0221716, acc 1
2016-09-07T04:33:52.068919: step 4472, loss 0.0196473, acc 0.98
2016-09-07T04:33:52.758019: step 4473, loss 0.0379027, acc 1
2016-09-07T04:33:53.478866: step 4474, loss 0.00703119, acc 1
2016-09-07T04:33:54.145208: step 4475, loss 0.0341053, acc 0.98
2016-09-07T04:33:54.838084: step 4476, loss 0.0562554, acc 0.96
2016-09-07T04:33:55.518640: step 4477, loss 0.0884859, acc 0.96
2016-09-07T04:33:56.197881: step 4478, loss 0.0304662, acc 0.98
2016-09-07T04:33:56.904579: step 4479, loss 0.00530281, acc 1
2016-09-07T04:33:57.594722: step 4480, loss 0.0416426, acc 0.98
2016-09-07T04:33:58.324854: step 4481, loss 0.103655, acc 0.98
2016-09-07T04:33:59.010361: step 4482, loss 0.0327175, acc 1
2016-09-07T04:33:59.700208: step 4483, loss 0.0256778, acc 0.98
2016-09-07T04:34:00.439351: step 4484, loss 0.0205805, acc 0.98
2016-09-07T04:34:01.127822: step 4485, loss 0.0099315, acc 1
2016-09-07T04:34:01.819631: step 4486, loss 0.0514715, acc 0.98
2016-09-07T04:34:02.495712: step 4487, loss 0.137731, acc 0.98
2016-09-07T04:34:03.206844: step 4488, loss 0.0589822, acc 0.96
2016-09-07T04:34:03.871233: step 4489, loss 0.0122216, acc 1
2016-09-07T04:34:04.555939: step 4490, loss 0.00204247, acc 1
2016-09-07T04:34:05.237551: step 4491, loss 0.0071076, acc 1
2016-09-07T04:34:05.921077: step 4492, loss 0.0304586, acc 0.98
2016-09-07T04:34:06.635294: step 4493, loss 0.00259464, acc 1
2016-09-07T04:34:07.310487: step 4494, loss 0.0240356, acc 0.98
2016-09-07T04:34:08.004306: step 4495, loss 0.0392483, acc 0.98
2016-09-07T04:34:08.697882: step 4496, loss 0.0441938, acc 0.98
2016-09-07T04:34:09.384049: step 4497, loss 0.0139767, acc 1
2016-09-07T04:34:10.079332: step 4498, loss 9.16184e-05, acc 1
2016-09-07T04:34:10.748521: step 4499, loss 0.013849, acc 1
2016-09-07T04:34:11.436685: step 4500, loss 0.0102507, acc 1

Evaluation:
2016-09-07T04:34:14.632590: step 4500, loss 2.16673, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-4500

2016-09-07T04:34:16.339009: step 4501, loss 0.0126535, acc 1
2016-09-07T04:34:17.010093: step 4502, loss 0.037882, acc 0.96
2016-09-07T04:34:17.693487: step 4503, loss 3.93906e-05, acc 1
2016-09-07T04:34:18.361864: step 4504, loss 0.00164139, acc 1
2016-09-07T04:34:19.044698: step 4505, loss 0.132675, acc 0.98
2016-09-07T04:34:19.734509: step 4506, loss 0.0139083, acc 1
2016-09-07T04:34:20.413743: step 4507, loss 0.0212061, acc 0.98
2016-09-07T04:34:21.142557: step 4508, loss 0.0633496, acc 0.98
2016-09-07T04:34:21.837448: step 4509, loss 0.0410109, acc 0.98
2016-09-07T04:34:22.527801: step 4510, loss 0.0949274, acc 0.98
2016-09-07T04:34:23.228763: step 4511, loss 0.112398, acc 0.94
2016-09-07T04:34:23.918491: step 4512, loss 0.00879015, acc 1
2016-09-07T04:34:24.623854: step 4513, loss 0.00862808, acc 1
2016-09-07T04:34:25.291421: step 4514, loss 0.0494182, acc 0.96
2016-09-07T04:34:26.025431: step 4515, loss 0.0850723, acc 0.96
2016-09-07T04:34:26.714853: step 4516, loss 0.0240595, acc 1
2016-09-07T04:34:27.414589: step 4517, loss 0.00839249, acc 1
2016-09-07T04:34:28.099466: step 4518, loss 0.0151798, acc 1
2016-09-07T04:34:28.781868: step 4519, loss 0.0196961, acc 1
2016-09-07T04:34:29.465234: step 4520, loss 0.0425023, acc 0.98
2016-09-07T04:34:30.141033: step 4521, loss 0.0671169, acc 0.96
2016-09-07T04:34:30.862326: step 4522, loss 0.098637, acc 0.98
2016-09-07T04:34:31.551468: step 4523, loss 0.0525779, acc 0.98
2016-09-07T04:34:32.248189: step 4524, loss 0.0910657, acc 0.94
2016-09-07T04:34:32.956122: step 4525, loss 0.00137273, acc 1
2016-09-07T04:34:33.643627: step 4526, loss 0.0163092, acc 0.98
2016-09-07T04:34:34.352248: step 4527, loss 0.033273, acc 0.98
2016-09-07T04:34:35.030948: step 4528, loss 0.00816208, acc 1
2016-09-07T04:34:35.730835: step 4529, loss 0.0272019, acc 0.98
2016-09-07T04:34:36.404242: step 4530, loss 0.054343, acc 0.98
2016-09-07T04:34:37.087320: step 4531, loss 0.000754576, acc 1
2016-09-07T04:34:37.780844: step 4532, loss 0.0386879, acc 0.98
2016-09-07T04:34:38.458745: step 4533, loss 0.010906, acc 1
2016-09-07T04:34:39.160980: step 4534, loss 0.0274021, acc 0.98
2016-09-07T04:34:39.857373: step 4535, loss 0.0316765, acc 0.98
2016-09-07T04:34:40.547392: step 4536, loss 0.0431387, acc 0.96
2016-09-07T04:34:41.225496: step 4537, loss 0.0101111, acc 1
2016-09-07T04:34:41.911994: step 4538, loss 0.0783277, acc 0.96
2016-09-07T04:34:42.633931: step 4539, loss 0.0120749, acc 1
2016-09-07T04:34:43.330822: step 4540, loss 0.0282012, acc 0.98
2016-09-07T04:34:44.027143: step 4541, loss 0.0576401, acc 0.94
2016-09-07T04:34:44.720525: step 4542, loss 0.0164853, acc 1
2016-09-07T04:34:45.406736: step 4543, loss 0.0419863, acc 0.98
2016-09-07T04:34:46.097042: step 4544, loss 0.0141504, acc 1
2016-09-07T04:34:46.791485: step 4545, loss 0.00678917, acc 1
2016-09-07T04:34:47.494103: step 4546, loss 0.0915855, acc 0.94
2016-09-07T04:34:48.162238: step 4547, loss 0.0596277, acc 0.96
2016-09-07T04:34:48.867676: step 4548, loss 0.0316829, acc 1
2016-09-07T04:34:49.570107: step 4549, loss 0.00165273, acc 1
2016-09-07T04:34:50.260464: step 4550, loss 0.0159382, acc 1
2016-09-07T04:34:50.944974: step 4551, loss 0.0158401, acc 1
2016-09-07T04:34:51.658570: step 4552, loss 0.0370082, acc 0.98
2016-09-07T04:34:52.387768: step 4553, loss 0.0700171, acc 0.98
2016-09-07T04:34:53.074989: step 4554, loss 0.0189527, acc 0.98
2016-09-07T04:34:53.781029: step 4555, loss 0.0108848, acc 1
2016-09-07T04:34:54.456288: step 4556, loss 0.112284, acc 0.98
2016-09-07T04:34:55.146363: step 4557, loss 0.0177096, acc 1
2016-09-07T04:34:55.821353: step 4558, loss 0.0379308, acc 0.96
2016-09-07T04:34:56.490422: step 4559, loss 0.0178367, acc 0.98
2016-09-07T04:34:57.203475: step 4560, loss 0.011454, acc 1
2016-09-07T04:34:57.868839: step 4561, loss 0.0530478, acc 0.98
2016-09-07T04:34:58.556384: step 4562, loss 0.0197804, acc 1
2016-09-07T04:34:59.219253: step 4563, loss 0.0320342, acc 0.98
2016-09-07T04:34:59.909785: step 4564, loss 0.0273318, acc 1
2016-09-07T04:35:00.624910: step 4565, loss 0.009048, acc 1
2016-09-07T04:35:01.322305: step 4566, loss 0.0420421, acc 0.98
2016-09-07T04:35:02.028534: step 4567, loss 0.0786779, acc 0.94
2016-09-07T04:35:02.705081: step 4568, loss 0.0722263, acc 0.98
2016-09-07T04:35:03.381650: step 4569, loss 0.0324779, acc 0.98
2016-09-07T04:35:04.064604: step 4570, loss 0.0613753, acc 0.98
2016-09-07T04:35:04.752214: step 4571, loss 0.0620017, acc 0.96
2016-09-07T04:35:05.460886: step 4572, loss 0.00710281, acc 1
2016-09-07T04:35:06.148544: step 4573, loss 0.000834642, acc 1
2016-09-07T04:35:06.845856: step 4574, loss 0.0268313, acc 0.98
2016-09-07T04:35:07.545765: step 4575, loss 0.000520365, acc 1
2016-09-07T04:35:08.234958: step 4576, loss 0.0157238, acc 1
2016-09-07T04:35:08.935485: step 4577, loss 0.00699951, acc 1
2016-09-07T04:35:09.656317: step 4578, loss 0.0467994, acc 0.98
2016-09-07T04:35:10.354373: step 4579, loss 0.0364423, acc 0.96
2016-09-07T04:35:11.034869: step 4580, loss 0.0628593, acc 0.96
2016-09-07T04:35:11.717286: step 4581, loss 0.0128594, acc 1
2016-09-07T04:35:12.405196: step 4582, loss 0.0341036, acc 1
2016-09-07T04:35:13.078299: step 4583, loss 0.0129982, acc 1
2016-09-07T04:35:13.772604: step 4584, loss 0.0169228, acc 0.98
2016-09-07T04:35:14.452543: step 4585, loss 0.000389233, acc 1
2016-09-07T04:35:15.133762: step 4586, loss 0.0551039, acc 0.96
2016-09-07T04:35:15.796079: step 4587, loss 0.0344254, acc 0.96
2016-09-07T04:35:16.522000: step 4588, loss 0.0100466, acc 1
2016-09-07T04:35:17.220774: step 4589, loss 0.0429972, acc 0.98
2016-09-07T04:35:17.936285: step 4590, loss 0.0619646, acc 0.96
2016-09-07T04:35:18.630073: step 4591, loss 0.0024708, acc 1
2016-09-07T04:35:19.301732: step 4592, loss 0.0247351, acc 1
2016-09-07T04:35:20.003764: step 4593, loss 0.0192605, acc 1
2016-09-07T04:35:20.681676: step 4594, loss 0.0342938, acc 0.98
2016-09-07T04:35:21.384586: step 4595, loss 0.070192, acc 0.98
2016-09-07T04:35:22.087927: step 4596, loss 0.0129464, acc 1
2016-09-07T04:35:22.785600: step 4597, loss 0.0112502, acc 1
2016-09-07T04:35:23.469775: step 4598, loss 0.150044, acc 0.98
2016-09-07T04:35:24.153086: step 4599, loss 0.132833, acc 0.96
2016-09-07T04:35:24.876736: step 4600, loss 0.0872353, acc 0.94

Evaluation:
2016-09-07T04:35:28.115626: step 4600, loss 2.08573, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-4600

2016-09-07T04:35:29.781428: step 4601, loss 0.00110241, acc 1
2016-09-07T04:35:30.478440: step 4602, loss 0.00694056, acc 1
2016-09-07T04:35:31.160793: step 4603, loss 0.0336814, acc 1
2016-09-07T04:35:31.848914: step 4604, loss 0.001505, acc 1
2016-09-07T04:35:32.547286: step 4605, loss 0.00929994, acc 1
2016-09-07T04:35:33.266167: step 4606, loss 0.0320152, acc 0.98
2016-09-07T04:35:33.937518: step 4607, loss 0.00307558, acc 1
2016-09-07T04:35:34.573530: step 4608, loss 0.00255976, acc 1
2016-09-07T04:35:35.272485: step 4609, loss 0.00448943, acc 1
2016-09-07T04:35:35.947882: step 4610, loss 0.0669265, acc 0.96
2016-09-07T04:35:36.638056: step 4611, loss 0.0203596, acc 0.98
2016-09-07T04:35:37.319396: step 4612, loss 0.0130261, acc 1
2016-09-07T04:35:38.022799: step 4613, loss 0.00102058, acc 1
2016-09-07T04:35:38.691039: step 4614, loss 0.0366056, acc 0.98
2016-09-07T04:35:39.409554: step 4615, loss 0.0760516, acc 0.98
2016-09-07T04:35:40.097481: step 4616, loss 0.130683, acc 0.94
2016-09-07T04:35:40.815311: step 4617, loss 0.028362, acc 0.98
2016-09-07T04:35:41.526007: step 4618, loss 0.00614659, acc 1
2016-09-07T04:35:42.184471: step 4619, loss 0.0221356, acc 1
2016-09-07T04:35:42.895712: step 4620, loss 0.0446595, acc 0.96
2016-09-07T04:35:43.574785: step 4621, loss 0.0350961, acc 0.98
2016-09-07T04:35:44.260288: step 4622, loss 0.175348, acc 0.96
2016-09-07T04:35:44.935265: step 4623, loss 0.0156087, acc 0.98
2016-09-07T04:35:45.622571: step 4624, loss 0.0033022, acc 1
2016-09-07T04:35:46.310224: step 4625, loss 0.0295446, acc 0.98
2016-09-07T04:35:46.990149: step 4626, loss 0.0104444, acc 1
2016-09-07T04:35:47.687750: step 4627, loss 0.0564695, acc 0.96
2016-09-07T04:35:48.363565: step 4628, loss 0.0189278, acc 1
2016-09-07T04:35:49.032386: step 4629, loss 0.016928, acc 1
2016-09-07T04:35:49.713018: step 4630, loss 0.025764, acc 0.98
2016-09-07T04:35:50.385385: step 4631, loss 0.0459506, acc 0.98
2016-09-07T04:35:51.061165: step 4632, loss 0.0512698, acc 0.98
2016-09-07T04:35:51.749228: step 4633, loss 0.0472851, acc 0.96
2016-09-07T04:35:52.454361: step 4634, loss 0.000850121, acc 1
2016-09-07T04:35:53.132987: step 4635, loss 0.00829813, acc 1
2016-09-07T04:35:53.849489: step 4636, loss 0.0352161, acc 0.98
2016-09-07T04:35:54.542069: step 4637, loss 0.0193953, acc 0.98
2016-09-07T04:35:55.232462: step 4638, loss 0.0541688, acc 0.98
2016-09-07T04:35:55.934524: step 4639, loss 0.00713747, acc 1
2016-09-07T04:35:56.616400: step 4640, loss 0.0200056, acc 0.98
2016-09-07T04:35:57.343382: step 4641, loss 0.050729, acc 0.96
2016-09-07T04:35:58.018301: step 4642, loss 0.0431642, acc 0.98
2016-09-07T04:35:58.701963: step 4643, loss 0.0204589, acc 0.98
2016-09-07T04:35:59.406520: step 4644, loss 0.0224951, acc 0.98
2016-09-07T04:36:00.106641: step 4645, loss 0.00184327, acc 1
2016-09-07T04:36:00.841739: step 4646, loss 0.0366646, acc 0.98
2016-09-07T04:36:01.504655: step 4647, loss 0.000936618, acc 1
2016-09-07T04:36:02.217044: step 4648, loss 0.000159796, acc 1
2016-09-07T04:36:02.918683: step 4649, loss 0.0220907, acc 0.98
2016-09-07T04:36:03.596090: step 4650, loss 0.0136473, acc 1
2016-09-07T04:36:04.271075: step 4651, loss 0.00209375, acc 1
2016-09-07T04:36:04.953741: step 4652, loss 0.0143398, acc 1
2016-09-07T04:36:05.648606: step 4653, loss 0.000550372, acc 1
2016-09-07T04:36:06.325496: step 4654, loss 0.014434, acc 1
2016-09-07T04:36:07.017379: step 4655, loss 0.0100968, acc 1
2016-09-07T04:36:07.697834: step 4656, loss 0.0959077, acc 0.98
2016-09-07T04:36:08.394576: step 4657, loss 0.0109843, acc 1
2016-09-07T04:36:09.106107: step 4658, loss 0.0526271, acc 0.98
2016-09-07T04:36:09.785684: step 4659, loss 0.00303618, acc 1
2016-09-07T04:36:10.485037: step 4660, loss 0.00974539, acc 1
2016-09-07T04:36:11.151459: step 4661, loss 0.00563746, acc 1
2016-09-07T04:36:11.865294: step 4662, loss 0.00850104, acc 1
2016-09-07T04:36:12.563947: step 4663, loss 0.0269977, acc 0.98
2016-09-07T04:36:13.252335: step 4664, loss 0.0548209, acc 0.96
2016-09-07T04:36:13.923536: step 4665, loss 0.0131769, acc 1
2016-09-07T04:36:14.606769: step 4666, loss 0.000364654, acc 1
2016-09-07T04:36:15.315408: step 4667, loss 0.00477454, acc 1
2016-09-07T04:36:16.005606: step 4668, loss 0.0738212, acc 0.98
2016-09-07T04:36:16.718089: step 4669, loss 0.0344917, acc 0.98
2016-09-07T04:36:17.410387: step 4670, loss 0.0258356, acc 1
2016-09-07T04:36:18.106060: step 4671, loss 0.0814613, acc 0.96
2016-09-07T04:36:18.791651: step 4672, loss 0.0149373, acc 0.98
2016-09-07T04:36:19.487835: step 4673, loss 0.00756211, acc 1
2016-09-07T04:36:20.182713: step 4674, loss 0.0357363, acc 0.98
2016-09-07T04:36:20.863164: step 4675, loss 0.00680094, acc 1
2016-09-07T04:36:21.543211: step 4676, loss 0.0254697, acc 1
2016-09-07T04:36:22.235507: step 4677, loss 0.00269576, acc 1
2016-09-07T04:36:22.907092: step 4678, loss 0.0389829, acc 0.98
2016-09-07T04:36:23.576270: step 4679, loss 0.0249114, acc 0.98
2016-09-07T04:36:24.274677: step 4680, loss 0.0409099, acc 0.98
2016-09-07T04:36:24.959374: step 4681, loss 0.000604555, acc 1
2016-09-07T04:36:25.638200: step 4682, loss 0.00119039, acc 1
2016-09-07T04:36:26.300585: step 4683, loss 0.0127886, acc 1
2016-09-07T04:36:26.991713: step 4684, loss 0.0305031, acc 1
2016-09-07T04:36:27.670641: step 4685, loss 0.0222582, acc 1
2016-09-07T04:36:28.372730: step 4686, loss 0.0581366, acc 0.98
2016-09-07T04:36:29.067245: step 4687, loss 0.00366645, acc 1
2016-09-07T04:36:29.771849: step 4688, loss 0.01648, acc 0.98
2016-09-07T04:36:30.446026: step 4689, loss 0.0480302, acc 0.98
2016-09-07T04:36:31.112516: step 4690, loss 0.0173388, acc 1
2016-09-07T04:36:31.802314: step 4691, loss 0.0274854, acc 0.98
2016-09-07T04:36:32.489294: step 4692, loss 0.0195616, acc 0.98
2016-09-07T04:36:33.189569: step 4693, loss 0.00467687, acc 1
2016-09-07T04:36:33.891148: step 4694, loss 0.0145081, acc 1
2016-09-07T04:36:34.582748: step 4695, loss 0.00684548, acc 1
2016-09-07T04:36:35.262763: step 4696, loss 0.0433837, acc 0.98
2016-09-07T04:36:35.969539: step 4697, loss 0.0100806, acc 1
2016-09-07T04:36:36.679148: step 4698, loss 0.0129024, acc 1
2016-09-07T04:36:37.386682: step 4699, loss 0.0231367, acc 0.98
2016-09-07T04:36:38.094903: step 4700, loss 0.0169265, acc 0.98

Evaluation:
2016-09-07T04:36:41.302326: step 4700, loss 2.33708, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-4700

2016-09-07T04:36:43.050381: step 4701, loss 0.0019844, acc 1
2016-09-07T04:36:43.723749: step 4702, loss 0.0356821, acc 0.98
2016-09-07T04:36:44.432493: step 4703, loss 0.0276569, acc 0.98
2016-09-07T04:36:45.112665: step 4704, loss 0.000832921, acc 1
2016-09-07T04:36:45.814521: step 4705, loss 0.00742274, acc 1
2016-09-07T04:36:46.505906: step 4706, loss 0.0514748, acc 1
2016-09-07T04:36:47.184065: step 4707, loss 0.00049915, acc 1
2016-09-07T04:36:47.887515: step 4708, loss 0.0412702, acc 0.96
2016-09-07T04:36:48.622258: step 4709, loss 0.0261521, acc 0.98
2016-09-07T04:36:49.331244: step 4710, loss 0.0498945, acc 0.96
2016-09-07T04:36:50.040923: step 4711, loss 0.0323523, acc 0.98
2016-09-07T04:36:50.747834: step 4712, loss 0.000746786, acc 1
2016-09-07T04:36:51.454123: step 4713, loss 0.0125097, acc 1
2016-09-07T04:36:52.122719: step 4714, loss 0.0152355, acc 0.98
2016-09-07T04:36:52.828171: step 4715, loss 0.0359437, acc 0.98
2016-09-07T04:36:53.510582: step 4716, loss 0.0110623, acc 1
2016-09-07T04:36:54.206188: step 4717, loss 0.0204309, acc 1
2016-09-07T04:36:54.916190: step 4718, loss 0.0637296, acc 0.96
2016-09-07T04:36:55.595327: step 4719, loss 0.0170101, acc 0.98
2016-09-07T04:36:56.291968: step 4720, loss 0.00013626, acc 1
2016-09-07T04:36:56.994074: step 4721, loss 0.0908283, acc 0.98
2016-09-07T04:36:57.686439: step 4722, loss 0.0197875, acc 1
2016-09-07T04:36:58.356646: step 4723, loss 0.00180032, acc 1
2016-09-07T04:36:59.035370: step 4724, loss 0.0430777, acc 0.96
2016-09-07T04:36:59.726627: step 4725, loss 0.0227856, acc 1
2016-09-07T04:37:00.428106: step 4726, loss 0.0123725, acc 1
2016-09-07T04:37:01.136455: step 4727, loss 0.0135992, acc 1
2016-09-07T04:37:01.830263: step 4728, loss 0.0336735, acc 0.96
2016-09-07T04:37:02.518617: step 4729, loss 0.0360363, acc 0.98
2016-09-07T04:37:03.201454: step 4730, loss 0.0743196, acc 0.98
2016-09-07T04:37:03.895434: step 4731, loss 0.000774607, acc 1
2016-09-07T04:37:04.607177: step 4732, loss 0.0118473, acc 1
2016-09-07T04:37:05.289369: step 4733, loss 0.0413403, acc 0.96
2016-09-07T04:37:05.984280: step 4734, loss 0.0324624, acc 0.98
2016-09-07T04:37:06.668275: step 4735, loss 0.000326859, acc 1
2016-09-07T04:37:07.352281: step 4736, loss 0.000134886, acc 1
2016-09-07T04:37:08.017142: step 4737, loss 0.00727422, acc 1
2016-09-07T04:37:08.712602: step 4738, loss 0.019307, acc 0.98
2016-09-07T04:37:09.417613: step 4739, loss 0.0513694, acc 0.96
2016-09-07T04:37:10.106114: step 4740, loss 0.0140812, acc 1
2016-09-07T04:37:10.790623: step 4741, loss 0.00941244, acc 1
2016-09-07T04:37:11.491377: step 4742, loss 0.000556632, acc 1
2016-09-07T04:37:12.178619: step 4743, loss 0.019329, acc 1
2016-09-07T04:37:12.852486: step 4744, loss 0.0429128, acc 0.98
2016-09-07T04:37:13.561164: step 4745, loss 0.0017515, acc 1
2016-09-07T04:37:14.270579: step 4746, loss 0.030587, acc 0.96
2016-09-07T04:37:14.955543: step 4747, loss 0.000311647, acc 1
2016-09-07T04:37:15.633038: step 4748, loss 0.0215271, acc 1
2016-09-07T04:37:16.301588: step 4749, loss 0.0301556, acc 1
2016-09-07T04:37:16.999602: step 4750, loss 0.0478116, acc 0.96
2016-09-07T04:37:17.704373: step 4751, loss 0.0394814, acc 0.98
2016-09-07T04:37:18.383980: step 4752, loss 0.0352159, acc 0.98
2016-09-07T04:37:19.086233: step 4753, loss 0.00517751, acc 1
2016-09-07T04:37:19.770017: step 4754, loss 0.0154307, acc 1
2016-09-07T04:37:20.454385: step 4755, loss 0.0241253, acc 0.98
2016-09-07T04:37:21.137969: step 4756, loss 0.0249251, acc 1
2016-09-07T04:37:21.852525: step 4757, loss 0.0181923, acc 0.98
2016-09-07T04:37:22.549953: step 4758, loss 0.000346049, acc 1
2016-09-07T04:37:23.234270: step 4759, loss 0.0402847, acc 0.98
2016-09-07T04:37:23.946564: step 4760, loss 0.0302523, acc 1
2016-09-07T04:37:24.614128: step 4761, loss 0.0269363, acc 0.98
2016-09-07T04:37:25.294548: step 4762, loss 0.0469918, acc 0.98
2016-09-07T04:37:25.980747: step 4763, loss 0.135643, acc 0.96
2016-09-07T04:37:26.669251: step 4764, loss 0.00524672, acc 1
2016-09-07T04:37:27.363125: step 4765, loss 0.000486743, acc 1
2016-09-07T04:37:28.020909: step 4766, loss 0.0945411, acc 0.96
2016-09-07T04:37:28.720627: step 4767, loss 0.00415964, acc 1
2016-09-07T04:37:29.413622: step 4768, loss 0.0193713, acc 0.98
2016-09-07T04:37:30.128491: step 4769, loss 0.00981316, acc 1
2016-09-07T04:37:30.816312: step 4770, loss 0.0390347, acc 0.98
2016-09-07T04:37:31.523812: step 4771, loss 0.00107166, acc 1
2016-09-07T04:37:32.234713: step 4772, loss 0.000109371, acc 1
2016-09-07T04:37:32.922679: step 4773, loss 0.000798746, acc 1
2016-09-07T04:37:33.633454: step 4774, loss 0.193524, acc 0.96
2016-09-07T04:37:34.321188: step 4775, loss 0.00511228, acc 1
2016-09-07T04:37:35.009087: step 4776, loss 0.00439006, acc 1
2016-09-07T04:37:35.683932: step 4777, loss 0.0581928, acc 0.98
2016-09-07T04:37:36.381447: step 4778, loss 0.0981953, acc 0.96
2016-09-07T04:37:37.088534: step 4779, loss 0.0223004, acc 0.98
2016-09-07T04:37:37.750473: step 4780, loss 0.00208797, acc 1
2016-09-07T04:37:38.456484: step 4781, loss 0.0262706, acc 0.98
2016-09-07T04:37:39.136415: step 4782, loss 0.00190972, acc 1
2016-09-07T04:37:39.841470: step 4783, loss 0.00312392, acc 1
2016-09-07T04:37:40.529198: step 4784, loss 0.0197078, acc 1
2016-09-07T04:37:41.227749: step 4785, loss 0.052081, acc 0.98
2016-09-07T04:37:41.931085: step 4786, loss 0.0404406, acc 0.96
2016-09-07T04:37:42.603873: step 4787, loss 0.012472, acc 1
2016-09-07T04:37:43.283401: step 4788, loss 0.0211148, acc 0.98
2016-09-07T04:37:43.965205: step 4789, loss 0.0111382, acc 1
2016-09-07T04:37:44.658055: step 4790, loss 0.00171019, acc 1
2016-09-07T04:37:45.337476: step 4791, loss 0.000640139, acc 1
2016-09-07T04:37:46.019734: step 4792, loss 0.0384437, acc 0.98
2016-09-07T04:37:46.722156: step 4793, loss 0.00833857, acc 1
2016-09-07T04:37:47.388834: step 4794, loss 0.0059586, acc 1
2016-09-07T04:37:48.074589: step 4795, loss 0.0341417, acc 0.98
2016-09-07T04:37:48.762684: step 4796, loss 0.0184386, acc 0.98
2016-09-07T04:37:49.444919: step 4797, loss 0.183547, acc 0.98
2016-09-07T04:37:50.138806: step 4798, loss 0.0346487, acc 0.98
2016-09-07T04:37:50.830345: step 4799, loss 0.0100673, acc 1
2016-09-07T04:37:51.470191: step 4800, loss 0.00443191, acc 1

Evaluation:
2016-09-07T04:37:54.704804: step 4800, loss 2.28481, acc 0.722326

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-4800

2016-09-07T04:37:56.401388: step 4801, loss 0.0367292, acc 1
2016-09-07T04:37:57.092352: step 4802, loss 0.0679582, acc 0.96
2016-09-07T04:37:57.799632: step 4803, loss 0.242148, acc 0.92
2016-09-07T04:37:58.500390: step 4804, loss 0.0309755, acc 0.98
2016-09-07T04:37:59.209314: step 4805, loss 0.0244822, acc 1
2016-09-07T04:37:59.913909: step 4806, loss 0.0224482, acc 0.98
2016-09-07T04:38:00.626388: step 4807, loss 0.0260435, acc 0.98
2016-09-07T04:38:01.314450: step 4808, loss 0.0275875, acc 1
2016-09-07T04:38:02.033758: step 4809, loss 0.00306453, acc 1
2016-09-07T04:38:02.740768: step 4810, loss 0.0215767, acc 0.98
2016-09-07T04:38:03.425686: step 4811, loss 0.0044238, acc 1
2016-09-07T04:38:04.086086: step 4812, loss 0.0614145, acc 0.94
2016-09-07T04:38:04.791024: step 4813, loss 0.0234479, acc 1
2016-09-07T04:38:05.468114: step 4814, loss 0.0403247, acc 0.98
2016-09-07T04:38:06.148112: step 4815, loss 0.0194963, acc 1
2016-09-07T04:38:06.840216: step 4816, loss 0.00372414, acc 1
2016-09-07T04:38:07.539328: step 4817, loss 0.0268631, acc 1
2016-09-07T04:38:08.226068: step 4818, loss 0.000521669, acc 1
2016-09-07T04:38:08.887208: step 4819, loss 0.0345601, acc 0.98
2016-09-07T04:38:09.585056: step 4820, loss 0.0136747, acc 1
2016-09-07T04:38:10.294821: step 4821, loss 0.00188656, acc 1
2016-09-07T04:38:10.993729: step 4822, loss 0.0573348, acc 0.98
2016-09-07T04:38:11.699633: step 4823, loss 0.0223491, acc 0.98
2016-09-07T04:38:12.380578: step 4824, loss 0.000517277, acc 1
2016-09-07T04:38:13.055842: step 4825, loss 0.00384301, acc 1
2016-09-07T04:38:13.707497: step 4826, loss 0.020936, acc 0.98
2016-09-07T04:38:14.414588: step 4827, loss 0.0130215, acc 1
2016-09-07T04:38:15.100633: step 4828, loss 0.00444257, acc 1
2016-09-07T04:38:15.807351: step 4829, loss 0.112039, acc 0.98
2016-09-07T04:38:16.493364: step 4830, loss 0.000740669, acc 1
2016-09-07T04:38:17.219828: step 4831, loss 0.0111672, acc 1
2016-09-07T04:38:17.916804: step 4832, loss 0.0263354, acc 0.98
2016-09-07T04:38:18.599425: step 4833, loss 0.112746, acc 0.98
2016-09-07T04:38:19.304196: step 4834, loss 0.0317023, acc 0.98
2016-09-07T04:38:19.981507: step 4835, loss 0.00778185, acc 1
2016-09-07T04:38:20.656182: step 4836, loss 0.0133484, acc 1
2016-09-07T04:38:21.355453: step 4837, loss 0.12341, acc 0.96
2016-09-07T04:38:22.037306: step 4838, loss 0.0390194, acc 0.96
2016-09-07T04:38:22.727643: step 4839, loss 0.0270472, acc 0.98
2016-09-07T04:38:23.412485: step 4840, loss 0.000452138, acc 1
2016-09-07T04:38:24.105416: step 4841, loss 0.0630635, acc 0.94
2016-09-07T04:38:24.807640: step 4842, loss 0.0216919, acc 0.98
2016-09-07T04:38:25.481676: step 4843, loss 0.0423381, acc 0.98
2016-09-07T04:38:26.153609: step 4844, loss 0.0187294, acc 0.98
2016-09-07T04:38:26.839822: step 4845, loss 0.0259883, acc 0.98
2016-09-07T04:38:27.542274: step 4846, loss 0.0157872, acc 1
2016-09-07T04:38:28.239369: step 4847, loss 0.00455186, acc 1
2016-09-07T04:38:28.934410: step 4848, loss 0.0277875, acc 0.98
2016-09-07T04:38:29.642704: step 4849, loss 0.0181955, acc 0.98
2016-09-07T04:38:30.329199: step 4850, loss 0.0874874, acc 0.96
2016-09-07T04:38:31.018756: step 4851, loss 0.0126977, acc 1
2016-09-07T04:38:31.703087: step 4852, loss 0.0138927, acc 1
2016-09-07T04:38:32.418712: step 4853, loss 0.00751605, acc 1
2016-09-07T04:38:33.090670: step 4854, loss 0.00394111, acc 1
2016-09-07T04:38:33.795044: step 4855, loss 0.0541491, acc 0.96
2016-09-07T04:38:34.479041: step 4856, loss 0.0393735, acc 1
2016-09-07T04:38:35.168003: step 4857, loss 0.0363218, acc 0.98
2016-09-07T04:38:35.884358: step 4858, loss 0.00108973, acc 1
2016-09-07T04:38:36.548463: step 4859, loss 0.0264018, acc 0.98
2016-09-07T04:38:37.253013: step 4860, loss 0.0178992, acc 0.98
2016-09-07T04:38:37.951892: step 4861, loss 0.0543902, acc 0.98
2016-09-07T04:38:38.633250: step 4862, loss 0.0121129, acc 1
2016-09-07T04:38:39.330844: step 4863, loss 0.0287522, acc 0.98
2016-09-07T04:38:40.005524: step 4864, loss 0.0322808, acc 0.98
2016-09-07T04:38:40.714343: step 4865, loss 0.0354548, acc 0.98
2016-09-07T04:38:41.389006: step 4866, loss 0.0646686, acc 0.98
2016-09-07T04:38:42.081640: step 4867, loss 0.00496581, acc 1
2016-09-07T04:38:42.783943: step 4868, loss 0.0283924, acc 0.98
2016-09-07T04:38:43.455093: step 4869, loss 0.0544955, acc 0.98
2016-09-07T04:38:44.131585: step 4870, loss 0.0235069, acc 1
2016-09-07T04:38:44.820701: step 4871, loss 0.034202, acc 0.98
2016-09-07T04:38:45.507436: step 4872, loss 0.0200437, acc 0.98
2016-09-07T04:38:46.155057: step 4873, loss 0.00380237, acc 1
2016-09-07T04:38:46.868847: step 4874, loss 0.000410152, acc 1
2016-09-07T04:38:47.562941: step 4875, loss 0.0208805, acc 0.98
2016-09-07T04:38:48.236694: step 4876, loss 0.0547698, acc 1
2016-09-07T04:38:48.947215: step 4877, loss 0.0187081, acc 0.98
2016-09-07T04:38:49.633340: step 4878, loss 0.0289347, acc 0.98
2016-09-07T04:38:50.335256: step 4879, loss 0.002195, acc 1
2016-09-07T04:38:50.986689: step 4880, loss 0.0151444, acc 1
2016-09-07T04:38:51.694622: step 4881, loss 0.0523998, acc 0.98
2016-09-07T04:38:52.378486: step 4882, loss 0.0621374, acc 0.96
2016-09-07T04:38:53.071094: step 4883, loss 0.0421842, acc 0.96
2016-09-07T04:38:53.779646: step 4884, loss 0.00327604, acc 1
2016-09-07T04:38:54.460927: step 4885, loss 0.0371277, acc 0.96
2016-09-07T04:38:55.131631: step 4886, loss 0.0282018, acc 0.98
2016-09-07T04:38:55.789765: step 4887, loss 0.0098152, acc 1
2016-09-07T04:38:56.488973: step 4888, loss 0.0135575, acc 1
2016-09-07T04:38:57.178835: step 4889, loss 0.0230861, acc 1
2016-09-07T04:38:57.852975: step 4890, loss 0.0380683, acc 0.98
2016-09-07T04:38:58.533885: step 4891, loss 0.0354132, acc 0.98
2016-09-07T04:38:59.215981: step 4892, loss 0.0270653, acc 0.98
2016-09-07T04:38:59.928768: step 4893, loss 0.118914, acc 0.98
2016-09-07T04:39:00.625156: step 4894, loss 0.00589736, acc 1
2016-09-07T04:39:01.350033: step 4895, loss 0.0702776, acc 0.98
2016-09-07T04:39:02.055379: step 4896, loss 0.0753582, acc 0.98
2016-09-07T04:39:02.737075: step 4897, loss 0.062362, acc 0.98
2016-09-07T04:39:03.411180: step 4898, loss 0.0255428, acc 1
2016-09-07T04:39:04.116039: step 4899, loss 0.0094188, acc 1
2016-09-07T04:39:04.838749: step 4900, loss 0.0383362, acc 0.96

Evaluation:
2016-09-07T04:39:08.114203: step 4900, loss 2.08642, acc 0.726079

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-4900

2016-09-07T04:39:09.757865: step 4901, loss 0.0802694, acc 0.96
2016-09-07T04:39:10.459781: step 4902, loss 0.0107308, acc 1
2016-09-07T04:39:11.140997: step 4903, loss 0.0536891, acc 0.98
2016-09-07T04:39:11.847651: step 4904, loss 0.0125832, acc 1
2016-09-07T04:39:12.540608: step 4905, loss 0.0207698, acc 0.98
2016-09-07T04:39:13.240798: step 4906, loss 0.0687237, acc 0.96
2016-09-07T04:39:13.921021: step 4907, loss 0.037574, acc 0.98
2016-09-07T04:39:14.607954: step 4908, loss 0.00594805, acc 1
2016-09-07T04:39:15.290443: step 4909, loss 0.0349612, acc 0.98
2016-09-07T04:39:15.962014: step 4910, loss 0.011934, acc 1
2016-09-07T04:39:16.651055: step 4911, loss 0.0313723, acc 1
2016-09-07T04:39:17.327684: step 4912, loss 0.00857469, acc 1
2016-09-07T04:39:18.004720: step 4913, loss 0.0269204, acc 1
2016-09-07T04:39:18.661203: step 4914, loss 0.00460609, acc 1
2016-09-07T04:39:19.378849: step 4915, loss 0.0275541, acc 0.98
2016-09-07T04:39:20.066201: step 4916, loss 0.0257057, acc 1
2016-09-07T04:39:20.748608: step 4917, loss 0.021011, acc 1
2016-09-07T04:39:21.424637: step 4918, loss 0.0546838, acc 0.98
2016-09-07T04:39:22.128018: step 4919, loss 0.00281765, acc 1
2016-09-07T04:39:22.822220: step 4920, loss 0.0414179, acc 0.98
2016-09-07T04:39:23.498043: step 4921, loss 0.0351913, acc 0.96
2016-09-07T04:39:24.226630: step 4922, loss 0.0794106, acc 0.98
2016-09-07T04:39:24.930428: step 4923, loss 0.00590792, acc 1
2016-09-07T04:39:25.606500: step 4924, loss 0.00200162, acc 1
2016-09-07T04:39:26.297535: step 4925, loss 0.150599, acc 0.96
2016-09-07T04:39:26.995348: step 4926, loss 0.0181267, acc 1
2016-09-07T04:39:27.732378: step 4927, loss 0.00160277, acc 1
2016-09-07T04:39:28.426642: step 4928, loss 0.00302902, acc 1
2016-09-07T04:39:29.091837: step 4929, loss 0.00473958, acc 1
2016-09-07T04:39:29.778294: step 4930, loss 0.00675662, acc 1
2016-09-07T04:39:30.473951: step 4931, loss 0.0147072, acc 1
2016-09-07T04:39:31.149836: step 4932, loss 0.0252489, acc 1
2016-09-07T04:39:31.831838: step 4933, loss 0.00135728, acc 1
2016-09-07T04:39:32.552436: step 4934, loss 0.0147879, acc 1
2016-09-07T04:39:33.234024: step 4935, loss 0.0568872, acc 0.96
2016-09-07T04:39:33.915459: step 4936, loss 0.0228636, acc 1
2016-09-07T04:39:34.614630: step 4937, loss 0.0117283, acc 1
2016-09-07T04:39:35.316732: step 4938, loss 0.0968352, acc 0.98
2016-09-07T04:39:36.036113: step 4939, loss 0.00175059, acc 1
2016-09-07T04:39:36.714832: step 4940, loss 0.0308854, acc 0.98
2016-09-07T04:39:37.405921: step 4941, loss 0.0424867, acc 0.98
2016-09-07T04:39:38.073512: step 4942, loss 0.0231669, acc 0.98
2016-09-07T04:39:38.771681: step 4943, loss 0.00601124, acc 1
2016-09-07T04:39:39.445361: step 4944, loss 0.0147288, acc 1
2016-09-07T04:39:40.147131: step 4945, loss 0.0165852, acc 1
2016-09-07T04:39:40.843950: step 4946, loss 0.0857637, acc 0.96
2016-09-07T04:39:41.509445: step 4947, loss 0.026005, acc 0.98
2016-09-07T04:39:42.224861: step 4948, loss 0.00505733, acc 1
2016-09-07T04:39:42.931492: step 4949, loss 0.00134078, acc 1
2016-09-07T04:39:43.612933: step 4950, loss 0.0442562, acc 0.98
2016-09-07T04:39:44.294519: step 4951, loss 0.0514885, acc 0.96
2016-09-07T04:39:44.969705: step 4952, loss 0.0306789, acc 1
2016-09-07T04:39:45.650917: step 4953, loss 0.180351, acc 0.96
2016-09-07T04:39:46.313824: step 4954, loss 0.0150156, acc 1
2016-09-07T04:39:46.996746: step 4955, loss 0.0242863, acc 1
2016-09-07T04:39:47.680834: step 4956, loss 0.00669662, acc 1
2016-09-07T04:39:48.376915: step 4957, loss 0.031369, acc 0.98
2016-09-07T04:39:49.079669: step 4958, loss 0.0556259, acc 0.98
2016-09-07T04:39:49.790795: step 4959, loss 0.00908126, acc 1
2016-09-07T04:39:50.493977: step 4960, loss 0.0280624, acc 0.98
2016-09-07T04:39:51.171371: step 4961, loss 0.0314324, acc 0.98
2016-09-07T04:39:51.874870: step 4962, loss 0.0777272, acc 0.96
2016-09-07T04:39:52.557893: step 4963, loss 8.54062e-05, acc 1
2016-09-07T04:39:53.242831: step 4964, loss 0.00301407, acc 1
2016-09-07T04:39:53.933931: step 4965, loss 0.0172789, acc 1
2016-09-07T04:39:54.619366: step 4966, loss 0.0642615, acc 0.96
2016-09-07T04:39:55.292113: step 4967, loss 0.0259191, acc 0.98
2016-09-07T04:39:55.952329: step 4968, loss 0.00197695, acc 1
2016-09-07T04:39:56.661454: step 4969, loss 0.0291973, acc 0.98
2016-09-07T04:39:57.357795: step 4970, loss 0.0217147, acc 1
2016-09-07T04:39:58.050115: step 4971, loss 0.028265, acc 0.98
2016-09-07T04:39:58.720986: step 4972, loss 0.00341881, acc 1
2016-09-07T04:39:59.411419: step 4973, loss 0.129289, acc 0.98
2016-09-07T04:40:00.111837: step 4974, loss 0.0779854, acc 0.98
2016-09-07T04:40:00.799914: step 4975, loss 0.0274022, acc 0.98
2016-09-07T04:40:01.510579: step 4976, loss 0.0172722, acc 1
2016-09-07T04:40:02.219366: step 4977, loss 0.00800301, acc 1
2016-09-07T04:40:02.922539: step 4978, loss 0.00395752, acc 1
2016-09-07T04:40:03.615977: step 4979, loss 0.00834547, acc 1
2016-09-07T04:40:04.305280: step 4980, loss 0.0150146, acc 1
2016-09-07T04:40:05.002629: step 4981, loss 0.0144594, acc 1
2016-09-07T04:40:05.693202: step 4982, loss 0.00149623, acc 1
2016-09-07T04:40:06.383797: step 4983, loss 0.000314307, acc 1
2016-09-07T04:40:07.068993: step 4984, loss 0.028255, acc 1
2016-09-07T04:40:07.767423: step 4985, loss 0.0126486, acc 1
2016-09-07T04:40:08.455708: step 4986, loss 0.019119, acc 1
2016-09-07T04:40:09.113688: step 4987, loss 0.0397034, acc 0.98
2016-09-07T04:40:09.847050: step 4988, loss 0.00232085, acc 1
2016-09-07T04:40:10.533282: step 4989, loss 0.0121783, acc 1
2016-09-07T04:40:11.207857: step 4990, loss 0.0543426, acc 0.98
2016-09-07T04:40:11.901599: step 4991, loss 0.0480103, acc 0.98
2016-09-07T04:40:12.540208: step 4992, loss 0.00625442, acc 1
2016-09-07T04:40:13.234376: step 4993, loss 0.0331131, acc 0.98
2016-09-07T04:40:13.919445: step 4994, loss 0.0183815, acc 0.98
2016-09-07T04:40:14.621082: step 4995, loss 0.109843, acc 0.92
2016-09-07T04:40:15.303008: step 4996, loss 0.0008929, acc 1
2016-09-07T04:40:16.004614: step 4997, loss 0.0254478, acc 0.98
2016-09-07T04:40:16.696023: step 4998, loss 0.0806655, acc 0.96
2016-09-07T04:40:17.394295: step 4999, loss 0.000566164, acc 1
2016-09-07T04:40:18.087791: step 5000, loss 0.00355701, acc 1

Evaluation:
2016-09-07T04:40:21.365391: step 5000, loss 2.15862, acc 0.737336

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-5000

2016-09-07T04:40:23.103515: step 5001, loss 0.0205053, acc 0.98
2016-09-07T04:40:23.784499: step 5002, loss 0.0040266, acc 1
2016-09-07T04:40:24.456573: step 5003, loss 0.0130531, acc 1
2016-09-07T04:40:25.145360: step 5004, loss 0.015425, acc 1
2016-09-07T04:40:25.820559: step 5005, loss 0.0683156, acc 0.96
2016-09-07T04:40:26.518816: step 5006, loss 0.0246134, acc 0.98
2016-09-07T04:40:27.205600: step 5007, loss 0.001249, acc 1
2016-09-07T04:40:27.898996: step 5008, loss 0.088019, acc 0.98
2016-09-07T04:40:28.591966: step 5009, loss 0.0294693, acc 0.98
2016-09-07T04:40:29.292091: step 5010, loss 0.054873, acc 0.98
2016-09-07T04:40:29.988826: step 5011, loss 0.0238017, acc 0.98
2016-09-07T04:40:30.675567: step 5012, loss 0.10249, acc 0.98
2016-09-07T04:40:31.387269: step 5013, loss 0.00456228, acc 1
2016-09-07T04:40:32.066647: step 5014, loss 0.0453401, acc 0.98
2016-09-07T04:40:32.772246: step 5015, loss 0.0115435, acc 1
2016-09-07T04:40:33.455010: step 5016, loss 0.0101007, acc 1
2016-09-07T04:40:34.153242: step 5017, loss 0.00446069, acc 1
2016-09-07T04:40:34.842950: step 5018, loss 0.000884126, acc 1
2016-09-07T04:40:35.515717: step 5019, loss 0.00312651, acc 1
2016-09-07T04:40:36.231179: step 5020, loss 0.0176189, acc 1
2016-09-07T04:40:36.922686: step 5021, loss 0.00404973, acc 1
2016-09-07T04:40:37.653588: step 5022, loss 0.0285525, acc 0.98
2016-09-07T04:40:38.344329: step 5023, loss 0.021172, acc 0.98
2016-09-07T04:40:39.047469: step 5024, loss 0.194235, acc 0.9
2016-09-07T04:40:39.748825: step 5025, loss 0.0136654, acc 1
2016-09-07T04:40:40.416071: step 5026, loss 0.190159, acc 0.94
2016-09-07T04:40:41.109612: step 5027, loss 0.0346849, acc 0.96
2016-09-07T04:40:41.829801: step 5028, loss 0.0241381, acc 0.98
2016-09-07T04:40:42.495825: step 5029, loss 0.00456978, acc 1
2016-09-07T04:40:43.177789: step 5030, loss 0.0308614, acc 0.98
2016-09-07T04:40:43.851508: step 5031, loss 0.0110853, acc 1
2016-09-07T04:40:44.547436: step 5032, loss 0.00711301, acc 1
2016-09-07T04:40:45.180659: step 5033, loss 0.0144155, acc 1
2016-09-07T04:40:45.884690: step 5034, loss 0.0153934, acc 1
2016-09-07T04:40:46.559367: step 5035, loss 0.00130754, acc 1
2016-09-07T04:40:47.243861: step 5036, loss 0.0431168, acc 0.96
2016-09-07T04:40:47.949147: step 5037, loss 0.0227135, acc 0.98
2016-09-07T04:40:48.634587: step 5038, loss 0.0222779, acc 0.98
2016-09-07T04:40:49.351408: step 5039, loss 0.00234256, acc 1
2016-09-07T04:40:50.009515: step 5040, loss 0.00622331, acc 1
2016-09-07T04:40:50.717130: step 5041, loss 0.000410263, acc 1
2016-09-07T04:40:51.371149: step 5042, loss 0.0626014, acc 0.96
2016-09-07T04:40:52.041854: step 5043, loss 0.0240158, acc 0.98
2016-09-07T04:40:52.734062: step 5044, loss 0.0305224, acc 1
2016-09-07T04:40:53.424585: step 5045, loss 0.000133433, acc 1
2016-09-07T04:40:54.104070: step 5046, loss 0.0212455, acc 1
2016-09-07T04:40:54.787280: step 5047, loss 0.0225196, acc 1
2016-09-07T04:40:55.505918: step 5048, loss 0.0186746, acc 1
2016-09-07T04:40:56.186410: step 5049, loss 0.0146067, acc 1
2016-09-07T04:40:56.861113: step 5050, loss 7.46359e-05, acc 1
2016-09-07T04:40:57.551901: step 5051, loss 0.00509272, acc 1
2016-09-07T04:40:58.233402: step 5052, loss 0.00334759, acc 1
2016-09-07T04:40:58.917022: step 5053, loss 0.026172, acc 0.98
2016-09-07T04:40:59.610595: step 5054, loss 0.0185931, acc 0.98
2016-09-07T04:41:00.346891: step 5055, loss 0.0329926, acc 0.98
2016-09-07T04:41:01.018765: step 5056, loss 0.0892213, acc 0.96
2016-09-07T04:41:01.714839: step 5057, loss 0.00680908, acc 1
2016-09-07T04:41:02.391594: step 5058, loss 0.025305, acc 0.98
2016-09-07T04:41:03.092367: step 5059, loss 0.00366857, acc 1
2016-09-07T04:41:03.775361: step 5060, loss 0.00829407, acc 1
2016-09-07T04:41:04.427896: step 5061, loss 0.00142762, acc 1
2016-09-07T04:41:05.124306: step 5062, loss 0.0514654, acc 0.98
2016-09-07T04:41:05.810000: step 5063, loss 0.00949951, acc 1
2016-09-07T04:41:06.502771: step 5064, loss 0.00314059, acc 1
2016-09-07T04:41:07.188210: step 5065, loss 0.100345, acc 0.96
2016-09-07T04:41:07.885608: step 5066, loss 0.0249156, acc 0.98
2016-09-07T04:41:08.593453: step 5067, loss 0.0136036, acc 1
2016-09-07T04:41:09.254193: step 5068, loss 0.0171384, acc 0.98
2016-09-07T04:41:09.984741: step 5069, loss 0.00311597, acc 1
2016-09-07T04:41:10.680565: step 5070, loss 0.0188347, acc 0.98
2016-09-07T04:41:11.374683: step 5071, loss 0.032363, acc 1
2016-09-07T04:41:12.064595: step 5072, loss 0.000364955, acc 1
2016-09-07T04:41:12.738639: step 5073, loss 0.0213486, acc 1
2016-09-07T04:41:13.430848: step 5074, loss 0.00638651, acc 1
2016-09-07T04:41:14.102297: step 5075, loss 0.0300273, acc 0.98
2016-09-07T04:41:14.804384: step 5076, loss 0.000797499, acc 1
2016-09-07T04:41:15.520898: step 5077, loss 0.00725388, acc 1
2016-09-07T04:41:16.196166: step 5078, loss 0.00439608, acc 1
2016-09-07T04:41:16.889125: step 5079, loss 0.000275907, acc 1
2016-09-07T04:41:17.561694: step 5080, loss 0.0153418, acc 1
2016-09-07T04:41:18.284239: step 5081, loss 0.00334591, acc 1
2016-09-07T04:41:18.954145: step 5082, loss 0.00662103, acc 1
2016-09-07T04:41:19.645268: step 5083, loss 0.0144741, acc 1
2016-09-07T04:41:20.345526: step 5084, loss 0.0228806, acc 0.98
2016-09-07T04:41:21.023147: step 5085, loss 0.00962884, acc 1
2016-09-07T04:41:21.711187: step 5086, loss 0.0179973, acc 1
2016-09-07T04:41:22.416919: step 5087, loss 0.02753, acc 1
2016-09-07T04:41:23.134013: step 5088, loss 0.0808231, acc 0.98
2016-09-07T04:41:23.812957: step 5089, loss 0.00637744, acc 1
2016-09-07T04:41:24.499797: step 5090, loss 0.0073897, acc 1
2016-09-07T04:41:25.192014: step 5091, loss 0.0392061, acc 0.98
2016-09-07T04:41:25.882854: step 5092, loss 0.0132861, acc 1
2016-09-07T04:41:26.555802: step 5093, loss 0.0111487, acc 1
2016-09-07T04:41:27.236947: step 5094, loss 0.011136, acc 1
2016-09-07T04:41:27.945616: step 5095, loss 0.0196346, acc 0.98
2016-09-07T04:41:28.654322: step 5096, loss 0.00882848, acc 1
2016-09-07T04:41:29.356145: step 5097, loss 0.0420765, acc 0.98
2016-09-07T04:41:30.052844: step 5098, loss 0.044045, acc 0.96
2016-09-07T04:41:30.751736: step 5099, loss 0.0096517, acc 1
2016-09-07T04:41:31.482337: step 5100, loss 0.0169445, acc 0.98

Evaluation:
2016-09-07T04:41:34.782607: step 5100, loss 2.5694, acc 0.731707

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-5100

2016-09-07T04:41:36.477986: step 5101, loss 0.00275151, acc 1
2016-09-07T04:41:37.159977: step 5102, loss 0.0901904, acc 0.98
2016-09-07T04:41:37.851269: step 5103, loss 0.0346092, acc 0.98
2016-09-07T04:41:38.546072: step 5104, loss 0.000803786, acc 1
2016-09-07T04:41:39.259428: step 5105, loss 0.00914793, acc 1
2016-09-07T04:41:39.984526: step 5106, loss 0.0111753, acc 1
2016-09-07T04:41:40.684252: step 5107, loss 0.000173521, acc 1
2016-09-07T04:41:41.368200: step 5108, loss 0.0347155, acc 0.98
2016-09-07T04:41:42.056017: step 5109, loss 0.036213, acc 0.98
2016-09-07T04:41:42.728113: step 5110, loss 0.0225176, acc 0.98
2016-09-07T04:41:43.417426: step 5111, loss 0.0160912, acc 1
2016-09-07T04:41:44.116062: step 5112, loss 0.00314807, acc 1
2016-09-07T04:41:44.824870: step 5113, loss 0.0319069, acc 0.98
2016-09-07T04:41:45.515885: step 5114, loss 0.0174031, acc 0.98
2016-09-07T04:41:46.198083: step 5115, loss 9.20229e-05, acc 1
2016-09-07T04:41:46.882208: step 5116, loss 0.055761, acc 0.98
2016-09-07T04:41:47.566248: step 5117, loss 0.0158062, acc 1
2016-09-07T04:41:48.240978: step 5118, loss 0.0823781, acc 0.98
2016-09-07T04:41:48.904559: step 5119, loss 0.0166127, acc 0.98
2016-09-07T04:41:49.604845: step 5120, loss 0.106521, acc 0.98
2016-09-07T04:41:50.270577: step 5121, loss 0.00148312, acc 1
2016-09-07T04:41:50.963904: step 5122, loss 0.00601858, acc 1
2016-09-07T04:41:51.653776: step 5123, loss 0.0284183, acc 0.98
2016-09-07T04:41:52.328921: step 5124, loss 0.0224343, acc 0.98
2016-09-07T04:41:53.025275: step 5125, loss 0.00329578, acc 1
2016-09-07T04:41:53.716061: step 5126, loss 0.0524732, acc 0.96
2016-09-07T04:41:54.436905: step 5127, loss 0.049926, acc 0.98
2016-09-07T04:41:55.112140: step 5128, loss 0.00917774, acc 1
2016-09-07T04:41:55.813027: step 5129, loss 0.0227411, acc 0.98
2016-09-07T04:41:56.510830: step 5130, loss 0.000141464, acc 1
2016-09-07T04:41:57.196159: step 5131, loss 0.0646954, acc 0.98
2016-09-07T04:41:57.894614: step 5132, loss 0.00826359, acc 1
2016-09-07T04:41:58.562504: step 5133, loss 0.00802676, acc 1
2016-09-07T04:41:59.268535: step 5134, loss 0.00712447, acc 1
2016-09-07T04:41:59.956970: step 5135, loss 0.0233955, acc 1
2016-09-07T04:42:00.666881: step 5136, loss 0.0563005, acc 0.96
2016-09-07T04:42:01.340884: step 5137, loss 0.00703858, acc 1
2016-09-07T04:42:02.027614: step 5138, loss 0.000801874, acc 1
2016-09-07T04:42:02.717336: step 5139, loss 0.0502568, acc 0.96
2016-09-07T04:42:03.384047: step 5140, loss 0.0767061, acc 0.98
2016-09-07T04:42:04.096393: step 5141, loss 0.00702219, acc 1
2016-09-07T04:42:04.802865: step 5142, loss 0.257393, acc 0.96
2016-09-07T04:42:05.511121: step 5143, loss 0.0802699, acc 0.96
2016-09-07T04:42:06.202199: step 5144, loss 0.0203591, acc 1
2016-09-07T04:42:06.889472: step 5145, loss 0.04728, acc 0.98
2016-09-07T04:42:07.605958: step 5146, loss 0.0565579, acc 0.94
2016-09-07T04:42:08.316110: step 5147, loss 0.03568, acc 0.98
2016-09-07T04:42:09.031766: step 5148, loss 0.00111985, acc 1
2016-09-07T04:42:09.732507: step 5149, loss 0.0450649, acc 0.98
2016-09-07T04:42:10.454706: step 5150, loss 0.0133515, acc 1
2016-09-07T04:42:11.140712: step 5151, loss 0.00883323, acc 1
2016-09-07T04:42:11.804547: step 5152, loss 0.0165205, acc 0.98
2016-09-07T04:42:12.520807: step 5153, loss 0.0222566, acc 1
2016-09-07T04:42:13.223327: step 5154, loss 0.0415803, acc 0.98
2016-09-07T04:42:13.926698: step 5155, loss 0.0216535, acc 0.98
2016-09-07T04:42:14.610021: step 5156, loss 0.0150843, acc 1
2016-09-07T04:42:15.298645: step 5157, loss 0.00983426, acc 1
2016-09-07T04:42:15.988901: step 5158, loss 0.0335604, acc 1
2016-09-07T04:42:16.654557: step 5159, loss 0.000145264, acc 1
2016-09-07T04:42:17.365261: step 5160, loss 0.0160007, acc 1
2016-09-07T04:42:18.029004: step 5161, loss 0.00207381, acc 1
2016-09-07T04:42:18.718670: step 5162, loss 0.0532793, acc 0.96
2016-09-07T04:42:19.403073: step 5163, loss 0.00283311, acc 1
2016-09-07T04:42:20.085573: step 5164, loss 0.0419274, acc 0.98
2016-09-07T04:42:20.771407: step 5165, loss 0.00866558, acc 1
2016-09-07T04:42:21.446533: step 5166, loss 0.0211813, acc 1
2016-09-07T04:42:22.139277: step 5167, loss 0.00031051, acc 1
2016-09-07T04:42:22.846152: step 5168, loss 0.0272653, acc 0.98
2016-09-07T04:42:23.527487: step 5169, loss 0.024036, acc 1
2016-09-07T04:42:24.211094: step 5170, loss 0.0171629, acc 1
2016-09-07T04:42:24.898619: step 5171, loss 0.0129948, acc 1
2016-09-07T04:42:25.599439: step 5172, loss 0.026431, acc 0.98
2016-09-07T04:42:26.262735: step 5173, loss 0.00693862, acc 1
2016-09-07T04:42:26.957042: step 5174, loss 0.000110681, acc 1
2016-09-07T04:42:27.633096: step 5175, loss 0.000195505, acc 1
2016-09-07T04:42:28.315219: step 5176, loss 0.00526997, acc 1
2016-09-07T04:42:29.002390: step 5177, loss 0.0103008, acc 1
2016-09-07T04:42:29.680933: step 5178, loss 0.0158506, acc 0.98
2016-09-07T04:42:30.379851: step 5179, loss 0.00671879, acc 1
2016-09-07T04:42:31.050308: step 5180, loss 0.0144496, acc 0.98
2016-09-07T04:42:31.766509: step 5181, loss 0.0348458, acc 0.98
2016-09-07T04:42:32.456937: step 5182, loss 0.0285893, acc 0.98
2016-09-07T04:42:33.135312: step 5183, loss 0.0624555, acc 0.98
2016-09-07T04:42:33.770595: step 5184, loss 0.0313071, acc 0.977273
2016-09-07T04:42:34.468029: step 5185, loss 0.014252, acc 1
2016-09-07T04:42:35.158775: step 5186, loss 0.0193714, acc 1
2016-09-07T04:42:35.837695: step 5187, loss 0.0172628, acc 1
2016-09-07T04:42:36.562687: step 5188, loss 0.0210622, acc 1
2016-09-07T04:42:37.255615: step 5189, loss 0.00700981, acc 1
2016-09-07T04:42:37.943083: step 5190, loss 0.0839943, acc 0.96
2016-09-07T04:42:38.646119: step 5191, loss 0.00348973, acc 1
2016-09-07T04:42:39.318110: step 5192, loss 0.0403223, acc 0.98
2016-09-07T04:42:40.011385: step 5193, loss 0.000679856, acc 1
2016-09-07T04:42:40.690411: step 5194, loss 0.0311485, acc 0.98
2016-09-07T04:42:41.454501: step 5195, loss 0.0274716, acc 0.98
2016-09-07T04:42:42.144253: step 5196, loss 0.0277682, acc 0.98
2016-09-07T04:42:42.813862: step 5197, loss 0.0738079, acc 0.94
2016-09-07T04:42:43.520322: step 5198, loss 0.0265458, acc 1
2016-09-07T04:42:44.217021: step 5199, loss 0.00110362, acc 1
2016-09-07T04:42:44.917379: step 5200, loss 0.000363825, acc 1

Evaluation:
2016-09-07T04:42:48.210279: step 5200, loss 2.567, acc 0.723265

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-5200

2016-09-07T04:42:49.840315: step 5201, loss 0.0942158, acc 0.96
2016-09-07T04:42:50.535999: step 5202, loss 0.0254648, acc 0.98
2016-09-07T04:42:51.234330: step 5203, loss 0.0427033, acc 0.98
2016-09-07T04:42:51.925543: step 5204, loss 0.0480027, acc 0.96
2016-09-07T04:42:52.595110: step 5205, loss 0.000413456, acc 1
2016-09-07T04:42:53.303620: step 5206, loss 0.0358143, acc 0.98
2016-09-07T04:42:54.003940: step 5207, loss 0.0034483, acc 1
2016-09-07T04:42:54.676491: step 5208, loss 0.000456657, acc 1
2016-09-07T04:42:55.354913: step 5209, loss 0.00461737, acc 1
2016-09-07T04:42:56.053428: step 5210, loss 0.00183567, acc 1
2016-09-07T04:42:56.750254: step 5211, loss 0.0141593, acc 0.98
2016-09-07T04:42:57.466336: step 5212, loss 0.000825389, acc 1
2016-09-07T04:42:58.171681: step 5213, loss 0.00455386, acc 1
2016-09-07T04:42:58.863849: step 5214, loss 0.0270895, acc 1
2016-09-07T04:42:59.552388: step 5215, loss 0.00167342, acc 1
2016-09-07T04:43:00.252179: step 5216, loss 0.0661191, acc 0.96
2016-09-07T04:43:00.909549: step 5217, loss 0.0135256, acc 1
2016-09-07T04:43:01.616430: step 5218, loss 0.00913518, acc 1
2016-09-07T04:43:02.288840: step 5219, loss 0.000444358, acc 1
2016-09-07T04:43:02.999710: step 5220, loss 0.1162, acc 0.94
2016-09-07T04:43:03.681684: step 5221, loss 0.0328718, acc 0.98
2016-09-07T04:43:04.359497: step 5222, loss 0.00329577, acc 1
2016-09-07T04:43:05.047751: step 5223, loss 0.0041814, acc 1
2016-09-07T04:43:05.744089: step 5224, loss 0.00188106, acc 1
2016-09-07T04:43:06.450072: step 5225, loss 0.0240043, acc 1
2016-09-07T04:43:07.118461: step 5226, loss 0.013132, acc 1
2016-09-07T04:43:07.821250: step 5227, loss 0.000691959, acc 1
2016-09-07T04:43:08.503517: step 5228, loss 0.0338327, acc 0.98
2016-09-07T04:43:09.176677: step 5229, loss 0.037673, acc 0.98
2016-09-07T04:43:09.859335: step 5230, loss 0.0794558, acc 0.96
2016-09-07T04:43:10.560388: step 5231, loss 0.0125804, acc 1
2016-09-07T04:43:11.248621: step 5232, loss 0.00548356, acc 1
2016-09-07T04:43:11.919630: step 5233, loss 0.0269513, acc 0.98
2016-09-07T04:43:12.627529: step 5234, loss 0.0173608, acc 1
2016-09-07T04:43:13.307882: step 5235, loss 0.0486871, acc 0.96
2016-09-07T04:43:13.997240: step 5236, loss 0.0121341, acc 1
2016-09-07T04:43:14.690881: step 5237, loss 0.00736577, acc 1
2016-09-07T04:43:15.358429: step 5238, loss 0.0388646, acc 0.98
2016-09-07T04:43:16.040769: step 5239, loss 0.00079573, acc 1
2016-09-07T04:43:16.730771: step 5240, loss 0.00875812, acc 1
2016-09-07T04:43:17.428799: step 5241, loss 0.018279, acc 1
2016-09-07T04:43:18.107820: step 5242, loss 0.000712626, acc 1
2016-09-07T04:43:18.800556: step 5243, loss 0.0386046, acc 0.98
2016-09-07T04:43:19.487746: step 5244, loss 0.0125214, acc 1
2016-09-07T04:43:20.175049: step 5245, loss 0.0201797, acc 0.98
2016-09-07T04:43:20.886987: step 5246, loss 0.000939066, acc 1
2016-09-07T04:43:21.581028: step 5247, loss 0.0408341, acc 0.98
2016-09-07T04:43:22.283892: step 5248, loss 0.0558883, acc 0.96
2016-09-07T04:43:22.949932: step 5249, loss 0.0378504, acc 0.98
2016-09-07T04:43:23.629530: step 5250, loss 0.0169302, acc 1
2016-09-07T04:43:24.302970: step 5251, loss 0.0219435, acc 1
2016-09-07T04:43:24.984155: step 5252, loss 0.0132295, acc 1
2016-09-07T04:43:25.675286: step 5253, loss 0.0233551, acc 0.98
2016-09-07T04:43:26.326099: step 5254, loss 0.00752124, acc 1
2016-09-07T04:43:27.037500: step 5255, loss 0.00324204, acc 1
2016-09-07T04:43:27.693939: step 5256, loss 0.0649492, acc 0.96
2016-09-07T04:43:28.380932: step 5257, loss 0.00132553, acc 1
2016-09-07T04:43:29.062177: step 5258, loss 0.00142101, acc 1
2016-09-07T04:43:29.762880: step 5259, loss 0.000489673, acc 1
2016-09-07T04:43:30.421506: step 5260, loss 0.0309745, acc 1
2016-09-07T04:43:31.090620: step 5261, loss 0.0229081, acc 1
2016-09-07T04:43:31.797120: step 5262, loss 0.00086949, acc 1
2016-09-07T04:43:32.466240: step 5263, loss 0.0194019, acc 1
2016-09-07T04:43:33.171495: step 5264, loss 0.087699, acc 0.94
2016-09-07T04:43:33.856503: step 5265, loss 0.00919624, acc 1
2016-09-07T04:43:34.545016: step 5266, loss 0.0384602, acc 0.98
2016-09-07T04:43:35.234356: step 5267, loss 0.0254776, acc 1
2016-09-07T04:43:35.931380: step 5268, loss 0.00763131, acc 1
2016-09-07T04:43:36.627394: step 5269, loss 0.0186523, acc 0.98
2016-09-07T04:43:37.304866: step 5270, loss 0.0169369, acc 1
2016-09-07T04:43:38.009526: step 5271, loss 0.0154028, acc 1
2016-09-07T04:43:38.693357: step 5272, loss 0.0528815, acc 0.96
2016-09-07T04:43:39.385970: step 5273, loss 0.000186543, acc 1
2016-09-07T04:43:40.061668: step 5274, loss 0.0971647, acc 0.98
2016-09-07T04:43:40.743272: step 5275, loss 0.0599726, acc 0.98
2016-09-07T04:43:41.461242: step 5276, loss 0.0297661, acc 0.98
2016-09-07T04:43:42.123782: step 5277, loss 0.0174561, acc 0.98
2016-09-07T04:43:42.843345: step 5278, loss 0.0319787, acc 1
2016-09-07T04:43:43.530656: step 5279, loss 0.0491373, acc 0.96
2016-09-07T04:43:44.227688: step 5280, loss 0.0548817, acc 0.98
2016-09-07T04:43:44.910854: step 5281, loss 0.00237421, acc 1
2016-09-07T04:43:45.572706: step 5282, loss 0.0403605, acc 0.96
2016-09-07T04:43:46.265637: step 5283, loss 0.000956656, acc 1
2016-09-07T04:43:46.938292: step 5284, loss 0.0534674, acc 0.94
2016-09-07T04:43:47.665998: step 5285, loss 0.0278016, acc 0.98
2016-09-07T04:43:48.364302: step 5286, loss 0.0282494, acc 1
2016-09-07T04:43:49.048570: step 5287, loss 0.0148572, acc 1
2016-09-07T04:43:49.746366: step 5288, loss 0.0481425, acc 0.98
2016-09-07T04:43:50.442395: step 5289, loss 0.00441663, acc 1
2016-09-07T04:43:51.142156: step 5290, loss 0.0457752, acc 0.96
2016-09-07T04:43:51.815943: step 5291, loss 0.0338254, acc 0.98
2016-09-07T04:43:52.500634: step 5292, loss 0.0350284, acc 0.96
2016-09-07T04:43:53.198226: step 5293, loss 0.0123545, acc 1
2016-09-07T04:43:53.884282: step 5294, loss 0.031626, acc 0.98
2016-09-07T04:43:54.572791: step 5295, loss 0.126426, acc 0.96
2016-09-07T04:43:55.235316: step 5296, loss 0.0309435, acc 0.98
2016-09-07T04:43:55.945508: step 5297, loss 0.0117547, acc 1
2016-09-07T04:43:56.626509: step 5298, loss 0.0124833, acc 1
2016-09-07T04:43:57.318654: step 5299, loss 0.0216811, acc 1
2016-09-07T04:43:58.009853: step 5300, loss 0.02051, acc 1

Evaluation:
2016-09-07T04:44:01.380774: step 5300, loss 2.62722, acc 0.727955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-5300

2016-09-07T04:44:03.114718: step 5301, loss 0.0173634, acc 1
2016-09-07T04:44:03.825541: step 5302, loss 0.00571645, acc 1
2016-09-07T04:44:04.520689: step 5303, loss 0.0116891, acc 1
2016-09-07T04:44:05.210133: step 5304, loss 0.00578918, acc 1
2016-09-07T04:44:05.904665: step 5305, loss 0.0302459, acc 0.98
2016-09-07T04:44:06.587491: step 5306, loss 0.00824266, acc 1
2016-09-07T04:44:07.264273: step 5307, loss 0.0270826, acc 0.98
2016-09-07T04:44:07.981744: step 5308, loss 0.00545444, acc 1
2016-09-07T04:44:08.669583: step 5309, loss 0.158726, acc 0.96
2016-09-07T04:44:09.361792: step 5310, loss 0.0180109, acc 1
2016-09-07T04:44:10.041066: step 5311, loss 0.000733946, acc 1
2016-09-07T04:44:10.730527: step 5312, loss 0.0339177, acc 0.98
2016-09-07T04:44:11.412978: step 5313, loss 0.0157608, acc 1
2016-09-07T04:44:12.092699: step 5314, loss 0.00169094, acc 1
2016-09-07T04:44:12.800620: step 5315, loss 0.0404407, acc 0.98
2016-09-07T04:44:13.461353: step 5316, loss 0.0149357, acc 1
2016-09-07T04:44:14.142641: step 5317, loss 0.041797, acc 0.98
2016-09-07T04:44:14.820252: step 5318, loss 0.0221521, acc 1
2016-09-07T04:44:15.507286: step 5319, loss 0.0257136, acc 0.98
2016-09-07T04:44:16.196069: step 5320, loss 0.0383369, acc 0.98
2016-09-07T04:44:16.865956: step 5321, loss 0.0286109, acc 0.98
2016-09-07T04:44:17.573878: step 5322, loss 0.0152192, acc 0.98
2016-09-07T04:44:18.248719: step 5323, loss 0.030187, acc 0.98
2016-09-07T04:44:18.916651: step 5324, loss 0.0306468, acc 0.98
2016-09-07T04:44:19.621221: step 5325, loss 0.0389977, acc 0.98
2016-09-07T04:44:20.299165: step 5326, loss 0.0295842, acc 0.98
2016-09-07T04:44:20.979532: step 5327, loss 0.0445191, acc 0.98
2016-09-07T04:44:21.657664: step 5328, loss 0.0501776, acc 0.98
2016-09-07T04:44:22.359303: step 5329, loss 0.16499, acc 0.96
2016-09-07T04:44:23.047909: step 5330, loss 0.00945228, acc 1
2016-09-07T04:44:23.730241: step 5331, loss 0.0252542, acc 0.98
2016-09-07T04:44:24.414715: step 5332, loss 0.0163461, acc 0.98
2016-09-07T04:44:25.109360: step 5333, loss 0.0443368, acc 0.98
2016-09-07T04:44:25.789393: step 5334, loss 0.00138988, acc 1
2016-09-07T04:44:26.473048: step 5335, loss 0.0188197, acc 0.98
2016-09-07T04:44:27.166566: step 5336, loss 0.0689751, acc 0.96
2016-09-07T04:44:27.848934: step 5337, loss 0.0372204, acc 0.96
2016-09-07T04:44:28.540051: step 5338, loss 0.0430224, acc 0.98
2016-09-07T04:44:29.235410: step 5339, loss 0.0520477, acc 0.98
2016-09-07T04:44:29.955237: step 5340, loss 0.00112477, acc 1
2016-09-07T04:44:30.641443: step 5341, loss 3.85402e-05, acc 1
2016-09-07T04:44:31.300126: step 5342, loss 0.0192856, acc 1
2016-09-07T04:44:32.018060: step 5343, loss 0.002728, acc 1
2016-09-07T04:44:32.701452: step 5344, loss 0.00960183, acc 1
2016-09-07T04:44:33.382885: step 5345, loss 0.00329507, acc 1
2016-09-07T04:44:34.057561: step 5346, loss 0.0146582, acc 1
2016-09-07T04:44:34.735641: step 5347, loss 0.0202046, acc 0.98
2016-09-07T04:44:35.442208: step 5348, loss 0.00655993, acc 1
2016-09-07T04:44:36.113917: step 5349, loss 0.0149242, acc 1
2016-09-07T04:44:36.816173: step 5350, loss 0.0115947, acc 1
2016-09-07T04:44:37.491383: step 5351, loss 0.0095598, acc 1
2016-09-07T04:44:38.183190: step 5352, loss 0.00821158, acc 1
2016-09-07T04:44:38.846608: step 5353, loss 0.00966041, acc 1
2016-09-07T04:44:39.528533: step 5354, loss 0.0300771, acc 1
2016-09-07T04:44:40.214039: step 5355, loss 0.0184423, acc 0.98
2016-09-07T04:44:40.897017: step 5356, loss 0.00155347, acc 1
2016-09-07T04:44:41.601559: step 5357, loss 0.0202713, acc 0.98
2016-09-07T04:44:42.260141: step 5358, loss 0.0212167, acc 1
2016-09-07T04:44:42.945727: step 5359, loss 0.00408277, acc 1
2016-09-07T04:44:43.630291: step 5360, loss 0.0199161, acc 1
2016-09-07T04:44:44.318279: step 5361, loss 0.0114591, acc 1
2016-09-07T04:44:45.013505: step 5362, loss 0.0688371, acc 0.94
2016-09-07T04:44:45.697376: step 5363, loss 0.0324486, acc 0.98
2016-09-07T04:44:46.415403: step 5364, loss 0.0704756, acc 0.98
2016-09-07T04:44:47.106442: step 5365, loss 0.0366738, acc 0.96
2016-09-07T04:44:47.811578: step 5366, loss 0.0333679, acc 0.98
2016-09-07T04:44:48.512038: step 5367, loss 0.00162665, acc 1
2016-09-07T04:44:49.194412: step 5368, loss 0.0307715, acc 0.98
2016-09-07T04:44:49.881116: step 5369, loss 0.000645504, acc 1
2016-09-07T04:44:50.547208: step 5370, loss 0.00814206, acc 1
2016-09-07T04:44:51.258335: step 5371, loss 0.0561226, acc 0.96
2016-09-07T04:44:51.954189: step 5372, loss 0.0131646, acc 1
2016-09-07T04:44:52.645109: step 5373, loss 0.0327891, acc 0.98
2016-09-07T04:44:53.342946: step 5374, loss 0.00412549, acc 1
2016-09-07T04:44:54.028518: step 5375, loss 0.0814295, acc 0.96
2016-09-07T04:44:54.681310: step 5376, loss 0.00679299, acc 1
2016-09-07T04:44:55.365056: step 5377, loss 0.0219205, acc 0.98
2016-09-07T04:44:56.072992: step 5378, loss 0.0131573, acc 1
2016-09-07T04:44:56.744078: step 5379, loss 0.00972126, acc 1
2016-09-07T04:44:57.427259: step 5380, loss 0.0426503, acc 0.96
2016-09-07T04:44:58.127107: step 5381, loss 0.0157588, acc 1
2016-09-07T04:44:58.817972: step 5382, loss 0.00702011, acc 1
2016-09-07T04:44:59.494352: step 5383, loss 0.0209819, acc 0.98
2016-09-07T04:45:00.151919: step 5384, loss 0.0374918, acc 0.96
2016-09-07T04:45:00.883278: step 5385, loss 0.00148468, acc 1
2016-09-07T04:45:01.575188: step 5386, loss 0.123785, acc 0.96
2016-09-07T04:45:02.267020: step 5387, loss 0.0276903, acc 0.98
2016-09-07T04:45:02.955734: step 5388, loss 0.00322074, acc 1
2016-09-07T04:45:03.654071: step 5389, loss 0.0123406, acc 1
2016-09-07T04:45:04.372012: step 5390, loss 0.0355844, acc 0.98
2016-09-07T04:45:05.038478: step 5391, loss 0.0305361, acc 0.98
2016-09-07T04:45:05.742468: step 5392, loss 0.0155064, acc 0.98
2016-09-07T04:45:06.421906: step 5393, loss 0.0142229, acc 1
2016-09-07T04:45:07.105229: step 5394, loss 0.152228, acc 0.96
2016-09-07T04:45:07.792972: step 5395, loss 0.000300682, acc 1
2016-09-07T04:45:08.478349: step 5396, loss 0.0540331, acc 0.98
2016-09-07T04:45:09.188999: step 5397, loss 0.0137988, acc 1
2016-09-07T04:45:09.851531: step 5398, loss 0.18267, acc 0.94
2016-09-07T04:45:10.564104: step 5399, loss 0.000612973, acc 1
2016-09-07T04:45:11.259825: step 5400, loss 0.00252354, acc 1

Evaluation:
2016-09-07T04:45:14.600390: step 5400, loss 2.4237, acc 0.726079

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-5400

2016-09-07T04:45:16.253571: step 5401, loss 0.0320267, acc 0.98
2016-09-07T04:45:16.960103: step 5402, loss 0.0381852, acc 0.98
2016-09-07T04:45:17.678147: step 5403, loss 0.0129782, acc 1
2016-09-07T04:45:18.344630: step 5404, loss 0.00562016, acc 1
2016-09-07T04:45:19.033523: step 5405, loss 0.038546, acc 0.98
2016-09-07T04:45:19.734889: step 5406, loss 0.0206155, acc 1
2016-09-07T04:45:20.434105: step 5407, loss 0.0537405, acc 0.96
2016-09-07T04:45:21.137451: step 5408, loss 0.0912499, acc 0.98
2016-09-07T04:45:21.792074: step 5409, loss 0.0392635, acc 1
2016-09-07T04:45:22.509995: step 5410, loss 0.00803619, acc 1
2016-09-07T04:45:23.201966: step 5411, loss 0.0127854, acc 1
2016-09-07T04:45:23.877955: step 5412, loss 0.0216116, acc 1
2016-09-07T04:45:24.594553: step 5413, loss 0.00352196, acc 1
2016-09-07T04:45:25.294341: step 5414, loss 0.00508039, acc 1
2016-09-07T04:45:25.996075: step 5415, loss 0.00101125, acc 1
2016-09-07T04:45:26.664653: step 5416, loss 9.64968e-05, acc 1
2016-09-07T04:45:27.355947: step 5417, loss 0.0342298, acc 0.98
2016-09-07T04:45:28.051045: step 5418, loss 0.0384081, acc 0.98
2016-09-07T04:45:28.748134: step 5419, loss 0.0136521, acc 1
2016-09-07T04:45:29.426462: step 5420, loss 0.076868, acc 0.98
2016-09-07T04:45:30.122894: step 5421, loss 0.106886, acc 0.98
2016-09-07T04:45:30.829451: step 5422, loss 0.00809888, acc 1
2016-09-07T04:45:31.505579: step 5423, loss 0.0369084, acc 0.98
2016-09-07T04:45:32.227859: step 5424, loss 0.0200286, acc 1
2016-09-07T04:45:32.918650: step 5425, loss 0.00146263, acc 1
2016-09-07T04:45:33.651629: step 5426, loss 0.0424368, acc 0.98
2016-09-07T04:45:34.334614: step 5427, loss 0.10456, acc 0.96
2016-09-07T04:45:35.011742: step 5428, loss 0.000260817, acc 1
2016-09-07T04:45:35.726724: step 5429, loss 0.00840897, acc 1
2016-09-07T04:45:36.405795: step 5430, loss 0.0289872, acc 0.98
2016-09-07T04:45:37.081417: step 5431, loss 0.150334, acc 0.96
2016-09-07T04:45:37.774187: step 5432, loss 0.0216134, acc 0.98
2016-09-07T04:45:38.441392: step 5433, loss 0.0258518, acc 1
2016-09-07T04:45:39.138208: step 5434, loss 0.000763141, acc 1
2016-09-07T04:45:39.808867: step 5435, loss 0.0152035, acc 1
2016-09-07T04:45:40.533348: step 5436, loss 0.0103299, acc 1
2016-09-07T04:45:41.224324: step 5437, loss 0.0290205, acc 0.98
2016-09-07T04:45:41.906451: step 5438, loss 0.00369221, acc 1
2016-09-07T04:45:42.586120: step 5439, loss 0.0402251, acc 0.96
2016-09-07T04:45:43.279415: step 5440, loss 0.0172675, acc 1
2016-09-07T04:45:43.970465: step 5441, loss 0.00416461, acc 1
2016-09-07T04:45:44.616549: step 5442, loss 0.0207802, acc 0.98
2016-09-07T04:45:45.325753: step 5443, loss 0.0220158, acc 0.98
2016-09-07T04:45:46.022482: step 5444, loss 0.00135358, acc 1
2016-09-07T04:45:46.686047: step 5445, loss 0.031711, acc 0.98
2016-09-07T04:45:47.384540: step 5446, loss 0.143784, acc 0.96
2016-09-07T04:45:48.072391: step 5447, loss 0.00224892, acc 1
2016-09-07T04:45:48.781557: step 5448, loss 0.0163409, acc 1
2016-09-07T04:45:49.464115: step 5449, loss 4.82846e-05, acc 1
2016-09-07T04:45:50.166895: step 5450, loss 0.00419692, acc 1
2016-09-07T04:45:50.856420: step 5451, loss 0.00874642, acc 1
2016-09-07T04:45:51.545605: step 5452, loss 0.158007, acc 0.92
2016-09-07T04:45:52.219678: step 5453, loss 0.00143547, acc 1
2016-09-07T04:45:52.913806: step 5454, loss 0.0177054, acc 1
2016-09-07T04:45:53.603216: step 5455, loss 0.0196093, acc 0.98
2016-09-07T04:45:54.267927: step 5456, loss 0.0595187, acc 0.98
2016-09-07T04:45:54.978718: step 5457, loss 0.0230441, acc 0.98
2016-09-07T04:45:55.659384: step 5458, loss 0.020269, acc 1
2016-09-07T04:45:56.361648: step 5459, loss 0.0294698, acc 0.98
2016-09-07T04:45:57.052010: step 5460, loss 0.0125533, acc 1
2016-09-07T04:45:57.757426: step 5461, loss 0.154349, acc 0.98
2016-09-07T04:45:58.461231: step 5462, loss 0.0178395, acc 1
2016-09-07T04:45:59.133008: step 5463, loss 0.00570361, acc 1
2016-09-07T04:45:59.845882: step 5464, loss 0.031401, acc 0.98
2016-09-07T04:46:00.577375: step 5465, loss 0.0258368, acc 0.98
2016-09-07T04:46:01.271540: step 5466, loss 0.0482251, acc 0.98
2016-09-07T04:46:01.970529: step 5467, loss 0.0314326, acc 0.98
2016-09-07T04:46:02.666309: step 5468, loss 0.0226709, acc 1
2016-09-07T04:46:03.381678: step 5469, loss 0.0654561, acc 0.96
2016-09-07T04:46:04.069840: step 5470, loss 0.00265775, acc 1
2016-09-07T04:46:04.746769: step 5471, loss 0.0245168, acc 1
2016-09-07T04:46:05.431849: step 5472, loss 0.0512856, acc 0.96
2016-09-07T04:46:06.114225: step 5473, loss 0.160821, acc 0.94
2016-09-07T04:46:06.817903: step 5474, loss 0.116309, acc 0.94
2016-09-07T04:46:07.516402: step 5475, loss 0.00423305, acc 1
2016-09-07T04:46:08.233814: step 5476, loss 0.0257779, acc 1
2016-09-07T04:46:08.943767: step 5477, loss 0.00910934, acc 1
2016-09-07T04:46:09.635358: step 5478, loss 0.0182749, acc 0.98
2016-09-07T04:46:10.335938: step 5479, loss 0.0745644, acc 0.96
2016-09-07T04:46:11.023051: step 5480, loss 0.0013026, acc 1
2016-09-07T04:46:11.732172: step 5481, loss 0.00763107, acc 1
2016-09-07T04:46:12.389865: step 5482, loss 0.0545633, acc 0.98
2016-09-07T04:46:13.087808: step 5483, loss 0.01795, acc 1
2016-09-07T04:46:13.766069: step 5484, loss 0.0606104, acc 0.96
2016-09-07T04:46:14.445520: step 5485, loss 0.00470643, acc 1
2016-09-07T04:46:15.138108: step 5486, loss 0.0115482, acc 1
2016-09-07T04:46:15.832929: step 5487, loss 0.0710086, acc 0.94
2016-09-07T04:46:16.510838: step 5488, loss 0.0099289, acc 1
2016-09-07T04:46:17.176961: step 5489, loss 0.0148622, acc 1
2016-09-07T04:46:17.891148: step 5490, loss 0.0618348, acc 0.96
2016-09-07T04:46:18.588098: step 5491, loss 0.0274002, acc 0.98
2016-09-07T04:46:19.289210: step 5492, loss 0.0979592, acc 0.92
2016-09-07T04:46:19.970198: step 5493, loss 0.0180547, acc 1
2016-09-07T04:46:20.672429: step 5494, loss 0.0599625, acc 0.94
2016-09-07T04:46:21.378135: step 5495, loss 0.0303431, acc 0.98
2016-09-07T04:46:22.042336: step 5496, loss 0.0334744, acc 0.96
2016-09-07T04:46:22.732484: step 5497, loss 0.0591406, acc 0.96
2016-09-07T04:46:23.411269: step 5498, loss 0.0448161, acc 0.96
2016-09-07T04:46:24.079886: step 5499, loss 0.00758673, acc 1
2016-09-07T04:46:24.781496: step 5500, loss 0.00650995, acc 1

Evaluation:
2016-09-07T04:46:28.125894: step 5500, loss 2.07375, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-5500

2016-09-07T04:46:29.838719: step 5501, loss 0.00742622, acc 1
2016-09-07T04:46:30.536171: step 5502, loss 0.0452209, acc 0.98
2016-09-07T04:46:31.222106: step 5503, loss 0.084003, acc 0.98
2016-09-07T04:46:31.918106: step 5504, loss 0.0255389, acc 1
2016-09-07T04:46:32.621615: step 5505, loss 0.0183164, acc 1
2016-09-07T04:46:33.334605: step 5506, loss 0.0126809, acc 1
2016-09-07T04:46:33.998134: step 5507, loss 0.0579425, acc 0.98
2016-09-07T04:46:34.695289: step 5508, loss 0.0223572, acc 1
2016-09-07T04:46:35.411505: step 5509, loss 0.00032139, acc 1
2016-09-07T04:46:36.102189: step 5510, loss 0.0543714, acc 0.96
2016-09-07T04:46:36.771885: step 5511, loss 0.00352603, acc 1
2016-09-07T04:46:37.464045: step 5512, loss 0.00166718, acc 1
2016-09-07T04:46:38.169165: step 5513, loss 0.0162899, acc 1
2016-09-07T04:46:38.836429: step 5514, loss 0.0022064, acc 1
2016-09-07T04:46:39.544662: step 5515, loss 0.00296523, acc 1
2016-09-07T04:46:40.204199: step 5516, loss 0.0276282, acc 0.98
2016-09-07T04:46:40.874137: step 5517, loss 0.0398822, acc 0.98
2016-09-07T04:46:41.564810: step 5518, loss 0.0781798, acc 0.94
2016-09-07T04:46:42.258267: step 5519, loss 0.000398896, acc 1
2016-09-07T04:46:42.937695: step 5520, loss 0.0076693, acc 1
2016-09-07T04:46:43.597100: step 5521, loss 0.0316015, acc 0.98
2016-09-07T04:46:44.290419: step 5522, loss 0.0270767, acc 1
2016-09-07T04:46:44.986785: step 5523, loss 0.0404954, acc 0.98
2016-09-07T04:46:45.658850: step 5524, loss 0.0474255, acc 0.96
2016-09-07T04:46:46.363263: step 5525, loss 0.0262691, acc 0.98
2016-09-07T04:46:47.042765: step 5526, loss 0.0523282, acc 0.96
2016-09-07T04:46:47.732817: step 5527, loss 0.0189333, acc 1
2016-09-07T04:46:48.421980: step 5528, loss 0.114195, acc 0.98
2016-09-07T04:46:49.120821: step 5529, loss 0.0425452, acc 0.98
2016-09-07T04:46:49.807668: step 5530, loss 0.0299652, acc 0.98
2016-09-07T04:46:50.535187: step 5531, loss 0.0216262, acc 1
2016-09-07T04:46:51.209028: step 5532, loss 0.000521713, acc 1
2016-09-07T04:46:51.906706: step 5533, loss 0.000383902, acc 1
2016-09-07T04:46:52.617346: step 5534, loss 0.198184, acc 0.98
2016-09-07T04:46:53.287205: step 5535, loss 0.0283949, acc 0.98
2016-09-07T04:46:53.997930: step 5536, loss 0.00556571, acc 1
2016-09-07T04:46:54.677673: step 5537, loss 0.0157756, acc 1
2016-09-07T04:46:55.380684: step 5538, loss 0.0984777, acc 0.96
2016-09-07T04:46:56.075625: step 5539, loss 0.110896, acc 0.94
2016-09-07T04:46:56.764606: step 5540, loss 0.0783132, acc 0.98
2016-09-07T04:46:57.464276: step 5541, loss 0.0186114, acc 0.98
2016-09-07T04:46:58.157241: step 5542, loss 0.0845549, acc 0.96
2016-09-07T04:46:58.849562: step 5543, loss 0.00940617, acc 1
2016-09-07T04:46:59.542783: step 5544, loss 0.0343012, acc 1
2016-09-07T04:47:00.261292: step 5545, loss 0.0403938, acc 0.98
2016-09-07T04:47:00.936474: step 5546, loss 0.235512, acc 0.9
2016-09-07T04:47:01.597410: step 5547, loss 0.0549038, acc 0.98
2016-09-07T04:47:02.296459: step 5548, loss 0.0395217, acc 0.98
2016-09-07T04:47:02.975100: step 5549, loss 0.0385655, acc 0.98
2016-09-07T04:47:03.649215: step 5550, loss 0.0595858, acc 0.98
2016-09-07T04:47:04.316840: step 5551, loss 0.0257237, acc 1
2016-09-07T04:47:05.007298: step 5552, loss 0.018114, acc 1
2016-09-07T04:47:05.691424: step 5553, loss 0.0301861, acc 0.98
2016-09-07T04:47:06.385277: step 5554, loss 0.0475789, acc 0.98
2016-09-07T04:47:07.108431: step 5555, loss 0.137355, acc 0.98
2016-09-07T04:47:07.780657: step 5556, loss 0.0181034, acc 1
2016-09-07T04:47:08.460847: step 5557, loss 0.0281373, acc 0.98
2016-09-07T04:47:09.140892: step 5558, loss 0.024748, acc 0.98
2016-09-07T04:47:09.837619: step 5559, loss 0.0170646, acc 1
2016-09-07T04:47:10.531886: step 5560, loss 0.0884899, acc 0.98
2016-09-07T04:47:11.204107: step 5561, loss 0.0473739, acc 0.98
2016-09-07T04:47:11.916761: step 5562, loss 0.0220901, acc 0.98
2016-09-07T04:47:12.604736: step 5563, loss 0.000905255, acc 1
2016-09-07T04:47:13.274108: step 5564, loss 0.022482, acc 0.98
2016-09-07T04:47:13.965769: step 5565, loss 0.0286177, acc 0.98
2016-09-07T04:47:14.669733: step 5566, loss 0.0609376, acc 0.96
2016-09-07T04:47:15.381585: step 5567, loss 0.0158985, acc 1
2016-09-07T04:47:16.012227: step 5568, loss 6.76194e-05, acc 1
2016-09-07T04:47:16.707083: step 5569, loss 0.0314051, acc 0.98
2016-09-07T04:47:17.391518: step 5570, loss 0.0195083, acc 0.98
2016-09-07T04:47:18.060477: step 5571, loss 0.0479219, acc 0.98
2016-09-07T04:47:18.739676: step 5572, loss 0.0593999, acc 0.96
2016-09-07T04:47:19.451547: step 5573, loss 0.0178802, acc 1
2016-09-07T04:47:20.140298: step 5574, loss 0.0335065, acc 0.98
2016-09-07T04:47:20.812696: step 5575, loss 0.0225658, acc 1
2016-09-07T04:47:21.499124: step 5576, loss 0.00798177, acc 1
2016-09-07T04:47:22.173970: step 5577, loss 0.00264239, acc 1
2016-09-07T04:47:22.856697: step 5578, loss 0.0212458, acc 1
2016-09-07T04:47:23.551100: step 5579, loss 0.000429907, acc 1
2016-09-07T04:47:24.231243: step 5580, loss 0.0270111, acc 0.98
2016-09-07T04:47:24.908029: step 5581, loss 0.0146952, acc 1
2016-09-07T04:47:25.607651: step 5582, loss 0.0132998, acc 1
2016-09-07T04:47:26.320143: step 5583, loss 0.0101501, acc 1
2016-09-07T04:47:26.989230: step 5584, loss 0.000485942, acc 1
2016-09-07T04:47:27.658742: step 5585, loss 0.00176911, acc 1
2016-09-07T04:47:28.347142: step 5586, loss 0.00316101, acc 1
2016-09-07T04:47:29.046600: step 5587, loss 0.0117771, acc 1
2016-09-07T04:47:29.742088: step 5588, loss 0.0611512, acc 0.98
2016-09-07T04:47:30.441649: step 5589, loss 0.121875, acc 0.98
2016-09-07T04:47:31.153264: step 5590, loss 0.019456, acc 1
2016-09-07T04:47:31.827704: step 5591, loss 0.0170667, acc 1
2016-09-07T04:47:32.512846: step 5592, loss 0.00348753, acc 1
2016-09-07T04:47:33.195153: step 5593, loss 0.0199641, acc 1
2016-09-07T04:47:33.876797: step 5594, loss 0.0114894, acc 1
2016-09-07T04:47:34.580014: step 5595, loss 0.00269055, acc 1
2016-09-07T04:47:35.289207: step 5596, loss 0.0182693, acc 0.98
2016-09-07T04:47:35.975479: step 5597, loss 0.0249904, acc 0.98
2016-09-07T04:47:36.652379: step 5598, loss 0.110436, acc 0.94
2016-09-07T04:47:37.336158: step 5599, loss 0.040194, acc 0.98
2016-09-07T04:47:38.012928: step 5600, loss 0.132037, acc 0.98

Evaluation:
2016-09-07T04:47:41.398013: step 5600, loss 1.93462, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-5600

2016-09-07T04:47:43.114043: step 5601, loss 0.0284435, acc 0.98
2016-09-07T04:47:43.770747: step 5602, loss 0.00470604, acc 1
2016-09-07T04:47:44.457936: step 5603, loss 0.00266435, acc 1
2016-09-07T04:47:45.133684: step 5604, loss 0.00308465, acc 1
2016-09-07T04:47:45.819634: step 5605, loss 0.0488761, acc 0.96
2016-09-07T04:47:46.509477: step 5606, loss 0.111051, acc 0.96
2016-09-07T04:47:47.198554: step 5607, loss 7.75344e-05, acc 1
2016-09-07T04:47:47.902276: step 5608, loss 0.00183882, acc 1
2016-09-07T04:47:48.552719: step 5609, loss 0.0814452, acc 0.96
2016-09-07T04:47:49.263826: step 5610, loss 0.110648, acc 0.94
2016-09-07T04:47:49.972548: step 5611, loss 0.0365702, acc 1
2016-09-07T04:47:50.658921: step 5612, loss 0.0045078, acc 1
2016-09-07T04:47:51.335564: step 5613, loss 0.0199908, acc 0.98
2016-09-07T04:47:52.035722: step 5614, loss 0.0120478, acc 1
2016-09-07T04:47:52.768577: step 5615, loss 0.0033736, acc 1
2016-09-07T04:47:53.434641: step 5616, loss 0.0131925, acc 1
2016-09-07T04:47:54.111537: step 5617, loss 0.0176301, acc 1
2016-09-07T04:47:54.797131: step 5618, loss 0.0321823, acc 1
2016-09-07T04:47:55.500894: step 5619, loss 0.00381795, acc 1
2016-09-07T04:47:56.186483: step 5620, loss 0.000402118, acc 1
2016-09-07T04:47:56.859531: step 5621, loss 0.00120122, acc 1
2016-09-07T04:47:57.558021: step 5622, loss 0.0158315, acc 0.98
2016-09-07T04:47:58.238259: step 5623, loss 0.0097647, acc 1
2016-09-07T04:47:58.913853: step 5624, loss 0.0579199, acc 0.96
2016-09-07T04:47:59.609825: step 5625, loss 0.0200763, acc 0.98
2016-09-07T04:48:00.319738: step 5626, loss 0.0122388, acc 1
2016-09-07T04:48:01.000631: step 5627, loss 0.020058, acc 1
2016-09-07T04:48:01.697584: step 5628, loss 0.00971096, acc 1
2016-09-07T04:48:02.392792: step 5629, loss 0.0160128, acc 1
2016-09-07T04:48:03.063153: step 5630, loss 0.00596499, acc 1
2016-09-07T04:48:03.785512: step 5631, loss 0.0530722, acc 0.98
2016-09-07T04:48:04.457437: step 5632, loss 0.013465, acc 1
2016-09-07T04:48:05.145443: step 5633, loss 0.0678095, acc 0.94
2016-09-07T04:48:05.845187: step 5634, loss 0.0362307, acc 0.98
2016-09-07T04:48:06.519627: step 5635, loss 0.00187553, acc 1
2016-09-07T04:48:07.215299: step 5636, loss 0.00318222, acc 1
2016-09-07T04:48:07.901884: step 5637, loss 0.0360701, acc 0.98
2016-09-07T04:48:08.611484: step 5638, loss 0.0376134, acc 0.98
2016-09-07T04:48:09.327482: step 5639, loss 0.148019, acc 0.96
2016-09-07T04:48:10.018808: step 5640, loss 0.0852269, acc 0.98
2016-09-07T04:48:10.709676: step 5641, loss 0.000455283, acc 1
2016-09-07T04:48:11.363137: step 5642, loss 0.0124276, acc 1
2016-09-07T04:48:12.094534: step 5643, loss 0.0290172, acc 0.98
2016-09-07T04:48:12.754989: step 5644, loss 0.0944755, acc 0.96
2016-09-07T04:48:13.450891: step 5645, loss 0.00570512, acc 1
2016-09-07T04:48:14.137758: step 5646, loss 0.0147165, acc 1
2016-09-07T04:48:14.829920: step 5647, loss 0.00643652, acc 1
2016-09-07T04:48:15.526008: step 5648, loss 0.0463617, acc 0.96
2016-09-07T04:48:16.185860: step 5649, loss 0.0225274, acc 0.98
2016-09-07T04:48:16.896220: step 5650, loss 0.00297801, acc 1
2016-09-07T04:48:17.565267: step 5651, loss 0.00279314, acc 1
2016-09-07T04:48:18.256794: step 5652, loss 0.0356106, acc 0.98
2016-09-07T04:48:18.962613: step 5653, loss 0.00458296, acc 1
2016-09-07T04:48:19.664583: step 5654, loss 0.00630179, acc 1
2016-09-07T04:48:20.368185: step 5655, loss 7.22126e-05, acc 1
2016-09-07T04:48:21.052333: step 5656, loss 0.0189037, acc 0.98
2016-09-07T04:48:21.749890: step 5657, loss 0.00332327, acc 1
2016-09-07T04:48:22.441904: step 5658, loss 0.0418718, acc 0.98
2016-09-07T04:48:23.176060: step 5659, loss 0.00295755, acc 1
2016-09-07T04:48:23.868177: step 5660, loss 0.00551925, acc 1
2016-09-07T04:48:24.558282: step 5661, loss 0.0267928, acc 0.98
2016-09-07T04:48:25.252066: step 5662, loss 0.0222614, acc 0.98
2016-09-07T04:48:25.915434: step 5663, loss 0.0395, acc 0.98
2016-09-07T04:48:26.606450: step 5664, loss 0.0215214, acc 0.98
2016-09-07T04:48:27.296879: step 5665, loss 0.0122187, acc 1
2016-09-07T04:48:27.980707: step 5666, loss 0.0269104, acc 1
2016-09-07T04:48:28.686296: step 5667, loss 0.0689324, acc 0.98
2016-09-07T04:48:29.361475: step 5668, loss 0.0547889, acc 0.96
2016-09-07T04:48:30.060051: step 5669, loss 9.73913e-05, acc 1
2016-09-07T04:48:30.724556: step 5670, loss 0.131751, acc 0.94
2016-09-07T04:48:31.407382: step 5671, loss 0.0132683, acc 1
2016-09-07T04:48:32.097429: step 5672, loss 0.0100748, acc 1
2016-09-07T04:48:32.783387: step 5673, loss 0.0538293, acc 0.98
2016-09-07T04:48:33.453469: step 5674, loss 0.0156853, acc 0.98
2016-09-07T04:48:34.141207: step 5675, loss 0.0100792, acc 1
2016-09-07T04:48:34.853924: step 5676, loss 0.0282695, acc 0.98
2016-09-07T04:48:35.526606: step 5677, loss 0.021675, acc 1
2016-09-07T04:48:36.235720: step 5678, loss 0.0326918, acc 0.98
2016-09-07T04:48:36.914986: step 5679, loss 0.0264951, acc 0.98
2016-09-07T04:48:37.611676: step 5680, loss 0.00835749, acc 1
2016-09-07T04:48:38.302386: step 5681, loss 0.0297687, acc 0.98
2016-09-07T04:48:38.974141: step 5682, loss 0.038442, acc 0.98
2016-09-07T04:48:39.681342: step 5683, loss 0.00147245, acc 1
2016-09-07T04:48:40.351248: step 5684, loss 0.0329944, acc 1
2016-09-07T04:48:41.033368: step 5685, loss 0.035328, acc 0.98
2016-09-07T04:48:41.719699: step 5686, loss 0.0430864, acc 0.98
2016-09-07T04:48:42.416319: step 5687, loss 0.0159697, acc 1
2016-09-07T04:48:43.116451: step 5688, loss 0.0282686, acc 0.98
2016-09-07T04:48:43.823290: step 5689, loss 0.028031, acc 0.98
2016-09-07T04:48:44.522654: step 5690, loss 0.0346547, acc 0.98
2016-09-07T04:48:45.185595: step 5691, loss 0.00128473, acc 1
2016-09-07T04:48:45.853801: step 5692, loss 0.0162982, acc 0.98
2016-09-07T04:48:46.538122: step 5693, loss 0.125983, acc 0.98
2016-09-07T04:48:47.226556: step 5694, loss 0.0058675, acc 1
2016-09-07T04:48:47.927434: step 5695, loss 0.0122252, acc 1
2016-09-07T04:48:48.604533: step 5696, loss 0.0335497, acc 0.98
2016-09-07T04:48:49.304296: step 5697, loss 0.0026303, acc 1
2016-09-07T04:48:49.981680: step 5698, loss 0.0892498, acc 0.96
2016-09-07T04:48:50.673772: step 5699, loss 0.0494598, acc 0.98
2016-09-07T04:48:51.370607: step 5700, loss 0.00588551, acc 1

Evaluation:
2016-09-07T04:48:54.727868: step 5700, loss 2.23283, acc 0.736398

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473190736/checkpoints/model-5700

2016-09-07T04:48:56.419058: step 5701, loss 0.00719932, acc 1
2016-09-07T04:48:57.103278: step 5702, loss 0.0147774, acc 1
2016-09-07T04:48:57.811530: step 5703, loss 0.0311449, acc 0.98
2016-09-07T04:48:58.495159: step 5704, loss 0.0550251, acc 0.98
2016-09-07T04:48:59.182639: step 5705, loss 0.0316443, acc 0.98
2016-09-07T04:48:59.857935: step 5706, loss 0.0641042, acc 0.98
2016-09-07T04:49:00.573346: step 5707, loss 0.0305716, acc 0.98
2016-09-07T04:49:01.265389: step 5708, loss 0.0276581, acc 0.98
2016-09-07T04:49:01.910518: step 5709, loss 0.00853981, acc 1
2016-09-07T04:49:02.619730: step 5710, loss 0.000966831, acc 1
2016-09-07T04:49:03.299514: step 5711, loss 0.0102364, acc 1
2016-09-07T04:49:03.990830: step 5712, loss 0.0532415, acc 0.96
2016-09-07T04:49:04.685629: step 5713, loss 0.185382, acc 0.96
2016-09-07T04:49:05.371814: step 5714, loss 0.0161299, acc 0.98
2016-09-07T04:49:06.141706: step 5715, loss 0.00115168, acc 1
2016-09-07T04:49:06.822990: step 5716, loss 0.0190516, acc 1
2016-09-07T04:49:07.495337: step 5717, loss 0.0503796, acc 0.98
2016-09-07T04:49:08.170557: step 5718, loss 0.000247563, acc 1
2016-09-07T04:49:08.857337: step 5719, loss 0.0331982, acc 0.98
2016-09-07T04:49:09.540770: step 5720, loss 0.0773297, acc 0.96
2016-09-07T04:49:10.233419: step 5721, loss 0.0894499, acc 0.96
2016-09-07T04:49:10.944585: step 5722, loss 0.0681688, acc 0.98
2016-09-07T04:49:11.623469: step 5723, loss 0.0442061, acc 0.96
2016-09-07T04:49:12.313497: step 5724, loss 0.00119836, acc 1
2016-09-07T04:49:13.005400: step 5725, loss 2.91882e-05, acc 1
2016-09-07T04:49:13.685434: step 5726, loss 0.0192518, acc 1
2016-09-07T04:49:14.373023: step 5727, loss 0.0043245, acc 1
2016-09-07T04:49:15.057158: step 5728, loss 0.010315, acc 1
2016-09-07T04:49:15.768360: step 5729, loss 0.0407187, acc 0.98
2016-09-07T04:49:16.447915: step 5730, loss 0.0143928, acc 1
2016-09-07T04:49:17.135078: step 5731, loss 0.000138239, acc 1
2016-09-07T04:49:17.828009: step 5732, loss 0.000730768, acc 1
2016-09-07T04:49:18.509983: step 5733, loss 0.00361238, acc 1
2016-09-07T04:49:19.184484: step 5734, loss 0.0287273, acc 0.98
2016-09-07T04:49:19.884930: step 5735, loss 0.000163015, acc 1
2016-09-07T04:49:20.577181: step 5736, loss 0.00195838, acc 1
2016-09-07T04:49:21.259196: step 5737, loss 0.03437, acc 0.96
2016-09-07T04:49:21.932585: step 5738, loss 0.144886, acc 0.92
2016-09-07T04:49:22.610528: step 5739, loss 0.0154864, acc 1
2016-09-07T04:49:23.296599: step 5740, loss 0.000602162, acc 1
2016-09-07T04:49:23.978174: step 5741, loss 0.0225328, acc 0.98
2016-09-07T04:49:24.653498: step 5742, loss 0.0646457, acc 0.96
2016-09-07T04:49:25.368493: step 5743, loss 0.0160948, acc 1
2016-09-07T04:49:26.046394: step 5744, loss 0.0109169, acc 1
2016-09-07T04:49:26.739802: step 5745, loss 0.0291596, acc 0.98
2016-09-07T04:49:27.424216: step 5746, loss 0.0369303, acc 0.98
2016-09-07T04:49:28.105540: step 5747, loss 0.0295785, acc 0.98
2016-09-07T04:49:28.801083: step 5748, loss 0.194502, acc 0.98
2016-09-07T04:49:29.472619: step 5749, loss 0.0139067, acc 1
2016-09-07T04:49:30.154501: step 5750, loss 0.00102781, acc 1
2016-09-07T04:49:30.834984: step 5751, loss 0.000265662, acc 1
2016-09-07T04:49:31.551477: step 5752, loss 0.00313571, acc 1
2016-09-07T04:49:32.249689: step 5753, loss 0.0862672, acc 0.96
2016-09-07T04:49:32.928333: step 5754, loss 0.0263333, acc 1
2016-09-07T04:49:33.601559: step 5755, loss 0.0344117, acc 1
2016-09-07T04:49:34.277445: step 5756, loss 0.00503652, acc 1
2016-09-07T04:49:34.979201: step 5757, loss 0.0270851, acc 1
2016-09-07T04:49:35.647324: step 5758, loss 0.0110686, acc 1
2016-09-07T04:49:36.343002: step 5759, loss 0.00480356, acc 1
2016-09-07T04:49:36.954422: step 5760, loss 0.00586265, acc 1
